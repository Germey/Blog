<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
  <meta name="theme-color" content="#222">
  <meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>
  <script id="hexo-configurations">
    var NexT = window.NexT ||
    {};
    var CONFIG = {
      "hostname": "cuiqingcai.com",
      "root": "/",
      "scheme": "Pisces",
      "version": "7.8.0",
      "exturl": false,
      "sidebar":
      {
        "position": "right",
        "width": 360,
        "display": "post",
        "padding": 18,
        "offset": 12,
        "onmobile": false,
        "widgets": [
          {
            "type": "image",
            "name": "阿布云",
            "enable": false,
            "url": "https://www.abuyun.com/http-proxy/introduce.html",
            "src": "https://cdn.cuiqingcai.com/88au8.jpg",
            "width": "100%"
      },
          {
            "type": "image",
            "name": "爬虫书",
            "url": "https://item.jd.com/13527222.html",
            "src": "https://cdn.cuiqingcai.com/ei5og.jpg",
            "width": "100%",
            "enable": true
      },
          {
            "type": "categories",
            "name": "分类",
            "enable": true
      },
          {
            "type": "image",
            "name": "IPIDEA",
            "url": "http://www.ipidea.net/?utm-source=cqc&utm-keyword=?cqc",
            "src": "https://cdn.cuiqingcai.com/0ywun.png",
            "width": "100%",
            "enable": false
      },
          {
            "type": "image",
            "name": "Storm Proxies",
            "src": "https://cdn.cuiqingcai.com/a2zad8.png",
            "url": "https://www.stormproxies.cn/?keyword=jingmi",
            "width": "100%",
            "enable": false
      },
          {
            "type": "friends",
            "name": "友情链接",
            "enable": true
      },
          {
            "type": "hot",
            "name": "猜你喜欢",
            "enable": true
      },
          {
            "type": "tags",
            "name": "标签云",
            "enable": true
      }]
      },
      "copycode":
      {
        "enable": true,
        "show_result": true,
        "style": "mac"
      },
      "back2top":
      {
        "enable": true,
        "sidebar": false,
        "scrollpercent": true
      },
      "bookmark":
      {
        "enable": false,
        "color": "#222",
        "save": "auto"
      },
      "fancybox": false,
      "mediumzoom": false,
      "lazyload": false,
      "pangu": true,
      "comments":
      {
        "style": "tabs",
        "active": "gitalk",
        "storage": true,
        "lazyload": false,
        "nav": null,
        "activeClass": "gitalk"
      },
      "algolia":
      {
        "hits":
        {
          "per_page": 10
        },
        "labels":
        {
          "input_placeholder": "Search for Posts",
          "hits_empty": "We didn't find any results for the search: ${query}",
          "hits_stats": "${hits} results found in ${time} ms"
        }
      },
      "localsearch":
      {
        "enable": true,
        "trigger": "auto",
        "top_n_per_article": 10,
        "unescape": false,
        "preload": false
      },
      "motion":
      {
        "enable": false,
        "async": false,
        "transition":
        {
          "post_block": "bounceDownIn",
          "post_header": "slideDownIn",
          "post_body": "slideDownIn",
          "coll_header": "slideLeftIn",
          "sidebar": "slideUpIn"
        }
      },
      "path": "search.xml"
    };

  </script>
  <meta name="keywords" content="爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书,静觅,崔庆才">
  <meta name="robots" content="index,follow">
  <meta name="GOOGLEBOT" content="index,follow">
  <meta name="author" content="静觅丨崔庆才的个人站点">
  <meta name="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
  <meta property="og:type" content="website">
  <meta property="og:title" content="静觅">
  <meta property="og:url" content="https://cuiqingcai.com/page/8/index.html">
  <meta property="og:site_name" content="静觅">
  <meta property="og:description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
  <meta property="og:locale" content="zh_CN">
  <meta property="article:author" content="崔庆才">
  <meta property="article:tag" content="爬虫教程">
  <meta property="article:tag" content="爬虫">
  <meta property="article:tag" content="Python">
  <meta property="article:tag" content="Python爬虫">
  <meta property="article:tag" content="Python爬虫教程">
  <meta property="article:tag" content="爬虫书">
  <meta property="article:tag" content="静觅">
  <meta property="article:tag" content="崔庆才">
  <meta name="twitter:card" content="summary">
  <link rel="canonical" href="https://cuiqingcai.com/page/8/">
  <script id="page-configurations">
    // https://hexo.io/docs/variables.html
    CONFIG.page = {
      sidebar: "",
      isHome: true,
      isPost: false,
      lang: 'zh-CN'
    };

  </script>
  <title>静觅丨崔庆才的个人站点 - Python爬虫教程</title>
  <meta name="google-site-verification" content="p_bIcnvirkFzG2dYKuNDivKD8-STet5W7D-01woA2fc" />
  <meta name="sogou_site_verification" content="kBOV53NQqT" />
  <noscript>
    <style>
      .use-motion .brand,
      .use-motion .menu-item,
      .sidebar-inner,
      .use-motion .post-block,
      .use-motion .pagination,
      .use-motion .comments,
      .use-motion .post-header,
      .use-motion .post-body,
      .use-motion .collection-header
      {
        opacity: initial;
      }

      .use-motion .site-title,
      .use-motion .site-subtitle
      {
        opacity: initial;
        top: initial;
      }

      .use-motion .logo-line-before i
      {
        left: initial;
      }

      .use-motion .logo-line-after i
      {
        right: initial;
      }

    </style>
  </noscript>
  <link rel="alternate" href="/atom.xml" title="静觅" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner">
        <div class="site-brand-container">
          <div class="site-nav-toggle">
            <div class="toggle" aria-label="切换导航栏">
              <span class="toggle-line toggle-line-first"></span>
              <span class="toggle-line toggle-line-middle"></span>
              <span class="toggle-line toggle-line-last"></span>
            </div>
          </div>
          <div class="site-meta">
            <a href="/" class="brand" rel="start">
              <span class="logo-line-before"><i></i></span>
              <h1 class="site-title">静觅 <span class="site-subtitle"> 崔庆才的个人站点 - Python爬虫教程 </span>
              </h1>
              <span class="logo-line-after"><i></i></span>
            </a>
          </div>
          <div class="site-nav-right">
            <div class="toggle popup-trigger">
              <i class="fa fa-search fa-fw fa-lg"></i>
            </div>
          </div>
        </div>
        <nav class="site-nav">
          <ul id="menu" class="main-menu menu">
            <li class="menu-item menu-item-home">
              <a href="/" rel="section">首页</a>
            </li>
            <li class="menu-item menu-item-archives">
              <a href="/archives/" rel="section">文章列表</a>
            </li>
            <li class="menu-item menu-item-tags">
              <a href="/tags/" rel="section">文章标签</a>
            </li>
            <li class="menu-item menu-item-categories">
              <a href="/categories/" rel="section">文章分类</a>
            </li>
            <li class="menu-item menu-item-about">
              <a href="/about/" rel="section">关于博主</a>
            </li>
            <li class="menu-item menu-item-message">
              <a href="/message/" rel="section">给我留言</a>
            </li>
            <li class="menu-item menu-item-search">
              <a role="button" class="popup-trigger">搜索 </a>
            </li>
          </ul>
        </nav>
        <div class="search-pop-overlay">
          <div class="popup search-popup">
            <div class="search-header">
              <span class="search-icon">
                <i class="fa fa-search"></i>
              </span>
              <div class="search-input-container">
                <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
              </div>
              <span class="popup-btn-close">
                <i class="fa fa-times-circle"></i>
              </span>
            </div>
            <div id="search-result">
              <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>
    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
      <span>0%</span>
    </div>
    <div class="reading-progress-bar"></div>
    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div class="content index posts-expand">
            <div class="carousel">
              <div id="wowslider-container">
                <div class="ws_images">
                  <ul>
                    <li><a target="_blank" href="https://item.jd.com/13527222.html"><img title="Python3网络爬虫开发实战（第二版）上市了！" src="https://cdn.cuiqingcai.com/prwgs.png" /></a></li>
                    <li><a target="_blank" href="https://t.lagou.com/fRCBRsRCSN6FA"><img title="52讲轻松搞定网络爬虫" src="https://cdn.cuiqingcai.com/fqq5e.png" /></a></li>
                    <li><a target="_blank" href="https://cuiqingcai.com/4320.html"><img title="Python3网络爬虫开发视频教程" src="https://cdn.cuiqingcai.com/bjrny.jpg" /></a></li>
                    <li><a target="_blank" href="https://cuiqingcai.com/5094.html"><img title="爬虫代理哪家强？十大付费代理详细对比评测出炉！" src="https://cdn.cuiqingcai.com/nifs6.jpg" /></a></li>
                  </ul>
                </div>
                <div class="ws_thumbs">
                  <div>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/prwgs.png" /></a>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/fqq5e.png" /></a>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/bjrny.jpg" /></a>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/nifs6.jpg" /></a>
                  </div>
                </div>
                <div class="ws_shadow"></div>
              </div>
            </div>
            <link rel="stylesheet" href="/lib/wowslide/slide.css">
            <script src="/lib/wowslide/jquery.min.js"></script>
            <script src="/lib/wowslide/slider.js"></script>
            <script>
              jQuery("#wowslider-container").wowSlider(
              {
                effect: "cube",
                prev: "",
                next: "",
                duration: 20 * 100,
                delay: 100 * 100,
                width: 716,
                height: 297,
                autoPlay: true,
                playPause: true,
                stopOnHover: false,
                loop: false,
                bullets: 0,
                caption: true,
                captionEffect: "slide",
                controls: true,
                onBeforeStep: 0,
                images: 0
              });

            </script>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8494.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8494.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.3–Scrapyd 对接 Docker</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-3-Scrapyd-对接-Docker"><a href="#15-3-Scrapyd-对接-Docker" class="headerlink" title="15.3 Scrapyd 对接 Docker"></a>15.3 Scrapyd 对接 Docker</h1>
                  <p>我们使用了 Scrapyd-Client 成功将 Scrapy 项目部署到 Scrapyd 运行，前提是需要提前在服务器上安装好 Scrapyd 并运行 Scrapyd 服务，而这个过程比较麻烦。如果同时将一个 Scrapy 项目部署到 100 台服务器上，我们需要手动配置每台服务器的 Python 环境，更改 Scrapyd 配置吗？如果这些服务器的 Python 环境是不同版本，同时还运行其他的项目，而版本冲突又会造成不必要的麻烦。 所以，我们需要解决一个痛点，那就是 Python 环境配置问题和版本冲突解决问题。如果我们将 Scrapyd 直接打包成一个 Docker 镜像，那么在服务器上只需要执行 Docker 命令就可以启动 Scrapyd 服务，这样就不用再关心 Python 环境问题，也不需要担心版本冲突问题。 接下来，我们就将 Scrapyd 打包制作成一个 Docker 镜像。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请确保本机已经正确安装好了 Docker，如没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="2-对接-Docker"><a href="#2-对接-Docker" class="headerlink" title="2. 对接 Docker"></a>2. 对接 Docker</h3>
                  <p>接下来我们首先新建一个项目，然后新建一个 scrapyd.conf，即 Scrapyd 的配置文件，内容如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[scrapyd]</span></span><br><span class="line"><span class="attr">eggs_dir</span>    = eggs</span><br><span class="line"><span class="attr">logs_dir</span>    = logs</span><br><span class="line"><span class="attr">items_dir</span>   =</span><br><span class="line"><span class="attr">jobs_to_keep</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">dbs_dir</span>     = dbs</span><br><span class="line"><span class="attr">max_proc</span>    = <span class="number">0</span></span><br><span class="line"><span class="attr">max_proc_per_cpu</span> = <span class="number">10</span></span><br><span class="line"><span class="attr">finished_to_keep</span> = <span class="number">100</span></span><br><span class="line"><span class="attr">poll_interval</span> = <span class="number">5.0</span></span><br><span class="line"><span class="attr">bind_address</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">http_port</span>   = <span class="number">6800</span></span><br><span class="line"><span class="attr">debug</span>       = <span class="literal">off</span></span><br><span class="line"><span class="attr">runner</span>      = scrapyd.runner</span><br><span class="line"><span class="attr">application</span> = scrapyd.app.application</span><br><span class="line"><span class="attr">launcher</span>    = scrapyd.launcher.Launcher</span><br><span class="line"><span class="attr">webroot</span>     = scrapyd.website.Root</span><br><span class="line"></span><br><span class="line"><span class="section">[services]</span></span><br><span class="line"><span class="attr">schedule.json</span>     = scrapyd.webservice.Schedule</span><br><span class="line"><span class="attr">cancel.json</span>       = scrapyd.webservice.Cancel</span><br><span class="line"><span class="attr">addversion.json</span>   = scrapyd.webservice.AddVersion</span><br><span class="line"><span class="attr">listprojects.json</span> = scrapyd.webservice.ListProjects</span><br><span class="line"><span class="attr">listversions.json</span> = scrapyd.webservice.ListVersions</span><br><span class="line"><span class="attr">listspiders.json</span>  = scrapyd.webservice.ListSpiders</span><br><span class="line"><span class="attr">delproject.json</span>   = scrapyd.webservice.DeleteProject</span><br><span class="line"><span class="attr">delversion.json</span>   = scrapyd.webservice.DeleteVersion</span><br><span class="line"><span class="attr">listjobs.json</span>     = scrapyd.webservice.ListJobs</span><br><span class="line"><span class="attr">daemonstatus.json</span> = scrapyd.webservice.DaemonStatus</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里实际上是修改自官方文档的配置文件：<a href="https://scrapyd.readthedocs.io/en/stable/config.html#example-configuration-file" target="_blank" rel="noopener">https://scrapyd.readthedocs.io/en/stable/config.html#example-configuration-file</a>，其中修改的地方有两个：</p>
                  <ul>
                    <li>max_proc_per_cpu = 10，原本是 4，即 CPU 单核最多运行 4 个 Scrapy 任务，也就是说 1 核的主机最多同时只能运行 4 个 Scrapy 任务，在这里设置上限为 10，也可以自行设置。</li>
                    <li>bind_address = 0.0.0.0，原本是 127.0.0.1，不能公开访问，在这里修改为 0.0.0.0 即可解除此限制。</li>
                  </ul>
                  <p>接下来新建一个 requirements.txt ，将一些 Scrapy 项目常用的库都列进去，内容如下：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">requests</span><br><span class="line">selenium</span><br><span class="line">aiohttp</span><br><span class="line"><span class="keyword">beautifulsoup4</span></span><br><span class="line"><span class="keyword">pyquery</span></span><br><span class="line"><span class="keyword">pymysql</span></span><br><span class="line"><span class="keyword">redis</span></span><br><span class="line"><span class="keyword">pymongo</span></span><br><span class="line"><span class="keyword">flask</span></span><br><span class="line"><span class="keyword">django</span></span><br><span class="line"><span class="keyword">scrapy</span></span><br><span class="line"><span class="keyword">scrapyd</span></span><br><span class="line"><span class="keyword">scrapyd-client</span></span><br><span class="line"><span class="keyword">scrapy-redis</span></span><br><span class="line"><span class="keyword">scrapy-splash</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果我们运行的 Scrapy 项目还有其他的库需要用到可以自行添加到此文件中。 最后我们新建一个 Dockerfile，内容如下：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.6</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /code</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /code</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./scrapyd.conf /etc/scrapyd/</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">6800</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip3 install -r requirements.txt</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> scrapyd</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一行 FROM 是指在 python:3.6 这个镜像上构建，也就是说在构建时就已经有了 Python 3.6 的环境。 第二行 ADD 是将本地的代码放置到虚拟容器中，它有两个参数，第一个参数是 . ，即代表本地当前路径，/code 代表虚拟容器中的路径，也就是将本地项目所有内容放置到虚拟容器的 /code 目录下。 第三行 WORKDIR 是指定工作目录，在这里将刚才我们添加的代码路径设成工作路径，在这个路径下的目录结构和我们当前本地目录结构是相同的，所以可以直接执行库安装命令等。 第四行 COPY 是将当前目录下的 scrapyd.conf 文件拷贝到虚拟容器的 /etc/scrapyd/ 目录下，Scrapyd 在运行的时候会默认读取这个配置。 第五行 EXPOSE 是声明运行时容器提供服务端口，注意这里只是一个声明，在运行时不一定就会在此端口开启服务。这样的声明一是告诉使用者这个镜像服务的运行端口，以方便配置映射。另一个用处则是在运行时使用随机端口映射时，会自动随机映射 EXPOSE 的端口。 第六行 RUN 是执行某些命令，一般做一些环境准备工作，由于 Docker 虚拟容器内只有 Python3 环境，而没有我们所需要的一些 Python 库，所以在这里我们运行此命令来在虚拟容器中安装相应的 Python 库，这样项目部署到 Scrapyd 中便可以正常运行了。 第七行 CMD 是容器启动命令，在容器运行时，会直接执行此命令，在这里我们直接用 scrapyd 来启动 Scrapyd 服务。 到现在基本的工作就完成了，运行如下命令进行构建：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">build </span>-t <span class="keyword">scrapyd:latest </span>.</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>构建成功后即可运行测试：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker run -d -p <span class="number">6800</span>:<span class="number">6800</span> scrapyd</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后我们打开：<a href="http://localhost:6800" target="_blank" rel="noopener">http://localhost:6800</a> 即可观察到 Scrapyd 服务，如图 15-2 所示： <img src="https://cdn.cuiqingcai.com/2019-11-29-114136.png" alt=""> 图 15-2 Scrapyd 主页 这样我们就完成了 Scrapyd Docker 镜像的构建并成功运行了。 然后我们可以将此镜像上传到 Docker Hub，例如我的 Docker Hub 用户名为 germey，新建了一个名为 scrapyd 的项目，首先可以打一个标签：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">tag</span> <span class="title">scrapyd</span>:latest germey/scrapyd:latest</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里请自行替换成你的项目名称。 然后 Push 即可：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">docker</span> <span class="keyword">push </span>germey/scrapyd:latest</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>之后我们在其他主机运行此命令即可启动 Scrapyd 服务：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker run -d -p <span class="number">6800</span>:<span class="number">6800</span> germey/scrapyd</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>执行命令后会发现 Scrapyd 就可以成功在其他服务器上运行了。</p>
                  <h3 id="3-结语"><a href="#3-结语" class="headerlink" title="3. 结语"></a>3. 结语</h3>
                  <p>这样我们就利用 Docker 解决了 Python 环境的问题，在后一节我们再解决一个批量部署 Docker 的问题就可以解决批量部署问题了。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-11 10:13:48" itemprop="dateCreated datePublished" datetime="2019-12-11T10:13:48+08:00">2019-12-11</time>
                </span>
                <span id="/8494.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.3–Scrapyd 对接 Docker" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8491.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8491.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.2–Scrapyd-Client 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-2-Scrapyd-Client-的使用"><a href="#15-2-Scrapyd-Client-的使用" class="headerlink" title="15.2 Scrapyd-Client 的使用"></a>15.2 Scrapyd-Client 的使用</h1>
                  <p>这里有现成的工具来完成部署过程，它叫作 Scrapyd-Client。本节将简单介绍使用 Scrapyd-Client 部署 Scrapy 项目的方法。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请先确保 Scrapyd-Client 已经正确安装，安装方式可以参考第 1 章的内容。</p>
                  <h3 id="2-Scrapyd-Client-的功能"><a href="#2-Scrapyd-Client-的功能" class="headerlink" title="2. Scrapyd-Client 的功能"></a>2. Scrapyd-Client 的功能</h3>
                  <p>Scrapyd-Client 为了方便 Scrapy 项目的部署，提供两个功能：</p>
                  <ul>
                    <li>将项目打包成 Egg 文件。</li>
                    <li>将打包生成的 Egg 文件通过 addversion.json 接口部署到 Scrapyd 上。</li>
                  </ul>
                  <p>也就是说，Scrapyd-Client 帮我们把部署全部实现了，我们不需要再去关心 Egg 文件是怎样生成的，也不需要再去读 Egg 文件并请求接口上传了，这一切的操作只需要执行一个命令即可一键部署。</p>
                  <h3 id="3-Scrapyd-Client-部署"><a href="#3-Scrapyd-Client-部署" class="headerlink" title="3. Scrapyd-Client 部署"></a>3. Scrapyd-Client 部署</h3>
                  <p>要部署 Scrapy 项目，我们首先需要修改一下项目的配置文件，例如我们之前写的 Scrapy 微博爬虫项目，在项目的第一层会有一个 scrapy.cfg 文件，它的内容如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[settings]</span><br><span class="line">default = weibo.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line"><span class="comment">#url = http://localhost:6800/</span></span><br><span class="line">project = weibo</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们需要配置一下 deploy 部分，例如我们要将项目部署到 120.27.34.25 的 Scrapyd 上，就需要修改为如下内容：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[deploy]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">120.27</span>.<span class="number">34.25</span>:<span class="number">6800</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们再在 scrapy.cfg 文件所在路径执行如下命令：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapyd-deploy</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Packing <span class="keyword">version</span> <span class="number">1501682277</span></span><br><span class="line">Deploying <span class="keyword">to</span> project "weibo" <span class="keyword">in</span> http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/addversion.json</span><br><span class="line"><span class="keyword">Server</span> response (<span class="number">200</span>):</span><br><span class="line">&#123;"status": "ok", "spiders": <span class="number">1</span>, "node_name": "datacrawl-vm", "project": "weibo", "version": "1501682277"&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回这样的结果就代表部署成功了。 我们也可以指定项目版本，如果不指定的话默认为当前时间戳，指定的话通过 version 参数传递即可，例如：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapyd-deploy <span class="comment">--version 201707131455</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>值得注意的是在 Python3 的 Scrapyd 1.2.0 版本中我们不要指定版本号为带字母的字符串，需要为纯数字，否则可能会出现报错。 另外如果我们有多台主机，我们可以配置各台主机的别名，例如可以修改配置文件为：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[deploy:vm1]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">120.27</span>.<span class="number">34.24</span>:<span class="number">6800</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br><span class="line"></span><br><span class="line"><span class="section">[deploy:vm2]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">139.217</span>.<span class="number">26.30</span>:<span class="number">6800</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>有多台主机的话就在此统一配置，一台主机对应一组配置，在 deploy 后面加上主机的别名即可，这样如果我们想将项目部署到 IP 为 139.217.26.30 的 vm2 主机，我们只需要执行如下命令：</p>
                  <figure class="highlight gcode">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapyd-deploy v<span class="name">m2</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就可以将项目部署到名称为 vm2 的主机上了。 如此一来，如果我们有多台主机，我们只需要在 scrapy.cfg 文件中配置好各台主机的 Scrapyd 地址，然后调用 scrapyd-deploy 命令加主机名称即可实现部署，非常方便。 如果 Scrapyd 设置了访问限制的话，我们可以在配置文件中加入用户名和密码的配置，同时端口修改一下，修改成 Nginx 代理端口，如在第一章我们使用的是 6801，那么这里就需要改成 6801，修改如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[deploy:vm1]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">120.27</span>.<span class="number">34.24</span>:<span class="number">6801</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br><span class="line"><span class="attr">username</span> = admin</span><br><span class="line"><span class="attr">password</span> = admin</span><br><span class="line"></span><br><span class="line"><span class="section">[deploy:vm2]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">139.217</span>.<span class="number">26.30</span>:<span class="number">6801</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br><span class="line"><span class="attr">username</span> = germey</span><br><span class="line"><span class="attr">password</span> = germey</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样通过加入 username 和 password 字段我们就可以在部署时自动进行 Auth 验证，然后成功实现部署。</p>
                  <h3 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h3>
                  <p>本节介绍了利用 Scrapyd-Client 来方便地将项目部署到 Scrapyd 的过程，有了它部署不再是麻烦事。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-11 09:55:47" itemprop="dateCreated datePublished" datetime="2019-12-11T09:55:47+08:00">2019-12-11</time>
                </span>
                <span id="/8491.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.2–Scrapyd-Client 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8475.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8475.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.1–Scrapyd 分布式部署</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-1-Scrapyd-分布式部署"><a href="#15-1-Scrapyd-分布式部署" class="headerlink" title="15.1 Scrapyd 分布式部署"></a>15.1 Scrapyd 分布式部署</h1>
                  <p>分布式爬虫完成并可以成功运行了，但是有个环节非常烦琐，那就是代码部署。 我们设想下面的几个场景。</p>
                  <ul>
                    <li>如果采用上传文件的方式部署代码，我们首先将代码压缩，然后采用 SFTP 或 FTP 的方式将文件上传到服务器，之后再连接服务器将文件解压，每个服务器都需要这样配置。</li>
                    <li>如果采用 Git 同步的方式部署代码，我们可以先把代码 Push 到某个 Git 仓库里，然后再远程连接各台主机执行 Pull 操作，同步代码，每个服务器同样需要做一次操作。</li>
                  </ul>
                  <p>如果代码突然有更新，那我们必须更新每个服务器，而且万一哪台主机的版本没控制好，这可能会影响整体的分布式爬取状况。 所以我们需要一个更方便的工具来部署 Scrapy 项目，如果可以省去一遍遍逐个登录服务器部署的操作，那将会方便很多。 本节我们就来看看提供分布式部署的工具 Scrapyd。</p>
                  <h3 id="1-了解-Scrapyd"><a href="#1-了解-Scrapyd" class="headerlink" title="1. 了解 Scrapyd"></a>1. 了解 Scrapyd</h3>
                  <p>Scrapyd 是一个运行 Scrapy 爬虫的服务程序，它提供一系列 HTTP 接口来帮助我们部署、启动、停止、删除爬虫程序。Scrapyd 支持版本管理，同时还可以管理多个爬虫任务，利用它我们可以非常方便地完成 Scrapy 爬虫项目的部署任务调度。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保本机或服务器已经正确安装好了 Scrapyd，安装和配置的方法可以参见第 1 章的内容。</p>
                  <h3 id="3-访问-Scrapyd"><a href="#3-访问-Scrapyd" class="headerlink" title="3. 访问 Scrapyd"></a>3. 访问 Scrapyd</h3>
                  <p>安装并运行了 Scrapyd 之后，我们就可以访问服务器的 6800 端口看到一个 WebUI 页面了，例如我的服务器地址为 120.27.34.25，在上面安装好了 Scrapyd 并成功运行，那么我就可以在本地的浏览器中打开：<a href="http://120.27.34.25:6800" target="_blank" rel="noopener">http://120.27.34.25:6800</a>，就可以看到 Scrapyd 的首页，这里请自行替换成你的服务器地址查看即可，如图 15-1 所示： <img src="https://cdn.cuiqingcai.com/2019-11-29-114054.png" alt=""> 图 15-1 Scrapyd 首页 如果可以成功访问到此页面，那么证明 Scrapyd 配置就没有问题了。</p>
                  <h3 id="4-Scrapyd-的功能"><a href="#4-Scrapyd-的功能" class="headerlink" title="4. Scrapyd 的功能"></a>4. Scrapyd 的功能</h3>
                  <p>Scrapyd 提供了一系列 HTTP 接口来实现各种操作，在这里我们可以将接口的功能梳理一下，以 Scrapyd 所在的 IP 为 120.27.34.25 为例：</p>
                  <h4 id="daemonstatus-json"><a href="#daemonstatus-json" class="headerlink" title="daemonstatus.json"></a>daemonstatus.json</h4>
                  <p>这个接口负责查看 Scrapyd 当前的服务和任务状态，我们可以用 curl 命令来请求这个接口，命令如下：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//139.217.26.30:6800/daemonstatus.json</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就会得到如下结果：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"finished"</span>: <span class="number">90</span>, <span class="attr">"running"</span>: <span class="number">9</span>, <span class="attr">"node_name"</span>: <span class="string">"datacrawl-vm"</span>, <span class="attr">"pending"</span>: <span class="number">0</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回结果是 Json 字符串，status 是当前运行状态， finished 代表当前已经完成的 Scrapy 任务，running 代表正在运行的 Scrapy 任务，pending 代表等待被调度的 Scrapyd 任务，node_name 就是主机的名称。</p>
                  <h4 id="addversion-json"><a href="#addversion-json" class="headerlink" title="addversion.json"></a>addversion.json</h4>
                  <p>这个接口主要是用来部署 Scrapy 项目用的，在部署的时候我们需要首先将项目打包成 Egg 文件，然后传入项目名称和部署版本。 我们可以用如下的方式实现项目部署：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="keyword">http</span>://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/addversion.json -F project=wenbo -F <span class="built_in">version</span>=<span class="keyword">first</span> -F egg=@weibo.egg</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里 -F 即代表添加一个参数，同时我们还需要将项目打包成 Egg 文件放到本地。 这样发出请求之后我们可以得到如下结果：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"spiders"</span>: <span class="number">3</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个结果表明部署成功，并且其中包含的 Spider 的数量为 3。 此方法部署可能比较繁琐，在后文会介绍更方便的工具来实现项目的部署。</p>
                  <h4 id="schedule-json"><a href="#schedule-json" class="headerlink" title="schedule.json"></a>schedule.json</h4>
                  <p>这个接口负责调度已部署好的 Scrapy 项目运行。 我们可以用如下接口实现任务调度：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/schedule.json -d project=weibo -d spider=weibocn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要传入两个参数，project 即 Scrapy 项目名称，spider 即 Spider 名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"jobid"</span>: <span class="string">"6487ec79947edab326d6db28a2d86511e8247444"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表 Scrapy 项目启动情况，jobid 代表当前正在运行的爬取任务代号。</p>
                  <h4 id="cancel-json"><a href="#cancel-json" class="headerlink" title="cancel.json"></a>cancel.json</h4>
                  <p>这个接口可以用来取消某个爬取任务，如果这个任务是 pending 状态，那么它将会被移除，如果这个任务是 running 状态，那么它将会被终止。 我们可以用下面的命令来取消任务的运行：</p>
                  <figure class="highlight dns">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl http://<span class="number">120.27.34.25</span>:<span class="number">6800</span>/cancel.json -d project=weibo -d job=<span class="number">6487</span>ec79947edab326d6db28a2d865<span class="number">11e8247444</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要传入两个参数，project 即项目名称，job 即爬取任务代号。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"prevstate"</span>: <span class="string">"running"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，prevstate 代表之前的运行状态。</p>
                  <h4 id="listprojects-json"><a href="#listprojects-json" class="headerlink" title="listprojects.json"></a>listprojects.json</h4>
                  <p>这个接口用来列出部署到 Scrapyd 服务上的所有项目描述。 我们可以用下面的命令来获取 Scrapyd 服务器上的所有项目描述：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listprojects.json</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里不需要传入任何参数。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"projects"</span>: [<span class="string">"weibo"</span>, <span class="string">"zhihu"</span>]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，projects 是项目名称列表。</p>
                  <h4 id="listversions-json"><a href="#listversions-json" class="headerlink" title="listversions.json"></a>listversions.json</h4>
                  <p>这个接口用来获取某个项目的所有版本号，版本号是按序排列的，最后一个条目是最新的版本号。 我们可以用如下命令来获取项目的版本号：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listversions.json?project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"versions"</span>: [<span class="string">"v1"</span>, <span class="string">"v2"</span>]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，versions 是版本号列表。</p>
                  <h4 id="listspiders-json"><a href="#listspiders-json" class="headerlink" title="listspiders.json"></a>listspiders.json</h4>
                  <p>这个接口用来获取某个项目最新的一个版本的所有 Spider 名称。 我们可以用如下命令来获取项目的 Spider 名称：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listspiders.json?project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"spiders"</span>: [<span class="string">"weibocn"</span>]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，spiders 是 Spider 名称列表。</p>
                  <h4 id="listjobs-json"><a href="#listjobs-json" class="headerlink" title="listjobs.json"></a>listjobs.json</h4>
                  <p>这个接口用来获取某个项目当前运行的所有任务详情。 我们可以用如下命令来获取所有任务详情：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listjobs.json?project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>,</span><br><span class="line"> <span class="attr">"pending"</span>: [&#123;<span class="attr">"id"</span>: <span class="string">"78391cc0fcaf11e1b0090800272a6d06"</span>, <span class="attr">"spider"</span>: <span class="string">"weibocn"</span>&#125;],</span><br><span class="line"> <span class="attr">"running"</span>: [&#123;<span class="attr">"id"</span>: <span class="string">"422e608f9f28cef127b3d5ef93fe9399"</span>, <span class="attr">"spider"</span>: <span class="string">"weibocn"</span>, <span class="attr">"start_time"</span>: <span class="string">"2017-07-12 10:14:03.594664"</span>&#125;],</span><br><span class="line"> <span class="attr">"finished"</span>: [&#123;<span class="attr">"id"</span>: <span class="string">"2f16646cfcaf11e1b0090800272a6d06"</span>, <span class="attr">"spider"</span>: <span class="string">"weibocn"</span>, <span class="attr">"start_time"</span>: <span class="string">"2017-07-12 10:14:03.594664"</span>, <span class="attr">"end_time"</span>: <span class="string">"2017-07-12 10:24:03.594664"</span>&#125;]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，pendings 代表当前正在等待的任务，running 代表当前正在运行的任务，finished 代表已经完成的任务。</p>
                  <h4 id="delversion-json"><a href="#delversion-json" class="headerlink" title="delversion.json"></a>delversion.json</h4>
                  <p>这个接口用来删除项目的某个版本。 我们可以用如下命令来删除项目版本：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/delversion.json -d project=weibo -d version=v1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称，还需要一个参数 version，就是项目的版本。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，这样就代表删除成功了。</p>
                  <h4 id="delproject-json"><a href="#delproject-json" class="headerlink" title="delproject.json"></a>delproject.json</h4>
                  <p>这个接口用来删除某个项目。 我们可以用如下命令来删除某个项目：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/delproject.json -d project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，这样就代表删除成功了。 以上就是 Scrapyd 所有的接口，我们可以直接请求 HTTP 接口即可控制项目的部署、启动、运行等操作。</p>
                  <h3 id="5-ScrapydAPI-的使用"><a href="#5-ScrapydAPI-的使用" class="headerlink" title="5. ScrapydAPI 的使用"></a>5. ScrapydAPI 的使用</h3>
                  <p>以上的这些接口可能使用起来还不是很方便，没关系，还有一个 ScrapydAPI 库对这些接口又做了一层封装，其安装方式也可以参考第一章的内容。 下面我们来看下 ScrapydAPI 的使用方法，其实核心原理和 HTTP 接口请求方式并无二致，只不过用 Python 封装后使用更加便捷。 我们可以用如下方式建立一个 ScrapydAPI 对象：</p>
                  <figure class="highlight clean">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapyd_api <span class="keyword">import</span> ScrapydAPI</span><br><span class="line">scrapyd = ScrapydAPI(<span class="string">'http://120.27.34.25:6800'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后就可以调用它的方法来实现对应接口的操作了，例如部署的操作可以使用如下方式：</p>
                  <figure class="highlight sas">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">egg =<span class="meta"> open(</span><span class="string">'weibo.egg'</span>, <span class="string">'rb'</span>)</span><br><span class="line">scrapyd.add_versi<span class="meta">on(</span><span class="string">'weibo'</span>, <span class="string">'v1'</span>, egg)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就可以将项目打包为 Egg 文件，然后把本地打包的的 Egg 项目部署到远程 Scrapyd 了。 另外 ScrapydAPI 还实现了所有 Scrapyd 提供的 API 接口，名称都是相同的，参数也是相同的。 例如我们可以调用 list_projects() 方法即可列出 Scrapyd 中所有已部署的项目：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapyd</span><span class="selector-class">.list_projects</span>()</span><br><span class="line"><span class="selector-attr">[<span class="string">'weibo'</span>, <span class="string">'zhihu'</span>]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外还有其他的方法在此不再一一列举了，名称和参数都是相同的，更加详细的操作可以参考其官方文档：<a href="http://python-scrapyd-api.readthedocs.io/" target="_blank" rel="noopener">http://python-scrapyd-api.readthedocs.io/</a>。</p>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>本节介绍了 Scrapyd 及 ScrapydAPI 的相关用法，我们可以通过它来部署项目，并通过 HTTP 接口来控制人物的运行，不过这里有一个不方便的地方就是部署过程，首先它需要打包 Egg 文件然后再上传，还是比较繁琐的，在下一节我们介绍一个更加方便的工具来完成部署过程。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-10 09:26:07" itemprop="dateCreated datePublished" datetime="2019-12-10T09:26:07+08:00">2019-12-10</time>
                </span>
                <span id="/8475.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.1–Scrapyd 分布式部署" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8472.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8472.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.4–Bloom Filter 的对接</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-4-Bloom-Filter-的对接"><a href="#14-4-Bloom-Filter-的对接" class="headerlink" title="14.4 Bloom Filter 的对接"></a>14.4 Bloom Filter 的对接</h1>
                  <p>首先回顾一下 Scrapy-Redis 的去重机制。Scrapy-Redis 将 Request 的指纹存储到了 Redis 集合中，每个指纹的长度为 40，例如 27adcc2e8979cdee0c9cecbbe8bf8ff51edefb61 就是一个指纹，它的每一位都是 16 进制数。 我们计算一下用这种方式耗费的存储空间。每个十六进制数占用 4 b，1 个指纹用 40 个十六进制数表示，占用空间为 20 B，1 万个指纹即占用空间 200 KB，1 亿个指纹占用 2 GB。当爬取数量达到上亿级别时，Redis 的占用的内存就会变得很大，而且这仅仅是指纹的存储。Redis 还存储了爬取队列，内存占用会进一步提高，更别说有多个 Scrapy 项目同时爬取的情况了。当爬取达到亿级别规模时，Scrapy-Redis 提供的集合去重已经不能满足我们的要求。所以我们需要使用一个更加节省内存的去重算法 Bloom Filter。</p>
                  <h3 id="1-了解-BloomFilter"><a href="#1-了解-BloomFilter" class="headerlink" title="1. 了解 BloomFilter"></a>1. 了解 BloomFilter</h3>
                  <p>Bloom Filter，中文名称叫作布隆过滤器，是 1970 年由 Bloom 提出的，它可以被用来检测一个元素是否在一个集合中。Bloom Filter 的空间利用效率很高，使用它可以大大节省存储空间。Bloom Filter 使用位数组表示一个待检测集合，并可以快速地通过概率算法判断一个元素是否存在于这个集合中。利用这个算法我们可以实现去重效果。 本节我们来了解 Bloom Filter 的基本算法，以及 Scrapy-Redis 中对接 Bloom Filter 的方法。</p>
                  <h3 id="2-BloomFilter-的算法"><a href="#2-BloomFilter-的算法" class="headerlink" title="2. BloomFilter 的算法"></a>2. BloomFilter 的算法</h3>
                  <p>在 Bloom Filter 中使用位数组来辅助实现检测判断。在初始状态下，我们声明一个包含 m 位的位数组，它的所有位都是 0，如图 14-7 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-29-113850.jpg" alt=""> 图 14-7 初始位数组 现在我们有了一个待检测集合，我们表示为 S={x1, x2, …, xn}，我们接下来需要做的就是检测一个 x 是否已经存在于集合 S 中。在 BloomFilter 算法中首先使用 k 个相互独立的、随机的哈希函数来将这个集合 S 中的每个元素 x1、x2、…、xn 映射到这个长度为 m 的位数组上，哈希函数得到的结果记作位置索引，然后将位数组该位置索引的位置 1。例如这里我们取 k 为 3，即有三个哈希函数，x1 经过三个哈希函数映射得到的结果分别为 1、4、8，x2 经过三个哈希函数映射得到的结果分别为 4、6、10，那么就会将位数组的 1、4、6、8、10 这五位置 1，如图 14-8 所示： <img src="https://cdn.cuiqingcai.com/2019-11-29-114343.jpg" alt=""> 图 14-8 映射后位数组 这时如果再有一个新的元素 x，我们要判断 x 是否属于 S 这个集合，我们便会将仍然用 k 个哈希函数对 x 求映射结果，如果所有结果对应的位数组位置均为 1，那么我们就认为 x 属于 S 这个集合，否则如果有一个不为 1，则 x 不属于 S 集合。 例如一个新元素 x 经过三个哈希函数映射的结果为 4、6、8，对应的位置均为 1，则判断 x 属于 S 这个集合。如果结果为 4、6、7，7 对应的位置为 0，则判定 x 不属于 S 这个集合。 注意这里 m、n、k 满足的关系是 m&gt;nk，也就是说位数组的长度 m 要比集合元素 n 和哈希函数 k 的乘积还要大。 这样的判定方法很高效，但是也是有代价的，它可能把不属于这个集合的元素误认为属于这个集合，我们来估计一下它的错误率。当集合 S={x1, x2,…, xn} 的所有元素都被 k 个哈希函数映射到 m 位的位数组中时，这个位数组中某一位还是 0 的概率是： <img src="https://cdn.cuiqingcai.com/2019-11-29-114353.jpg" alt=""> 因为哈希函数是随机的，所以任意一个哈希函数选中这一位的概率为 1/m，那么 1-1/m 就代表哈希函数一次没有选中这一位的概率，要把 S 完全映射到 m 位数组中，需要做 kn 次哈希运算，所以最后的概率就是 1-1/m 的 kn 次方。 一个不属于 S 的元素 x 如果要被误判定为在 S 中，那么这个概率就是 k 次哈希运算得到的结果对应的位数组位置都为 1，所以误判概率为： <img src="https://cdn.cuiqingcai.com/2019-11-29-114430.jpg" alt=""> 根据： <img src="https://cdn.cuiqingcai.com/2019-11-29-114441.jpg" alt=""> 可以将误判概率转化为： <img src="https://cdn.cuiqingcai.com/2019-11-29-114445.jpg" alt=""> 在给定 m、n 时，可以求出使得 f 最小化的 k 值为： <img src="https://cdn.cuiqingcai.com/2019-11-29-114452.jpg" alt=""> 在这里将误判概率归纳如下： 表 14-1 　误判概率</p>
                  <p>m/n</p>
                  <p>最优 k</p>
                  <p>k=1</p>
                  <p>k=2</p>
                  <p>k=3</p>
                  <p>k=4</p>
                  <p>k=5</p>
                  <p>k=6</p>
                  <p>k=7</p>
                  <p>k=8</p>
                  <p>2</p>
                  <p>1.39</p>
                  <p>0.393</p>
                  <p>0.400</p>
                  <p>3</p>
                  <p>2.08</p>
                  <p>0.283</p>
                  <p>0.237</p>
                  <p>0.253</p>
                  <p>4</p>
                  <p>2.77</p>
                  <p>0.221</p>
                  <p>0.155</p>
                  <p>0.147</p>
                  <p>0.160</p>
                  <p>5</p>
                  <p>3.46</p>
                  <p>0.181</p>
                  <p>0.109</p>
                  <p>0.092</p>
                  <p>0.092</p>
                  <p>0.101</p>
                  <p>6</p>
                  <p>4.16</p>
                  <p>0.154</p>
                  <p>0.0804</p>
                  <p>0.0609</p>
                  <p>0.0561</p>
                  <p>0.0578</p>
                  <p>0.0638</p>
                  <p>7</p>
                  <p>4.85</p>
                  <p>0.133</p>
                  <p>0.0618</p>
                  <p>0.0423</p>
                  <p>0.0359</p>
                  <p>0.0347</p>
                  <p>0.0364</p>
                  <p>8</p>
                  <p>5.55</p>
                  <p>0.118</p>
                  <p>0.0489</p>
                  <p>0.0306</p>
                  <p>0.024</p>
                  <p>0.0217</p>
                  <p>0.0216</p>
                  <p>0.0229</p>
                  <p>9</p>
                  <p>6.24</p>
                  <p>0.105</p>
                  <p>0.0397</p>
                  <p>0.0228</p>
                  <p>0.0166</p>
                  <p>0.0141</p>
                  <p>0.0133</p>
                  <p>0.0135</p>
                  <p>0.0145</p>
                  <p>10</p>
                  <p>6.93</p>
                  <p>0.0952</p>
                  <p>0.0329</p>
                  <p>0.0174</p>
                  <p>0.0118</p>
                  <p>0.00943</p>
                  <p>0.00844</p>
                  <p>0.00819</p>
                  <p>0.00846</p>
                  <p>11</p>
                  <p>7.62</p>
                  <p>0.0869</p>
                  <p>0.0276</p>
                  <p>0.0136</p>
                  <p>0.00864</p>
                  <p>0.0065</p>
                  <p>0.00552</p>
                  <p>0.00513</p>
                  <p>0.00509</p>
                  <p>12</p>
                  <p>8.32</p>
                  <p>0.08</p>
                  <p>0.0236</p>
                  <p>0.0108</p>
                  <p>0.00646</p>
                  <p>0.00459</p>
                  <p>0.00371</p>
                  <p>0.00329</p>
                  <p>0.00314</p>
                  <p>13</p>
                  <p>9.01</p>
                  <p>0.074</p>
                  <p>0.0203</p>
                  <p>0.00875</p>
                  <p>0.00492</p>
                  <p>0.00332</p>
                  <p>0.00255</p>
                  <p>0.00217</p>
                  <p>0.00199</p>
                  <p>14</p>
                  <p>9.7</p>
                  <p>0.0689</p>
                  <p>0.0177</p>
                  <p>0.00718</p>
                  <p>0.00381</p>
                  <p>0.00244</p>
                  <p>0.00179</p>
                  <p>0.00146</p>
                  <p>0.00129</p>
                  <p>15</p>
                  <p>10.4</p>
                  <p>0.0645</p>
                  <p>0.0156</p>
                  <p>0.00596</p>
                  <p>0.003</p>
                  <p>0.00183</p>
                  <p>0.00128</p>
                  <p>0.001</p>
                  <p>0.000852</p>
                  <p>16</p>
                  <p>11.1</p>
                  <p>0.0606</p>
                  <p>0.0138</p>
                  <p>0.005</p>
                  <p>0.00239</p>
                  <p>0.00139</p>
                  <p>0.000935</p>
                  <p>0.000702</p>
                  <p>0.000574</p>
                  <p>17</p>
                  <p>11.8</p>
                  <p>0.0571</p>
                  <p>0.0123</p>
                  <p>0.00423</p>
                  <p>0.00193</p>
                  <p>0.00107</p>
                  <p>0.000692</p>
                  <p>0.000499</p>
                  <p>0.000394</p>
                  <p>18</p>
                  <p>12.5</p>
                  <p>0.054</p>
                  <p>0.0111</p>
                  <p>0.00362</p>
                  <p>0.00158</p>
                  <p>0.000839</p>
                  <p>0.000519</p>
                  <p>0.00036</p>
                  <p>0.000275</p>
                  <p>19</p>
                  <p>13.2</p>
                  <p>0.0513</p>
                  <p>0.00998</p>
                  <p>0.00312</p>
                  <p>0.0013</p>
                  <p>0.000663</p>
                  <p>0.000394</p>
                  <p>0.000264</p>
                  <p>0.000194</p>
                  <p>20</p>
                  <p>13.9</p>
                  <p>0.0488</p>
                  <p>0.00906</p>
                  <p>0.0027</p>
                  <p>0.00108</p>
                  <p>0.00053</p>
                  <p>0.000303</p>
                  <p>0.000196</p>
                  <p>0.00014</p>
                  <p>21</p>
                  <p>14.6</p>
                  <p>0.0465</p>
                  <p>0.00825</p>
                  <p>0.00236</p>
                  <p>0.000905</p>
                  <p>0.000427</p>
                  <p>0.000236</p>
                  <p>0.000147</p>
                  <p>0.000101</p>
                  <p>22</p>
                  <p>15.2</p>
                  <p>0.0444</p>
                  <p>0.00755</p>
                  <p>0.00207</p>
                  <p>0.000764</p>
                  <p>0.000347</p>
                  <p>0.000185</p>
                  <p>0.000112</p>
                  <p>7.46e-05</p>
                  <p>23</p>
                  <p>15.9</p>
                  <p>0.0425</p>
                  <p>0.00694</p>
                  <p>0.00183</p>
                  <p>0.000649</p>
                  <p>0.000285</p>
                  <p>0.000147</p>
                  <p>8.56e-05</p>
                  <p>5.55e-05</p>
                  <p>24</p>
                  <p>16.6</p>
                  <p>0.0408</p>
                  <p>0.00639</p>
                  <p>0.00162</p>
                  <p>0.000555</p>
                  <p>0.000235</p>
                  <p>0.000117</p>
                  <p>6.63e-05</p>
                  <p>4.17e-05</p>
                  <p>25</p>
                  <p>17.3</p>
                  <p>0.0392</p>
                  <p>0.00591</p>
                  <p>0.00145</p>
                  <p>0.000478</p>
                  <p>0.000196</p>
                  <p>9.44e-05</p>
                  <p>5.18e-05</p>
                  <p>3.16e-05</p>
                  <p>26</p>
                  <p>18</p>
                  <p>0.0377</p>
                  <p>0.00548</p>
                  <p>0.00129</p>
                  <p>0.000413</p>
                  <p>0.000164</p>
                  <p>7.66e-05</p>
                  <p>4.08e-05</p>
                  <p>2.42e-05</p>
                  <p>27</p>
                  <p>18.7</p>
                  <p>0.0364</p>
                  <p>0.0051</p>
                  <p>0.00116</p>
                  <p>0.000359</p>
                  <p>0.000138</p>
                  <p>6.26e-05</p>
                  <p>3.24e-05</p>
                  <p>1.87e-05</p>
                  <p>28</p>
                  <p>19.4</p>
                  <p>0.0351</p>
                  <p>0.00475</p>
                  <p>0.00105</p>
                  <p>0.000314</p>
                  <p>0.000117</p>
                  <p>5.15e-05</p>
                  <p>2.59e-05</p>
                  <p>1.46e-05</p>
                  <p>29</p>
                  <p>20.1</p>
                  <p>0.0339</p>
                  <p>0.00444</p>
                  <p>0.000949</p>
                  <p>0.000276</p>
                  <p>9.96e-05</p>
                  <p>4.26e-05</p>
                  <p>2.09e-05</p>
                  <p>1.14e-05</p>
                  <p>30</p>
                  <p>20.8</p>
                  <p>0.0328</p>
                  <p>0.00416</p>
                  <p>0.000862</p>
                  <p>0.000243</p>
                  <p>8.53e-05</p>
                  <p>3.55e-05</p>
                  <p>1.69e-05</p>
                  <p>9.01e-06</p>
                  <p>31</p>
                  <p>21.5</p>
                  <p>0.0317</p>
                  <p>0.0039</p>
                  <p>0.000785</p>
                  <p>0.000215</p>
                  <p>7.33e-05</p>
                  <p>2.97e-05</p>
                  <p>1.38e-05</p>
                  <p>7.16e-06</p>
                  <p>32</p>
                  <p>22.2</p>
                  <p>0.0308</p>
                  <p>0.00367</p>
                  <p>0.000717</p>
                  <p>0.000191</p>
                  <p>6.33e-05</p>
                  <p>2.5e-05</p>
                  <p>1.13e-05</p>
                  <p>5.73e-06</p>
                  <p>表 14-1 中第一列为 m/n 的值，第二列为最优 k 值，其后列为不同 k 值的误判概率，可以看到当 k 值确定时，随着 m/n 的增大，误判概率逐渐变小。当 m/n 的值确定时，当 k 越靠近最优 K 值，误判概率越小。另外误判概率总体来看都是极小的，在容忍此误判概率的情况下，大幅减小存储空间和判定速度是完全值得的。 接下来我们就将 BloomFilter 算法应用到 Scrapy-Redis 分布式爬虫的去重过程中，以解决 Redis 内存不足的问题。</p>
                  <h3 id="3-对接-Scrapy-Redis"><a href="#3-对接-Scrapy-Redis" class="headerlink" title="3. 对接 Scrapy-Redis"></a>3. 对接 Scrapy-Redis</h3>
                  <p>实现 BloomFilter 时，我们首先要保证不能破坏 Scrapy-Redis 分布式爬取的运行架构，所以我们需要修改 Scrapy-Redis 的源码，将它的去重类替换掉。同时 BloomFilter 的实现需要借助于一个位数组，所以既然当前架构还是依赖于 Redis 的，那么正好位数组的维护直接使用 Redis 就好了。 首先我们实现一个基本的哈希算法，可以实现将一个值经过哈希运算后映射到一个 m 位位数组的某一位上，代码实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashMap</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, m, seed)</span>:</span></span><br><span class="line">        self.m = m</span><br><span class="line">        self.seed = seed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Hash Algorithm</span></span><br><span class="line"><span class="string">        :param value: Value</span></span><br><span class="line"><span class="string">        :return: Hash Value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ret = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(value)):</span><br><span class="line">            ret += self.seed * ret + ord(value[i])</span><br><span class="line">        <span class="keyword">return</span> (self.m - <span class="number">1</span>) &amp; ret</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里新建了一个 HashMap 类，构造函数传入两个值，一个是 m 位数组的位数，另一个是种子值 seed，不同的哈希函数需要有不同的 seed，这样可以保证不同的哈希函数的结果不会碰撞。 在 hash() 方法的实现中，value 是要被处理的内容，在这里我们遍历了该字符的每一位并利用 ord() 方法取到了它的 ASCII 码值，然后混淆 seed 进行迭代求和运算，最终会得到一个数值。这个数值的结果就由 value 和 seed 唯一确定，然后我们再将它和 m 进行按位与运算，即可获取到 m 位数组的映射结果，这样我们就实现了一个由字符串和 seed 来确定的哈希函数。当 m 固定时，只要 seed 值相同，就代表是同一个哈希函数，相同的 value 必然会映射到相同的位置。所以如果我们想要构造几个不同的哈希函数，只需要改变其 seed 就好了，以上便是一个简易的哈希函数的实现。 接下来我们再实现 BloomFilter，BloomFilter 里面需要用到 k 个哈希函数，所以在这里我们需要对这几个哈希函数指定相同的 m 值和不同的 seed 值，在这里构造如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">BLOOMFILTER_HASH_NUMBER = 6</span><br><span class="line">BLOOMFILTER_BIT = 30</span><br><span class="line"></span><br><span class="line">class BloomFilter(object):</span><br><span class="line">    def __init__(self, server, key, <span class="attribute">bit</span>=BLOOMFILTER_BIT, <span class="attribute">hash_number</span>=BLOOMFILTER_HASH_NUMBER):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Initialize BloomFilter</span></span><br><span class="line"><span class="string">        :param server: Redis Server</span></span><br><span class="line"><span class="string">        :param key: BloomFilter Key</span></span><br><span class="line"><span class="string">        :param bit: m = 2 ^ bit</span></span><br><span class="line"><span class="string">        :param hash_number: the number of hash function</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        #<span class="built_in"> default </span><span class="keyword">to</span> 1 &lt;&lt; 30 = 10,7374,1824 = 2^30 = 128MB, max<span class="built_in"> filter </span>2^30/hash_number = 1,7895,6970 fingerprints</span><br><span class="line">        self.m = 1 &lt;&lt; bit</span><br><span class="line">        self.seeds = range(hash_number)</span><br><span class="line">        self.maps = [HashMap(self.m, seed) <span class="keyword">for</span> seed <span class="keyword">in</span> self.seeds]</span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于我们需要亿级别的数据的去重，即前文介绍的算法中的 n 为 1 亿以上，哈希函数的个数 k 大约取 10 左右的量级，而 m&gt;kn，所以这里 m 值大约保底在 10 亿，由于这个数值比较大，所以这里用移位操作来实现，传入位数 bit，定义 30，然后做一个移位操作 1 &lt;&lt; 30，相当于 2 的 30 次方，等于 1073741824，量级也是恰好在 10 亿左右，由于是位数组，所以这个位数组占用的大小就是 2^30b=128MB，而本文开头我们计算过 Scrapy-Redis 集合去重的占用空间大约在 2G 左右，可见 BloomFilter 的空间利用效率之高。 随后我们再传入哈希函数的个数，用它来生成几个不同的 seed，用不同的 seed 来定义不同的哈希函数，这样我们就可以构造一个哈希函数列表，遍历 seed，构造带有不同 seed 值的 HashMap 对象，保存成变量 maps 供后续使用。 另外 server 就是 Redis 连接对象，key 就是这个 m 位数组的名称。 接下来我们就要实现比较关键的两个方法了，一个是判定元素是否重复的方法 exists()，另一个是添加元素到集合中的方法 insert()，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span><span class="params">(self, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    if value exists</span></span><br><span class="line"><span class="string">    :param value:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> value:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    exist = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> map <span class="keyword">in</span> self.maps:</span><br><span class="line">        offset = map.hash(value)</span><br><span class="line">        exist = exist &amp; self.server.getbit(self.key, offset)</span><br><span class="line">    <span class="keyword">return</span> exist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    add value to bloom</span></span><br><span class="line"><span class="string">    :param value:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> self.maps:</span><br><span class="line">        offset = f.hash(value)</span><br><span class="line">        self.server.setbit(self.key, offset, <span class="number">1</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先我们先看下 insert() 方法，BloomFilter 算法中会逐个调用哈希函数对放入集合中的元素进行运算得到在 m 位位数组中的映射位置，然后将位数组对应的位置置 1，所以这里在代码中我们遍历了初始化好的哈希函数，然后调用其 hash() 方法算出映射位置 offset，再利用 Redis 的 setbit() 方法将该位置 1。 在 exists() 方法中我们就需要实现判定是否重复的逻辑了，方法参数 value 即为待判断的元素，在这里我们首先定义了一个变量 exist，然后遍历了所有哈希函数对 value 进行哈希运算，得到映射位置，然后我们用 getbit() 方法取得该映射位置的结果，依次进行与运算。这样只有每次 getbit() 得到的结果都为 1 时，最后的 exist 才为 True，即代表 value 属于这个集合。如果其中只要有一次 getbit() 得到的结果为 0，即 m 位数组中有对应的 0 位，那么最终的结果 exist 就为 False，即代表 value 不属于这个集合。这样此方法最后的返回结果就是判定重复与否的结果了。 到现在为止 BloomFilter 的实现就已经完成了，我们可以用一个实例来测试一下，代码如下：</p>
                  <figure class="highlight vim">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">conn = StrictRedis(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, password=<span class="string">'foobared'</span>)</span><br><span class="line"><span class="keyword">bf</span> = BloomFilter(conn, <span class="string">'testbf'</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">bf</span>.<span class="keyword">insert</span>(<span class="string">'Hello'</span>)</span><br><span class="line"><span class="keyword">bf</span>.<span class="keyword">insert</span>(<span class="string">'World'</span>)</span><br><span class="line">result = <span class="keyword">bf</span>.<span class="built_in">exists</span>(<span class="string">'Hello'</span>)</span><br><span class="line"><span class="keyword">print</span>(bool(result))</span><br><span class="line">result = <span class="keyword">bf</span>.<span class="built_in">exists</span>(<span class="string">'Python'</span>)</span><br><span class="line"><span class="keyword">print</span>(bool(result))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们首先定义了一个 Redis 连接对象，然后传递给 BloomFilter，为了避免内存占用过大这里传的位数 bit 比较小，设置为 5，哈希函数的个数设置为 6。 首先我们调用 insert() 方法插入了 Hello 和 World 两个字符串，随后判断了一下 Hello 和 Python 这两个字符串是否存在，最后输出它的结果，运行结果如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>很明显，结果完全没有问题，这样我们就借助于 Redis 成功实现了 BloomFilter 的算法。 接下来我们需要继续修改 Scrapy-Redis 的源码，将它的 dupefilter 逻辑替换为 BloomFilter 的逻辑，在这里主要是修改 RFPDupeFilter 类的 request_seen() 方法，实现如下：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(<span class="keyword">self</span>, request)</span></span><span class="symbol">:</span></span><br><span class="line">    fp = <span class="keyword">self</span>.request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.bf.exists(fp)<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> True</span><br><span class="line">    <span class="keyword">self</span>.bf.insert(fp)</span><br><span class="line">    <span class="keyword">return</span> False</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先还是利用 request_fingerprint() 方法获取了 Request 的指纹，然后调用 BloomFilter 的 exists() 方法判定了该指纹是否存在，如果存在，则证明该 Request 是重复的，返回 True，否则调用 BloomFilter 的 insert() 方法将该指纹添加并返回 False，这样就成功利用 BloomFilter 替换了 Scrapy-Redis 的集合去重。 对于 BloomFilter 的初始化定义，我们可以将 <strong>init</strong>() 方法修改为如下内容：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def __init__(self, server, key, debug, bit, hash_number):</span><br><span class="line">    self.server = server</span><br><span class="line">    self.key = key</span><br><span class="line">    self.<span class="builtin-name">debug</span> = debug</span><br><span class="line">    self.bit = bit</span><br><span class="line">    self.hash_number = hash_number</span><br><span class="line">    self.logdupes = <span class="literal">True</span></span><br><span class="line">    self.bf = BloomFilter(server, self.key, bit, hash_number)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中 bit 和 hash_number 需要使用 from_settings() 方法传递，修改如下：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">@classmethod</span><br><span class="line">def from_settings(cls, settings):</span><br><span class="line">    <span class="keyword">server</span> = get_redis_from_settings(settings)</span><br><span class="line">    key = defaults.DUPEFILTER_KEY % &#123;<span class="string">'timestamp'</span>: <span class="type">int</span>(<span class="type">time</span>.time())&#125;</span><br><span class="line">    <span class="keyword">debug</span> = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>, DUPEFILTER_DEBUG)</span><br><span class="line">    <span class="type">bit</span> = settings.getint(<span class="string">'BLOOMFILTER_BIT'</span>, BLOOMFILTER_BIT)</span><br><span class="line">    hash_number = settings.getint(<span class="string">'BLOOMFILTER_HASH_NUMBER'</span>, BLOOMFILTER_HASH_NUMBER)</span><br><span class="line">    <span class="keyword">return</span> cls(<span class="keyword">server</span>, key=key, <span class="keyword">debug</span>=<span class="keyword">debug</span>, <span class="type">bit</span>=<span class="type">bit</span>, hash_number=hash_number)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中常量的定义 DUPEFILTER_DEBUG 和 BLOOMFILTER_BIT 统一定义在 defaults.py 中，默认如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">BLOOMFILTER_HASH_NUMBER</span> = <span class="number">6</span></span><br><span class="line"><span class="attr">BLOOMFILTER_BIT</span> = <span class="number">30</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>到此为止我们就成功实现了 BloomFilter 和 Scrapy-Redis 的对接。</p>
                  <h3 id="4-本节代码"><a href="#4-本节代码" class="headerlink" title="4. 本节代码"></a>4. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyRedisBloomFilter" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyRedisBloomFilter</a>。</p>
                  <h3 id="5-使用"><a href="#5-使用" class="headerlink" title="5. 使用"></a>5. 使用</h3>
                  <p>为了方便使用，本节的代码已经打包成了一个 Python 包并发布到了 PyPi，链接为：<a href="https://pypi.python.org/pypi/scrapy-redis-bloomfilter" target="_blank" rel="noopener">https://pypi.python.org/pypi/scrapy-redis-bloomfilter</a>，因此我们以后如果想使用 ScrapyRedisBloomFilter 直接使用就好了，不需要再自己实现一遍。 我们可以直接使用 Pip 来安装，命令如下：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> scrapy-redis-bloomfilter</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>使用的方法和 Scrapy-Redis 基本相似，在这里说明几个关键配置：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 去重类，要使用 BloomFilter 请替换 DUPEFILTER_CLASS</span></span><br><span class="line"><span class="attr">DUPEFILTER_CLASS</span> = <span class="string">"scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter"</span></span><br><span class="line"><span class="comment"># 哈希函数的个数，默认为 6，可以自行修改</span></span><br><span class="line"><span class="attr">BLOOMFILTER_HASH_NUMBER</span> = <span class="number">6</span></span><br><span class="line"><span class="comment"># BloomFilter 的 bit 参数，默认 30，占用 128MB 空间，去重量级 1 亿</span></span><br><span class="line"><span class="attr">BLOOMFILTER_BIT</span> = <span class="number">30</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>DUPEFILTER_CLASS 是去重类，如果要使用 BloomFilter 需要将 DUPEFILTER_CLASS 修改为该包的去重类。 BLOOMFILTER_HASH_NUMBER 是 BloomFilter 使用的哈希函数的个数，默认为 6，可以根据去重量级自行修改。 BLOOMFILTER_BIT 即前文所介绍的 BloomFilter 类的 bit 参数，它决定了位数组的位数，如果 BLOOMFILTER_BIT 为 30，那么位数组位数为 2 的 30 次方，将占用 Redis 128MB 的存储空间，去重量级在 1 亿左右，即对应爬取量级 1 亿左右。如果爬取量级在 10 亿、20 亿甚至 100 亿，请务必将此参数对应调高。</p>
                  <h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3>
                  <p>在源代码中附有一个测试项目，放在 tests 文件夹，该项目使用了 Scrapy-RedisBloomFilter 来去重，Spider 的实现如下：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy import Request, Spider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSpider</span>(<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'test'</span></span><br><span class="line">    base_url = <span class="string">'https://www.baidu.com/s?wd='</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)<span class="symbol">:</span></span><br><span class="line">            url = <span class="keyword">self</span>.base_url + str(i)</span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=<span class="keyword">self</span>.parse)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Here contains 10 duplicated Requests</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)<span class="symbol">:</span></span><br><span class="line">            url = <span class="keyword">self</span>.base_url + str(i)</span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=<span class="keyword">self</span>.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.logger.debug(<span class="string">'Response of '</span> + response.url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 start_requests() 方法中首先循环 10 次，构造参数为 0-9 的 URL，然后重新循环了 100 次，构造了参数为 0-99 的 URL，那么这里就会包含 10 个重复的 Request，我们运行项目测试一下：</p>
                  <figure class="highlight bash">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl <span class="built_in">test</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到最后的输出结果如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">&#123;'bloomfilter/filtered':</span> <span class="number">10</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/request_bytes':</span> <span class="number">34021</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/request_count':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/request_method_count/GET':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/response_bytes':</span> <span class="number">72943</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/response_count':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/response_status_count/200':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'finish_reason':</span> <span class="string">'finished'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'finish_time':</span> <span class="string">datetime.datetime(2017,</span> <span class="number">8</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">9</span><span class="string">,</span> <span class="number">34</span><span class="string">,</span> <span class="number">30</span><span class="string">,</span> <span class="number">419597</span><span class="string">),</span></span><br><span class="line"> <span class="attr">'log_count/DEBUG':</span> <span class="number">202</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'log_count/INFO':</span> <span class="number">7</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'memusage/max':</span> <span class="number">54153216</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'memusage/startup':</span> <span class="number">54153216</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'response_received_count':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'scheduler/dequeued/redis':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'scheduler/enqueued/redis':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'start_time':</span> <span class="string">datetime.datetime(2017,</span> <span class="number">8</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">9</span><span class="string">,</span> <span class="number">34</span><span class="string">,</span> <span class="number">26</span><span class="string">,</span> <span class="number">495018</span><span class="string">)&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到最后统计的第一行的结果：</p>
                  <figure class="highlight sml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">'bloomfilter</span>/filtered': <span class="number">10</span>,</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是 BloomFilter 过滤后的统计结果，可以看到它的过滤个数为 10 个，也就是它成功将重复的 10 个 Reqeust 识别出来了，测试通过。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>以上便是 BloomFilter 的原理及对接实现，使用了 BloomFilter 可以大大节省 Redis 内存，在数据量大的情况下推荐使用此方案。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-10 09:24:45" itemprop="dateCreated datePublished" datetime="2019-12-10T09:24:45+08:00">2019-12-10</time>
                </span>
                <span id="/8472.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.4–Bloom Filter 的对接" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>10k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>9 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8468.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8468.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.3–Scrapy 分布式实现</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-3-Scrapy-分布式实现"><a href="#14-3-Scrapy-分布式实现" class="headerlink" title="14.3 Scrapy 分布式实现"></a>14.3 Scrapy 分布式实现</h1>
                  <p>接下来，我们会利用 Scrapy-Redis 来实现分布式的对接。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请确保已经成功实现了 Scrapy 新浪微博爬虫，Scrapy-Redis 库已经正确安装，如果还没安装，请参考第 1 章的安装说明。</p>
                  <h3 id="2-搭建-Redis-服务器"><a href="#2-搭建-Redis-服务器" class="headerlink" title="2. 搭建 Redis 服务器"></a>2. 搭建 Redis 服务器</h3>
                  <p>要实现分布式部署，多台主机需要共享爬取队列和去重集合，而这两部分内容都是存于 Redis 数据库中的，我们需要搭建一个可公网访问的 Redis 服务器。 推荐使用 Linux 服务器，可以购买阿里云、腾讯云、Azure 等提供的云主机，一般都会配有公网 IP，具体的搭建方式可以参考第 1 章中 Redis 数据库的安装方式。 Redis 安装完成之后就可以远程连接了，注意部分商家（如阿里云、腾讯云）的服务器需要配置安全组放通 Redis 运行端口才可以远程访问。如果遇到不能远程连接的问题，可以排查安全组的设置。 需要记录 Redis 的运行 IP、端口、地址，供后面配置分布式爬虫使用。当前配置好的 Redis 的 IP 为服务器的 IP 120.27.34.25，端口为默认的 6379，密码为 foobared。</p>
                  <h3 id="3-部署代理池和-Cookies-池"><a href="#3-部署代理池和-Cookies-池" class="headerlink" title="3. 部署代理池和 Cookies 池"></a>3. 部署代理池和 Cookies 池</h3>
                  <p>新浪微博项目需要用到代理池和 Cookies 池，而之前我们的代理池和 Cookies 池都是在本地运行的。所以我们需要将二者放到可以被公网访问的服务器上运行，将代码上传到服务器，修改 Redis 的连接信息配置，用同样的方式运行代理池和 Cookies 池。 远程访问代理池和 Cookies 池提供的接口，来获取随机代理和 Cookies。如果不能远程访问，先确保其在 0.0.0.0 这个 Host 上运行，再检查安全组的配置。 如我当前配置好的代理池和 Cookies 池的运行 IP 都是服务器的 IP，120.27.34.25，端口分别为 5555 和 5556，如图 14-3 和图 14-4 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-29-113501.jpg" alt=""> 图 14-3 代理池接口 <img src="https://cdn.cuiqingcai.com/2019-11-29-113506.jpg" alt=""> 图 14-4 Cookies 池接口 所以接下来我们就需要把 Scrapy 新浪微博项目中的访问链接修改如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">PROXY_URL</span> = <span class="string">'http://120.27.34.25:5555/random'</span></span><br><span class="line"><span class="attr">COOKIES_URL</span> = <span class="string">'http://120.27.34.25:5556/weibo/random'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>具体的修改方式根据实际配置的 IP 和端口做相应调整。</p>
                  <h3 id="4-配置-Scrapy-Redis"><a href="#4-配置-Scrapy-Redis" class="headerlink" title="4. 配置 Scrapy-Redis"></a>4. 配置 Scrapy-Redis</h3>
                  <p>配置 Scrapy-Redis 非常简单，只需要修改一下 settings.py 配置文件即可。</p>
                  <h4 id="核心配置"><a href="#核心配置" class="headerlink" title="核心配置"></a>核心配置</h4>
                  <p>首先最主要的是，需要将调度器的类和去重的类替换为 Scrapy-Redis 提供的类，在 settings.py 里面添加如下配置即可：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER</span> = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></span><br><span class="line"><span class="attr">DUPEFILTER_CLASS</span> = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="Redis-连接配置"><a href="#Redis-连接配置" class="headerlink" title="Redis 连接配置"></a>Redis 连接配置</h4>
                  <p>接下来配置 Redis 的连接信息，这里有两种配置方式。 第一种方式是通过连接字符串配置。我们可以用 Redis 的地址、端口、密码来构造一个 Redis 连接字符串，支持的连接形式如下所示：</p>
                  <figure class="highlight dts">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">redis:</span><span class="comment">//[:password]@host:port/db</span></span><br><span class="line"><span class="symbol">rediss:</span><span class="comment">//[:password]@host:port/db</span></span><br><span class="line"><span class="symbol">unix:</span><span class="comment">//[:password]@/path/to/socket.sock?db=db</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>password 是密码，比如要以冒号开头，中括号代表此选项可有可无，host 是 Redis 的地址，port 是运行端口，db 是数据库代号，其值默认是 0。 根据上文中提到我的 Redis 连接信息，构造这个 Redis 的连接字符串如下所示：</p>
                  <figure class="highlight avrasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">redis:</span>//:foobared<span class="subst">@120</span><span class="number">.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6379</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>直接在 settings.py 里面配置为 REDIS_URL 变量即可：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">REDIS_URL</span> = <span class="string">'redis://:foobared@120.27.34.25:6379'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第二种配置方式是分项单独配置。这个配置就更加直观明了，如根据我的 Redis 连接信息，可以在 settings.py 中配置如下代码：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">REDIS_HOST</span> = <span class="string">'120.27.34.25'</span></span><br><span class="line"><span class="attr">REDIS_PORT</span> = <span class="number">6379</span></span><br><span class="line"><span class="attr">REDIS_PASSWORD</span> = <span class="string">'foobared'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这段代码分开配置了 Redis 的地址、端口和密码。 注意，如果配置了 REDIS_URL，那么 Scrapy-Redis 将优先使用 REDIS_URL 连接，会覆盖上面的三项配置。如果想要分项单独配置的话，请不要配置 REDIS_URL。 在本项目中，我选择的是配置 REDIS_URL。</p>
                  <h4 id="配置调度队列"><a href="#配置调度队列" class="headerlink" title="配置调度队列"></a>配置调度队列</h4>
                  <p>此项配置是可选的，默认使用 PriorityQueue。如果想要更改配置，可以配置 SCHEDULER_QUEUE_CLASS 变量，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER_QUEUE_CLASS</span> = <span class="string">'scrapy_redis.queue.PriorityQueue'</span></span><br><span class="line"><span class="attr">SCHEDULER_QUEUE_CLASS</span> = <span class="string">'scrapy_redis.queue.FifoQueue'</span></span><br><span class="line"><span class="attr">SCHEDULER_QUEUE_CLASS</span> = <span class="string">'scrapy_redis.queue.LifoQueue'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上三行任选其一配置，即可切换爬取队列的存储方式。 在本项目中不进行任何配置，我们使用默认配置。</p>
                  <h4 id="配置持久化"><a href="#配置持久化" class="headerlink" title="配置持久化"></a>配置持久化</h4>
                  <p>此配置是可选的，默认是 False。Scrapy-Redis 默认会在爬取全部完成后清空爬取队列和去重指纹集合。 如果不想自动清空爬取队列和去重指纹集合，可以增加如下配置：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER_PERSIST</span> = <span class="literal">True</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>将 SCHEDULER_PERSIST 设置为 True 之后，爬取队列和去重指纹集合不会在爬取完成后自动清空，如果不配置，默认是 False，即自动清空。 值得注意的是，如果强制中断爬虫的运行，爬取队列和去重指纹集合是不会自动清空的。 在本项目中不进行任何配置，我们使用默认配置。</p>
                  <h4 id="配置重爬"><a href="#配置重爬" class="headerlink" title="配置重爬"></a>配置重爬</h4>
                  <p>此配置是可选的，默认是 False。如果配置了持久化或者强制中断了爬虫，那么爬取队列和指纹集合不会被清空，爬虫重新启动之后就会接着上次爬取。如果想重新爬取，我们可以配置重爬的选项：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER_FLUSH_ON_START</span> = <span class="literal">True</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样将 SCHEDULER_FLUSH_ON_START 设置为 True 之后，爬虫每次启动时，爬取队列和指纹集合都会清空。所以要做分布式爬取，我们必须保证只能清空一次，否则每个爬虫任务在启动时都清空一次，就会把之前的爬取队列清空，势必会影响分布式爬取。 注意，此配置在单机爬取的时候比较方便，分布式爬取不常用此配置。 在本项目中不进行任何配置，我们使用默认配置。</p>
                  <h4 id="Pipeline-配置"><a href="#Pipeline-配置" class="headerlink" title="Pipeline 配置"></a>Pipeline 配置</h4>
                  <p>此配置是可选的，默认不启动 Pipeline。Scrapy-Redis 实现了一个存储到 Redis 的 Item Pipeline，启用了这个 Pipeline 的话，爬虫会把生成的 Item 存储到 Redis 数据库中。在数据量比较大的情况下，我们一般不会这么做。因为 Redis 是基于内存的，我们利用的是它处理速度快的特性，用它来做存储未免太浪费了，配置如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ITEM_PIPELINES</span> = &#123;<span class="string">'scrapy_redis.pipelines.RedisPipeline'</span>: <span class="number">300</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>本项目不进行任何配置，即不启动 Pipeline。 到此为止，Scrapy-Redis 的配置就完成了。有的选项我们没有配置，但是这些配置在其他 Scrapy 项目中可能用到，要根据具体情况而定。</p>
                  <h3 id="5-配置存储目标"><a href="#5-配置存储目标" class="headerlink" title="5. 配置存储目标"></a>5. 配置存储目标</h3>
                  <p>之前 Scrapy 新浪微博爬虫项目使用的存储是 MongoDB，而且 MongoDB 是本地运行的，即连接的是 localhost。但是，当爬虫程序分发到各台主机运行的时候，爬虫就会连接各自的的 MongoDB。所以我们需要在各台主机上都安装 MongoDB，这样有两个缺点：一是搭建 MongoDB 环境比较烦琐；二是这样各台主机的爬虫会把爬取结果分散存到各自主机上，不方便统一管理。 所以我们最好将存储目标存到同一个地方，例如都存到同一个 MongoDB 数据库中。我们可以在服务器上搭建一个 MongoDB 服务，或者直接购买 MongoDB 数据存储服务。 这里使用的就是服务器上搭建的的 MongoDB 服务，IP 仍然为 120.27.34.25，用户名为 admin，密码为 admin123。 修改配置 MONGO_URI 为如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'mongodb://admin:admin123@120.27.34.25:27017'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>到此为止，我们就成功完成了 Scrapy 分布式爬虫的配置了。</p>
                  <h3 id="6-运行"><a href="#6-运行" class="headerlink" title="6. 运行"></a>6. 运行</h3>
                  <p>接下来将代码部署到各台主机上，记得每台主机都需要配好对应的 Python 环境。 每台主机上都执行如下命令，即可启动爬取：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl weibocn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>每台主机启动了此命令之后，就会从配置的 Redis 数据库中调度 Request，做到爬取队列共享和指纹集合共享。同时每台主机占用各自的带宽和处理器，不会互相影响，爬取效率成倍提高。</p>
                  <h3 id="7-结果"><a href="#7-结果" class="headerlink" title="7. 结果"></a>7. 结果</h3>
                  <p>一段时间后，我们可以用 RedisDesktop 观察远程 Redis 数据库的信息。这里会出现两个 Key：一个叫作 weibocn:dupefilter，用来储存指纹；另一个叫作 weibocn:requests，即爬取队列，如图 14-5 和图 14-6 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-29-113521.jpg" alt=""> 图 14-5 去重指纹 <img src="/Users/maqian/Downloads/python3webspiderupload/assets/14-6.jpg" alt=""> 图 14-6 爬取队列 随着时间的推移，指纹集合会不断增长，爬取队列会动态变化，爬取的数据也会被储存到 MongoDB 数据库中。 至此 Scrapy 分布式的配置已全部完成。</p>
                  <h3 id="8-本节代码"><a href="#8-本节代码" class="headerlink" title="8. 本节代码"></a>8. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/Weibo/tree/distributed" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weibo/tree/distributed</a>，注意这里是 distributed 分支。</p>
                  <h3 id="9-结语"><a href="#9-结语" class="headerlink" title="9. 结语"></a>9. 结语</h3>
                  <p>本节通过对接 Scrapy-Redis 成功实现了分布式爬虫，但是部署还是有很多不方便的地方。另外，如果爬取量特别大的话，Redis 的内存也是个问题。在后文我们会继续了解相关优化方案。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-09 10:14:40" itemprop="dateCreated datePublished" datetime="2019-12-09T10:14:40+08:00">2019-12-09</time>
                </span>
                <span id="/8468.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.3–Scrapy 分布式实现" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8465.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8465.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.2–Scrapy-Redis 源码解析</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-2-Scrapy-Redis-源码解析"><a href="#14-2-Scrapy-Redis-源码解析" class="headerlink" title="14.2 Scrapy-Redis 源码解析"></a>14.2 Scrapy-Redis 源码解析</h1>
                  <p>Scrapy-Redis 库已经为我们提供了 Scrapy 分布式的队列、调度器、去重等功能，其 GitHub 地址为：<a href="https://github.com/rmax/scrapy-redis" target="_blank" rel="noopener">https://github.com/rmax/scrapy-redis</a>。 本节我们深入了解一下，利用 Redis 如何实现 Scrapy 分布式。</p>
                  <h3 id="1-获取源码"><a href="#1-获取源码" class="headerlink" title="1. 获取源码"></a>1. 获取源码</h3>
                  <p>可以把源码克隆下来，执行如下命令：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/rmax/scrapy-redis.git</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>核心源码在 scrapy-redis/src/scrapy_redis 目录下。</p>
                  <h3 id="2-爬取队列"><a href="#2-爬取队列" class="headerlink" title="2. 爬取队列"></a>2. 爬取队列</h3>
                  <p>从爬取队列入手，看看它的具体实现。源码文件为 queue.py，它有三个队列的实现，首先它实现了一个父类 Base，提供一些基本方法和属性，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider base queue class"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, spider, key, serializer=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> serializer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            serializer = picklecompat</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(serializer, <span class="string">'loads'</span>):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"serializer does not implement 'loads' function: % r"</span></span><br><span class="line">                            % serializer)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(serializer, <span class="string">'dumps'</span>):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"serializer '% s' does not implement 'dumps' function: % r"</span></span><br><span class="line">                            % serializer)</span><br><span class="line">        self.server = server</span><br><span class="line">        self.spider = spider</span><br><span class="line">        self.key = key % &#123;<span class="string">'spider'</span>: spider.name&#125;</span><br><span class="line">        self.serializer = serializer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_encode_request</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        obj = request_to_dict(request, self.spider)</span><br><span class="line">        <span class="keyword">return</span> self.serializer.dumps(obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_decode_request</span><span class="params">(self, encoded_request)</span>:</span></span><br><span class="line">        obj = self.serializer.loads(encoded_request)</span><br><span class="line">        <span class="keyword">return</span> request_from_dict(obj, self.spider)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Clear queue/stack"""</span></span><br><span class="line">        self.server.delete(self.key)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先看一下 <em>encode<em>request() 和 _decode_request() 方法，因为我们需要把一 个 Request 对象存储到数据库中，但数据库无法直接存储对象，所以需要将 Request 序列化转成字符串再存储，而这两个方法就分别是序列化和反序列化的操作，利用 pickle 库来实现，一般在调用 push() 将 Request 存入数据库时会调用 _encode_request() 方法进行序列化，在调用 pop() 取出 Request 的时候会调用 _decode_request() 进行反序列化。 在父类中 __len</em></em>()、push() 和 pop() 方法都是未实现的，会直接抛出 NotImplementedError，因此这个类是不能直接被使用的，所以必须要实现一个子类来重写这三个方法，而不同的子类就会有不同的实现，也就有着不同的功能。 那么接下来就需要定义一些子类来继承 Base 类，并重写这几个方法，那在源码中就有三个子类的实现，它们分别是 FifoQueue、PriorityQueue、LifoQueue，我们分别来看下它们的实现原理。 首先是 FifoQueue：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FifoQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider FIFO queue"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">if</span> timeout &gt; <span class="number">0</span>:</span><br><span class="line">            data = self.server.brpop(self.key, timeout)</span><br><span class="line">            <span class="keyword">if</span> isinstance(data, tuple):</span><br><span class="line">                data = data[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = self.server.rpop(self.key)</span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到这个类继承了 Base 类，并重写了 <strong>len</strong>()、push()、pop() 这三个方法，在这三个方法中都是对 server 对象的操作，而 server 对象就是一个 Redis 连接对象，我们可以直接调用其操作 Redis 的方法对数据库进行操作，可以看到这里的操作方法有 llen()、lpush()、rpop() 等，那这就代表此爬取队列是使用的 Redis 的列表，序列化后的 Request 会被存入列表中，就是列表的其中一个元素，<strong>len</strong>() 方法是获取列表的长度，push() 方法中调用了 lpush() 操作，这代表从列表左侧存入数据，pop() 方法中调用了 rpop() 操作，这代表从列表右侧取出数据。 所以 Request 在列表中的存取顺序是左侧进、右侧出，所以这是有序的进出，即先进先出，英文叫做 First Input First Output，也被简称作 Fifo，而此类的名称就叫做 FifoQueue。 另外还有一个与之相反的实现类，叫做 LifoQueue，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LifoQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider LIFO queue."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the stack"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">if</span> timeout &gt; <span class="number">0</span>:</span><br><span class="line">            data = self.server.blpop(self.key, timeout)</span><br><span class="line">            <span class="keyword">if</span> isinstance(data, tuple):</span><br><span class="line">                data = data[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = self.server.lpop(self.key)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>与 FifoQueue 不同的就是它的 pop() 方法，在这里使用的是 lpop() 操作，也就是从左侧出，而 push() 方法依然是使用的 lpush() 操作，是从左侧入。那么这样达到的效果就是先进后出、后进先出，英文叫做 Last In First Out，简称为 Lifo，而此类名称就叫做 LifoQueue。同时这个存取方式类似栈的操作，所以其实也可以称作 StackQueue。 另外在源码中还有一个子类实现，叫做 PriorityQueue，顾名思义，它叫做优先级队列，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PriorityQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider priority queue abstraction using redis' sorted set"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.zcard(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        data = self._encode_request(request)</span><br><span class="line">        score = -request.priority</span><br><span class="line">        self.server.execute_command(<span class="string">'ZADD'</span>, self.key, score, data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Pop a request</span></span><br><span class="line"><span class="string">        timeout not support in this queue class</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pipe = self.server.pipeline()</span><br><span class="line">        pipe.multi()</span><br><span class="line">        pipe.zrange(self.key, <span class="number">0</span>, <span class="number">0</span>).zremrangebyrank(self.key, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        results, count = pipe.execute()</span><br><span class="line">        <span class="keyword">if</span> results:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(results[<span class="number">0</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们可以看到 <strong>len</strong>()、push()、pop() 方法中使用了 server 对象的 zcard()、zadd()、zrange() 操作，可以知道这里使用的存储结果是有序集合 Sorted Set，在这个集合中每个元素都可以设置一个分数，那么这个分数就代表优先级。 在 <strong>len</strong>() 方法里调用了 zcard() 操作，返回的就是有序集合的大小，也就是爬取队列的长度，在 push() 方法中调用了 zadd() 操作，就是向集合中添加元素，这里的分数指定成 Request 的优先级的相反数，因为分数低的会排在集合的前面，所以这里高优先级的 Request 就会存在集合的最前面。pop() 方法是首先调用了 zrange() 操作取出了集合的第一个元素，因为最高优先级的 Request 会存在集合最前面，所以第一个元素就是最高优先级的 Request，然后再调用 zremrangebyrank() 操作将这个元素删除，这样就完成了取出并删除的操作。 此队列是默认使用的队列，也就是爬取队列默认是使用有序集合来存储的。</p>
                  <h3 id="3-去重过滤"><a href="#3-去重过滤" class="headerlink" title="3. 去重过滤"></a>3. 去重过滤</h3>
                  <p>前面说过 Scrapy 的去重是利用集合来实现的，而在 Scrapy 分布式中的去重就需要利用共享的集合，那么这里使用的就是 Redis 中的集合数据结构。我们来看看去重类是怎样实现的，源码文件是 dupefilter.py，其内实现了一个 RFPDupeFilter 类，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RFPDupeFilter</span><span class="params">(BaseDupeFilter)</span>:</span></span><br><span class="line">    <span class="string">"""Redis-based request duplicates filter.</span></span><br><span class="line"><span class="string">    This class can also be used with default Scrapy's scheduler.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger = logger</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, key, debug=False)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the duplicates filter.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        server : redis.StrictRedis</span></span><br><span class="line"><span class="string">            The redis server instance.</span></span><br><span class="line"><span class="string">        key : str</span></span><br><span class="line"><span class="string">            Redis key Where to store fingerprints.</span></span><br><span class="line"><span class="string">        debug : bool, optional</span></span><br><span class="line"><span class="string">            Whether to log filtered requests.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br><span class="line">        self.debug = debug</span><br><span class="line">        self.logdupes = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        <span class="string">"""Returns an instance from given settings.</span></span><br><span class="line"><span class="string">        This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the</span></span><br><span class="line"><span class="string">        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as</span></span><br><span class="line"><span class="string">        it needs to pass the spider name in the key.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        settings : scrapy.settings.Settings</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        RFPDupeFilter</span></span><br><span class="line"><span class="string">            A RFPDupeFilter instance.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        server = get_redis_from_settings(settings)</span><br><span class="line">        key = defaults.DUPEFILTER_KEY % &#123;<span class="string">'timestamp'</span>: int(time.time())&#125;</span><br><span class="line">        debug = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(server, key=key, debug=debug)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="string">"""Returns instance from crawler.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        crawler : scrapy.crawler.Crawler</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        RFPDupeFilter</span></span><br><span class="line"><span class="string">            Instance of RFPDupeFilter.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Returns True if request was already seen.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        fp = self.request_fingerprint(request)</span><br><span class="line">        added = self.server.sadd(self.key, fp)</span><br><span class="line">        <span class="keyword">return</span> added == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_fingerprint</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a fingerprint for a given request.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> request_fingerprint(request)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Delete data on close. Called by Scrapy's scheduler.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        reason : str, optional</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.clear()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Clears fingerprints data."""</span></span><br><span class="line">        self.server.delete(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""Logs given request.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string">        spider : scrapy.spiders.Spider</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.debug:</span><br><span class="line">            msg = <span class="string">"Filtered duplicate request: %(request) s"</span></span><br><span class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</span><br><span class="line">        <span class="keyword">elif</span> self.logdupes:</span><br><span class="line">            msg = (<span class="string">"Filtered duplicate request %(request) s"</span></span><br><span class="line">                   <span class="string">"- no more duplicates will be shown"</span></span><br><span class="line">                   <span class="string">"(see DUPEFILTER_DEBUG to show all duplicates)"</span>)</span><br><span class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</span><br><span class="line">            self.logdupes = <span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里同样实现了一个 request_seen() 方法，和 Scrapy 中的 request_seen() 方法实现极其类似。不过这里集合使用的是 server 对象的 sadd() 操作，也就是集合不再是一个简单数据结构了，而是直接换成了数据库的存储方式。 鉴别重复的方式还是使用指纹，指纹同样是依靠 request_fingerprint() 方法来获取的。获取指纹之后就直接向集合添加指纹，如果添加成功，说明这个指纹原本不存在于集合中，返回值 1。代码中最后的返回结果是判定添加结果是否为 0，如果刚才的返回值为 1，那这个判定结果就是 False，也就是不重复，否则判定为重复。 这样我们就成功利用 Redis 的集合完成了指纹的记录和重复的验证。</p>
                  <h3 id="4-调度器"><a href="#4-调度器" class="headerlink" title="4. 调度器"></a>4. 调度器</h3>
                  <p>Scrapy-Redis 还帮我们实现了配合 Queue、DupeFilter 使用的调度器 Scheduler，源文件名称是 scheduler.py。我们可以指定一些配置，如 SCHEDULER_FLUSH_ON_START 即是否在爬取开始的时候清空爬取队列，SCHEDULER_PERSIST 即是否在爬取结束后保持爬取队列不清除。我们可以在 settings.py 里自由配置，而此调度器很好地实现了对接。 接下来我们看看两个核心的存取方法，实现如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enqueue_request</span><span class="params">(<span class="keyword">self</span>, request)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> request.dont_filter <span class="keyword">and</span> <span class="keyword">self</span>.df.request_seen(request)<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.df.log(request, <span class="keyword">self</span>.spider)</span><br><span class="line">        <span class="keyword">return</span> False</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">stats:</span></span><br><span class="line">        <span class="keyword">self</span>.stats.inc_value(<span class="string">'scheduler/enqueued/redis'</span>, spider=<span class="keyword">self</span>.spider)</span><br><span class="line">    <span class="keyword">self</span>.queue.push(request)</span><br><span class="line">    <span class="keyword">return</span> True</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_request</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    block_pop_timeout = <span class="keyword">self</span>.idle_before_close</span><br><span class="line">    request = <span class="keyword">self</span>.queue.pop(block_pop_timeout)</span><br><span class="line">    <span class="keyword">if</span> request <span class="keyword">and</span> <span class="keyword">self</span>.<span class="symbol">stats:</span></span><br><span class="line">        <span class="keyword">self</span>.stats.inc_value(<span class="string">'scheduler/dequeued/redis'</span>, spider=<span class="keyword">self</span>.spider)</span><br><span class="line">    <span class="keyword">return</span> request</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>enqueue_request() 可以向队列中添加 Request，核心操作就是调用 Queue 的 push() 操作，还有一些统计和日志操作。next_request() 就是从队列中取 Request，核心操作就是调用 Queue 的 pop() 操作，此时如果队列中还有 Request，则 Request 会直接取出来，爬取继续，否则如果队列为空，爬取则会重新开始。</p>
                  <h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3>
                  <p>那么到现在为止我们就把之前所说的三个分布式的问题解决了，总结如下：</p>
                  <ul>
                    <li>爬取队列的实现，在这里提供了三种队列，使用了 Redis 的列表或有序集合来维护。</li>
                    <li>去重的实现，使用了 Redis 的集合来保存 Request 的指纹来提供重复过滤。</li>
                    <li>中断后重新爬取的实现，中断后 Redis 的队列没有清空，再次启动时调度器的 next_request() 会从队列中取到下一个 Request，继续爬取。</li>
                  </ul>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>以上内容便是 Scrapy-Redis 的核心源码解析。Scrapy-Redis 中还提供了 Spider、Item Pipeline 的实现，不过它们并不是必须使用。 在下一节，我们会将 Scrapy-Redis 集成到之前所实现的 Scrapy 新浪微博项目中，实现多台主机协同爬取。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-09 10:08:42" itemprop="dateCreated datePublished" datetime="2019-12-09T10:08:42+08:00">2019-12-09</time>
                </span>
                <span id="/8465.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.2–Scrapy-Redis 源码解析" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>8.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>8 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8456.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8456.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.1–分布式爬虫原理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-1-分布式爬虫原理"><a href="#14-1-分布式爬虫原理" class="headerlink" title="14.1 分布式爬虫原理"></a>14.1 分布式爬虫原理</h1>
                  <p>我们在前面已经实现了 Scrapy 微博爬虫，虽然爬虫是异步加多线程的，但是我们只能在一台主机上运行，所以爬取效率还是有限的，分布式爬虫则是将多台主机组合起来，共同完成一个爬取任务，这将大大提高爬取的效率。</p>
                  <h3 id="1-分布式爬虫架构"><a href="#1-分布式爬虫架构" class="headerlink" title="1. 分布式爬虫架构"></a>1. 分布式爬虫架构</h3>
                  <p>在了解分布式爬虫架构之前，首先回顾一下 Scrapy 的架构，如图 13-1 所示。 Scrapy 单机爬虫中有一个本地爬取队列 Queue，这个队列是利用 deque 模块实现的。如果新的 Request 生成就会放到队列里面，随后 Request 被 Scheduler 调度。之后，Request 交给 Downloader 执行爬取，简单的调度架构如图 14-1 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-29-113355.jpg" alt=""> 图 14-1 调度架构 如果两个 Scheduler 同时从队列里面取 Request，每个 Scheduler 都有其对应的 Downloader，那么在带宽足够、正常爬取且不考虑队列存取压力的情况下，爬取效率会有什么变化？没错，爬取效率会翻倍。 这样，Scheduler 可以扩展多个，Downloader 也可以扩展多个。而爬取队列 Queue 必须始终为一个，也就是所谓的共享爬取队列。这样才能保证 Scheduer 从队列里调度某个 Request 之后，其他 Scheduler 不会重复调度此 Request，就可以做到多个 Schduler 同步爬取。这就是分布式爬虫的基本雏形，简单调度架构如图 14-2 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-29-113406.jpg" alt=""> 图 14-2 调度架构 我们需要做的就是在多台主机上同时运行爬虫任务协同爬取，而协同爬取的前提就是共享爬取队列。这样各台主机就不需要各自维护爬取队列，而是从共享爬取队列存取 Request。但是各台主机还是有各自的 Scheduler 和 Downloader，所以调度和下载功能分别完成。如果不考虑队列存取性能消耗，爬取效率还是会成倍提高。</p>
                  <h3 id="2-维护爬取队列"><a href="#2-维护爬取队列" class="headerlink" title="2. 维护爬取队列"></a>2. 维护爬取队列</h3>
                  <p>那么这个队列用什么维护来好呢？我们首先需要考虑的就是性能问题，什么数据库存取效率高？我们自然想到基于内存存储的 Redis，而且 Redis 还支持多种数据结构，例如列表 List、集合 Set、有序集合 Sorted Set 等等，存取的操作也非常简单，所以在这里我们采用 Redis 来维护爬取队列。 这几种数据结构存储实际各有千秋，分析如下：</p>
                  <ul>
                    <li>列表数据结构有 lpush()、lpop()、rpush()、rpop() 方法，所以我们可以用它来实现一个先进先出式爬取队列，也可以实现一个先进后出栈式爬取队列。</li>
                    <li>集合的元素是无序的且不重复的，这样我们可以非常方便地实现一个随机排序的不重复的爬取队列。</li>
                    <li>有序集合带有分数表示，而 Scrapy 的 Request 也有优先级的控制，所以用有集合我们可以实现一个带优先级调度的队列。</li>
                  </ul>
                  <p>这些不同的队列我们需要根据具体爬虫的需求灵活选择。</p>
                  <h3 id="3-怎样来去重"><a href="#3-怎样来去重" class="headerlink" title="3. 怎样来去重"></a>3. 怎样来去重</h3>
                  <p>Scrapy 有自动去重，它的去重使用了 Python 中的集合。这个集合记录了 Scrapy 中每个 Request 的指纹，这个指纹实际上就是 Request 的散列值。我们可以看看 Scrapy 的源代码，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import hashlib</span><br><span class="line">def request<span class="constructor">_fingerprint(<span class="params">request</span>, <span class="params">include_headers</span>=None)</span>:</span><br><span class="line">    <span class="keyword">if</span> include_headers:</span><br><span class="line">        include_headers = tuple(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">h</span>.<span class="params">lower</span>()</span>)</span><br><span class="line">                                 for h <span class="keyword">in</span> sorted(include_headers))</span><br><span class="line">    cache = <span class="module-access"><span class="module"><span class="identifier">_fingerprint_cache</span>.</span></span>setdefault(request, &#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> include_headers not <span class="keyword">in</span> cache:</span><br><span class="line">        fp = hashlib.sha1<span class="literal">()</span></span><br><span class="line">        fp.update(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">request</span>.<span class="params">method</span>)</span>)</span><br><span class="line">        fp.update(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">canonicalize_url</span>(<span class="params">request</span>.<span class="params">url</span>)</span>))</span><br><span class="line">        fp.update(request.body <span class="keyword">or</span> b'')</span><br><span class="line">        <span class="keyword">if</span> include_headers:</span><br><span class="line">            for hdr <span class="keyword">in</span> include_headers:</span><br><span class="line">                <span class="keyword">if</span> hdr <span class="keyword">in</span> request.headers:</span><br><span class="line">                    fp.update(hdr)</span><br><span class="line">                    for v <span class="keyword">in</span> request.headers.getlist(hdr):</span><br><span class="line">                        fp.update(v)</span><br><span class="line">        cache<span class="literal">[<span class="identifier">include_headers</span>]</span> = fp.hexdigest<span class="literal">()</span></span><br><span class="line">    return cache<span class="literal">[<span class="identifier">include_headers</span>]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>request_fingerprint() 就是计算 Request 指纹的方法，其方法内部使用的是 hashlib 的 sha1() 方法。计算的字段包括 Request 的 Method、URL、Body、Headers 这几部分内容，这里只要有一点不同，那么计算的结果就不同。计算得到的结果是加密后的字符串，也就是指纹。每个 Request 都有独有的指纹，指纹就是一个字符串，判定字符串是否重复比判定 Request 对象是否重复容易得多，所以指纹可以作为判定 Request 是否重复的依据。 那么我们如何判定重复呢？Scrapy 是这样实现的，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.fingerprints = set()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(<span class="keyword">self</span>, request)</span></span><span class="symbol">:</span></span><br><span class="line">    fp = <span class="keyword">self</span>.request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> fp <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">fingerprints:</span></span><br><span class="line">        <span class="keyword">return</span> True</span><br><span class="line">    <span class="keyword">self</span>.fingerprints.add(fp)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在去重的类 RFPDupeFilter 中，有一个 request_seen() 方法，这个方法有一个参数 request，它的作用就是检测该 Request 对象是否重复。这个方法调用 request_fingerprint() 获取该 Request 的指纹，检测这个指纹是否存在于 fingerprints 变量中，而 fingerprints 是一个集合，集合的元素都是不重复的。如果指纹存在，那么就返回 True，说明该 Request 是重复的，否则这个指纹加入到集合中。如果下次还有相同的 Request 传递过来，指纹也是相同的，那么这时指纹就已经存在于集合中，Request 对象就会直接判定为重复。这样去重的目的就实现了。 Scrapy 的去重过程就是，利用集合元素的不重复特性来实现 Request 的去重。 对于分布式爬虫来说，我们肯定不能再用每个爬虫各自的集合来去重了。因为这样还是每个主机单独维护自己的集合，不能做到共享。多台主机如果生成了相同的 Request，只能各自去重，各个主机之间就无法做到去重了。 那么要实现去重，这个指纹集合也需要是共享的，Redis 正好有集合的存储数据结构，我们可以利用 Redis 的集合作为指纹集合，那么这样去重集合也是利用 Redis 共享的。每台主机新生成 Request 之后，把该 Request 的指纹与集合比对，如果指纹已经存在，说明该 Request 是重复的，否则将 Request 的指纹加入到这个集合中即可。利用同样的原理不同的存储结构我们也实现了分布式 Reqeust 的去重。</p>
                  <h3 id="4-防止中断"><a href="#4-防止中断" class="headerlink" title="4. 防止中断"></a>4. 防止中断</h3>
                  <p>在 Scrapy 中，爬虫运行时的 Request 队列放在内存中。爬虫运行中断后，这个队列的空间就被释放，此队列就被销毁了。所以一旦爬虫运行中断，爬虫再次运行就相当于全新的爬取过程。 要做到中断后继续爬取，我们可以将队列中的 Request 保存起来，下次爬取直接读取保存数据即可获取上次爬取的队列。我们在 Scrapy 中指定一个爬取队列的存储路径即可，这个路径使用 JOB_DIR 变量来标识，我们可以用如下命令来实现：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl spider -s <span class="attribute">JOBDIR</span>=crawls/spider</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更加详细的使用方法可以参见官方文档，链接为：<a href="https://doc.scrapy.org/en/latest/topics/jobs.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/jobs.html</a>。 在 Scrapy 中，我们实际是把爬取队列保存到本地，第二次爬取直接读取并恢复队列即可。那么在分布式架构中我们还用担心这个问题吗？不需要。因为爬取队列本身就是用数据库保存的，如果爬虫中断了，数据库中的 Request 依然是存在的，下次启动就会接着上次中断的地方继续爬取。 所以，当 Redis 的队列为空时，爬虫会重新爬取；当 Redis 的队列不为空时，爬虫便会接着上次中断之处继续爬取。</p>
                  <h3 id="5-架构实现"><a href="#5-架构实现" class="headerlink" title="5. 架构实现"></a>5. 架构实现</h3>
                  <p>我们接下来就需要在程序中实现这个架构了。首先实现一个共享的爬取队列，还要实现去重的功能。另外，重写一个 Scheduer 的实现，使之可以从共享的爬取队列存取 Request。 幸运的是，已经有人实现了这些逻辑和架构，并发布成叫 Scrapy-Redis 的 Python 包。接下来，我们看看 Scrapy-Redis 的源码实现，以及它的详细工作原理。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-08 09:52:07" itemprop="dateCreated datePublished" datetime="2019-12-08T09:52:07+08:00">2019-12-08</time>
                </span>
                <span id="/8456.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.1–分布式爬虫原理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8453.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8453.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.13–Scrapy 爬取新浪微博</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-13-Scrapy-爬取新浪微博"><a href="#13-13-Scrapy-爬取新浪微博" class="headerlink" title="13.13 Scrapy 爬取新浪微博"></a>13.13 Scrapy 爬取新浪微博</h1>
                  <p>前面讲解了 Scrapy 中各个模块基本使用方法以及代理池、Cookies 池。接下来我们以一个反爬比较强的网站新浪微博为例，来实现一下 Scrapy 的大规模爬取。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本次爬取的目标是新浪微博用户的公开基本信息，如用户昵称、头像、用户的关注、粉丝列表以及发布的微博等，这些信息抓取之后保存至 MongoDB。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保前文所讲的代理池、Cookies 池已经实现并可以正常运行，安装 Scrapy、PyMongo 库，如没有安装可以参考前文内容。</p>
                  <h3 id="3-爬取思路"><a href="#3-爬取思路" class="headerlink" title="3. 爬取思路"></a>3. 爬取思路</h3>
                  <p>首先我们要实现用户的大规模爬取。这里采用的爬取方式是，以微博的几个大 V 为起始点，爬取他们各自的粉丝和关注列表，然后获取粉丝和关注列表的粉丝和关注列表，以此类推，这样下去就可以实现递归爬取。如果一个用户与其他用户有社交网络上的关联，那他们的信息就会被爬虫抓取到，这样我们就可以做到对所有用户的爬取。通过这种方式，我们可以得到用户的唯一 ID，再根据 ID 获取每个用户发布的微博即可。</p>
                  <h3 id="4-爬取分析"><a href="#4-爬取分析" class="headerlink" title="4. 爬取分析"></a>4. 爬取分析</h3>
                  <p>这里我们选取的爬取站点是：<a href="https://m.weibo.cn，此站点是微博移动端的站点。打开该站点会跳转到登录页面，这是因为主页做了登录限制。不过我们可以直接打开某个用户详情页面，如图" target="_blank" rel="noopener">https://m.weibo.cn，此站点是微博移动端的站点。打开该站点会跳转到登录页面，这是因为主页做了登录限制。不过我们可以直接打开某个用户详情页面，如图</a> 13-32 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034836.jpg" alt=""> 图 13-32 个人详情页面 我们在页面最上方可以看到她的关注和粉丝数量。我们点击关注，进入到她的关注列表，如图 13-33 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034839.jpg" alt=""> 图 13-33 关注列表 我们打开开发者工具，切换到 XHR 过滤器，一直下拉关注列表，即可看到下方会出现很多 Ajax 请求，这些请求就是获取关注列表的 Ajax 请求，如图 13-34 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034852.png" alt=""> 图 13-34 请求列表 我们打开第一个 Ajax 请求看一下，发现它的链接为：<a href="https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&amp;luicode=10000011&amp;lfid=1005051916655407&amp;featurecode=20000320&amp;type=uid&amp;value=1916655407&amp;page=2" target="_blank" rel="noopener">https://m.weibo.cn/api/container/getIndex?containerid=231051<em>-_followers</em>-_1916655407&amp;luicode=10000011&amp;lfid=1005051916655407&amp;featurecode=20000320&amp;type=uid&amp;value=1916655407&amp;page=2</a>，详情如图 13-35 和 13-36 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034901.jpg" alt=""> 图 13-35 请求详情 <img src="https://cdn.cuiqingcai.com/2019-11-27-034906.jpg" alt=""> 图 13-36 响应结果 请求类型是 GET 类型，返回结果是 JSON 格式，我们将其展开之后即可看到其关注的用户的基本信息。接下来我们只需要构造这个请求的参数。此链接一共有 7 个参数，如图 13-37 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034931.jpg" alt=""> 图 13-37 参数信息 其中最主要的参数就是 containerid 和 page。有了这两个参数，我们同样可以获取请求结果。我们可以将接口精简为：<a href="https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&amp;page=2" target="_blank" rel="noopener">https://m.weibo.cn/api/container/getIndex?containerid=231051<em>-_followers</em>-_1916655407&amp;page=2</a>，这里的 containerid 的前半部分是固定的，后半部分是用户的 id。所以这里参数就可以构造出来了，只需要修改 containerid 最后的 id 和 page 参数即可获取分页形式的关注列表信息。 利用同样的方法，我们也可以分析用户详情的 Ajax 链接、用户微博列表的 Ajax 链接，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 用户详情 API</span></span><br><span class="line"><span class="attr">user_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;value=&#123;uid&#125;&amp;containerid=100505&#123;uid&#125;'</span></span><br><span class="line"><span class="comment"># 关注列表 API</span></span><br><span class="line"><span class="attr">follow_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line"><span class="comment"># 粉丝列表 API</span></span><br><span class="line"><span class="attr">fan_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_fans_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line"><span class="comment"># 微博列表 API</span></span><br><span class="line"><span class="attr">weibo_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;page=&#123;page&#125;&amp;containerid=107603&#123;uid&#125;'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此处的 uid 和 page 分别代表用户 ID 和分页页码。 注意，这个 API 可能随着时间的变化或者微博的改版而变化，以实测为准。 我们从几个大 V 开始抓取，抓取他们的粉丝、关注列表、微博信息，然后递归抓取他们的粉丝和关注列表的粉丝、关注列表、微博信息，递归抓取，最后保存微博用户的基本信息、关注和粉丝列表、发布的微博。 我们选择 MongoDB 作为存储的数据库，可以更方便地存储用户的粉丝和关注列表。</p>
                  <h3 id="5-新建项目"><a href="#5-新建项目" class="headerlink" title="5. 新建项目"></a>5. 新建项目</h3>
                  <p>接下来，我们用 Scrapy 来实现这个抓取过程。首先创建一个项目，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>进入项目中，新建一个 Spider，名为 weibocn，命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">weibocn</span> <span class="selector-tag">m</span><span class="selector-class">.weibo</span><span class="selector-class">.cn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们首先修改 Spider，配置各个 Ajax 的 URL，选取几个大 V，将他们的 ID 赋值成一个列表，实现 start_requests() 方法，也就是依次抓取各个大 V 的个人详情，然后用 parse_user() 进行解析，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy import Request, Spider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboSpider</span>(<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'weibocn'</span></span><br><span class="line">    allowed_domains = [<span class="string">'m.weibo.cn'</span>]</span><br><span class="line">    user_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;value=&#123;uid&#125;&amp;containerid=100505&#123;uid&#125;'</span></span><br><span class="line">    follow_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line">    fan_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_fans_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line">    weibo_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;page=&#123;page&#125;&amp;containerid=107603&#123;uid&#125;'</span></span><br><span class="line">    start_users = [<span class="string">'3217179555'</span>, <span class="string">'1742566624'</span>, <span class="string">'2282991915'</span>, <span class="string">'1288739185'</span>, <span class="string">'3952070245'</span>, <span class="string">'5878659096'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> uid <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">start_users:</span></span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="keyword">self</span>.user_url.format(uid=uid), callback=<span class="keyword">self</span>.parse_user)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_user</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.logger.debug(response)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="6-创建-Item"><a href="#6-创建-Item" class="headerlink" title="6. 创建 Item"></a>6. 创建 Item</h3>
                  <p>接下来，我们解析用户的基本信息并生成 Item。这里我们先定义几个 Item，如用户、用户关系、微博的 Item，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="symbol">UserItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">users</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">name</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">avatar</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">cover</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">gender</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">description</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">fans_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">follows_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">weibos_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">verified</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">verified_reason</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">verified_type</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">follows</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">fans</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">crawled_at</span> = <span class="symbol">Field</span>()</span><br><span class="line"></span><br><span class="line"><span class="symbol">class</span> <span class="symbol">UserRelationItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">users</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">follows</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">fans</span> = <span class="symbol">Field</span>()</span><br><span class="line"></span><br><span class="line"><span class="symbol">class</span> <span class="symbol">WeiboItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">weibos</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">attitudes_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">comments_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">reposts_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">picture</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">pictures</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">source</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">text</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">raw_text</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">thumbnail</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">user</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">created_at</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">crawled_at</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了 collection 字段，指明保存的 Collection 的名称。用户的关注和粉丝列表直接定义为一个单独的 UserRelationItem，其中 id 就是用户的 ID，follows 就是用户关注列表，fans 是粉丝列表，但这并不意味着我们会将关注和粉丝列表存到一个单独的 Collection 里。后面我们会用 Pipeline 对各个 Item 进行处理、合并存储到用户的 Collection 里，因此 Item 和 Collection 并不一定是完全对应的。</p>
                  <h3 id="7-提取数据"><a href="#7-提取数据" class="headerlink" title="7. 提取数据"></a>7. 提取数据</h3>
                  <p>我们开始解析用户的基本信息，实现 parse_user() 方法，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_user(self, response):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    解析用户信息</span></span><br><span class="line"><span class="string">    :param response: Response 对象</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    <span class="keyword">if</span> result.<span class="builtin-name">get</span>(<span class="string">'userInfo'</span>):</span><br><span class="line">        user_info = result.<span class="builtin-name">get</span>(<span class="string">'userInfo'</span>)</span><br><span class="line">        user_item = UserItem()</span><br><span class="line">        field_map = &#123;</span><br><span class="line">            <span class="string">'id'</span>: <span class="string">'id'</span>, <span class="string">'name'</span>: <span class="string">'screen_name'</span>, <span class="string">'avatar'</span>: <span class="string">'profile_image_url'</span>, <span class="string">'cover'</span>: <span class="string">'cover_image_phone'</span>,</span><br><span class="line">            <span class="string">'gender'</span>: <span class="string">'gender'</span>, <span class="string">'description'</span>: <span class="string">'description'</span>, <span class="string">'fans_count'</span>: <span class="string">'followers_count'</span>,</span><br><span class="line">            <span class="string">'follows_count'</span>: <span class="string">'follow_count'</span>, <span class="string">'weibos_count'</span>: <span class="string">'statuses_count'</span>, <span class="string">'verified'</span>: <span class="string">'verified'</span>,</span><br><span class="line">            <span class="string">'verified_reason'</span>: <span class="string">'verified_reason'</span>, <span class="string">'verified_type'</span>: <span class="string">'verified_type'</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> field, attr <span class="keyword">in</span> field_map.items():</span><br><span class="line">            user_item[field] = user_info.<span class="builtin-name">get</span>(attr)</span><br><span class="line">        yield user_item</span><br><span class="line">        # 关注</span><br><span class="line">        uid = user_info.<span class="builtin-name">get</span>(<span class="string">'id'</span>)</span><br><span class="line">        yield Request(self.follow_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=1), <span class="attribute">callback</span>=self.parse_follows,</span><br><span class="line">                      meta=&#123;<span class="string">'page'</span>: 1, <span class="string">'uid'</span>: uid&#125;)</span><br><span class="line">        # 粉丝</span><br><span class="line">        yield Request(self.fan_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=1), <span class="attribute">callback</span>=self.parse_fans,</span><br><span class="line">                      meta=&#123;<span class="string">'page'</span>: 1, <span class="string">'uid'</span>: uid&#125;)</span><br><span class="line">        # 微博</span><br><span class="line">        yield Request(self.weibo_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=1), <span class="attribute">callback</span>=self.parse_weibos,</span><br><span class="line">                      meta=&#123;<span class="string">'page'</span>: 1, <span class="string">'uid'</span>: uid&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们一共完成了两个操作。</p>
                  <ul>
                    <li>解析 JSON 提取用户信息并生成 UserItem 返回。我们并没有采用常规的逐个赋值的方法，而是定义了一个字段映射关系。我们定义的字段名称可能和 JSON 中用户的字段名称不同，所以在这里定义成一个字典，然后遍历字典的每个字段实现逐个字段的赋值。</li>
                    <li>构造用户的关注、粉丝、微博的第一页的链接，并生成 Request，这里需要的参数只有用户的 ID。另外，初始分页页码直接设置为 1 即可。</li>
                  </ul>
                  <p>接下来，我们还需要保存用户的关注和粉丝列表。以关注列表为例，其解析方法为 parse_follows()，实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_follows(self, response):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    解析用户关注</span></span><br><span class="line"><span class="string">    :param response: Response 对象</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    <span class="keyword">if</span> result.<span class="builtin-name">get</span>(<span class="string">'ok'</span>) <span class="keyword">and</span> result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>) <span class="keyword">and</span> len(result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)) <span class="keyword">and</span> result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)[-1].<span class="builtin-name">get</span>(<span class="string">'card_group'</span>):</span><br><span class="line">        # 解析用户</span><br><span class="line">        follows = result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)[-1].<span class="builtin-name">get</span>(<span class="string">'card_group'</span>)</span><br><span class="line">        <span class="keyword">for</span> follow <span class="keyword">in</span> follows:</span><br><span class="line">            <span class="keyword">if</span> follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>):</span><br><span class="line">                uid = follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>).<span class="builtin-name">get</span>(<span class="string">'id'</span>)</span><br><span class="line">                yield Request(self.user_url.format(<span class="attribute">uid</span>=uid), <span class="attribute">callback</span>=self.parse_user)</span><br><span class="line">        # 关注列表</span><br><span class="line">        uid = response.meta.<span class="builtin-name">get</span>(<span class="string">'uid'</span>)</span><br><span class="line">        user_relation_item = UserRelationItem()</span><br><span class="line">        follows = [&#123;<span class="string">'id'</span>: follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>).<span class="builtin-name">get</span>(<span class="string">'id'</span>), <span class="string">'name'</span>: follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>).<span class="builtin-name">get</span>(<span class="string">'screen_name'</span>)&#125; <span class="keyword">for</span> follow <span class="keyword">in</span></span><br><span class="line">                   follows]</span><br><span class="line">        user_relation_item[<span class="string">'id'</span>] = uid</span><br><span class="line">        user_relation_item[<span class="string">'follows'</span>] = follows</span><br><span class="line">        user_relation_item[<span class="string">'fans'</span>] = []</span><br><span class="line">        yield user_relation_item</span><br><span class="line">        # 下一页关注</span><br><span class="line">       <span class="built_in"> page </span>= response.meta.<span class="builtin-name">get</span>(<span class="string">'page'</span>) + 1</span><br><span class="line">        yield Request(self.follow_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=page),</span><br><span class="line">                      <span class="attribute">callback</span>=self.parse_follows, meta=&#123;<span class="string">'page'</span>: page, <span class="string">'uid'</span>: uid&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>那么在这个方法里面我们做了如下三件事。</p>
                  <ul>
                    <li>解析关注列表中的每个用户信息并发起新的解析请求。我们首先解析关注列表的信息，得到用户的 ID，然后再利用 user_url 构造访问用户详情的 Request，回调就是刚才所定义的 parse_user() 方法。</li>
                    <li>提取用户关注列表内的关键信息并生成 UserRelationItem。id 字段直接设置成用户的 ID，JSON 返回数据中的用户信息有很多冗余字段。在这里我们只提取了关注用户的 ID 和用户名，然后把它们赋值给 follows 字段，fans 字段设置成空列表。这样我们就建立了一个存有用户 ID 和用户部分关注列表的 UserRelationItem，之后合并且保存具有同一个 ID 的 UserRelationItem 的关注和粉丝列表。</li>
                    <li>提取下一页关注。只需要将此请求的分页页码加 1 即可。分页页码通过 Request 的 meta 属性进行传递，Response 的 meta 来接收。这样我们构造并返回下一页的关注列表的 Request。</li>
                  </ul>
                  <p>抓取粉丝列表的原理和抓取关注列表原理相同，在此不再赘述。 接下来我们还差一个方法的实现，即 parse_weibos()，它用来抓取用户的微博信息，实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_weibos(self, response):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    解析微博列表</span></span><br><span class="line"><span class="string">    :param response: Response 对象</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    <span class="keyword">if</span> result.<span class="builtin-name">get</span>(<span class="string">'ok'</span>) <span class="keyword">and</span> result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>):</span><br><span class="line">        weibos = result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)</span><br><span class="line">        <span class="keyword">for</span> weibo <span class="keyword">in</span> weibos:</span><br><span class="line">            mblog = weibo.<span class="builtin-name">get</span>(<span class="string">'mblog'</span>)</span><br><span class="line">            <span class="keyword">if</span> mblog:</span><br><span class="line">                weibo_item = WeiboItem()</span><br><span class="line">                field_map = &#123;</span><br><span class="line">                    <span class="string">'id'</span>: <span class="string">'id'</span>, <span class="string">'attitudes_count'</span>: <span class="string">'attitudes_count'</span>, <span class="string">'comments_count'</span>: <span class="string">'comments_count'</span>, <span class="string">'created_at'</span>: <span class="string">'created_at'</span>,</span><br><span class="line">                    <span class="string">'reposts_count'</span>: <span class="string">'reposts_count'</span>, <span class="string">'picture'</span>: <span class="string">'original_pic'</span>, <span class="string">'pictures'</span>: <span class="string">'pics'</span>,</span><br><span class="line">                    <span class="string">'source'</span>: <span class="string">'source'</span>, <span class="string">'text'</span>: <span class="string">'text'</span>, <span class="string">'raw_text'</span>: <span class="string">'raw_text'</span>, <span class="string">'thumbnail'</span>: <span class="string">'thumbnail_pic'</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">for</span> field, attr <span class="keyword">in</span> field_map.items():</span><br><span class="line">                    weibo_item[field] = mblog.<span class="builtin-name">get</span>(attr)</span><br><span class="line">                weibo_item[<span class="string">'user'</span>] = response.meta.<span class="builtin-name">get</span>(<span class="string">'uid'</span>)</span><br><span class="line">                yield weibo_item</span><br><span class="line">        # 下一页微博</span><br><span class="line">        uid = response.meta.<span class="builtin-name">get</span>(<span class="string">'uid'</span>)</span><br><span class="line">       <span class="built_in"> page </span>= response.meta.<span class="builtin-name">get</span>(<span class="string">'page'</span>) + 1</span><br><span class="line">        yield Request(self.weibo_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=page), <span class="attribute">callback</span>=self.parse_weibos,</span><br><span class="line">                      meta=&#123;<span class="string">'uid'</span>: uid, <span class="string">'page'</span>: page&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 parse_weibos() 方法完成了两件事。</p>
                  <ul>
                    <li>提取用户的微博信息，并生成 WeiboItem。这里同样建立了一个字段映射表，实现批量字段赋值。</li>
                    <li>提取下一页的微博列表。这里同样需要传入用户 ID 和分页页码。</li>
                  </ul>
                  <p>到目前为止，微博的 Spider 已经完成。后面还需要对数据进行数据清洗存储，以及对接代理池、Cookies 池来防止反爬虫。</p>
                  <h3 id="8-数据清洗"><a href="#8-数据清洗" class="headerlink" title="8. 数据清洗"></a>8. 数据清洗</h3>
                  <p>有些微博的时间可能不是标准的时间，比如它可能显示为刚刚、几分钟前、几小时前、昨天等。这里我们需要统一转化这些时间，实现一个 parse_time() 方法，如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_time(self, <span class="type">date</span>):</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">' 刚刚 '</span>, <span class="type">date</span>):</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d % H:% M'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time()))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d + 分钟前 '</span>, <span class="type">date</span>):</span><br><span class="line">        minute = re.match(<span class="string">'(d+)'</span>, <span class="type">date</span>).<span class="keyword">group</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d % H:% M'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time() - <span class="type">float</span>(minute) * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d + 小时前 '</span>, <span class="type">date</span>):</span><br><span class="line">        hour = re.match(<span class="string">'(d+)'</span>, <span class="type">date</span>).<span class="keyword">group</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d % H:% M'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time() - <span class="type">float</span>(hour) * <span class="number">60</span> * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">' 昨天.*'</span>, <span class="type">date</span>):</span><br><span class="line">        <span class="type">date</span> = re.match(<span class="string">' 昨天 (.*)'</span>, <span class="type">date</span>).<span class="keyword">group</span>(<span class="number">1</span>).strip()</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>() - <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>) + <span class="string">' '</span> + <span class="type">date</span></span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d&#123;2&#125;-d&#123;2&#125;'</span>, <span class="type">date</span>):</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>()) + <span class="type">date</span> + <span class="string">' 00:00'</span></span><br><span class="line">    <span class="keyword">return</span> <span class="type">date</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们用正则来提取一些关键数字，用 time 库来实现标准时间的转换。 以 X 分钟前的处理为例，爬取的时间会赋值为 created_at 字段。我们首先用正则匹配这个时间，表达式写作 d + 分钟前，如果提取到的时间符合这个表达式，那么就提取出其中的数字，这样就可以获取分钟数了。接下来使用 time 模块的 strftime() 方法，第一个参数传入要转换的时间格式，第二个参数就是时间戳。这里我们用当前的时间戳减去此分钟数乘以 60 就是当时的时间戳，这样我们就可以得到格式化后的正确时间了。 然后 Pipeline 可以实现如下处理：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class WeiboPipeline():</span><br><span class="line">    def process_item(self, <span class="keyword">item</span>, spider):</span><br><span class="line">        <span class="keyword">if</span> isinstance(<span class="keyword">item</span>, WeiboItem):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">item</span>.<span class="built_in">get</span>(<span class="string">'created_at'</span>):</span><br><span class="line">                <span class="keyword">item</span>[<span class="string">'created_at'</span>] = <span class="keyword">item</span>[<span class="string">'created_at'</span>].strip()</span><br><span class="line">                <span class="keyword">item</span>[<span class="string">'created_at'</span>] = self.parse_time(<span class="keyword">item</span>.<span class="built_in">get</span>(<span class="string">'created_at'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们在 Spider 里没有对 crawled_at 字段赋值，它代表爬取时间，我们可以统一将其赋值为当前时间，实现如下所示：</p>
                  <figure class="highlight haskell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">TimePipeline</span>():</span></span><br><span class="line"><span class="class">    def process_item(<span class="title">self</span>, <span class="title">item</span>, <span class="title">spider</span>):</span></span><br><span class="line"><span class="class">        if isinstance(<span class="title">item</span>, <span class="type">UserItem</span>) or isinstance(<span class="title">item</span>, <span class="type">WeiboItem</span>):</span></span><br><span class="line"><span class="class">            now = time.strftime('% <span class="type">Y</span>-% <span class="title">m</span>-% <span class="title">d</span> % <span class="type">H</span>:% <span class="type">M</span>', <span class="title">time</span>.<span class="title">localtime</span>())</span></span><br><span class="line"><span class="class">            item['crawled_at'] = now</span></span><br><span class="line"><span class="class">        return item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们判断了 item 如果是 UserItem 或 WeiboItem 类型，那么就给它的 crawled_at 字段赋值为当前时间。 通过上面的两个 Pipeline，我们便完成了数据清洗工作，这里主要是时间的转换。</p>
                  <h3 id="9-数据存储"><a href="#9-数据存储" class="headerlink" title="9. 数据存储"></a>9. 数据存储</h3>
                  <p>数据清洗完毕之后，我们就要将数据保存到 MongoDB 数据库。我们在这里实现 MongoPipeline 类，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, mongo_uri, mongo_db)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.mongo_uri = mongo_uri</span><br><span class="line">        <span class="keyword">self</span>.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>), mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client = pymongo.MongoClient(<span class="keyword">self</span>.mongo_uri)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[<span class="keyword">self</span>.mongo_db]</span><br><span class="line">        <span class="keyword">self</span>.db[UserItem.collection].create_index([(<span class="string">'id'</span>, pymongo.ASCENDING)])</span><br><span class="line">        <span class="keyword">self</span>.db[WeiboItem.collection].create_index([(<span class="string">'id'</span>, pymongo.ASCENDING)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item, UserItem) <span class="keyword">or</span> isinstance(item, WeiboItem)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.db[item.collection].update(&#123;<span class="string">'id'</span>: item.get(<span class="string">'id'</span>)&#125;, &#123;<span class="string">'$set'</span>: item&#125;, True)</span><br><span class="line">        <span class="keyword">if</span> isinstance(item, UserRelationItem)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.db[item.collection].update(&#123;<span class="string">'id'</span>: item.get(<span class="string">'id'</span>)&#125;,</span><br><span class="line">                &#123;<span class="string">'$addToSet'</span><span class="symbol">:</span></span><br><span class="line">                    &#123;<span class="string">'follows'</span>: &#123;<span class="string">'$each'</span>: item[<span class="string">'follows'</span>]&#125;,</span><br><span class="line">                        <span class="string">'fans'</span>: &#123;<span class="string">'$each'</span>: item[<span class="string">'fans'</span>]&#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;, True)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当前的 MongoPipeline 和前面我们所写的有所不同，主要有以下几点。</p>
                  <ul>
                    <li>在 open_spider() 方法里面添加了 Collection 的索引，在这里为两个 Item 都做了索引，索引的字段是 id，由于我们这次是大规模爬取，同时在爬取过程中涉及到数据的更新问题，所以我们为每个 Collection 建立了索引，建立了索引之后可以大大提高检索效率。</li>
                    <li>在 process_item() 方法里存储使用的是 update() 方法，第一个参数是查询条件，第二个参数是爬取的 Item，这里我们使用了 $set 操作符，这样我们如果爬取到了重复的数据即可对数据进行更新，同时不会删除已存在的字段，如果这里不加 $set 操作符，那么会直接进行 item 替换，这样可能会导致已存在的字段如关注和粉丝列表清空，所以这里必须要加上 $set 操作符。第三个参数我们设置为了 True，这个参数起到的作用是如果数据不存在，则插入数据。这样我们就可以做到数据存在即更新、数据不存在即插入，这样就达到了去重的效果。</li>
                    <li>对于用户的关注和粉丝列表，我们在这里使用了一个新的操作符，叫做 $addToSet，这个操作符可以向列表类型的字段插入数据同时去重，接下来它的值就是需要操作的字段名称，我们在这里又利用了 $each 操作符对需要插入的列表数据进行了遍历，这样就可以逐条插入用户的关注或粉丝数据到指定的字段了，关于该操作更多的解释可以参考 MongoDB 的官方文档，链接为：<a href="https://docs.mongodb.com/manual/reference/operator/update/addToSet/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/operator/update/addToSet/</a>。</li>
                  </ul>
                  <h3 id="10-Cookies-池对接"><a href="#10-Cookies-池对接" class="headerlink" title="10. Cookies 池对接"></a>10. Cookies 池对接</h3>
                  <p>新浪微博的反爬能力非常强，我们需要做一些防范反爬虫的措施才可以顺利完成数据爬取。 如果没有登录而直接请求微博的 API 接口，这非常容易导致 403 状态码。这个情况我们在 10.2 节也提过。所以在这里我们实现一个 Middleware，为每个 Request 添加随机的 Cookies。 我们先开启 Cookies 池，使 API 模块正常运行。例如在本地运行 5000 端口，访问：<a href="http://localhost:5000/weibo/random" target="_blank" rel="noopener">http://localhost:5000/weibo/random</a> 即可获取随机的 Cookies，当然也可以将 Cookies 池部署到远程的服务器，这样只需要更改一下访问的链接就好了。 那么在这里我们将 Cookies 池在本地启动起来，再实现一个 Middleware 如下：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CookiesMiddleware</span>():</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, cookies_url)</span></span><span class="symbol">:</span></span><br><span class="line">       <span class="keyword">self</span>.logger = logging.getLogger(__name_<span class="number">_</span>)</span><br><span class="line">       <span class="keyword">self</span>.cookies_url = cookies_url</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">get_random_cookies</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">       <span class="symbol">try:</span></span><br><span class="line">           response = requests.get(<span class="keyword">self</span>.cookies_url)</span><br><span class="line">           <span class="keyword">if</span> response.status_code == <span class="number">200</span><span class="symbol">:</span></span><br><span class="line">               cookies = json.loads(response.text)</span><br><span class="line">               <span class="keyword">return</span> cookies</span><br><span class="line">       except requests.<span class="symbol">ConnectionError:</span></span><br><span class="line">           <span class="keyword">return</span> False</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(<span class="keyword">self</span>, request, spider)</span></span><span class="symbol">:</span></span><br><span class="line">       <span class="keyword">self</span>.logger.debug(<span class="string">' 正在获取 Cookies'</span>)</span><br><span class="line">       cookies = <span class="keyword">self</span>.get_random_cookies()</span><br><span class="line">       <span class="keyword">if</span> <span class="symbol">cookies:</span></span><br><span class="line">           request.cookies = cookies</span><br><span class="line">           <span class="keyword">self</span>.logger.debug(<span class="string">' 使用 Cookies '</span> + json.dumps(cookies))</span><br><span class="line"></span><br><span class="line">   @classmethod</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">       settings = crawler.settings</span><br><span class="line">       <span class="keyword">return</span> cls(cookies_url=settings.get(<span class="string">'COOKIES_URL'</span>)</span><br><span class="line">       )</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们首先利用 from_crawler() 方法获取了 COOKIES_URL 变量，它定义在 settings.py 里，这就是刚才我们所说的接口。接下来实现 get_random_cookies() 方法，这个方法主要就是请求此 Cookies 池接口并获取接口返回的随机 Cookies。如果成功获取，则返回 Cookies；否则返回 False。 接下来，在 process_request() 方法里，我们给 request 对象的 cookies 属性赋值，其值就是获取的随机 Cookies，这样我们就成功地为每一次请求赋值 Cookies 了。 如果启用了该 Middleware，每个请求都会被赋值随机的 Cookies。这样我们就可以模拟登录之后的请求，403 状态码基本就不会出现。</p>
                  <h3 id="11-代理池对接"><a href="#11-代理池对接" class="headerlink" title="11. 代理池对接"></a>11. 代理池对接</h3>
                  <p>微博还有一个反爬措施就是，检测到同一 IP 请求量过大时就会出现 414 状态码。如果遇到这样的情况可以切换代理。例如，在本地 5555 端口运行，获取随机可用代理的地址为：<a href="http://localhost:5555/random" target="_blank" rel="noopener">http://localhost:5555/random</a>，访问这个接口即可获取一个随机可用代理。接下来我们再实现一个 Middleware，代码如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class ProxyMiddleware():</span><br><span class="line">    def __init__(self, proxy_url):</span><br><span class="line">        self.logger = logging.getLogger(__name__)</span><br><span class="line">        self.proxy_url = proxy_url</span><br><span class="line"></span><br><span class="line">    def get_random_proxy(self):</span><br><span class="line">        try:</span><br><span class="line">            response = requests.<span class="builtin-name">get</span>(self.proxy_url)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == 200:</span><br><span class="line">               <span class="built_in"> proxy </span>= response.text</span><br><span class="line">                return proxy</span><br><span class="line">        except requests.ConnectionError:</span><br><span class="line">            return <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        <span class="keyword">if</span> request.meta.<span class="builtin-name">get</span>(<span class="string">'retry_times'</span>):</span><br><span class="line">           <span class="built_in"> proxy </span>= self.get_random_proxy()</span><br><span class="line">            <span class="keyword">if</span> proxy:</span><br><span class="line">                uri = <span class="string">'https://&#123;proxy&#125;'</span>.format(<span class="attribute">proxy</span>=proxy)</span><br><span class="line">                self.logger.<span class="builtin-name">debug</span>(<span class="string">' 使用代理 '</span> + proxy)</span><br><span class="line">                request.meta[<span class="string">'proxy'</span>] = uri</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">       <span class="built_in"> settings </span>= crawler.settings</span><br><span class="line">        return cls(<span class="attribute">proxy_url</span>=settings.get('PROXY_URL')</span><br><span class="line">        )</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>同样的原理，我们实现了一个 get_random_proxy() 方法用于请求代理池的接口获取随机代理。如果获取成功，则返回改代理，否则返回 False。在 process_request() 方法中，我们给 request 对象的 meta 属性赋值一个 proxy 字段，该字段的值就是代理。 另外，赋值代理的判断条件是当前 retry_times 不为空，也就是说第一次请求失败之后才启用代理，因为使用代理后访问速度会慢一些。所以我们在这里设置了只有重试的时候才启用代理，否则直接请求。这样就可以保证在没有被封禁的情况下直接爬取，保证了爬取速度。</p>
                  <h3 id="12-启用-Middleware"><a href="#12-启用-Middleware" class="headerlink" title="12. 启用 Middleware"></a>12. 启用 Middleware</h3>
                  <p>接下来，我们在配置文件中启用这两个 Middleware，修改 settings.py 如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">DOWNLOADER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'weibo.middlewares.CookiesMiddleware':</span> <span class="number">554</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'weibo.middlewares.ProxyMiddleware':</span> <span class="number">555</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>注意这里的优先级设置，前文提到了 Scrapy 的默认 Downloader Middleware 的设置如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware':</span> <span class="number">300</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware':</span> <span class="number">350</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware':</span> <span class="number">400</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware':</span> <span class="number">500</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.retry.RetryMiddleware':</span> <span class="number">550</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware':</span> <span class="number">560</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware':</span> <span class="number">580</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">590</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware':</span> <span class="number">600</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware':</span> <span class="number">700</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware':</span> <span class="number">750</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.stats.DownloaderStats':</span> <span class="number">850</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware':</span> <span class="number">900</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>要使得我们自定义的 CookiesMiddleware 生效，它在内置的 CookiesMiddleware 之前调用。内置的 CookiesMiddleware 的优先级为 700，所以这里我们设置一个比 700 小的数字即可。 要使得我们自定义的 ProxyMiddleware 生效，它在内置的 HttpProxyMiddleware 之前调用。内置的 HttpProxyMiddleware 的优先级为 750，所以这里我们设置一个比 750 小的数字即可。</p>
                  <h3 id="13-运行"><a href="#13-运行" class="headerlink" title="13. 运行"></a>13. 运行</h3>
                  <p>到此为止，整个微博爬虫就实现完毕了，我们运行如下命令启动一下爬虫：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl weibocn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>类似的输出结果如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[urllib3.connectionpool]</span> <span class="attr">DEBUG:</span> <span class="string">http://localhost:5000</span> <span class="string">"GET /weibo/random HTTP/1.1"</span> <span class="number">200</span> <span class="number">339</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[weibo.middlewares]</span> <span class="attr">DEBUG:</span> <span class="string">使用</span> <span class="string">Cookies</span> <span class="string">&#123;"SCF":</span> <span class="string">"AhzwTr_DxIGjgri_dt46_DoPzUqq-PSupu545JdozdHYJ7HyEb4pD3pe05VpbIpVyY1ciKRRWwUgojiO3jYwlBE."</span><span class="string">,</span> <span class="attr">"_T_WM":</span> <span class="string">"8fe0bc1dad068d09b888d8177f1c1218"</span><span class="string">,</span> <span class="attr">"SSOLoginState":</span> <span class="string">"1501496388"</span><span class="string">,</span> <span class="attr">"M_WEIBOCN_PARAMS":</span> <span class="string">"uicode%3D20000174"</span><span class="string">,</span> <span class="attr">"SUHB":</span> <span class="string">"0tKqV4asxqYl4J"</span><span class="string">,</span> <span class="attr">"SUB":</span> <span class="string">"_2A250e3QUDeRhGeBM6VYX8y7NwjiIHXVXhBxcrDV6PUJbkdBeLXjckW2fUT8MWloekO4FCWVlIYJGJdGLnA.."</span><span class="string">&#125;</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[weibocn]</span> <span class="attr">DEBUG:</span> <span class="string">&lt;200</span> <span class="string">https://m.weibo.cn/api/container/getIndex?uid=1742566624&amp;type=uid&amp;value=1742566624&amp;containerid=1005051742566624&gt;</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[scrapy.core.scraper]</span> <span class="attr">DEBUG:</span> <span class="string">Scraped</span> <span class="string">from</span> <span class="string">&lt;200</span> <span class="string">https://m.weibo.cn/api/container/getIndex?uid=1742566624&amp;type=uid&amp;value=1742566624&amp;containerid=1005051742566624&gt;</span></span><br><span class="line"><span class="string">&#123;'avatar':</span> <span class="string">'https://tva4.sinaimg.cn/crop.0.0.180.180.180/67dd74e0jw1e8qgp5bmzyj2050050aa8.jpg'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'cover':</span> <span class="string">'https://tva3.sinaimg.cn/crop.0.0.640.640.640/6ce2240djw1e9oaqhwllzj20hs0hsdir.jpg'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'crawled_at':</span> <span class="string">'2017-07-11 17:27'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'description':</span> <span class="string">' 成长，就是一个不断觉得以前的自己是个傻逼的过程 '</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'fans_count':</span> <span class="number">19202906</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'follows_count':</span> <span class="number">1599</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'gender':</span> <span class="string">'m'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'id':</span> <span class="number">1742566624</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'name':</span> <span class="string">' 思想聚焦 '</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'verified':</span> <span class="literal">True</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'verified_reason':</span> <span class="string">' 微博知名博主，校导网编辑 '</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'verified_type':</span> <span class="number">0</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'weibos_count':</span> <span class="number">58393</span><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行一段时间后，我们便可以到 MongoDB 数据库查看数据，爬取下来的数据如图 13-38 和图 13-39 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034947.jpg" alt=""> 图 13-38 用户信息 <img src="https://cdn.cuiqingcai.com/2019-11-27-035001.jpg" alt=""> 图 13-39 微博信息 针对用户信息，我们不仅爬取了其基本信息，还把关注和粉丝列表加到了 follows 和 fans 字段并做了去重操作。针对微博信息，我们成功进行了时间转换处理，同时还保存了微博的图片列表信息。</p>
                  <h3 id="14-本节代码"><a href="#14-本节代码" class="headerlink" title="14. 本节代码"></a>14. 本节代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/Weibo" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weibo</a>。</p>
                  <h3 id="15-结语"><a href="#15-结语" class="headerlink" title="15. 结语"></a>15. 结语</h3>
                  <p>本节实现了新浪微博的用户及其粉丝关注列表和微博信息的爬取，还对接了 Cookies 池和代理池来处理反爬虫。不过现在是针对单机的爬取，后面我们会将此项目修改为分布式爬虫，以进一步提高抓取效率。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-08 09:50:48" itemprop="dateCreated datePublished" datetime="2019-12-08T09:50:48+08:00">2019-12-08</time>
                </span>
                <span id="/8453.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.13–Scrapy 爬取新浪微博" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>17k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>15 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8448.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8448.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.12–Scrapy 对接 Docker</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-12-Scrapy-对接-Docker"><a href="#13-12-Scrapy-对接-Docker" class="headerlink" title="13.12 Scrapy 对接 Docker"></a>13.12 Scrapy 对接 Docker</h1>
                  <p>环境配置问题可能一直是我们头疼的，我们可能遇到过如下的情况：</p>
                  <ul>
                    <li>我们在本地写好了一个 Scrapy 爬虫项目，想要把它放到服务器上运行，但是服务器上没有安装 Python 环境。</li>
                    <li>别人给了我们一个 Scrapy 爬虫项目，项目中使用包的版本和我们本地环境版本不一致，无法直接运行。</li>
                    <li>我们需要同时管理不同版本的 Scrapy 项目，如早期的项目依赖于 Scrapy 0.25，现在的项目依赖于 Scrapy 1.4.0。</li>
                  </ul>
                  <p>在这些情况下，我们需要解决的就是环境的安装配置、环境的版本冲突解决等问题。 对于 Python 来说，VirtualEnv 的确可以解决版本冲突的问题。但是，VirtualEnv 不太方便做项目部署，我们还是需要安装 Python 环境， 如何解决上述问题呢？答案是用 Docker。Docker 可以提供操作系统级别的虚拟环境，一个 Docker 镜像一般都包含一个完整的操作系统，而这些系统内也有已经配置好的开发环境，如 Python 3.6 环境等。 我们可以直接使用此 Docker 的 Python 3 镜像运行一个容器，将项目直接放到容器里运行，就不用再额外配置 Python 3 环境。这样就解决了环境配置的问题。 我们也可以进一步将 Scrapy 项目制作成一个新的 Docker 镜像，镜像里只包含适用于本项目的 Python 环境。如果要部署到其他平台，只需要下载该镜像并运行就好了，因为 Docker 运行时采用虚拟环境，和宿主机是完全隔离的，所以也不需要担心环境冲突问题。 如果我们能够把 Scrapy 项目制作成一个 Docker 镜像，只要其他主机安装了 Docker，那么只要将镜像下载并运行即可，而不必再担心环境配置问题或版本冲突问题。 接下来，我们尝试把一个 Scrapy 项目制作成一个 Docker 镜像。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们要实现把前文 Scrapy 的入门项目打包成一个 Docker 镜像的过程。项目爬取的网址为：<a href="http://quotes.toscrape.com/" target="_blank" rel="noopener">http://quotes.toscrape.com/</a>，本章 Scrapy 入门一节已经实现了 Scrapy 对此站点的爬取过程，项目代码为：<a href="https://github.com/Python3WebSpider/ScrapyTutorial" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyTutorial</a>，如果本地不存在的话可以 Clone 下来。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保已经安装好 Docker 和 MongoDB 并可以正常运行，如果没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="3-创建-Dockerfile"><a href="#3-创建-Dockerfile" class="headerlink" title="3. 创建 Dockerfile"></a>3. 创建 Dockerfile</h3>
                  <p>首先在项目的根目录下新建一个 requirements.txt 文件，将整个项目依赖的 Python 环境包都列出来，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy</span></span><br><span class="line"><span class="attribute">pymongo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果库需要特定的版本，我们还可以指定版本号，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy&gt;=<span class="number">1.4</span><span class="number">.0</span></span><br><span class="line">pymongo&gt;=<span class="number">3.4</span><span class="number">.0</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在项目根目录下新建一个 Dockerfile 文件，文件不加任何后缀名，修改内容如下所示：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.6</span></span><br><span class="line"><span class="keyword">ENV</span> PATH /usr/local/bin:$PATH</span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /code</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /code</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip3 install -r requirements.txt</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> scrapy crawl quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一行的 FROM 代表使用的 Docker 基础镜像，在这里我们直接使用 python:3.6 的镜像，在此基础上运行 Scrapy 项目。 第二行 ENV 是环境变量设置，将 /usr/local/bin:$PATH 赋值给 PATH，即增加 /usr/local/bin 这个环境变量路径。 第三行 ADD 是将本地的代码放置到虚拟容器中。它有两个参数：第一个参数是.，代表本地当前路径；第二个参数是 /code，代表虚拟容器中的路径，也就是将本地项目所有内容放置到虚拟容器的 /code 目录下，以便于在虚拟容器中运行代码。 第四行 WORKDIR 是指定工作目录，这里将刚才添加的代码路径设成工作路径。这个路径下的目录结构和当前本地目录结构是相同的，所以我们可以直接执行库安装命令、爬虫运行命令等。 第五行 RUN 是执行某些命令来做一些环境准备工作。由于 Docker 虚拟容器内只有 Python 3 环境，而没有所需要的 Python 库，所以我们运行此命令来在虚拟容器中安装相应的 Python 库如 Scrapy，这样就可以在虚拟容器中执行 Scrapy 命令了。 第六行 CMD 是容器启动命令。在容器运行时，此命令会被执行。在这里我们直接用 scrapy crawl quotes 来启动爬虫。</p>
                  <h3 id="4-修改-MongoDB-连接"><a href="#4-修改-MongoDB-连接" class="headerlink" title="4. 修改 MongoDB 连接"></a>4. 修改 MongoDB 连接</h3>
                  <p>接下来我们需要修改 MongoDB 的连接信息。如果我们继续用 localhost 是无法找到 MongoDB 的，因为在 Docker 虚拟容器里 localhost 实际指向容器本身的运行 IP，而容器内部并没有安装 MongoDB，所以爬虫无法连接 MongoDB。 这里的 MongoDB 地址可以有如下两种选择。</p>
                  <ul>
                    <li>如果只想在本机测试，我们可以将地址修改为宿主机的 IP，也就是容器外部的本机 IP，一般是一个局域网 IP，使用 ifconfig 命令即可查看。</li>
                    <li>如果要部署到远程主机运行，一般 MongoDB 都是可公网访问的地址，修改为此地址即可。</li>
                  </ul>
                  <p>在本节中，我们的目标是将项目打包成一个镜像，让其他远程主机也可运行这个项目。所以我们直接将此处 MongoDB 地址修改为某个公网可访问的远程数据库地址，修改 MONGO_URI 如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'mongodb://admin:admin123@120.27.34.25:27017'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此处地址可以修改为自己的远程 MongoDB 数据库地址。 这样项目的配置就完成了。</p>
                  <h3 id="5-构建镜像"><a href="#5-构建镜像" class="headerlink" title="5. 构建镜像"></a>5. 构建镜像</h3>
                  <p>接下来我们便可以构建镜像了，执行如下命令：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">docker</span> <span class="keyword">build </span>-t quotes:latest .</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的输出就说明镜像构建成功。这时我们查看一下构建的镜像，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Sending build context to Docker daemon <span class="number">191.5</span> kB</span><br><span class="line">Step <span class="number">1</span>/<span class="number">6</span> : FROM python:<span class="number">3.6</span></span><br><span class="line"> ---&gt; <span class="number">968120</span>d8cbe8</span><br><span class="line">Step <span class="number">2</span>/<span class="number">6</span> : ENV PATH /usr/local/bin:$PATH</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; <span class="number">387</span>abbba1189</span><br><span class="line">Step <span class="number">3</span>/<span class="number">6</span> : ADD . /code</span><br><span class="line"> ---&gt; a844ee0db9c6</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container <span class="number">4</span>dc41779c573</span><br><span class="line">Step <span class="number">4</span>/<span class="number">6</span> : WORKDIR /code</span><br><span class="line"> ---&gt; <span class="number">619</span>b2c064ae9</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container bcd7cd7f7337</span><br><span class="line">Step <span class="number">5</span>/<span class="number">6</span> : RUN pip3 install -r requirements.txt</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> <span class="number">9452</span>c83a12c5</span><br><span class="line">...</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container <span class="number">9452</span>c83a12c5</span><br><span class="line">Step <span class="number">6</span>/<span class="number">6</span> : CMD scrapy crawl quotes</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> c092b5557ab8</span><br><span class="line"> ---&gt; c8101aca6e2a</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container c092b5557ab8</span><br><span class="line">Successfully built c8101aca6e2a</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>出现类似输出就证明镜像构建成功了，这时执行如我们查看一下构建的镜像：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">docker images</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回结果中其中有一行就是：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">quotes  latest  <span class="number">41</span>c8499ce210    <span class="number">2</span> minutes ago   <span class="number">769</span> MB</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是我们新构建的镜像。</p>
                  <h3 id="6-运行"><a href="#6-运行" class="headerlink" title="6. 运行"></a>6. 运行</h3>
                  <p>我们可以先在本地测试运行，执行如下命令：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">run</span><span class="bash"> quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就利用此镜像新建并运行了一个 Docker 容器，运行效果完全一致，如图 13-29 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034816.jpg" alt=""> 图 13-32 运行结果 如果出现类似图 13-29 的运行结果，这就证明构建的镜像没有问题。</p>
                  <h3 id="7-推送至-Docker-Hub"><a href="#7-推送至-Docker-Hub" class="headerlink" title="7. 推送至 Docker Hub"></a>7. 推送至 Docker Hub</h3>
                  <p>构建完成之后，我们可以将镜像 Push 到 Docker 镜像托管平台，如 Docker Hub 或者私有的 Docker Registry 等，这样我们就可以从远程服务器下拉镜像并运行了。 以 Docker Hub 为例，如果项目包含一些私有的连接信息（如数据库），我们最好将 Repository 设为私有或者直接放到私有的 Docker Registry。 首先在 <a href="https://hub.docker.com" target="_blank" rel="noopener">https://hub.docker.com</a> 注册一个账号，新建一个 Repository，名为 quotes。比如，我的用户名为 germey，新建的 Repository 名为 quotes，那么此 Repository 的地址就可以用 germey/quotes 来表示。 为新建的镜像打一个标签，命令如下所示：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">tag</span> <span class="title">quotes</span>:latest germey/quotes:latest</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>推送镜像到 Docker Hub 即可，命令如下所示：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">docker</span> <span class="keyword">push </span>germey/quotes</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Docker Hub 便会出现新推送的 Docker 镜像了，如图 13-30 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034823.png" alt=""> 图 13-30 推送结果 如果我们想在其他的主机上运行这个镜像，主机上装好 Docker 后，可以直接执行如下命令：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">run</span><span class="bash"> germey/quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就会自动下载镜像，然后启动容器运行，不需要配置 Python 环境，不需要关心版本冲突问题。 运行效果如图 13-31 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-034829.jpg" alt=""> 图 13-31 运行效果 整个项目爬取完成后，数据就可以存储到指定的数据库中。</p>
                  <h3 id="8-结语"><a href="#8-结语" class="headerlink" title="8. 结语"></a>8. 结语</h3>
                  <p>我们讲解了将 Scrapy 项目制作成 Docker 镜像并部署到远程服务器运行的过程。使用此种方式，我们在本节开头所列出的问题都迎刃而解。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-07 10:23:45" itemprop="dateCreated datePublished" datetime="2019-12-07T10:23:45+08:00">2019-12-07</time>
                </span>
                <span id="/8448.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.12–Scrapy 对接 Docker" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8445.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8445.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.11–Scrapyrt 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-11-Scrapyrt-的使用"><a href="#13-11-Scrapyrt-的使用" class="headerlink" title="13.11 Scrapyrt 的使用"></a>13.11 Scrapyrt 的使用</h1>
                  <p>Scrapyrt 为 Scrapy 提供了一个调度的 HTTP 接口。有了它我们不需要再执行 Scrapy 命令，而是通过请求一个 HTTP 接口即可调度 Scrapy 任务，我们就不需要借助于命令行来启动项目了。如果项目是在远程服务器运行，利用它来启动项目是个不错的选择。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们以本章 Scrapy 入门项目为例来说明 Scrapyrt 的使用方法，项目源代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyTutorial" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyTutorial</a>。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保 Scrapyrt 已经正确安装并正常运行，具体安装可以参考第 1 章的说明。</p>
                  <h3 id="3-启动服务"><a href="#3-启动服务" class="headerlink" title="3. 启动服务"></a>3. 启动服务</h3>
                  <p>首先将项目下载下来，在项目目录下运行 Scrapyrt，假设当前服务运行在 9080 端口上。下面将简单介绍 Scrapyrt 的使用方法。</p>
                  <h3 id="4-GET-请求"><a href="#4-GET-请求" class="headerlink" title="4. GET 请求"></a>4. GET 请求</h3>
                  <p>目前，GET 请求方式支持如下的参数。</p>
                  <ul>
                    <li>spider_name，Spider 名称，字符串类型，必传参数，如果传递的 Spider 名称不存在则会返回 404 错误。</li>
                    <li>url，爬取链接，字符串类型，如果起始链接没有定义的话就必须要传递，如果传递了该参数，Scrapy 会直接用该 URL 生成 Request，而直接忽略 start_requests() 方法和 start_urls 属性的定义。</li>
                    <li>callback，回调函数名称，字符串类型，可选参数，如果传递了就会使用此回调函数处理，否则会默认使用 Spider 内定义的回调函数。</li>
                    <li>max_requests，最大请求数量，数值类型，可选参数，它定义了 Scrapy 执行请求的 Request 的最大限制，如定义为 5，则最多只执行 5 次 Request 请求，其余的则会被忽略。</li>
                    <li>start_requests，是否要执行 start_request() 函数，布尔类型，可选参数，在 Scrapy 项目中如果定义了 start_requests() 方法，那么在项目启动时会默认调用该方法，但是在 Scrapyrt 就不一样了，它默认不执行 start_requests() 方法，如果要执行，需要将它设置为 true。</li>
                  </ul>
                  <p>例如我们执行如下命令：</p>
                  <figure class="highlight awk">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl http:<span class="regexp">//</span>localhost:<span class="number">9080</span><span class="regexp">/crawl.json?spider_name=quotes&amp;url=http:/</span><span class="regexp">/quotes.toscrape.com/</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>得到类似如下结果，如图 13-28 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-034631.jpg" alt=""> 图 13-28 输出结果 返回的是一个 JSON 格式的字符串，我们解析它的结构，如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"status"</span>: <span class="string">"ok"</span>,</span><br><span class="line">  <span class="attr">"items"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"text"</span>: <span class="string">"“The world as we have created it is a process of o..."</span>,</span><br><span class="line">      <span class="attr">"author"</span>: <span class="string">"Albert Einstein"</span>,</span><br><span class="line">      <span class="attr">"tags"</span>: [</span><br><span class="line">        <span class="string">"change"</span>,</span><br><span class="line">        <span class="string">"deep-thoughts"</span>,</span><br><span class="line">        <span class="string">"thinking"</span>,</span><br><span class="line">        <span class="string">"world"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    ...</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"text"</span>: <span class="string">"“... a mind needs books as a sword needs a whetsto..."</span>,</span><br><span class="line">      <span class="attr">"author"</span>: <span class="string">"George R.R. Martin"</span>,</span><br><span class="line">      <span class="attr">"tags"</span>: [</span><br><span class="line">        <span class="string">"books"</span>,</span><br><span class="line">        <span class="string">"mind"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">"items_dropped"</span>: [],</span><br><span class="line">  <span class="attr">"stats"</span>: &#123;</span><br><span class="line">    <span class="attr">"downloader/request_bytes"</span>: <span class="number">2892</span>,</span><br><span class="line">    <span class="attr">"downloader/request_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"downloader/request_method_count/GET"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"downloader/response_bytes"</span>: <span class="number">24812</span>,</span><br><span class="line">    <span class="attr">"downloader/response_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"downloader/response_status_count/200"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"downloader/response_status_count/404"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"dupefilter/filtered"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"finish_reason"</span>: <span class="string">"finished"</span>,</span><br><span class="line">    <span class="attr">"finish_time"</span>: <span class="string">"2017-07-12 15:09:02"</span>,</span><br><span class="line">    <span class="attr">"item_scraped_count"</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="attr">"log_count/DEBUG"</span>: <span class="number">112</span>,</span><br><span class="line">    <span class="attr">"log_count/INFO"</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="attr">"memusage/max"</span>: <span class="number">52510720</span>,</span><br><span class="line">    <span class="attr">"memusage/startup"</span>: <span class="number">52510720</span>,</span><br><span class="line">    <span class="attr">"request_depth_max"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"response_received_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"scheduler/dequeued"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"scheduler/dequeued/memory"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"scheduler/enqueued"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"scheduler/enqueued/memory"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"start_time"</span>: <span class="string">"2017-07-12 15:08:56"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"spider_name"</span>: <span class="string">"quotes"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里省略了 items 绝大部分。status 显示了爬取的状态，items 部分是 Scrapy 项目的爬取结果，items_dropped 是被忽略的 Item 列表，stats 是爬取结果的统计情况。此结果和直接运行 Scrapy 项目得到的统计是相同的。 这样一来，我们就通过 HTTP 接口调度 Scrapy 项目并获取爬取结果，如果 Scrapy 项目部署在服务器上，我们可以通过开启一个 Scrapyrt 服务实现任务的调度并直接取到爬取结果，这很方便。</p>
                  <h3 id="5-POST-请求"><a href="#5-POST-请求" class="headerlink" title="5. POST 请求"></a>5. POST 请求</h3>
                  <p>除了 GET 请求，我们还可以通过 POST 请求来请求 Scrapyrt。但是此处 Request Body 必须是一个合法的 JSON 配置，在 JSON 里面可以配置相应的参数，支持的配置参数更多。 目前，JSON 配置支持如下参数。</p>
                  <ul>
                    <li><strong>spider_name</strong>：Spider 名称，字符串类型，必传参数。如果传递的 Spider 名称不存在，则返回 404 错误。</li>
                    <li><strong>max_requests</strong>：最大请求数量，数值类型，可选参数。它定义了 Scrapy 执行请求的 Request 的最大限制，如定义为 5，则表示最多只执行 5 次 Request 请求，其余的则会被忽略。</li>
                    <li><strong>request</strong>：Request 配置，JSON 对象，必传参数。通过该参数可以定义 Request 的各个参数，必须指定 url 字段来指定爬取链接，其他字段可选。</li>
                  </ul>
                  <p>我们看一个 JSON 配置实例，如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"request"</span>: &#123;</span><br><span class="line">        <span class="attr">"url"</span>: <span class="string">"http://quotes.toscrape.com/"</span>,</span><br><span class="line">        <span class="attr">"callback"</span>: <span class="string">"parse"</span>,</span><br><span class="line">        <span class="attr">"dont_filter"</span>: <span class="string">"True"</span>,</span><br><span class="line">        <span class="attr">"cookies"</span>: &#123;<span class="attr">"foo"</span>: <span class="string">"bar"</span>&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"max_requests"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="attr">"spider_name"</span>: <span class="string">"quotes"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们执行如下命令传递该 Json 配置并发起 POST 请求：</p>
                  <figure class="highlight awk">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl http:<span class="regexp">//</span>localhost:<span class="number">9080</span><span class="regexp">/crawl.json -d '&#123;"request": &#123;"url": "http:/</span><span class="regexp">/quotes.toscrape.com/</span><span class="string">", "</span>dont_filte<span class="string">r": "</span>True<span class="string">", "</span>callback<span class="string">": "</span>parse<span class="string">", "</span>cookies<span class="string">": &#123;"</span>foo<span class="string">": "</span>ba<span class="string">r"&#125;&#125;, "</span>max_requests<span class="string">": 2, "</span>spider_name<span class="string">": "</span>quotes<span class="string">"&#125;'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果和上文类似，同样是输出了爬取状态、结果、统计信息等内容。</p>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>以上内容便是 Scrapyrt 的相关用法介绍。通过它，我们方便地调度 Scrapy 项目的运行并获取爬取结果。更多的使用方法可以参考官方文档：<a href="http://scrapyrt.readthedocs.io" target="_blank" rel="noopener">http://scrapyrt.readthedocs.io</a>。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-07 10:20:01" itemprop="dateCreated datePublished" datetime="2019-12-07T10:20:01+08:00">2019-12-07</time>
                </span>
                <span id="/8445.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.11–Scrapyrt 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.3k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8443.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8443.html" class="post-title-link" itemprop="url">Nginx 反向代理返回结果为空的问题</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>最近在开发过程中遇到了这么一个问题： 现在有一个 Web 项目，前端是使用 Vue.js 开发的，整个前端需要部署到 K8S 上，后端和前端分开，同样也需要部署到 K8S 上，因此二者需要打包为 Docker 镜像。 对前端来说，打包 Docker 就遇到了一个问题：跨域访问问题。 因此一个普遍的解决方案就是使用 Nginx 做反向代理。 一般来说，我们需要在打包时配置一下 nginx.conf 文件，然后在 Dockerfile 里面指定即可。</p>
                  <h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2>
                  <p>首先看下 Dockerfile：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># build stage</span></span><br><span class="line"><span class="keyword">FROM</span> node:lts-alpine as build-stage</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /app</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package*.json ./</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . .</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm run build</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># production stage</span></span><br><span class="line"><span class="keyword">FROM</span> nginx:lts-alpine as production-stage</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=build-stage /app/dist /usr/share/nginx/html</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> nginx.conf /etc/nginx/conf.d/</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> rm /etc/nginx/conf.d/default.conf</span></span><br><span class="line">    &amp;&amp; mv /etc/nginx/conf.d/nginx.conf /etc/nginx/conf.d/default.conf</span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">80</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"nginx"</span>, <span class="string">"-g"</span>, <span class="string">"daemon off;"</span>]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>一般来说，对于常规的 Vue.js 前端项目，Dockerfile 就这么写就行了。 简单介绍一下：</p>
                  <ul>
                    <li>第一步，使用 Node.js 镜像，在 Node.js 环境下对项目进行编译，默认会输出到 dist 文件夹下。</li>
                    <li>第二步，使用新的 Nginx 镜像，将编译得到的前端文件拷贝到 nginx 默认 serve 的目录，然后把自定义的 nginx.conf 文件替换为 Nginx 默认的 conf 文件，运行即可。</li>
                  </ul>
                  <h2 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h2>
                  <p>这里比较关键的就是 nginx.conf 文件了，为了解决跨域问题，我们一般会将后端的接口进行反向代理。 一般来说，后端的 API 接口都是以 api 为开头的，所以我们需要代理 api 开头的接口地址，nginx.conf 内容一般可以这么写：</p>
                  <figure class="highlight nginx">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span>       <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span>  localhost;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> /api/ &#123;</span><br><span class="line">                <span class="attribute">proxy_pass</span> http://domain.com/api/;</span><br><span class="line">                <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">                <span class="attribute">proxy_set_header</span> Host <span class="variable">$http_host</span>;</span><br><span class="line">                <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">        <span class="attribute">root</span>   /usr/share/nginx/html;</span><br><span class="line">        <span class="attribute">index</span>  index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> = /50x.html &#123;</span><br><span class="line">        <span class="attribute">root</span>   /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">error_page</span>  <span class="number">404</span>              /<span class="number">404</span>.html;</span><br><span class="line">    <span class="attribute">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /50x.html;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>一般来说，以上的写法是没有问题的，proxy_set_header 也把一些 Header 进行设置，转发到后端服务器。 如果你这么写，打包 Docker 之后，测试没有遇到问题，那就完事了。</p>
                  <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2>
                  <p>但我遇到了一个奇怪的问题，某个接口在请求的时候，状态码还是 200，但其返回值总是为空，即 Response Data 的内容完全为空。 但是服务器端看 Log 确实有正常返回 Response，使用 Vue 的 devServer 也是正常的，使用 Postman 来请求也是正常的，但是经过 Nginx 这么一反向代理就不行了，什么 Response 都接收不到。 部署到 Prod 环境之后，浏览器上面可以得到这么个错误：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">ERR_INCOMPLETE_CHUNKED_ENCODING</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p><img src="https://cdn.cuiqingcai.com/2019-12-06-202934.png" alt="image-20191207042932549"> 最后经排查，发现后端接口使用时设定了 <code>Transfer-Encoding: chunked</code> 响应头：</p>
                  <figure class="highlight fortran">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="built_in">Transfer</span>-Encoding: chunked</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这是啥？这时候就需要引出 Keep-Alive 的相关问题了。</p>
                  <h2 id="什么是-Keep-Alive？"><a href="#什么是-Keep-Alive？" class="headerlink" title="什么是 Keep-Alive？"></a>什么是 Keep-Alive？</h2>
                  <p>我们知道 HTTP 协议采用「请求-应答」模式，当使用普通模式，即非 Keep-Alive 模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接（HTTP 协议为无连接的协议）。当使用 Keep-Alive 模式（又称持久连接、连接重用）时，Keep-Alive 功能使客户端到服务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive 功能避免了建立或者重新建立连接。</p>
                  <ul>
                    <li>HTTP 1.0 中默认是关闭 Keep-Alive 的，需要在 HTTP 头加入<code>Connection: Keep-Alive</code>，才能启用 Keep-Alive</li>
                    <li>HTTP 1.1 中默认启用 Keep-Alive，如果请求头中加入 <code>Connection: close</code>，Keep-Alive 才关闭。</li>
                  </ul>
                  <p>目前大部分浏览器都是用 HTTP 1.1 协议，也就是说默认都会发起 Keep-Alive 的连接请求了，所以是否能完成一个完整的 Keep-Alive 连接就看服务器设置情况。 启用 Keep-Alive 模式肯定更高效，性能更高。因为避免了建立/释放连接的开销。</p>
                  <h2 id="Keep-Alive-模式下如何传输数据"><a href="#Keep-Alive-模式下如何传输数据" class="headerlink" title="Keep-Alive 模式下如何传输数据"></a>Keep-Alive 模式下如何传输数据</h2>
                  <p>Keep-Alive 模式，客户端如何判断请求所得到的响应数据已经接收完成呢？或者说如何知道服务器已经发生完了数据？ 我们已经知道了，Keep-Alive 模式发送完数据，HTTP 服务器不会自动断开连接，所有不能再使用返回 EOF（-1）来判断。 那么怎么判断呢？一个是使用 Content-Length ，一个是使用 Transfer-Encoding。</p>
                  <h3 id="Content-Length"><a href="#Content-Length" class="headerlink" title="Content-Length"></a>Content-Length</h3>
                  <p>顾名思义，Conent-Length 表示实体内容长度，客户端（服务器）可以根据这个值来判断数据是否接收完成。 由于 <code>Content-Length</code> 字段必须真实反映实体长度，但实际应用中，有些时候实体长度并没那么好获得，例如实体来自于网络文件，或者由动态语言生成。这时候要想准确获取长度，只能开一个足够大的 buffer，等内容全部生成好再计算。但这样做一方面需要更大的内存开销，另一方面也会让客户端等更久。 我们在做 WEB 性能优化时，有一个重要的指标叫 TTFB（Time To First Byte），它代表的是从客户端发出请求到收到响应的第一个字节所花费的时间。大部分浏览器自带的 Network 面板都可以看到这个指标，越短的 TTFB 意味着用户可以越早看到页面内容，体验越好。可想而知，服务端为了计算响应实体长度而缓存所有内容，跟更短的 TTFB 理念背道而驰。但在 HTTP 报文中，实体一定要在头部之后，顺序不能颠倒，为此我们需要一个新的机制：不依赖头部的长度信息，也能知道实体的边界。 但是如果消息中没有 Conent-Length，那该如何来判断呢？又在什么情况下会没有 Conent-Length 呢？</p>
                  <h3 id="Transfer-Encoding"><a href="#Transfer-Encoding" class="headerlink" title="Transfer-Encoding"></a>Transfer-Encoding</h3>
                  <p>当客户端向服务器请求一个静态页面或者一张图片时，服务器可以很清楚地知道内容大小，然后通过 Content-length 消息首部字段告诉客户端需要接收多少数据。但是如果是动态页面等时，服务器是不可能预先知道内容大小，这时就可以使用 分块编码模式来传输数据了。即如果要一边产生数据，一边发给客户端，服务器就需要在请求头中使用<code>Transfer-Encoding: chunked</code> 这样的方式来代替 Content-Length，这就是分块编码。 分块编码相当简单，在头部加入 <code>Transfer-Encoding: chunked</code> 之后，就代表这个报文采用了分块编码。这时，报文中的实体需要改为用一系列分块来传输。每个分块包含十六进制的长度值和数据，长度值独占一行，长度不包括它结尾的 CRLF（rn），也不包括分块数据结尾的 CRLF。最后一个分块长度值必须为 0，对应的分块数据没有内容，表示实体结束。</p>
                  <h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2>
                  <p>那么我说了这么一大通有什么用呢？ OK，在我遇到的业务场景中，我发现服务器的响应头中就包含了<code>Transfer-Encoding: chunked</code> 这个字段。 而这个字段，在 HTTP 1.0 是不被支持的。 而 Nginx 的反向代理，默认用的就是 HTTP 1.0，那就导致了数据无法获取的问题，可以参考 Nginx 的官方文档说明：<a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass" target="_blank" rel="noopener">http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass</a>。 原文中：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Syntax: proxy_http_version <span class="number">1.0</span> | <span class="number">1.1</span>;</span><br><span class="line"><span class="keyword">Default</span>: proxy_http_version <span class="number">1.0</span>;</span><br><span class="line"><span class="keyword">By</span> <span class="keyword">default</span>, <span class="keyword">version</span> <span class="number">1.0</span> <span class="keyword">is</span> used. <span class="keyword">Version</span> <span class="number">1.1</span> <span class="keyword">is</span> recommended <span class="keyword">for</span> use <span class="keyword">with</span> keepalive connections <span class="keyword">and</span> NTLM authentication.</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>所以，我们如果要解决这个问题，只需要设置一下 HTTP 版本为 1.1 就好了： 修改 nginx.conf 文件如下：</p>
                  <figure class="highlight nginx">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">location</span> /api/ &#123;</span><br><span class="line">    <span class="attribute">proxy_pass</span> http://domain.com/api/;</span><br><span class="line">    <span class="attribute">proxy_http_version</span> <span class="number">1</span>.<span class="number">1</span>;</span><br><span class="line">    <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">    <span class="attribute">proxy_set_header</span> Host <span class="variable">$http_host</span>;</span><br><span class="line">    <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里就增加了一行：</p>
                  <figure class="highlight abnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">proxy_http_version <span class="number">1.1</span><span class="comment">;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样再测试，反向代理就会支持 <code>Transfer-Encoding: chunked</code> 模式了，这也就呼应了之前在浏览器中遇到的 ERR_INCOMPLETE_CHUNKED_ENCODING 错误。 自此，问题完美解决。</p>
                  <h2 id="复盘记录"><a href="#复盘记录" class="headerlink" title="复盘记录"></a>复盘记录</h2>
                  <p>一开始本来只想简单一记录就了事的，但一边写，发现某个地方还可以展开写得更详细。 所以干脆最后我对这个问题进行了详细的复盘和记录。在写本文之前，我其实只思考到了 Keep-Alive 和 HTTP 1.1 的问题，其实我对 Transfer-Encoding 这个并没有去深入思考。在边写边总结的过程中，为了把整个脉络讲明白，我又查询了一些 Transfer-Encoding 和 Nginx 的官方文档，对这块的了解变得更加深入，相当于我在整个记录的过程中，又对整个流程梳理了一遍，同时又有额外的收获。 所以，遇到问题，深入去思考、总结和复盘，是很有帮助的，这会让我们对问题的看法和理解更加透彻。 怎么说呢？在开发过程中，难免会遇到一些奇奇怪怪的 Bug，但这其实只是技术问题，总会解决的。 但怎样在开发过程中，不断提高自己的技术能力，我觉得需要从每一个细节出发，去思考一些事情的来龙去脉。思考得越多，我们对整个事件的把握也会越清晰，以后如果再遇到类似的或者关联的事情，就会迎刃而解了。 平时我们可能很多情况下都在写业务代码，可能比较枯燥，感觉对技术没有实质性的提升，但如果我们能从中提炼出一些核心的问题或解决方案，这才是能真正提高技术的时候，这才是最有价值的。</p>
                  <h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2>
                  <p>本文部分内容改写或摘自下列内容。</p>
                  <ul>
                    <li>HTTP Keep-Alive 模式：<a href="https://www.cnblogs.com/skynet/archive/2010/12/11/1903347.html" target="_blank" rel="noopener">https://www.cnblogs.com/skynet/archive/2010/12/11/1903347.html</a></li>
                    <li>Nginx proxy_set_header 理解：<a href="https://www.jianshu.com/p/cc5167032525" target="_blank" rel="noopener">https://www.jianshu.com/p/cc5167032525</a></li>
                    <li>使用 Docker 打造超溜的前端环境：<a href="https://github.com/axetroy/blog/issues/178" target="_blank" rel="noopener">https://github.com/axetroy/blog/issues/178</a></li>
                    <li>HTTP 协议中的 Transfer-Encoding：<a href="https://imququ.com/post/transfer-encoding-header-in-http.html" target="_blank" rel="noopener">https://imququ.com/post/transfer-encoding-header-in-http.html</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-07 06:26:39" itemprop="dateCreated datePublished" datetime="2019-12-07T06:26:39+08:00">2019-12-07</time>
                </span>
                <span id="/8443.html" class="post-meta-item leancloud_visitors" data-flag-title="Nginx 反向代理返回结果为空的问题" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>5 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8418.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8418.html" class="post-title-link" itemprop="url">阿里云服务器活动！阿里云代金券 + 1 折优惠码</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>阿里云作为国内最大的云服务商家，个人与企业上云都纷纷首选阿里云。但是在价格方面比整个市场有些许昂贵，让不少用户却而止步。因此<a href="https://www.xingsuyun58.com/" target="_blank" rel="noopener">星速云</a>小编呕心沥血整理阿里云最新优惠折扣【汇总篇】，让大家不用花时间到处寻找优惠信息，帮助站长、开发者和企业们上云购节省项目开支。</p>
                  <hr>
                  <h1 id="最全：阿里云最新优惠获取教程【长期有效】"><a href="#最全：阿里云最新优惠获取教程【长期有效】" class="headerlink" title="最全：阿里云最新优惠获取教程【长期有效】"></a>最全：阿里云最新优惠获取教程【长期有效】</h1>
                  <hr>
                  <h2 id="①：阿里云代金券2000元红包"><a href="#①：阿里云代金券2000元红包" class="headerlink" title="①：阿里云代金券2000元红包"></a>①：阿里云代金券2000元红包</h2>
                  <p>阿里云代金券领取很简单，点击下面链接进行领取。 <a href="https://www.xingsuyun58.com/wp-content/uploads/2019/11/1575037236.png" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/11/1575037236.png" alt="阿里云代金券领取和使用步骤教程"></a> 阿里云代金券领取地址：<a href="https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=ahxhg8oc" target="_blank" rel="noopener">点击领取2000元代金券礼包</a> 点击“立即领取”按钮就可以一键领取到所有满减代金券，最高2000元。别忘记通过购物车一键批量购买哟！</p>
                  <h2 id="②：阿里云9折优惠码"><a href="#②：阿里云9折优惠码" class="headerlink" title="②：阿里云9折优惠码"></a>②：阿里云9折优惠码</h2>
                  <p>新用户还可以使用手机扫码领取一个阿里云9折折扣码叠加上述阿里云代金券使用。该9折码只能通过阿里云手机客户端扫描领取，PC端无法领取，（限ECS首购并且优惠高于7折才可以使用，比如优惠已经为5折，则该折扣码无效） <a href="https://www.xingsuyun58.com/wp-content/uploads/2019/11/15750316201.png" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/11/15750316201.png" alt="阿里云代金券"></a> 注明：阿里云9折优惠码与阿里云2000元红包可叠加优惠折扣。</p>
                  <hr>
                  <h1 id="阿里云双12期间（2019-12-3-2019-12-31）最新优惠活动"><a href="#阿里云双12期间（2019-12-3-2019-12-31）最新优惠活动" class="headerlink" title="阿里云双12期间（2019.12.3-2019.12.31）最新优惠活动"></a>阿里云双12期间（2019.12.3-2019.12.31）最新优惠活动</h1>
                  <hr>
                  <p>阿里云双12优惠活动终于开启了，新用户1折甩卖，老用户五折，还可以领取2000元红包，优惠力度不亚于双11优惠活动哟！还不赶紧上云呢？错过双11优惠活动，那么双12不容错过了！ <a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/12/aliyun12pintuan.jpg" alt="阿里云双12活动"></a></p>
                  <h2 id="什么？您还不知道云服务器用途"><a href="#什么？您还不知道云服务器用途" class="headerlink" title="什么？您还不知道云服务器用途"></a>什么？您还不知道云服务器用途</h2>
                  <p>不管是做web网站、APP程序后端部署、应用程序后端、小程序后端等，还是打算创业的小伙伴，或者传统IDC自建机房的企业，上云已成为趋势。云服务器更便捷省心、节约IT运维的成本。</p>
                  <h2 id="新用户1折优惠售卖："><a href="#新用户1折优惠售卖：" class="headerlink" title="新用户1折优惠售卖："></a>新用户1折优惠售卖：</h2>
                  <p>实例规格</p>
                  <p>配置</p>
                  <p>带宽</p>
                  <p>时长</p>
                  <p>价格</p>
                  <p>官网购买</p>
                  <p>ECS突发性能型t5</p>
                  <p>1核2G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>89.00元</p>
                  <p>【<a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener">立即抢购</a>】</p>
                  <p>ECS突发性能型t5</p>
                  <p>1核2G40G高效云盘</p>
                  <p>1M</p>
                  <p>3年</p>
                  <p>229.00元</p>
                  <p>ECS共享型n4</p>
                  <p>2核4G40G高效云盘</p>
                  <p>3M</p>
                  <p>2年</p>
                  <p>469.00元</p>
                  <p>ECS突发性能t5</p>
                  <p>2核4G40G高效云盘</p>
                  <p>5M</p>
                  <p>3年</p>
                  <p>899.00元</p>
                  <p>ECS突发性能t5</p>
                  <p>2核4G40G高效云盘</p>
                  <p>3M</p>
                  <p>3年</p>
                  <p>639.00元</p>
                  <p>ECS共享型n4</p>
                  <p>2核4G40G高效云盘</p>
                  <p>3M</p>
                  <p>3年</p>
                  <p>799.00元</p>
                  <p>ECS共享通用型mn4</p>
                  <p>2核8G40G高效云盘</p>
                  <p>5M</p>
                  <p>3年</p>
                  <p>1399.00元</p>
                  <p>ECS突发性能t5（香港）</p>
                  <p>1核1G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>119.00元</p>
                  <p>ECS网络增强型sn1ne</p>
                  <p>4核8G40G高效云盘</p>
                  <p>5M</p>
                  <p>3年</p>
                  <p>5621.00元</p>
                  <p>8核16G40G高效云盘</p>
                  <p>8M</p>
                  <p>3年</p>
                  <p>12209.00元</p>
                  <hr>
                  <p>注明：突发性t5实例，别看到价格比较便宜就直接购买，里面很多套路，购买页面有提示：限制20%性能基线。释义：依靠CPU 积分来提升 CPU 性能，满足业务需求。当实例实际工作性能高于基准 CPU 计算性能时，会把服务器 CPU 的性能限制在 20%以下，如果这时20%CPU性能满足不了业务需求，云服务器CPU会跑满100%，到那时候你以为是被某大佬攻击了，很有可能是你突发性t5实例CPU 积分消耗完了。笔者建议：如果用户业务对 CPU 要求高的，可以直接略过，选择t5实例（无限制CPU性能）、n4共享型、通用型mn4。以下笔者建议爆款：</p>
                  <h2 id="个人博客与企业微服务首选"><a href="#个人博客与企业微服务首选" class="headerlink" title="个人博客与企业微服务首选"></a>个人博客与企业微服务首选</h2>
                  <p><a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/12/15754425651.png" alt="阿里云双12云服务器爆款"></a></p>
                  <h2 id="老用户五折优惠甩卖："><a href="#老用户五折优惠甩卖：" class="headerlink" title="老用户五折优惠甩卖："></a>老用户五折优惠甩卖：</h2>
                  <p>实例规格</p>
                  <p>CPU/内存/云盘</p>
                  <p>带宽</p>
                  <p>时长</p>
                  <p>价格</p>
                  <p>老用户优惠购买</p>
                  <p>云服务器计算型ic5</p>
                  <p>8核8G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>4433.94元</p>
                  <p>【<a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener">立即抢购</a>】</p>
                  <p>计算网络增强型sn1ne</p>
                  <p>8核16G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>3751.20元</p>
                  <p>通用网络增强型sn2ne</p>
                  <p>8核32G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>5353.20元</p>
                  <p>内存网络增强型se1ne</p>
                  <p>8核64G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>6793.20元</p>
                  <blockquote>
                    <p>注明：本文为星速云原创版权所有，禁止转载，一经发现将追究版权责任！</p>
                  </blockquote>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-06 18:43:17" itemprop="dateCreated datePublished" datetime="2019-12-06T18:43:17+08:00">2019-12-06</time>
                </span>
                <span id="/8418.html" class="post-meta-item leancloud_visitors" data-flag-title="阿里云服务器活动！阿里云代金券 + 1 折优惠码" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8413.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8413.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.10–Scrapy 通用爬虫</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-10-Scrapy-通用爬虫"><a href="#13-10-Scrapy-通用爬虫" class="headerlink" title="13.10 Scrapy 通用爬虫"></a>13.10 Scrapy 通用爬虫</h1>
                  <p>通过 Scrapy，我们可以轻松地完成一个站点爬虫的编写。但如果抓取的站点量非常大，比如爬取各大媒体的新闻信息，多个 Spider 则可能包含很多重复代码。 如果我们将各个站点的 Spider 的公共部分保留下来，不同的部分提取出来作为单独的配置，如爬取规则、页面解析方式等抽离出来做成一个配置文件，那么我们在新增一个爬虫的时候，只需要实现这些网站的爬取规则和提取规则即可。 本节我们就来探究一下 Scrapy 通用爬虫的实现方法。</p>
                  <h3 id="1-CrawlSpider"><a href="#1-CrawlSpider" class="headerlink" title="1. CrawlSpider"></a>1. CrawlSpider</h3>
                  <p>在实现通用爬虫之前我们需要先了解一下 CrawlSpider，其官方文档链接为：<a href="http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider" target="_blank" rel="noopener">http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider</a>。 CrawlSpider 是 Scrapy 提供的一个通用 Spider。在 Spider 里，我们可以指定一些爬取规则来实现页面的提取，这些爬取规则由一个专门的数据结构 Rule 表示。Rule 里包含提取和跟进页面的配置，Spider 会根据 Rule 来确定当前页面中的哪些链接需要继续爬取、哪些页面的爬取结果需要用哪个方法解析等。 CrawlSpider 继承自 Spider 类。除了 Spider 类的所有方法和属性，它还提供了一个非常重要的属性和方法。</p>
                  <ul>
                    <li>rules，它是爬取规则属性，是包含一个或多个 Rule 对象的列表。每个 Rule 对爬取网站的动作都做了定义，CrawlSpider 会读取 rules 的每一个 Rule 并进行解析。</li>
                    <li>parse_start_url()，它是一个可重写的方法。当 start_urls 里对应的 Request 得到 Response 时，该方法被调用，它会分析 Response 并必须返回 Item 对象或者 Request 对象。</li>
                  </ul>
                  <p>这里最重要的内容莫过于 Rule 的定义了，它的定义和参数如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class scrapy.contrib.spiders.Rule(link_extractor, <span class="attribute">callback</span>=None, <span class="attribute">cb_kwargs</span>=None, <span class="attribute">follow</span>=None, <span class="attribute">process_links</span>=None, <span class="attribute">process_request</span>=None)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>下面对其参数依次说明：</p>
                  <ul>
                    <li>link_extractor，是一个 Link Extractor 对象。通过它，Spider 可以知道从爬取的页面中提取哪些链接。提取出的链接会自动生成 Request。它又是一个数据结构，一般常用 LxmlLinkExtractor 对象作为参数，其定义和参数如下所示：</li>
                  </ul>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), <span class="attribute">deny_extensions</span>=None, restrict_xpaths=(), restrict_css=(), tags=(<span class="string">'a'</span>, <span class="string">'area'</span>), attrs=(<span class="string">'href'</span>,), <span class="attribute">canonicalize</span>=<span class="literal">False</span>, <span class="attribute">unique</span>=<span class="literal">True</span>, <span class="attribute">process_value</span>=None, <span class="attribute">strip</span>=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>allow 是一个正则表达式或正则表达式列表，它定义了从当前页面提取出的链接哪些是符合要求的，只有符合要求的链接才会被跟进。deny 则相反。allow_domains 定义了符合要求的域名，只有此域名的链接才会被跟进生成新的 Request，它相当于域名白名单。deny_domains 则相反，相当于域名黑名单。restrict_xpaths 定义了从当前页面中 XPath 匹配的区域提取链接，其值是 XPath 表达式或 XPath 表达式列表。restrict_css 定义了从当前页面中 CSS 选择器匹配的区域提取链接，其值是 CSS 选择器或 CSS 选择器列表。还有一些其他参数代表了提取链接的标签、是否去重、链接的处理等内容，使用的频率不高。可以参考文档的参数说明：<a href="http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml" target="_blank" rel="noopener">http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml</a>。</p>
                  <ul>
                    <li>callback，即回调函数，和之前定义 Request 的 callback 有相同的意义。每次从 link_extractor 中获取到链接时，该函数将会调用。该回调函数接收一个 response 作为其第一个参数，并返回一个包含 Item 或 Request 对象的列表。注意，避免使用 parse() 作为回调函数。由于 CrawlSpider 使用 parse() 方法来实现其逻辑，如果 parse() 方法覆盖了，CrawlSpider 将会运行失败。</li>
                    <li>cb_kwargs，字典，它包含传递给回调函数的参数。</li>
                    <li>follow，布尔值，即 True 或 False，它指定根据该规则从 response 提取的链接是否需要跟进。如果 callback 参数为 None，follow 默认设置为 True，否则默认为 False。</li>
                    <li>process_links，指定处理函数，从 link_extractor 中获取到链接列表时，该函数将会调用，它主要用于过滤。</li>
                    <li>process_request，同样是指定处理函数，根据该 Rule 提取到每个 Request 时，该函数都会调用，对 Request 进行处理。该函数必须返回 Request 或者 None。</li>
                  </ul>
                  <p>以上内容便是 CrawlSpider 中的核心 Rule 的基本用法。但这些内容可能还不足以完成一个 CrawlSpider 爬虫。下面我们利用 CrawlSpider 实现新闻网站的爬取实例，来更好地理解 Rule 的用法。</p>
                  <h3 id="2-Item-Loader"><a href="#2-Item-Loader" class="headerlink" title="2. Item Loader"></a>2. Item Loader</h3>
                  <p>我们了解了利用 CrawlSpider 的 Rule 来定义页面的爬取逻辑，这是可配置化的一部分内容。但是，Rule 并没有对 Item 的提取方式做规则定义。对于 Item 的提取，我们需要借助另一个模块 Item Loader 来实现。 Item Loader 提供一种便捷的机制来帮助我们方便地提取 Item。它提供的一系列 API 可以分析原始数据对 Item 进行赋值。Item 提供的是保存抓取数据的容器，而 Item Loader 提供的是填充容器的机制。有了它，数据的提取会变得更加规则化。 Item Loader 的 API 如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">class</span> scrapy.loader.<span class="constructor">ItemLoader([<span class="params">item</span>, <span class="params">selector</span>, <span class="params">response</span>,] <span class="operator">**</span><span class="params">kwargs</span>)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Item Loader 的 API 返回一个新的 Item Loader 来填充给定的 Item。如果没有给出 Item，则使用 default_item_class 中的类自动实例化。另外，它传入 selector 和 response 参数来使用选择器或响应参数实例化。 下面将依次说明 Item Loader 的 API 参数。</p>
                  <ul>
                    <li>item，Item 对象，可以调用 add_xpath()、add_css() 或 add_value() 等方法来填充 Item 对象。</li>
                    <li>selector，Selector 对象，用来提取填充数据的选择器。</li>
                    <li>response，Response 对象，用于使用构造选择器的 Response。</li>
                  </ul>
                  <p>一个比较典型的 Item Loader 实例如下：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy.loader import ItemLoader</span><br><span class="line">from project.items import Product</span><br><span class="line"></span><br><span class="line">def parse(self, response):</span><br><span class="line">    loader = <span class="constructor">ItemLoader(<span class="params">item</span>=Product()</span>, response=response)</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">name</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"product_name"</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">name</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"product_title"</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">price</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">p</span>[@<span class="params">id</span>=<span class="string">"price"</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_css('<span class="params">stock</span>', '<span class="params">p</span>#<span class="params">stock</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">last_updated</span>', '<span class="params">today</span>')</span></span><br><span class="line">    return loader.load<span class="constructor">_item()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先声明一个 Product Item，用该 Item 和 Response 对象实例化 ItemLoader，调用 add_xpath() 方法把来自两个不同位置的数据提取出来，分配给 name 属性，再用 add_xpath()、add_css()、add_value() 等方法对不同属性依次赋值，最后调用 load_item() 方法实现 Item 的解析。这种方式比较规则化，我们可以把一些参数和规则单独提取出来做成配置文件或存到数据库，即可实现可配置化。 另外，Item Loader 每个字段中都包含了一个 Input Processor（输入处理器）和一个 Output Processor（输出处理器）。Input Processor 收到数据时立刻提取数据，Input Processor 的结果被收集起来并且保存在 ItemLoader 内，但是不分配给 Item。收集到所有的数据后，load_item() 方法被调用来填充再生成 Item 对象。在调用时会先调用 Output Processor 来处理之前收集到的数据，然后再存入 Item 中，这样就生成了 Item。 下面将介绍一些内置的 Processor。</p>
                  <h4 id="Identity"><a href="#Identity" class="headerlink" title="Identity"></a>Identity</h4>
                  <p>Identity 是最简单的 Processor，不进行任何处理，直接返回原来的数据。</p>
                  <h4 id="TakeFirst"><a href="#TakeFirst" class="headerlink" title="TakeFirst"></a>TakeFirst</h4>
                  <p>TakeFirst 返回列表的第一个非空值，类似 extract_first() 的功能，常用作 Output Processor，如下所示：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy<span class="selector-class">.loader</span><span class="selector-class">.processors</span> import TakeFirst</span><br><span class="line">processor = TakeFirst()</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(processor([<span class="string">''</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span></span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>经过此 Processor 处理后的结果返回了第一个不为空的值。</p>
                  <h4 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h4>
                  <p>Join 方法相当于字符串的 join() 方法，可以把列表拼合成字符串，字符串默认使用空格分隔，如下所示：</p>
                  <figure class="highlight gradle">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> <span class="keyword">Join</span></span><br><span class="line">processor = <span class="keyword">Join</span>()</span><br><span class="line"><span class="keyword">print</span>(processor([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">one</span> <span class="literal">two</span> <span class="literal">three</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>它也可以通过参数更改默认的分隔符，例如改成逗号：</p>
                  <figure class="highlight gradle">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> <span class="keyword">Join</span></span><br><span class="line">processor = <span class="keyword">Join</span>(<span class="string">','</span>)</span><br><span class="line"><span class="keyword">print</span>(processor([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">one</span>,<span class="literal">two</span>,<span class="literal">three</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="Compose"><a href="#Compose" class="headerlink" title="Compose"></a>Compose</h4>
                  <p>Compose 是用给定的多个函数的组合而构造的 Processor，每个输入值被传递到第一个函数，其输出再传递到第二个函数，依次类推，直到最后一个函数返回整个处理器的输出，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy.loader.processors import Compose</span><br><span class="line">processor = <span class="constructor">Compose(<span class="params">str</span>.<span class="params">upper</span>, <span class="params">lambda</span> <span class="params">s</span>: <span class="params">s</span>.<span class="params">strip</span>()</span>)</span><br><span class="line">print(processor(' hello world'))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">HELLO WORLD</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们构造了一个 Compose Processor，传入一个开头带有空格的字符串。Compose Processor 的参数有两个：第一个是 str.upper，它可以将字母全部转为大写；第二个是一个匿名函数，它调用 strip() 方法去除头尾空白字符。Compose 会顺次调用两个参数，最后返回结果的字符串全部转化为大写并且去除了开头的空格。</p>
                  <h4 id="MapCompose"><a href="#MapCompose" class="headerlink" title="MapCompose"></a>MapCompose</h4>
                  <p>与 Compose 类似，MapCompose 可以迭代处理一个列表输入值，如下所示：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy<span class="selector-class">.loader</span><span class="selector-class">.processors</span> import MapCompose</span><br><span class="line">processor = MapCompose(str<span class="selector-class">.upper</span>, lambda s: s.strip())</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(processor([<span class="string">'Hello'</span>, <span class="string">'World'</span>, <span class="string">'Python'</span>])</span></span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight scheme">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="symbol">'HELLO</span>', <span class="symbol">'WORLD</span>', <span class="symbol">'PYTHON</span>']</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>被处理的内容是一个可迭代对象，MapCompose 会将该对象遍历然后依次处理。</p>
                  <h4 id="SelectJmes"><a href="#SelectJmes" class="headerlink" title="SelectJmes"></a>SelectJmes</h4>
                  <p>SelectJmes 可以查询 JSON，传入 Key，返回查询所得的 Value。不过需要先安装 jmespath 库才可以使用它，命令如下所示：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> jmespath</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>安装好 jmespath 之后，便可以使用这个 Processor 了，如下所示：</p>
                  <figure class="highlight isbl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="variable">from</span> <span class="variable">scrapy.loader.processors</span> <span class="variable">import</span> <span class="variable">SelectJmes</span></span><br><span class="line"><span class="variable">proc</span> = <span class="function"><span class="title">SelectJmes</span>(<span class="string">'foo'</span>)</span></span><br><span class="line"><span class="variable">processor</span> = <span class="function"><span class="title">SelectJmes</span>(<span class="string">'foo'</span>)</span></span><br><span class="line"><span class="function"><span class="title">print</span>(<span class="title">processor</span>(&#123;<span class="string">'foo'</span>: <span class="string">'bar'</span>&#125;))</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">bar</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上内容便是一些常用的 Processor，在本节的实例中我们会使用 Processor 来进行数据的处理。 接下来，我们用一个实例来了解 Item Loader 的用法。</p>
                  <h3 id="3-本节目标"><a href="#3-本节目标" class="headerlink" title="3. 本节目标"></a>3. 本节目标</h3>
                  <p>我们以中华网科技类新闻为例，来了解 CrawlSpider 和 Item Loader 的用法，再提取其可配置信息实现可配置化。官网链接为：<a href="http://tech.china.com/。我们需要爬取它的科技类新闻内容，链接为：http://tech.china.com/articles/，页面如图" target="_blank" rel="noopener">http://tech.china.com/。我们需要爬取它的科技类新闻内容，链接为：http://tech.china.com/articles/，页面如图</a> 13-19 所示。 我们要抓取新闻列表中的所有分页的新闻详情，包括标题、正文、时间、来源等信息。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034144.png" alt=""> 图 13-19 爬取站点</p>
                  <h3 id="4-新建项目"><a href="#4-新建项目" class="headerlink" title="4. 新建项目"></a>4. 新建项目</h3>
                  <p>首先新建一个 Scrapy 项目，名为 scrapyuniversal，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject scrapyuniversal</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>创建一个 CrawlSpider，需要先制定一个模板。我们可以先看看有哪些可用模板，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy genspider -l</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">Available</span> templates:</span><br><span class="line">  <span class="keyword">basic</span></span><br><span class="line"><span class="keyword"> </span> crawl</span><br><span class="line">  csvfeed</span><br><span class="line">  xmlfeed</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>之前创建 Spider 的时候，我们默认使用了第一个模板 basic。这次要创建 CrawlSpider，就需要使用第二个模板 crawl，创建命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">-t</span> <span class="selector-tag">crawl</span> <span class="selector-tag">china</span> <span class="selector-tag">tech</span><span class="selector-class">.china</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后便会生成一个 CrawlSpider，其内容如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChinaSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'china'</span></span><br><span class="line">    allowed_domains = [<span class="string">'tech.china.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://tech.china.com/'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (Rule(LinkExtractor(allow=<span class="string">r'Items/'</span>), callback=<span class="string">'parse_item'</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        <span class="comment">#i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()</span></span><br><span class="line">        <span class="comment">#i['name'] = response.xpath('//div[@id="name"]').extract()</span></span><br><span class="line">        <span class="comment">#i['description'] = response.xpath('//div[@id="description"]').extract()</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次生成的 Spider 内容多了一个 rules 属性的定义。Rule 的第一个参数是 LinkExtractor，就是上文所说的 LxmlLinkExtractor，只是名称不同。同时，默认的回调函数也不再是 parse，而是 parse_item。</p>
                  <h3 id="5-定义-Rule"><a href="#5-定义-Rule" class="headerlink" title="5. 定义 Rule"></a>5. 定义 Rule</h3>
                  <p>要实现新闻的爬取，我们需要做的就是定义好 Rule，然后实现解析函数。下面我们就来一步步实现这个过程。 首先将 start_urls 修改为起始链接，代码如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">start_urls</span> = [<span class="string">'http://tech.china.com/articles/'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>之后，Spider 爬取 start_urls 里面的每一个链接。所以这里第一个爬取的页面就是我们刚才所定义的链接。得到 Response 之后，Spider 就会根据每一个 Rule 来提取这个页面内的超链接，去生成进一步的 Request。接下来，我们就需要定义 Rule 来指定提取哪些链接。 当前页面如图 13-20 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-034532.png" alt=""> 图 13-20 页面内容 这是新闻的列表页，下一步自然就是将列表中的每条新闻详情的链接提取出来。这里直接指定这些链接所在区域即可。查看源代码，所有链接都在 ID 为 left_side 的节点内，具体来说是它内部的 class 为 con_item 的节点，如图 13-21 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034527.jpg" alt=""> 图 13-21 列表源码 此处我们可以用 LinkExtractor 的 restrict_xpaths 属性来指定，之后 Spider 就会从这个区域提取所有的超链接并生成 Request。但是，每篇文章的导航中可能还有一些其他的超链接标签，我们只想把需要的新闻链接提取出来。真正的新闻链接路径都是以 article 开头的，我们用一个正则表达式将其匹配出来再赋值给 allow 参数即可。另外，这些链接对应的页面其实就是对应的新闻详情页，而我们需要解析的就是新闻的详情信息，所以此处还需要指定一个回调函数 callback。 到现在我们就可以构造出一个 Rule 了，代码如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="constructor">Rule(LinkExtractor(<span class="params">allow</span>='<span class="params">article</span><span class="operator">/</span>.<span class="operator">*</span>.<span class="params">html</span>', <span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"left_side"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"con_item"</span>]')</span>, callback='parse_item')</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来，我们还要让当前页面实现分页功能，所以还需要提取下一页的链接。分析网页源码之后可以发现下一页链接是在 ID 为 pageStyle 的节点内，如图 13-22 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034519.jpg" alt=""> 图 13-22 分页源码 但是，下一页节点和其他分页链接区分度不高，要取出此链接我们可以直接用 XPath 的文本匹配方式，所以这里我们直接用 LinkExtractor 的 restrict_xpaths 属性来指定提取的链接即可。另外，我们不需要像新闻详情页一样去提取此分页链接对应的页面详情信息，也就是不需要生成 Item，所以不需要加 callback 参数。另外这下一页的页面如果请求成功了就需要继续像上述情况一样分析，所以它还需要加一个 follow 参数为 True，代表继续跟进匹配分析。其实，follow 参数也可以不加，因为当 callback 为空的时候，follow 默认为 True。此处 Rule 定义为如下所示：</p>
                  <figure class="highlight lisp">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Rule(<span class="name">LinkExtractor</span>(<span class="name">restrict_xpaths=</span>'//div[@id=<span class="string">"pageStyle"</span>]//a[contains(., <span class="string">"下一页"</span>)]'))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>所以现在 rules 就变成了：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">rules = (<span class="constructor">Rule(LinkExtractor(<span class="params">allow</span>='<span class="params">article</span><span class="operator">/</span>.<span class="operator">*</span>.<span class="params">html</span>', <span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"left_side"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"con_item"</span>]')</span>, callback='parse_item'),</span><br><span class="line">    <span class="constructor">Rule(LinkExtractor(<span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"pageStyle"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">a</span>[<span class="params">contains</span>(., <span class="string">"下一页"</span>)</span>]'))</span><br><span class="line">)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接着我们运行一下代码，命令如下：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl china</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>现在已经实现页面的翻页和详情页的抓取了，我们仅仅通过定义了两个 Rule 即实现了这样的功能，运行效果如图 13-23 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034515.jpg" alt=""> 图 13-23 运行效果</p>
                  <h3 id="6-解析页面"><a href="#6-解析页面" class="headerlink" title="6. 解析页面"></a>6. 解析页面</h3>
                  <p>接下来我们需要做的就是解析页面内容了，将标题、发布时间、正文、来源提取出来即可。首先定义一个 Item，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Field, Item</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="symbol">NewsItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">title</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">url</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">text</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">datetime</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">source</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">website</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的字段分别指新闻标题、链接、正文、发布时间、来源、站点名称，其中站点名称直接赋值为中华网。因为既然是通用爬虫，肯定还有很多爬虫也来爬取同样结构的其他站点的新闻内容，所以需要一个字段来区分一下站点名称。 详情页的预览图如图 13-24 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034510.png" alt=""> 图 13-24 详情页面 如果像之前一样提取内容，就直接调用 response 变量的 xpath()、css() 等方法即可。这里 parse_item() 方法的实现如下所示：</p>
                  <figure class="highlight xquery">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_item(self, response):</span><br><span class="line">    <span class="type">item</span> = NewsItem()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'title'</span>] = response.xpath(<span class="string">'//h1[@id="chan_newsTitle"]/text()'</span>).extract_first()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'url'</span>] = response.url</span><br><span class="line">    <span class="type">item</span>[<span class="string">'text'</span>] = <span class="string">''</span>.join(response.xpath(<span class="string">'//div[@id="chan_newsDetail"]//text()'</span>).extract()).<span class="keyword">strip</span>()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'datetime'</span>] = response.xpath(<span class="string">'//div[@id="chan_newsInfo"]/text()'</span>).re_first(<span class="string">'(d+-d+-d+sd+:d+:d+)'</span>)</span><br><span class="line">    <span class="type">item</span>[<span class="string">'source'</span>] = response.xpath(<span class="string">'//div[@id="chan_newsInfo"]/text()'</span>).re_first(<span class="string">' 来源：(.*)'</span>).<span class="keyword">strip</span>()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'website'</span>] = <span class="string">' 中华网 '</span></span><br><span class="line">    yield <span class="type">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就把每条新闻的信息提取形成了一个 NewsItem 对象。 这时实际上我们就已经完成了 Item 的提取。再运行一下 Spider，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl china</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出内容如图 13-25 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-034503.jpg" alt=""> 图 13-25 输出内容 现在我们就可以成功将每条新闻的信息提取出来。 不过我们发现这种提取方式非常不规整。下面我们再用 Item Loader，通过 add_xpath()、add_css()、add_value() 等方式实现配置化提取。我们可以改写 parse_item()，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse<span class="constructor">_item(<span class="params">self</span>, <span class="params">response</span>)</span>:</span><br><span class="line">    loader = <span class="constructor">ChinaLoader(<span class="params">item</span>=NewsItem()</span>, response=response)</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">title</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">h1</span>[@<span class="params">id</span>=<span class="string">"chan_newsTitle"</span>]<span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">url</span>', <span class="params">response</span>.<span class="params">url</span>)</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">text</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsDetail"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">datetime</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re='(d+-d+-d+sd+:d+:d+)')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">source</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re=' 来源：(.*)')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">website</span>', ' 中华网 ')</span></span><br><span class="line">    yield loader.load<span class="constructor">_item()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 ItemLoader 的子类，名为 ChinaLoader，其实现如下所示：</p>
                  <figure class="highlight haskell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="title">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="title">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst, Join, Compose</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">NewsLoader</span>(<span class="type">ItemLoader</span>):</span></span><br><span class="line"><span class="class">    default_output_processor = <span class="type">TakeFirst</span>()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">ChinaLoader</span>(<span class="type">NewsLoader</span>):</span></span><br><span class="line"><span class="class">    text_out = <span class="type">Compose</span>(<span class="type">Join</span>(), lambda s: s.strip())</span></span><br><span class="line"><span class="class">    source_out = <span class="type">Compose</span>(<span class="type">Join</span>(), lambda s: s.strip())</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>ChinaLoader 继承了 NewsLoader 类，其内定义了一个通用的 Out Processor 为 TakeFirst，这相当于之前所定义的 extract_first() 方法的功能。我们在 ChinaLoader 中定义了 text_out 和 source_out 字段。这里使用了一个 Compose Processor，它有两个参数：第一个参数 Join 也是一个 Processor，它可以把列表拼合成一个字符串；第二个参数是一个匿名函数，可以将字符串的头尾空白字符去掉。经过这一系列处理之后，我们就将列表形式的提取结果转化为去除头尾空白字符的字符串。 代码重新运行，提取效果是完全一样的。 至此，我们已经实现了爬虫的半通用化配置。</p>
                  <h3 id="7-通用配置抽取"><a href="#7-通用配置抽取" class="headerlink" title="7. 通用配置抽取"></a>7. 通用配置抽取</h3>
                  <p>为什么现在只做到了半通用化？如果我们需要扩展其他站点，仍然需要创建一个新的 CrawlSpider，定义这个站点的 Rule，单独实现 parse_item() 方法。还有很多代码是重复的，如 CrawlSpider 的变量、方法名几乎都是一样的。那么我们可不可以把多个类似的几个爬虫的代码共用，把完全不相同的地方抽离出来，做成可配置文件呢？ 当然可以。那我们可以抽离出哪些部分？所有的变量都可以抽取，如 name、allowed_domains、start_urls、rules 等。这些变量在 CrawlSpider 初始化的时候赋值即可。我们就可以新建一个通用的 Spider 来实现这个功能，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy genspider -t crawl universal universal</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个全新的 Spider 名为 universal。接下来，我们将刚才所写的 Spider 内的属性抽离出来配置成一个 JSON，命名为 china.json，放到 configs 文件夹内，和 spiders 文件夹并列，代码如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"spider"</span>: <span class="string">"universal"</span>,</span><br><span class="line">  <span class="attr">"website"</span>: <span class="string">"中华网科技"</span>,</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"新闻"</span>,</span><br><span class="line">  <span class="attr">"index"</span>: <span class="string">"http://tech.china.com/"</span>,</span><br><span class="line">  <span class="attr">"settings"</span>: &#123;<span class="attr">"USER_AGENT"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"start_urls"</span>: [<span class="string">"http://tech.china.com/articles/"</span>],</span><br><span class="line">  <span class="attr">"allowed_domains"</span>: [<span class="string">"tech.china.com"</span>],</span><br><span class="line">  <span class="attr">"rules"</span>: <span class="string">"china"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一个字段 spider 即 Spider 的名称，在这里是 universal。后面是站点的描述，比如站点名称、类型、首页等。随后的 settings 是该 Spider 特有的 settings 配置，如果要覆盖全局项目，settings.py 内的配置可以单独为其配置。随后是 Spider 的一些属性，如 start_urls、allowed_domains、rules 等。rules 也可以单独定义成一个 rules.py 文件，做成配置文件，实现 Rule 的分离，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import Rule</span><br><span class="line"></span><br><span class="line">rules = &#123;</span><br><span class="line">    'china': (<span class="constructor">Rule(LinkExtractor(<span class="params">allow</span>='<span class="params">article</span><span class="operator">/</span>.<span class="operator">*</span>.<span class="params">html</span>', <span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"left_side"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"con_item"</span>]')</span>,</span><br><span class="line">             callback='parse_item'),</span><br><span class="line">        <span class="constructor">Rule(LinkExtractor(<span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"pageStyle"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">a</span>[<span class="params">contains</span>(., <span class="string">"下一页"</span>)</span>]'))</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们将基本的配置抽取出来。如果要启动爬虫，只需要从该配置文件中读取然后动态加载到 Spider 中即可。所以我们需要定义一个读取该 JSON 文件的方法，如下所示：</p>
                  <figure class="highlight xl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from os.<span class="built_in">path</span> <span class="keyword">import</span> realpath, dirname</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">def get_config(<span class="keyword">name</span>):</span><br><span class="line">    <span class="built_in">path</span> = dirname(realpath(__file__)) + <span class="string">'/configs/'</span> + <span class="keyword">name</span> + <span class="string">'.json'</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="built_in">path</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        return json.loads(f.read())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>定义了 get_config() 方法之后，我们只需要向其传入 JSON 配置文件的名称即可获取此 JSON 配置信息。随后我们定义入口文件 run.py，把它放在项目根目录下，它的作用是启动 Spider，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import sys</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project import get_project_settings</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.spiders.universal import UniversalSpider</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.utils import get_config</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler import CrawlerProcess</span><br><span class="line"></span><br><span class="line">def <span class="builtin-name">run</span>():</span><br><span class="line">    name = sys.argv[1]</span><br><span class="line">    custom_settings = get_config(name)</span><br><span class="line">    # 爬取使用的 Spider 名称</span><br><span class="line">    spider = custom_settings.<span class="builtin-name">get</span>(<span class="string">'spider'</span>, <span class="string">'universal'</span>)</span><br><span class="line">    project_settings = get_project_settings()</span><br><span class="line">   <span class="built_in"> settings </span>= dict(project_settings.copy())</span><br><span class="line">    # 合并配置</span><br><span class="line">    settings.update(custom_settings.<span class="builtin-name">get</span>(<span class="string">'settings'</span>))</span><br><span class="line">    process = CrawlerProcess(settings)</span><br><span class="line">    # 启动爬虫</span><br><span class="line">    process.crawl(spider, **&#123;<span class="string">'name'</span>: name&#125;)</span><br><span class="line">    process.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="builtin-name">run</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行入口为 run()。首先获取命令行的参数并赋值为 name，name 就是 JSON 文件的名称，其实就是要爬取的目标网站的名称。我们首先利用 get_config() 方法，传入该名称读取刚才定义的配置文件。获取爬取使用的 spider 的名称、配置文件中的 settings 配置，然后将获取到的 settings 配置和项目全局的 settings 配置做了合并。新建一个 CrawlerProcess，传入爬取使用的配置。调用 crawl() 和 start() 方法即可启动爬取。 在 universal 中，我们新建一个<strong>init</strong>() 方法，进行初始化配置，实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors import LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.utils import get_config</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.rules import rules</span><br><span class="line"></span><br><span class="line">class UniversalSpider(CrawlSpider):</span><br><span class="line">    name = <span class="string">'universal'</span></span><br><span class="line">    def __init__(self, name, <span class="number">*a</span>rgs, **kwargs):</span><br><span class="line">       <span class="built_in"> config </span>= get_config(name)</span><br><span class="line">        self.config = config</span><br><span class="line">        self.rules = rules.<span class="builtin-name">get</span>(config.<span class="builtin-name">get</span>(<span class="string">'rules'</span>))</span><br><span class="line">        self.start_urls = config.<span class="builtin-name">get</span>(<span class="string">'start_urls'</span>)</span><br><span class="line">        self.allowed_domains = config.<span class="builtin-name">get</span>(<span class="string">'allowed_domains'</span>)</span><br><span class="line">        super(UniversalSpider, self).__init__(<span class="number">*a</span>rgs, **kwargs)</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        return i</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 <strong>init</strong>() 方法中，start_urls、allowed_domains、rules 等属性被赋值。其中，rules 属性另外读取了 rules.py 的配置，这样就成功实现爬虫的基础配置。 接下来，执行如下命令运行爬虫：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 <span class="keyword">run</span>.<span class="bash">py china</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>程序会首先读取 JSON 配置文件，将配置中的一些属性赋值给 Spider，然后启动爬取。运行效果完全相同，运行结果如图 13-26 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034600.jpg" alt=""> 图 13-26 运行结果 现在我们已经对 Spider 的基础属性实现了可配置化。剩下的解析部分同样需要实现可配置化，原来的解析函数如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse<span class="constructor">_item(<span class="params">self</span>, <span class="params">response</span>)</span>:</span><br><span class="line">    loader = <span class="constructor">ChinaLoader(<span class="params">item</span>=NewsItem()</span>, response=response)</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">title</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">h1</span>[@<span class="params">id</span>=<span class="string">"chan_newsTitle"</span>]<span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">url</span>', <span class="params">response</span>.<span class="params">url</span>)</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">text</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsDetail"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">datetime</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re='(d+-d+-d+sd+:d+:d+)')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">source</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re=' 来源：(.*)')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">website</span>', ' 中华网 ')</span></span><br><span class="line">    yield loader.load<span class="constructor">_item()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们需要将这些配置也抽离出来。这里的变量主要有 Item Loader 类的选用、Item 类的选用、Item Loader 方法参数的定义，我们可以在 JSON 文件中添加如下 item 的配置：</p>
                  <figure class="highlight prolog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">"item"</span>: &#123;</span><br><span class="line">  <span class="string">"class"</span>: <span class="string">"NewsItem"</span>,</span><br><span class="line">  <span class="string">"loader"</span>: <span class="string">"ChinaLoader"</span>,</span><br><span class="line">  <span class="string">"attrs"</span>: &#123;</span><br><span class="line">    <span class="string">"title"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//h1[@id='chan_newsTitle']/text()"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"url"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"attr"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"url"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"text"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsDetail']//text()"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"datetime"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsInfo']/text()"</span>],</span><br><span class="line">        <span class="string">"re"</span>: <span class="string">"(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"source"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsInfo']/text()"</span>],</span><br><span class="line">        <span class="string">"re"</span>: <span class="string">"来源：(.*)"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"website"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"value"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"中华网"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了 class 和 loader 属性，它们分别代表 Item 和 Item Loader 所使用的类。定义了 attrs 属性来定义每个字段的提取规则，例如，title 定义的每一项都包含一个 method 属性，它代表使用的提取方法，如 xpath 即代表调用 Item Loader 的 add_xpath() 方法。args 即参数，就是 add_xpath() 的第二个参数，即 XPath 表达式。针对 datetime 字段，我们还用了一次正则提取，所以这里还可以定义一个 re 参数来传递提取时所使用的正则表达式。 我们还要将这些配置之后动态加载到 parse_item() 方法里。最后，最重要的就是实现 parse_item() 方法，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_item(self, response):</span><br><span class="line">   item = self.config.<span class="builtin-name">get</span>(<span class="string">'item'</span>)</span><br><span class="line">   <span class="keyword">if</span> item:</span><br><span class="line">       cls = eval(item.<span class="builtin-name">get</span>(<span class="string">'class'</span>))()</span><br><span class="line">       loader = eval(item.<span class="builtin-name">get</span>(<span class="string">'loader'</span>))(cls, <span class="attribute">response</span>=response)</span><br><span class="line">       # 动态获取属性配置</span><br><span class="line">       <span class="keyword">for</span> key, value <span class="keyword">in</span> item.<span class="builtin-name">get</span>(<span class="string">'attrs'</span>).items():</span><br><span class="line">           <span class="keyword">for</span> extractor <span class="keyword">in</span> value:</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'xpath'</span>:</span><br><span class="line">                   loader.add_xpath(key, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.<span class="builtin-name">get</span>(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'css'</span>:</span><br><span class="line">                   loader.add_css(key, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.<span class="builtin-name">get</span>(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'value'</span>:</span><br><span class="line">                   loader.add_value(key, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.<span class="builtin-name">get</span>(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'attr'</span>:</span><br><span class="line">                   loader.add_value(key, getattr(response, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>)))</span><br><span class="line">       yield loader.load_item()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先获取 Item 的配置信息，然后获取 class 的配置，将其初始化，初始化 Item Loader，遍历 Item 的各个属性依次进行提取。判断 method 字段，调用对应的处理方法进行处理。如 method 为 css，就调用 Item Loader 的 add_css() 方法进行提取。所有配置动态加载完毕之后，调用 load_item() 方法将 Item 提取出来。 重新运行程序，结果如图 13-27 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034606.jpg" alt=""> 图 13-27 运行结果 运行结果是完全相同的。 我们再回过头看一下 start_urls 的配置。这里 start_urls 只可以配置具体的链接。如果这些链接有 100 个、1000 个，我们总不能将所有的链接全部列出来吧？在某些情况下，start_urls 也需要动态配置。我们将 start_urls 分成两种，一种是直接配置 URL 列表，一种是调用方法生成，它们分别定义为 static 和 dynamic 类型。 本例中的 start_urls 很明显是 static 类型的，所以 start_urls 配置改写如下所示： ```json”start_urls”: {“type”:”static”,”value”: [“<a href="http://tech.china.com/articles/" target="_blank" rel="noopener">http://tech.china.com/articles/</a>“] }</p>
                  <figure class="highlight autohotkey">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">如果 start_urls 是动态生成的，我们可以调用方法传参数，如下所示：</span><br><span class="line">```json</span><br><span class="line"><span class="string">"start_urls"</span>: &#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"dynamic"</span>,</span><br><span class="line">  <span class="string">"method"</span>: <span class="string">"china"</span>,</span><br><span class="line">  <span class="string">"args"</span>: [<span class="number">5</span>, <span class="number">10</span>]</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 start_urls 定义为 dynamic 类型，指定方法为 urls_china()，然后传入参数 5 和 10，来生成第 5 到 10 页的链接。这样我们只需要实现该方法即可，统一新建一个 urls.py 文件，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def china(start, end):</span><br><span class="line">    <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(start, end + 1):</span><br><span class="line">        yield <span class="string">'http://tech.china.com/articles/index_'</span> + str(page) + <span class="string">'.html'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其他站点可以自行配置。如某些链接需要用到时间戳，加密参数等，均可通过自定义方法实现。 接下来在 Spider 的 <strong>init</strong>() 方法中，start_urls 的配置改写如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapyuniversal import urls</span><br><span class="line"></span><br><span class="line">start_urls = config.<span class="builtin-name">get</span>(<span class="string">'start_urls'</span>)</span><br><span class="line"><span class="keyword">if</span> start_urls:</span><br><span class="line">    <span class="keyword">if</span> start_urls.<span class="builtin-name">get</span>(<span class="string">'type'</span>) == <span class="string">'static'</span>:</span><br><span class="line">        self.start_urls = start_urls.<span class="builtin-name">get</span>(<span class="string">'value'</span>)</span><br><span class="line">    elif start_urls.<span class="builtin-name">get</span>(<span class="string">'type'</span>) == <span class="string">'dynamic'</span>:</span><br><span class="line">        self.start_urls = list(eval(<span class="string">'urls.'</span> + start_urls.<span class="builtin-name">get</span>(<span class="string">'method'</span>))(*start_urls.<span class="builtin-name">get</span>(<span class="string">'args'</span>, [])))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里通过判定 start_urls 的类型分别进行不同的处理，这样我们就可以实现 start_urls 的配置了。 至此，Spider 的设置、起始链接、属性、提取方法都已经实现了全部的可配置化。 综上所述，整个项目的配置包括如下内容。</p>
                  <ul>
                    <li>spider，指定所使用的 Spider 的名称。</li>
                    <li>settings，可以专门为 Spider 定制配置信息，会覆盖项目级别的配置。</li>
                    <li>start_urls，指定爬虫爬取的起始链接。</li>
                    <li>allowed_domains，允许爬取的站点。</li>
                    <li>rules，站点的爬取规则。</li>
                    <li>item，数据的提取规则。</li>
                  </ul>
                  <p>我们实现了 Scrapy 的通用爬虫，每个站点只需要修改 JSON 文件即可实现自由配置。</p>
                  <h3 id="7-本节代码"><a href="#7-本节代码" class="headerlink" title="7. 本节代码"></a>7. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyUniversal" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyUniversal</a>。</p>
                  <h3 id="8-结语"><a href="#8-结语" class="headerlink" title="8. 结语"></a>8. 结语</h3>
                  <p>本节介绍了 Scrapy 通用爬虫的实现。我们将所有配置抽离出来，每增加一个爬虫，就只需要增加一个 JSON 文件配置。之后我们只需要维护这些配置文件即可。如果要更加方便的管理，可以将规则存入数据库，再对接可视化管理页面即可。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-06 09:30:05" itemprop="dateCreated datePublished" datetime="2019-12-06T09:30:05+08:00">2019-12-06</time>
                </span>
                <span id="/8413.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.10–Scrapy 通用爬虫" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>18k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>16 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8410.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8410.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.9–Scrapy 对接 Splash</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-9-Scrapy-对接-Splash"><a href="#13-9-Scrapy-对接-Splash" class="headerlink" title="13.9 Scrapy 对接 Splash"></a>13.9 Scrapy 对接 Splash</h1>
                  <p>在上一节我们实现了 Scrapy 对接 Selenium 抓取淘宝商品的过程，这是一种抓取 JavaScript 动态渲染页面的方式。除了 Selenium，Splash 也可以实现同样的功能。本节我们来了解 Scrapy 对接 Splash 来进行页面抓取的方式。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请确保 Splash 已经正确安装并正常运行，同时安装好 Scrapy-Splash 库，如果没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="2-新建项目"><a href="#2-新建项目" class="headerlink" title="2. 新建项目"></a>2. 新建项目</h3>
                  <p>首先新建一个项目，名为 scrapysplashtest，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject scrapysplashtest</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建一个 Spider，命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">taobao</span> <span class="selector-tag">www</span><span class="selector-class">.taobao</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="3-添加配置"><a href="#3-添加配置" class="headerlink" title="3. 添加配置"></a>3. 添加配置</h3>
                  <p>可以参考 Scrapy-Splash 的配置说明进行一步步的配置，链接如下：<a href="https://github.com/scrapy-plugins/scrapy-splash#configuration" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-splash#configuration</a>。 修改 settings.py，配置 SPLASH_URL。在这里我们的 Splash 是在本地运行的，所以可以直接配置本地的地址：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SPLASH_URL</span> = <span class="string">'http://localhost:8050'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果 Splash 是在远程服务器运行的，那此处就应该配置为远程的地址。例如运行在 IP 为 120.27.34.25 的服务器上，则此处应该配置为：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SPLASH_URL</span> = <span class="string">'http://120.27.34.25:8050'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>还需要配置几个 Middleware，代码如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">DOWNLOADER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashCookiesMiddleware':</span> <span class="number">723</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashMiddleware':</span> <span class="number">725</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">810</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;'scrapy_splash.SplashDeduplicateArgsMiddleware':</span> <span class="number">100</span><span class="string">,&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里配置了三个 Downloader Middleware 和一个 Spider Middleware，这是 Scrapy-Splash 的核心部分。我们不再需要像对接 Selenium 那样实现一个 Downloader Middleware，Scrapy-Splash 库都为我们准备好了，直接配置即可。 还需要配置一个去重的类 DUPEFILTER_CLASS，代码如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">DUPEFILTER_CLASS</span> = <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>最后配置一个 Cache 存储 HTTPCACHE_STORAGE，代码如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">HTTPCACHE_STORAGE</span> = <span class="string">'scrapy_splash.SplashAwareFSCacheStorage'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="4-新建请求"><a href="#4-新建请求" class="headerlink" title="4. 新建请求"></a>4. 新建请求</h3>
                  <p>配置完成之后，我们就可以利用 Splash 来抓取页面了。我们可以直接生成一个 SplashRequest 对象并传递相应的参数，Scrapy 会将此请求转发给 Splash，Splash 对页面进行渲染加载，然后再将渲染结果传递回来。此时 Response 的内容就是渲染完成的页面结果了，最后交给 Spider 解析即可。 我们来看一个示例，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">yield SplashRequest(url, self.parse_result,</span><br><span class="line">    args=&#123;</span><br><span class="line">        # optional; parameters passed <span class="keyword">to</span> Splash HTTP API</span><br><span class="line">        <span class="string">'wait'</span>: 0.5,</span><br><span class="line">        # <span class="string">'url'</span> is prefilled <span class="keyword">from</span> request url</span><br><span class="line">        # <span class="string">'http_method'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> <span class="string">'POST'</span> <span class="keyword">for</span> POST requests</span><br><span class="line">        # <span class="string">'body'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> request body <span class="keyword">for</span> POST requests</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attribute">endpoint</span>=<span class="string">'render.json'</span>, # optional;<span class="built_in"> default </span>is render.html</span><br><span class="line">    <span class="attribute">splash_url</span>=<span class="string">'&lt;url&gt;'</span>,     # optional; overrides SPLASH_URL</span><br><span class="line">)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里构造了一个 SplashRequest 对象，前两个参数依然是请求的 URL 和回调函数，另外还可以通过 args 传递一些渲染参数，例如等待时间 wait 等，还可以根据 endpoint 参数指定渲染接口，另外还有更多的参数可以参考文档的说明：<a href="https://github.com/scrapy-plugins/scrapy-splash#requests" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-splash#requests</a>。 另外我们也可以生成 Request 对象，关于 Splash 的配置通过 meta 属性配置即可，代码如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">yield scrapy.Request(url, self.parse_result, meta=&#123;</span><br><span class="line">    <span class="string">'splash'</span>: &#123;</span><br><span class="line">        <span class="string">'args'</span>: &#123;</span><br><span class="line">            # <span class="builtin-name">set</span> rendering arguments here</span><br><span class="line">            <span class="string">'html'</span>: 1,</span><br><span class="line">            <span class="string">'png'</span>: 1,</span><br><span class="line">            # <span class="string">'url'</span> is prefilled <span class="keyword">from</span> request url</span><br><span class="line">            # <span class="string">'http_method'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> <span class="string">'POST'</span> <span class="keyword">for</span> POST requests</span><br><span class="line">            # <span class="string">'body'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> request body <span class="keyword">for</span> POST requests</span><br><span class="line">        &#125;,</span><br><span class="line">        # optional parameters</span><br><span class="line">        <span class="string">'endpoint'</span>: <span class="string">'render.json'</span>,  # optional;<span class="built_in"> default </span>is render.json</span><br><span class="line">        <span class="string">'splash_url'</span>: <span class="string">'&lt;url&gt;'</span>,      # optional; overrides SPLASH_URL</span><br><span class="line">        <span class="string">'slot_policy'</span>: scrapy_splash.SlotPolicy.PER_DOMAIN,</span><br><span class="line">        <span class="string">'splash_headers'</span>: &#123;&#125;,       # optional; a dict with headers sent <span class="keyword">to</span> Splash</span><br><span class="line">        <span class="string">'dont_process_response'</span>: <span class="literal">True</span>, # optional,<span class="built_in"> default </span>is <span class="literal">False</span></span><br><span class="line">        <span class="string">'dont_send_headers'</span>: <span class="literal">True</span>,  # optional,<span class="built_in"> default </span>is <span class="literal">False</span></span><br><span class="line">        <span class="string">'magic_response'</span>: <span class="literal">False</span>,    # optional,<span class="built_in"> default </span>is <span class="literal">True</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>SplashRequest 对象通过 args 来配置和 Request 对象通过 meta 来配置，两种方式达到的效果是相同的。 本节我们要做的抓取是淘宝商品信息，涉及页面加载等待、模拟点击翻页等操作。我们可以首先定义一个 Lua 脚本，来实现页面加载、模拟点击翻页的功能，代码如下所示：</p>
                  <figure class="highlight irpf90">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">function</span></span> main(splash, args)</span><br><span class="line">  args = &#123;</span><br><span class="line">    url=<span class="string">"https://s.taobao.com/search?q=iPad"</span>,</span><br><span class="line">    <span class="keyword">wait</span>=<span class="number">5</span>,</span><br><span class="line">    page=<span class="number">5</span></span><br><span class="line">  &#125;</span><br><span class="line">  splash.images_enabled = false</span><br><span class="line">  <span class="keyword">assert</span>(splash:go(args.url))</span><br><span class="line">  <span class="keyword">assert</span>(splash:<span class="keyword">wait</span>(args.<span class="keyword">wait</span>))</span><br><span class="line">  js = string.<span class="keyword">format</span>(<span class="string">"document.querySelector('#mainsrp-pager div.form&gt; input').value=% d;document.querySelector('#mainsrp-pager div.form&gt; span.btn.J_Submit').click()"</span>, args.page)</span><br><span class="line">  splash:evaljs(js)</span><br><span class="line">  <span class="keyword">assert</span>(splash:<span class="keyword">wait</span>(args.<span class="keyword">wait</span>))</span><br><span class="line">  <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们定义了三个参数：请求的链接 url、等待时间 wait、分页页码 page。然后禁用图片加载，请求淘宝的商品列表页面，通过 evaljs() 方法调用 JavaScript 代码，实现页码填充和翻页点击，最后返回页面截图。我们将脚本放到 Splash 中运行，正常获取到页面截图，如图 13-15 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034117.jpg" alt=""> 图 13-15 页面截图 翻页操作也成功实现，如图 13-16 所示即为当前页码，和我们传入的页码 page 参数是相同的。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034121.jpg" alt=""> 图 13-16 翻页结果 我们只需要在 Spider 里用 SplashRequest 对接 Lua 脚本就好了，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy import Spider</span><br><span class="line"><span class="keyword">from</span> urllib.parse import quote</span><br><span class="line"><span class="keyword">from</span> scrapysplashtest.items import ProductItem</span><br><span class="line"><span class="keyword">from</span> scrapy_splash import SplashRequest</span><br><span class="line"></span><br><span class="line">script = <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">function main(splash, args)</span></span><br><span class="line"><span class="string">  splash.images_enabled = false</span></span><br><span class="line"><span class="string">  assert(splash:go(args.url))</span></span><br><span class="line"><span class="string">  assert(splash:wait(args.wait))</span></span><br><span class="line"><span class="string">  js = string.format("</span>document.querySelector(<span class="string">'#mainsrp-pager div.form&gt; input'</span>).<span class="attribute">value</span>=% d;document.querySelector(<span class="string">'#mainsrp-pager div.form&gt; span.btn.J_Submit'</span>).click()<span class="string">", args.page)</span></span><br><span class="line"><span class="string">  splash:evaljs(js)</span></span><br><span class="line"><span class="string">  assert(splash:wait(args.wait))</span></span><br><span class="line"><span class="string">  return splash:html()</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">class TaobaoSpider(Spider):</span><br><span class="line">    name = <span class="string">'taobao'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.taobao.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://s.taobao.com/search?q='</span></span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.settings.<span class="builtin-name">get</span>(<span class="string">'KEYWORDS'</span>):</span><br><span class="line">            <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(1, self.settings.<span class="builtin-name">get</span>(<span class="string">'MAX_PAGE'</span>) + 1):</span><br><span class="line">                url = self.base_url + quote(keyword)</span><br><span class="line">                yield SplashRequest(url, <span class="attribute">callback</span>=self.parse, <span class="attribute">endpoint</span>=<span class="string">'execute'</span>, args=&#123;<span class="string">'lua_source'</span>: script, <span class="string">'page'</span>: page, <span class="string">'wait'</span>: 7&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们把 Lua 脚本定义成长字符串，通过 SplashRequest 的 args 来传递参数，接口修改为 execute。另外，args 参数里还有一个 lua_source 字段用于指定 Lua 脚本内容。这样我们就成功构造了一个 SplashRequest，对接 Splash 的工作就完成了。 其他的配置不需要更改，Item、Item Pipeline 等设置与上节对接 Selenium 的方式相同，parse() 回调函数也是完全一致的。</p>
                  <h3 id="5-运行"><a href="#5-运行" class="headerlink" title="5. 运行"></a>5. 运行</h3>
                  <p>接下来，我们通过如下命令运行爬虫：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl taobao</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如图 13-17 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034129.jpg" alt=""> 图 13-17 运行结果 由于 Splash 和 Scrapy 都支持异步处理，我们可以看到同时会有多个抓取成功的结果。在 Selenium 的对接过程中，每个页面渲染下载是在 Downloader Middleware 里完成的，所以整个过程是阻塞式的。Scrapy 会等待这个过程完成后再继续处理和调度其他请求，这影响了爬取效率。因此使用 Splash 的爬取效率比 Selenium 高很多。 最后我们再看看 MongoDB 的结果，如图 13-18 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034134.jpg" alt=""> 图 13-18 存储结果 结果同样正常保存到了 MongoDB 中。</p>
                  <h3 id="6-本节代码"><a href="#6-本节代码" class="headerlink" title="6. 本节代码"></a>6. 本节代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/ScrapySplashTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapySplashTest</a>。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>在 Scrapy 中，建议使用 Splash 处理 JavaScript 动态渲染的页面。这样不会破坏 Scrapy 中的异步处理过程，会大大提高爬取效率。而且 Splash 的安装和配置比较简单，通过 API 调用的方式实现了模块分离，大规模爬取的部署也更加方便。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-06 09:27:03" itemprop="dateCreated datePublished" datetime="2019-12-06T09:27:03+08:00">2019-12-06</time>
                </span>
                <span id="/8410.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.9–Scrapy 对接 Splash" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>5.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>5 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8397.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8397.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.8–Scrapy 对接 Selenium</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-8-Scrapy-对接-Selenium"><a href="#13-8-Scrapy-对接-Selenium" class="headerlink" title="13.8 Scrapy 对接 Selenium"></a>13.8 Scrapy 对接 Selenium</h1>
                  <p>Scrapy 抓取页面的方式和 requests 库类似，都是直接模拟 HTTP 请求，而 Scrapy 也不能抓取 JavaScript 动态渲染的页面。在前文中抓取 JavaScript 渲染的页面有两种方式。一种是分析 Ajax 请求，找到其对应的接口抓取，Scrapy 同样可以用此种方式抓取。另一种是直接用 Selenium 或 Splash 模拟浏览器进行抓取，我们不需要关心页面后台发生的请求，也不需要分析渲染过程，只需要关心页面最终结果即可，可见即可爬。那么，如果 Scrapy 可以对接 Selenium，那 Scrapy 就可以处理任何网站的抓取了。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本节我们来看看 Scrapy 框架如何对接 Selenium，以 PhantomJS 进行演示。我们依然抓取淘宝商品信息，抓取逻辑和前文中用 Selenium 抓取淘宝商品完全相同。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保 PhantomJS 和 MongoDB 已经安装好并可以正常运行，安装好 Scrapy、Selenium、PyMongo 库，安装方式可以参考第 1 章的安装说明。</p>
                  <h3 id="3-新建项目"><a href="#3-新建项目" class="headerlink" title="3. 新建项目"></a>3. 新建项目</h3>
                  <p>首先新建项目，名为 scrapyseleniumtest，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject scrapyseleniumtest</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建一个 Spider，命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">taobao</span> <span class="selector-tag">www</span><span class="selector-class">.taobao</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>修改 ROBOTSTXT_OBEY 为 False，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ROBOTSTXT_OBEY</span> = <span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="4-定义-Item"><a href="#4-定义-Item" class="headerlink" title="4. 定义 Item"></a>4. 定义 Item</h3>
                  <p>首先定义 Item 对象，名为 ProductItem，代码如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="symbol">ProductItem</span>(<span class="symbol">Item</span>):</span><br><span class="line"></span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">products</span>'</span><br><span class="line">    <span class="symbol">image</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">price</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">deal</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">title</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">shop</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">location</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了 6 个 Field，也就是 6 个字段，跟之前的案例完全相同。然后定义了一个 collection 属性，即此 Item 保存到 MongoDB 的 Collection 名称。 初步实现 Spider 的 start_requests() 方法，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy import Request, Spider</span><br><span class="line"><span class="keyword">from</span> urllib.parse import quote</span><br><span class="line"><span class="keyword">from</span> scrapyseleniumtest.items import ProductItem</span><br><span class="line"></span><br><span class="line">class TaobaoSpider(Spider):</span><br><span class="line">    name = <span class="string">'taobao'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.taobao.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://s.taobao.com/search?q='</span></span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.settings.<span class="builtin-name">get</span>(<span class="string">'KEYWORDS'</span>):</span><br><span class="line">            <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(1, self.settings.<span class="builtin-name">get</span>(<span class="string">'MAX_PAGE'</span>) + 1):</span><br><span class="line">                url = self.base_url + quote(keyword)</span><br><span class="line">                yield Request(<span class="attribute">url</span>=url, <span class="attribute">callback</span>=self.parse, meta=&#123;<span class="string">'page'</span>: page&#125;, <span class="attribute">dont_filter</span>=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先定义了一个 base_url，即商品列表的 URL，其后拼接一个搜索关键字就是该关键字在淘宝的搜索结果商品列表页面。 关键字用 KEYWORDS 标识，定义为一个列表。最大翻页页码用 MAX_PAGE 表示。它们统一定义在 setttings.py 里面，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">KEYWORDS</span> = [<span class="string">'iPad'</span>]</span><br><span class="line"><span class="attr">MAX_PAGE</span> = <span class="number">100</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 start_requests() 方法里，我们首先遍历了关键字，遍历了分页页码，构造并生成 Request。由于每次搜索的 URL 是相同的，所以分页页码用 meta 参数来传递，同时设置 dont_filter 不去重。这样爬虫启动的时候，就会生成每个关键字对应的商品列表的每一页的请求了。</p>
                  <h3 id="5-对接-Selenium"><a href="#5-对接-Selenium" class="headerlink" title="5. 对接 Selenium"></a>5. 对接 Selenium</h3>
                  <p>接下来我们需要处理这些请求的抓取。这次我们对接 Selenium 进行抓取，采用 Downloader Middleware 来实现。在 Middleware 里面的 process_request() 方法里对每个抓取请求进行处理，启动浏览器并进行页面渲染，再将渲染后的结果构造一个 HtmlResponse 对象返回。代码实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium import webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions import TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by import By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support import expected_conditions as EC</span><br><span class="line"><span class="keyword">from</span> scrapy.http import HtmlResponse</span><br><span class="line"><span class="keyword">from</span><span class="built_in"> logging </span>import getLogger</span><br><span class="line"></span><br><span class="line">class SeleniumMiddleware():</span><br><span class="line">    def __init__(self, <span class="attribute">timeout</span>=None, service_args=[]):</span><br><span class="line">        self.logger = getLogger(__name__)</span><br><span class="line">        self.timeout = timeout</span><br><span class="line">        self.browser = webdriver.PhantomJS(<span class="attribute">service_args</span>=service_args)</span><br><span class="line">        self.browser.set_window_size(1400, 700)</span><br><span class="line">        self.browser.set_page_load_timeout(self.timeout)</span><br><span class="line">        self.wait = WebDriverWait(self.browser, self.timeout)</span><br><span class="line"></span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.browser.close()</span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        用 PhantomJS 抓取页面</span></span><br><span class="line"><span class="string">        :param request: Request 对象</span></span><br><span class="line"><span class="string">        :param spider: Spider 对象</span></span><br><span class="line"><span class="string">        :return: HtmlResponse</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        self.logger.<span class="builtin-name">debug</span>(<span class="string">'PhantomJS is Starting'</span>)</span><br><span class="line">       <span class="built_in"> page </span>= request.meta.<span class="builtin-name">get</span>(<span class="string">'page'</span>, 1)</span><br><span class="line">        try:</span><br><span class="line">            self.browser.<span class="builtin-name">get</span>(request.url)</span><br><span class="line">            <span class="keyword">if</span><span class="built_in"> page </span>&gt; 1:</span><br><span class="line">                input = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager div.form&gt; input'</span>)))</span><br><span class="line">                submit = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager div.form&gt; span.btn.J_Submit'</span>)))</span><br><span class="line">                input.clear()</span><br><span class="line">                input.send_keys(page)</span><br><span class="line">                submit.click()</span><br><span class="line">            self.wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager li.item.active&gt; span'</span>), str(page)))</span><br><span class="line">            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'.m-itemlist .items .item'</span>)))</span><br><span class="line">            return HtmlResponse(<span class="attribute">url</span>=request.url, <span class="attribute">body</span>=self.browser.page_source, <span class="attribute">request</span>=request, <span class="attribute">encoding</span>=<span class="string">'utf-8'</span>, <span class="attribute">status</span>=200)</span><br><span class="line">        except TimeoutException:</span><br><span class="line">            return HtmlResponse(<span class="attribute">url</span>=request.url, <span class="attribute">status</span>=500, <span class="attribute">request</span>=request)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(<span class="attribute">timeout</span>=crawler.settings.get('SELENIUM_TIMEOUT'),</span><br><span class="line">                   <span class="attribute">service_args</span>=crawler.settings.get('PHANTOMJS_SERVICE_ARGS'))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先我们在 <strong>init</strong>() 里对一些对象进行初始化，包括 PhantomJS、WebDriverWait 等对象，同时设置页面大小和页面加载超时时间。在 process_request() 方法中，我们通过 Request 的 meta 属性获取当前需要爬取的页码，调用 PhantomJS 对象的 get() 方法访问 Request 的对应的 URL。这就相当于从 Request 对象里获取请求链接，然后再用 PhantomJS 加载，而不再使用 Scrapy 里的 Downloader。 随后的处理等待和翻页的方法在此不再赘述，和前文的原理完全相同。最后，页面加载完成之后，我们调用 PhantomJS 的 page_source 属性即可获取当前页面的源代码，然后用它来直接构造并返回一个 HtmlResponse 对象。构造这个对象的时候需要传入多个参数，如 url、body 等，这些参数实际上就是它的基础属性。可以在官方文档查看 HtmlResponse 对象的结构：<a href="https://doc.scrapy.org/en/latest/topics/request-response.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/request-response.html</a>，这样我们就成功利用 PhantomJS 来代替 Scrapy 完成了页面的加载，最后将 Response 返回即可。 有人可能会纳闷：为什么实现这么一个 Downloader Middleware 就可以了？之前的 Request 对象怎么办？Scrapy 不再处理了吗？Response 返回后又传递给了谁？ 是的，Request 对象到这里就不会再处理了，也不会再像以前一样交给 Downloader 下载。Response 会直接传给 Spider 进行解析。 我们需要回顾一下 Downloader Middleware 的 process_request() 方法的处理逻辑，内容如下所示： 当 process_request() 方法返回 Response 对象的时候，更低优先级的 Downloader Middleware 的 process_request() 和 process_exception() 方法就不会被继续调用了，转而开始执行每个 Downloader Middleware 的 process_response() 方法，调用完毕之后直接将 Response 对象发送给 Spider 来处理。 这里直接返回了一个 HtmlResponse 对象，它是 Response 的子类，返回之后便顺次调用每个 Downloader Middleware 的 process_response() 方法。而在 process_response() 中我们没有对其做特殊处理，它会被发送给 Spider，传给 Request 的回调函数进行解析。 到现在，我们应该能了解 Downloader Middleware 实现 Selenium 对接的原理了。 在 settings.py 里，我们设置调用刚才定义的 SeleniumMiddleware、设置等待超时变量 SELENIUM_TIMEOUT、设置 PhantomJS 配置参数 PHANTOMJS_SERVICE_ARGS，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">DOWNLOADER_MIDDLEWARES</span> = &#123;<span class="string">'scrapyseleniumtest.middlewares.SeleniumMiddleware'</span>: <span class="number">543</span>,&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="6-解析页面"><a href="#6-解析页面" class="headerlink" title="6. 解析页面"></a>6. 解析页面</h3>
                  <p>Response 对象就会回传给 Spider 内的回调函数进行解析。所以下一步我们就实现其回调函数，对网页来进行解析，代码如下所示：</p>
                  <figure class="highlight nginx">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">def</span> parse(self, response):</span><br><span class="line">    products = response.xpath(<span class="string">'//div[<span class="variable">@id</span>="mainsrp-itemlist"]//div[<span class="variable">@class</span>="items"][1]//div[contains(<span class="variable">@class</span>, "item")]'</span>)</span><br><span class="line">    for product in products:</span><br><span class="line">        item = ProductItem()</span><br><span class="line">        item[<span class="string">'price'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "price")]//text()'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'title'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "title")]//text()'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'shop'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "shop")]//text()'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'image'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[<span class="variable">@class</span>="pic"]//img[contains(<span class="variable">@class</span>, "img")]/<span class="variable">@data</span>-src'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'deal'</span>] = product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "deal-cnt")]//text()'</span>).extract_first()</span><br><span class="line">        item[<span class="string">'location'</span>] = product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "location")]//text()'</span>).extract_first()</span><br><span class="line">        yield item</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们使用 XPath 进行解析，调用 response 变量的 xpath() 方法即可。首先我们传递选取所有商品对应的 XPath，可以匹配所有商品，随后对结果进行遍历，依次选取每个商品的名称、价格、图片等内容，构造并返回一个 ProductItem 对象。</p>
                  <h3 id="7-存储结果"><a href="#7-存储结果" class="headerlink" title="7. 存储结果"></a>7. 存储结果</h3>
                  <p>最后我们实现一个 Item Pipeline，将结果保存到 MongoDB，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, mongo_uri, mongo_db)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.mongo_uri = mongo_uri</span><br><span class="line">        <span class="keyword">self</span>.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>), mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client = pymongo.MongoClient(<span class="keyword">self</span>.mongo_uri)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[<span class="keyword">self</span>.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db[item.collection].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此实现和前文中存储到 MongoDB 的方法完全一致，原理不再赘述。记得在 settings.py 中开启它的调用，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ITEM_PIPELINES</span> = &#123;<span class="string">'scrapyseleniumtest.pipelines.MongoPipeline'</span>: <span class="number">300</span>,&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中，MONGO_URI 和 MONGO_DB 的定义如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'localhost'</span></span><br><span class="line"><span class="attr">MONGO_DB</span> = <span class="string">'taobao'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="8-运行"><a href="#8-运行" class="headerlink" title="8. 运行"></a>8. 运行</h3>
                  <p>整个项目就完成了，执行如下命令启动抓取即可：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl taobao</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如图 13-13 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-034100.jpg" alt=""> 图 13-13 运行结果 再查看一下 MongoDB，结果如图 13-14 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-034105.jpg" alt=""> 图 13-14 MongoDB 结果 这样我们便成功在 Scrapy 中对接 Selenium 并实现了淘宝商品的抓取。</p>
                  <h3 id="9-本节代码"><a href="#9-本节代码" class="headerlink" title="9. 本节代码"></a>9. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/ScrapySeleniumTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapySeleniumTest</a>。</p>
                  <h3 id="10-结语"><a href="#10-结语" class="headerlink" title="10. 结语"></a>10. 结语</h3>
                  <p>我们通过改写 Downloader Middleware 的方式实现了 Selenium 的对接。但这种方法其实是阻塞式的，也就是说这样就破坏了 Scrapy 异步处理的逻辑，速度会受到影响。为了不破坏其异步加载逻辑，我们可以使用 Splash 实现。下一节我们再来看看 Scrapy 对接 Splash 的方式。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-05 09:30:23" itemprop="dateCreated datePublished" datetime="2019-12-05T09:30:23+08:00">2019-12-05</time>
                </span>
                <span id="/8397.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.8–Scrapy 对接 Selenium" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>7.2k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>7 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8394.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8394.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.7–Item Pipeline 的用法</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-7-Item-Pipeline-的用法"><a href="#13-7-Item-Pipeline-的用法" class="headerlink" title="13.7 Item Pipeline 的用法"></a>13.7 Item Pipeline 的用法</h1>
                  <p>Item Pipeline 是项目管道。在前面我们已经了解了 Item Pipeline 的基本用法，本节我们再作详细了解它的用法。 首先我们看看 Item Pipeline 在 Scrapy 中的架构，如图 13-1 所示。 图中的最左侧即为 Item Pipeline，它的调用发生在 Spider 产生 Item 之后。当 Spider 解析完 Response 之后，Item 就会传递到 Item Pipeline，被定义的 Item Pipeline 组件会顺次调用，完成一连串的处理过程，比如数据清洗、存储等。 它的主要功能有：</p>
                  <ul>
                    <li>清洗 HTML 数据</li>
                    <li>验证爬取数据，检查爬取字段</li>
                    <li>查重并丢弃重复内容</li>
                    <li>将爬取结果储存到数据库</li>
                  </ul>
                  <h3 id="1-核心方法"><a href="#1-核心方法" class="headerlink" title="1. 核心方法"></a>1. 核心方法</h3>
                  <p>我们可以自定义 Item Pipeline，只需要实现指定的方法就好，其中必须要实现的一个方法是：</p>
                  <ul>
                    <li>process_item(item, spider)</li>
                  </ul>
                  <p>另外还有几个比较实用的方法，它们分别是：</p>
                  <ul>
                    <li>open_spider(spider)</li>
                    <li>close_spider(spider)</li>
                    <li>from_crawler(cls, crawler)</li>
                  </ul>
                  <p>下面我们对这几个方法的用法作下详细的介绍：</p>
                  <h4 id="process-item-item-spider"><a href="#process-item-item-spider" class="headerlink" title="process_item(item, spider)"></a>process_item(item, spider)</h4>
                  <p>process_item() 是必须要实现的方法，被定义的 Item Pipeline 会默认调用这个方法对 Item 进行处理。比如，我们可以进行数据处理或者将数据写入到数据库等操作。它必须返回 Item 类型的值或者抛出一个 DropItem 异常。 process_item() 方法的参数有如下两个。</p>
                  <ul>
                    <li>item，是 Item 对象，即被处理的 Item</li>
                    <li>spider，是 Spider 对象，即生成该 Item 的 Spider</li>
                  </ul>
                  <p>下面对该方法的返回类型归纳如下：</p>
                  <ul>
                    <li>如果返回的是 Item 对象，那么此 Item 会接着被低优先级的 Item Pipeline 的 process_item() 方法进行处理，直到所有的方法被调用完毕。</li>
                    <li>如果抛出的是 DropItem 异常，那么此 Item 就会被丢弃，不再进行处理。</li>
                  </ul>
                  <h4 id="open-spider-self-spider"><a href="#open-spider-self-spider" class="headerlink" title="open_spider(self, spider)"></a>open_spider(self, spider)</h4>
                  <p>open_spider() 方法是在 Spider 开启的时候被自动调用的，在这里我们可以做一些初始化操作，如开启数据库连接等。其中参数 spider 就是被开启的 Spider 对象。</p>
                  <h4 id="close-spider-spider"><a href="#close-spider-spider" class="headerlink" title="close_spider(spider)"></a>close_spider(spider)</h4>
                  <p>close_spider() 方法是在 Spider 关闭的时候自动调用的，在这里我们可以做一些收尾工作，如关闭数据库连接等，其中参数 spider 就是被关闭的 Spider 对象。</p>
                  <h4 id="from-crawler-cls-crawler"><a href="#from-crawler-cls-crawler" class="headerlink" title="from_crawler(cls, crawler)"></a>from_crawler(cls, crawler)</h4>
                  <p>from_crawler() 方法是一个类方法，用 @classmethod 标识，是一种依赖注入的方式。它的参数是 crawler，通过 crawler 对象，我们可以拿到 Scrapy 的所有核心组件，如全局配置的每个信息，然后创建一个 Pipeline 实例。参数 cls 就是 Class，最后返回一个 Class 实例。 下面我们用一个实例来加深对 Item Pipeline 用法的理解。</p>
                  <h3 id="2-本节目标"><a href="#2-本节目标" class="headerlink" title="2. 本节目标"></a>2. 本节目标</h3>
                  <p>我们以爬取 360 摄影美图为例，来分别实现 MongoDB 存储、MySQL 存储、Image 图片存储的三个 Pipeline。</p>
                  <h3 id="3-准备工作"><a href="#3-准备工作" class="headerlink" title="3. 准备工作"></a>3. 准备工作</h3>
                  <p>请确保已经安装好 MongoDB 和 MySQL 数据库，安装好 Python 的 PyMongo、PyMySQL、Scrapy 框架，另外需要安装 pillow 图像处理库，如没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="4-抓取分析"><a href="#4-抓取分析" class="headerlink" title="4. 抓取分析"></a>4. 抓取分析</h3>
                  <p>我们这次爬取的目标网站为：<a href="https://image.so.com。打开此页面，切换到摄影页面，网页中呈现了许许多多的摄影美图。我们打开浏览器开发者工具，过滤器切换到" target="_blank" rel="noopener">https://image.so.com。打开此页面，切换到摄影页面，网页中呈现了许许多多的摄影美图。我们打开浏览器开发者工具，过滤器切换到</a> XHR 选项，然后下拉页面，可以看到下面就会呈现许多 Ajax 请求，如图 13-6 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033958.png" alt=""> 图 13-6 请求列表 我们查看一个请求的详情，观察返回的数据结构，如图 13-7 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034003.jpg" alt=""> 图 13-7 返回结果 返回格式是 JSON。其中 list 字段就是一张张图片的详情信息，包含了 30 张图片的 ID、名称、链接、缩略图等信息。另外观察 Ajax 请求的参数信息，有一个参数 sn 一直在变化，这个参数很明显就是偏移量。当 sn 为 30 时，返回的是前 30 张图片，sn 为 60 时，返回的就是第 31~60 张图片。另外，ch 参数是摄影类别，listtype 是排序方式，temp 参数可以忽略。 所以我们抓取时只需要改变 sn 的数值就好了。 下面我们用 Scrapy 来实现图片的抓取，将图片的信息保存到 MongoDB、MySQL，同时将图片存储到本地。</p>
                  <h3 id="5-新建项目"><a href="#5-新建项目" class="headerlink" title="5. 新建项目"></a>5. 新建项目</h3>
                  <p>首先新建一个项目，命令如下：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">scrapy </span>startproject images360</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来新建一个 Spider，命令如下：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">images</span> <span class="selector-tag">images</span><span class="selector-class">.so</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就成功创建了一个 Spider。</p>
                  <h3 id="6-构造请求"><a href="#6-构造请求" class="headerlink" title="6. 构造请求"></a>6. 构造请求</h3>
                  <p>接下来定义爬取的页数。比如爬取 50 页、每页 30 张，也就是 1500 张图片，我们可以先在 settings.py 里面定义一个变量 MAX_PAGE，添加如下定义：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MAX_PAGE</span> = <span class="number">50</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>定义 start_requests() 方法，用来生成 50 次请求，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def start_requests(self):</span><br><span class="line">    data = &#123;<span class="string">'ch'</span>: <span class="string">'photography'</span>, <span class="string">'listtype'</span>: <span class="string">'new'</span>&#125;</span><br><span class="line">    base_url = <span class="string">'https://image.so.com/zj?'</span></span><br><span class="line">    <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(1, self.settings.<span class="builtin-name">get</span>(<span class="string">'MAX_PAGE'</span>) + 1):</span><br><span class="line">        data[<span class="string">'sn'</span>] =<span class="built_in"> page </span>* 30</span><br><span class="line">        params = urlencode(data)</span><br><span class="line">        url = base_url + params</span><br><span class="line">        yield Request(url, self.parse)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们首先定义了初始的两个参数，sn 参数是遍历循环生成的。然后利用 urlencode() 方法将字典转化为 URL 的 GET 参数，构造出完整的 URL，构造并生成 Request。 还需要引入 scrapy.Request 和 urllib.parse 模块，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>再修改 settings.py 中的 ROBOTSTXT_OBEY 变量，将其设置为 False，否则无法抓取，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ROBOTSTXT_OBEY</span> = <span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行爬虫，即可以看到链接都请求成功，执行命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl images</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行示例结果如图 13-8 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034009.jpg" alt=""> 图 13-8 运行结果 所有请求的状态码都是 200，这就证明图片信息爬取成功了。</p>
                  <h3 id="7-提取信息"><a href="#7-提取信息" class="headerlink" title="7. 提取信息"></a>7. 提取信息</h3>
                  <p>首先定义一个 Item，叫作 ImageItem，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"><span class="keyword">class</span> <span class="symbol">ImageItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = <span class="symbol">table</span> = '<span class="symbol">images</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">url</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">title</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">thumb</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们定义了 4 个字段，包括图片的 ID、链接、标题、缩略图。另外还有两个属性 collection 和 table，都定义为 images 字符串，分别代表 MongoDB 存储的 Collection 名称和 MySQL 存储的表名称。 接下来我们提取 Spider 里有关信息，将 parse() 方法改写为如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse(self, response):</span><br><span class="line">    <span class="built_in">result</span> = json.loads(response.<span class="keyword">text</span>)</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> <span class="built_in">result</span>.<span class="built_in">get</span>(<span class="string">'list'</span>):</span><br><span class="line">        <span class="keyword">item</span> = ImageItem()</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'id'</span>] = image.<span class="built_in">get</span>(<span class="string">'imageid'</span>)</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'url'</span>] = image.<span class="built_in">get</span>(<span class="string">'qhimg_url'</span>)</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'title'</span>] = image.<span class="built_in">get</span>(<span class="string">'group_title'</span>)</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'thumb'</span>] = image.<span class="built_in">get</span>(<span class="string">'qhimg_thumb_url'</span>)</span><br><span class="line">        yield <span class="keyword">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先解析 JSON，遍历其 list 字段，取出一个个图片信息，然后再对 ImageItem 赋值，生成 Item 对象。 这样我们就完成了信息的提取。</p>
                  <h3 id="8-存储信息"><a href="#8-存储信息" class="headerlink" title="8. 存储信息"></a>8. 存储信息</h3>
                  <p>接下来我们需要将图片的信息保存到 MongoDB、MySQL，同时将图片保存到本地。</p>
                  <h4 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h4>
                  <p>首先确保 MongoDB 已经正常安装并且正常运行。 我们用一个 MongoPipeline 将信息保存到 MongoDB，在 pipelines.py 里添加如下类的实现：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, mongo_uri, mongo_db)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.mongo_uri = mongo_uri</span><br><span class="line">        <span class="keyword">self</span>.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client = pymongo.MongoClient(<span class="keyword">self</span>.mongo_uri)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[<span class="keyword">self</span>.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db[item.collection].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里需要用到两个变量，MONGO_URI 和 MONGO_DB，即存储到 MongoDB 的链接地址和数据库名称。我们在 settings.py 里添加这两个变量，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'localhost'</span></span><br><span class="line"><span class="attr">MONGO_DB</span> = <span class="string">'images360'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样一个保存到 MongoDB 的 Pipeline 的就创建好了。这里最主要的方法是 process_item() 方法，直接调用 Collection 对象的 insert() 方法即可完成数据的插入，最后返回 Item 对象。</p>
                  <h4 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h4>
                  <p>首先确保 MySQL 已经正确安装并且正常运行。 新建一个数据库，名字还是 images360，SQL 语句如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">CREATE DATABASE images360<span class="built_in"> DEFAULT </span>CHARACTER <span class="builtin-name">SET</span> utf8 COLLATE utf8_general_ci</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建一个数据表，包含 id、url、title、thumb 四个字段，SQL 语句如下所示：</p>
                  <figure class="highlight sql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> images (<span class="keyword">id</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, <span class="keyword">url</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span> , title <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span> , thumb <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>执行完 SQL 语句之后，我们就成功创建好了数据表。接下来就可以往表里存储数据了。 接下来我们实现一个 MySQLPipeline，代码如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, host, database, user, password, port)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.host = host</span><br><span class="line">        <span class="keyword">self</span>.database = database</span><br><span class="line">        <span class="keyword">self</span>.user = user</span><br><span class="line">        <span class="keyword">self</span>.password = password</span><br><span class="line">        <span class="keyword">self</span>.port = port</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(host=crawler.settings.get(<span class="string">'MYSQL_HOST'</span>),</span><br><span class="line">            database=crawler.settings.get(<span class="string">'MYSQL_DATABASE'</span>),</span><br><span class="line">            user=crawler.settings.get(<span class="string">'MYSQL_USER'</span>),</span><br><span class="line">            password=crawler.settings.get(<span class="string">'MYSQL_PASSWORD'</span>),</span><br><span class="line">            port=crawler.settings.get(<span class="string">'MYSQL_PORT'</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db = pymysql.connect(<span class="keyword">self</span>.host, <span class="keyword">self</span>.user, <span class="keyword">self</span>.password, <span class="keyword">self</span>.database, charset=<span class="string">'utf8'</span>, port=<span class="keyword">self</span>.port)</span><br><span class="line">        <span class="keyword">self</span>.cursor = <span class="keyword">self</span>.db.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        data = dict(item)</span><br><span class="line">        keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">        values = <span class="string">', '</span>.join([<span class="string">'% s'</span>] * len(data))</span><br><span class="line">        sql = <span class="string">'insert into % s (% s) values (% s)'</span> % (item.table, keys, values)</span><br><span class="line">        <span class="keyword">self</span>.cursor.execute(sql, tuple(data.values()))</span><br><span class="line">        <span class="keyword">self</span>.db.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如前所述，这里用到的数据插入方法是一个动态构造 SQL 语句的方法。 这里又需要几个 MySQL 的配置，我们在 settings.py 里添加几个变量，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MYSQL_HOST</span> = <span class="string">'localhost'</span></span><br><span class="line"><span class="attr">MYSQL_DATABASE</span> = <span class="string">'images360'</span></span><br><span class="line"><span class="attr">MYSQL_PORT</span> = <span class="number">3306</span></span><br><span class="line"><span class="attr">MYSQL_USER</span> = <span class="string">'root'</span></span><br><span class="line"><span class="attr">MYSQL_PASSWORD</span> = <span class="string">'123456'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里分别定义了 MySQL 的地址、数据库名称、端口、用户名、密码。 这样，MySQL Pipeline 就完成了。</p>
                  <h4 id="Image-Pipeline"><a href="#Image-Pipeline" class="headerlink" title="Image Pipeline"></a>Image Pipeline</h4>
                  <p>Scrapy 提供了专门处理下载的 Pipeline，包括文件下载和图片下载。下载文件和图片的原理与抓取页面的原理一样，因此下载过程支持异步和多线程，下载十分高效。下面我们来看看具体的实现过程。 官方文档地址为：<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/media-pipeline.html</a>。 首先定义存储文件的路径，需要定义一个 IMAGES_STORE 变量，在 settings.py 中添加如下代码：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">IMAGES_STORE</span> = <span class="string">'./images'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们将路径定义为当前路径下的 images 子文件夹，即下载的图片都会保存到本项目的 images 文件夹中。 内置的 ImagesPipeline 会默认读取 Item 的 image_urls 字段，并认为该字段是一个列表形式，它会遍历 Item 的 image_urls 字段，然后取出每个 URL 进行图片下载。 但是现在生成的 Item 的图片链接字段并不是 image_urls 字段表示的，也不是列表形式，而是单个的 URL。所以为了实现下载，我们需要重新定义下载的部分逻辑，即要自定义 ImagePipeline，继承内置的 ImagesPipeline，重写几个方法。 我们定义 ImagePipeline，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        url = request.url</span><br><span class="line">        file_name = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> file_name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        image_paths = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'Image Downloaded Failed'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(item[<span class="string">'url'</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们实现了 ImagePipeline，继承 Scrapy 内置的 ImagesPipeline，重写下面几个方法。</p>
                  <ul>
                    <li>get_media_requests()。它的第一个参数 item 是爬取生成的 Item 对象。我们将它的 url 字段取出来，然后直接生成 Request 对象。此 Request 加入到调度队列，等待被调度，执行下载。</li>
                    <li>file_path()。它的第一个参数 request 就是当前下载对应的 Request 对象。这个方法用来返回保存的文件名，直接将图片链接的最后一部分当作文件名即可。它利用 split() 函数分割链接并提取最后一部分，返回结果。这样此图片下载之后保存的名称就是该函数返回的文件名。</li>
                    <li>item_completed()，它是当单个 Item 完成下载时的处理方法。因为并不是每张图片都会下载成功，所以我们需要分析下载结果并剔除下载失败的图片。如果某张图片下载失败，那么我们就不需保存此 Item 到数据库。该方法的第一个参数 results 就是该 Item 对应的下载结果，它是一个列表形式，列表每一个元素是一个元组，其中包含了下载成功或失败的信息。这里我们遍历下载结果找出所有成功的下载列表。如果列表为空，那么该 Item 对应的图片下载失败，随即抛出异常 DropItem，该 Item 忽略。否则返回该 Item，说明此 Item 有效。</li>
                  </ul>
                  <p>现在为止，三个 Item Pipeline 的定义就完成了。最后只需要启用就可以了，修改 settings.py，设置 ITEM_PIPELINES，如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">ITEM_PIPELINES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'images360.pipelines.ImagePipeline':</span> <span class="number">300</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'images360.pipelines.MongoPipeline':</span> <span class="number">301</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'images360.pipelines.MysqlPipeline':</span> <span class="number">302</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里注意调用的顺序。我们需要优先调用 ImagePipeline 对 Item 做下载后的筛选，下载失败的 Item 就直接忽略，它们就不会保存到 MongoDB 和 MySQL 里。随后再调用其他两个存储的 Pipeline，这样就能确保存入数据库的图片都是下载成功的。 接下来运行程序，执行爬取，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl images</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>爬虫一边爬取一边下载，下载速度非常快，对应的输出日志如图 13-9 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034019.jpg" alt=""> 图 13-9 输出日志 查看本地 images 文件夹，发现图片都已经成功下载，如图 13-10 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034024.jpg" alt=""> 图 13-10 下载结果 查看 MySQL，下载成功的图片信息也已成功保存，如图 13-11 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034029.jpg" alt=""> 图 13-11 MySQL 结果 查看 MongoDB，下载成功的图片信息同样已成功保存，如图 13-12 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-034034.jpg" alt=""> 图 13-12 MongoDB 结果 这样我们就可以成功实现图片的下载并把图片的信息存入数据库了。</p>
                  <h3 id="9-本节代码"><a href="#9-本节代码" class="headerlink" title="9. 本节代码"></a>9. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/Images360" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Images360</a>。</p>
                  <h3 id="10-结语"><a href="#10-结语" class="headerlink" title="10. 结语"></a>10. 结语</h3>
                  <p>Item Pipeline 是 Scrapy 非常重要的组件，数据存储几乎都是通过此组件实现的。请读者认真掌握此内容。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-05 09:26:32" itemprop="dateCreated datePublished" datetime="2019-12-05T09:26:32+08:00">2019-12-05</time>
                </span>
                <span id="/8394.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.7–Item Pipeline 的用法" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>8.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>7 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8385.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8385.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.6–Spider Middleware 的用法</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-6-Spider-Middleware-的用法"><a href="#13-6-Spider-Middleware-的用法" class="headerlink" title="13.6 Spider Middleware 的用法"></a>13.6 Spider Middleware 的用法</h1>
                  <p>Spider Middleware 是介入到 Scrapy 的 Spider 处理机制的钩子框架。我们首先来看看它的架构，如图 13-1 所示。 当 Downloader 生成 Response 之后，Response 会被发送给 Spider，在发送给 Spider 之前，Response 会首先经过 Spider Middleware 处理，当 Spider 处理生成 Item 和 Request 之后，Item 和 Request 还会经过 Spider Middleware 的处理。 Spider Middleware 有如下三个作用。</p>
                  <ul>
                    <li>我们可以在 Downloader 生成的 Response 发送给 Spider 之前，也就是在 Response 发送给 Spider 之前对 Response 进行处理。</li>
                    <li>我们可以在 Spider 生成的 Request 发送给 Scheduler 之前，也就是在 Request 发送给 Scheduler 之前对 Request 进行处理。</li>
                    <li>我们可以在 Spider 生成的 Item 发送给 Item Pipeline 之前，也就是在 Item 发送给 Item Pipeline 之前对 Item 进行处理。</li>
                  </ul>
                  <h3 id="1-使用说明"><a href="#1-使用说明" class="headerlink" title="1. 使用说明"></a>1. 使用说明</h3>
                  <p>需要说明的是，Scrapy 其实已经提供了许多 Spider Middleware，它们被 SPIDER_MIDDLEWARES_BASE 这个变量所定义。 SPIDER_MIDDLEWARES_BASE 变量的内容如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware':</span> <span class="number">50</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware':</span> <span class="number">500</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.spidermiddlewares.referer.RefererMiddleware':</span> <span class="number">700</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware':</span> <span class="number">800</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.spidermiddlewares.depth.DepthMiddleware':</span> <span class="number">900</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>和 Downloader Middleware 一样，Spider Middleware 首先加入到 SPIDER_MIDDLEWARES 设置中，该设置会和 Scrapy 中 SPIDER_MIDDLEWARES_BASE 定义的 Spider Middleware 合并。然后根据键值的数字优先级排序，得到一个有序列表。第一个 Middleware 是最靠近引擎的，最后一个 Middleware 是最靠近 Spider 的。</p>
                  <h3 id="2-核心方法"><a href="#2-核心方法" class="headerlink" title="2. 核心方法"></a>2. 核心方法</h3>
                  <p>Scrapy 内置的 Spider Middleware 为 Scrapy 提供了基础的功能。如果我们想要扩展其功能，只需要实现某几个方法即可。 每个 Spider Middleware 都定义了以下一个或多个方法的类，核心方法有如下 4 个。</p>
                  <ul>
                    <li>process_spider_input(response, spider)</li>
                    <li>process_spider_output(response, result, spider)</li>
                    <li>process_spider_exception(response, exception, spider)</li>
                    <li>process_start_requests(start_requests, spider)</li>
                  </ul>
                  <p>只需要实现其中一个方法就可以定义一个 Spider Middleware。下面我们来看看这 4 个方法的详细用法。</p>
                  <h4 id="process-spider-input-response-spider"><a href="#process-spider-input-response-spider" class="headerlink" title="process_spider_input(response, spider)"></a>process_spider_input(response, spider)</h4>
                  <p>当 Response 通过 Spider Middleware 时，该方法被调用，处理该 Response。 方法的参数有两个：</p>
                  <ul>
                    <li>response，即 Response 对象，即被处理的 Response</li>
                    <li>spider，即 Spider 对象，即该 response 对应的 Spider</li>
                  </ul>
                  <p>process_spider_input() 应该返回 None 或者抛出一个异常。</p>
                  <ul>
                    <li>如果其返回 None ，Scrapy 将会继续处理该 Response，调用所有其他的 Spider Middleware 直到 Spider 处理该 Response。</li>
                    <li>如果其抛出一个异常，Scrapy 将不会调用任何其他 Spider Middlewar e 的 process_spider_input() 方法，并调用 Request 的 errback() 方法。 errback 的输出将会以另一个方向被重新输入到中间件中，使用 process_spider_output() 方法来处理，当其抛出异常时则调用 process_spider_exception() 来处理。</li>
                  </ul>
                  <h4 id="process-spider-output-response-result-spider"><a href="#process-spider-output-response-result-spider" class="headerlink" title="process_spider_output(response, result, spider)"></a>process_spider_output(response, result, spider)</h4>
                  <p>当 Spider 处理 Response 返回结果时，该方法被调用。 方法的参数有三个：</p>
                  <ul>
                    <li>response，即 Response 对象，即生成该输出的 Response</li>
                    <li>result，包含 Request 或 Item 对象的可迭代对象，即 Spider 返回的结果</li>
                    <li>spider，即 Spider 对象，即其结果对应的 Spider</li>
                  </ul>
                  <p>process_spider_output() 必须返回包含 Request 或 Item 对象的可迭代对象。</p>
                  <h4 id="process-spider-exception-response-exception-spider"><a href="#process-spider-exception-response-exception-spider" class="headerlink" title="process_spider_exception(response, exception, spider)"></a>process_spider_exception(response, exception, spider)</h4>
                  <p>当 Spider 或 Spider Middleware 的 process_spider_input() 方法抛出异常时， 该方法被调用。 方法的参数有三个：</p>
                  <ul>
                    <li>response，即 Response 对象，即异常被抛出时被处理的 Response</li>
                    <li>exception，即 Exception 对象，被抛出的异常</li>
                    <li>spider，即 Spider 对象，即抛出该异常的 Spider</li>
                  </ul>
                  <p>process_spider_exception() 必须要么返回 None ， 要么返回一个包含 Response 或 Item 对象的可迭代对象。</p>
                  <ul>
                    <li>如果其返回 None ，Scrapy 将继续处理该异常，调用其他 Spider Middleware 中的 process_spider_exception() 方法，直到所有 Spider Middleware 都被调用。</li>
                    <li>如果其返回一个可迭代对象，则其他 Spider Middleware 的 process_spider_output() 方法被调用， 其他的 process_spider_exception() 将不会被调用。</li>
                  </ul>
                  <h4 id="process-start-requests-start-requests-spider"><a href="#process-start-requests-start-requests-spider" class="headerlink" title="process_start_requests(start_requests, spider)"></a>process_start_requests(start_requests, spider)</h4>
                  <p>该方法以 Spider 启动的 Request 为参数被调用，执行的过程类似于 process_spider_output() ，只不过其没有相关联的 Response 并且必须返回 Request。 方法的参数有两个：</p>
                  <ul>
                    <li>start_requests，即包含 Request 的可迭代对象，即 Start Requests</li>
                    <li>spider，即 Spider 对象，即 Start Requests 所属的 Spider</li>
                  </ul>
                  <p>其必须返回另一个包含 Request 对象的可迭代对象。</p>
                  <h3 id="3-结语"><a href="#3-结语" class="headerlink" title="3. 结语"></a>3. 结语</h3>
                  <p>本节介绍了 Spider Middleware 的基本原理和自定义 Spider Middleware 的方法。Spider Middleware 使用的频率不如 Downloader Middleware 的高，在必要的情况下它可以用来方便数据的处理。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-04 15:18:30" itemprop="dateCreated datePublished" datetime="2019-12-04T15:18:30+08:00">2019-12-04</time>
                </span>
                <span id="/8385.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.6–Spider Middleware 的用法" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8381.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8381.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.5–Downloader Middleware 的用法</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-5-Downloader-Middleware-的用法"><a href="#13-5-Downloader-Middleware-的用法" class="headerlink" title="13.5 Downloader Middleware 的用法"></a>13.5 Downloader Middleware 的用法</h1>
                  <p>Downloader Middleware 即下载中间件，它是处于 Scrapy 的 Request 和 Response 之间的处理模块。我们首先来看看它的架构，如图 13-1 所示。 Scheduler 从队列中拿出一个 Request 发送给 Downloader 执行下载，这个过程会经过 Downloader Middleware 的处理。另外，当 Downloader 将 Request 下载完成得到 Response 返回给 Spider 时会再次经过 Downloader Middleware 处理。 也就是说，Downloader Middleware 在整个架构中起作用的位置是以下两个。</p>
                  <ul>
                    <li>在 Scheduler 调度出队列的 Request 发送给 Downloader 下载之前，也就是我们可以在 Request 执行下载之前对其进行修改。</li>
                    <li>在下载后生成的 Response 发送给 Spider 之前，也就是我们可以在生成 Resposne 被 Spider 解析之前对其进行修改。</li>
                  </ul>
                  <p>Downloader Middleware 的功能十分强大，修改 User-Agent、处理重定向、设置代理、失败重试、设置 Cookies 等功能都需要借助它来实现。下面我们来了解一下 Downloader Middleware 的详细用法。</p>
                  <h3 id="1-使用说明"><a href="#1-使用说明" class="headerlink" title="1. 使用说明"></a>1. 使用说明</h3>
                  <p>需要说明的是，Scrapy 其实已经提供了许多 Downloader Middleware，比如负责失败重试、自动重定向等功能的 Middleware，它们被 DOWNLOADER_MIDDLEWARES_BASE 变量所定义。 DOWNLOADER_MIDDLEWARES_BASE 变量的内容如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware':</span> <span class="number">300</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware':</span> <span class="number">350</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware':</span> <span class="number">400</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware':</span> <span class="number">500</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.retry.RetryMiddleware':</span> <span class="number">550</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware':</span> <span class="number">560</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware':</span> <span class="number">580</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">590</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware':</span> <span class="number">600</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware':</span> <span class="number">700</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware':</span> <span class="number">750</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.stats.DownloaderStats':</span> <span class="number">850</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware':</span> <span class="number">900</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这是一个字典格式，字典的键名是 Scrapy 内置的 Downloader Middleware 的名称，键值代表了调用的优先级，优先级是一个数字，数字越小代表越靠近 Scrapy 引擎，数字越大代表越靠近 Downloader。每个 Downloader Middleware 都可以定义 process_request() 和 request_response() 方法来分别处理请求和响应，对于 process_request() 方法来说，优先级数字越小越先被调用，对于 process_response() 方法来说，优先级数字越大越先被调用。。 如果自己定义的 Downloader Middleware 要添加到项目里，DOWNLOADER_MIDDLEWARES_BASE 变量不能直接修改。Scrapy 提供了另外一个设置变量 DOWNLOADER_MIDDLEWARES，我们直接修改这个变量就可以添加自己定义的 Downloader Middleware，以及禁用 DOWNLOADER_MIDDLEWARES_BASE 里面定义的 Downloader Middleware。下面我们具体来看看 Downloader Middleware 的使用方法。</p>
                  <h3 id="2-核心方法"><a href="#2-核心方法" class="headerlink" title="2. 核心方法"></a>2. 核心方法</h3>
                  <p>Scrapy 内置的 Downloader Middleware 为 Scrapy 提供了基础的功能，但在项目实战中我们往往需要单独定义 Downloader Middleware。不用担心，这个过程非常简单，我们只需要实现某几个方法即可。 每个 Downloader Middleware 都定义了一个或多个方法的类，核心的方法有如下三个。</p>
                  <ul>
                    <li>process_request(request, spider)</li>
                    <li>process_response(request, response, spider)</li>
                    <li>process_exception(request, exception, spider)</li>
                  </ul>
                  <p>我们只需要实现至少一个方法，就可以定义一个 Downloader Middleware。下面我们来看看这三个方法的详细用法。</p>
                  <h4 id="process-request-request-spider"><a href="#process-request-request-spider" class="headerlink" title="process_request(request, spider)"></a>process_request(request, spider)</h4>
                  <p>Request 被 Scrapy 引擎调度给 Downloader 之前，process_request() 方法就会被调用，也就是在 Request 从队列里调度出来到 Downloader 下载执行之前，我们都可以用 process_request() 方法对 Request 进行处理。方法的返回值必须为 None、Response 对象、Request 对象之一，或者抛出 IgnoreRequest 异常。 process_request() 方法的参数有如下两个。</p>
                  <ul>
                    <li>request，即 Request 对象，即被处理的 Request</li>
                    <li>spider，即 Spdier 对象，即此 Request 对应的 Spider</li>
                  </ul>
                  <p>返回类型不同，产生的效果也不同。下面归纳一下不同的返回情况。</p>
                  <ul>
                    <li>当返回是 None 时，Scrapy 将继续处理该 Request，接着执行其他 Downloader Middleware 的 process_request() 方法，一直到 Downloader 把 Request 执行后得到 Response 才结束。这个过程其实就是修改 Request 的过程，不同的 Downloader Middleware 按照设置的优先级顺序依次对 Request 进行修改，最后送至 Downloader 执行。</li>
                    <li>当返回为 Response 对象时，更低优先级的 Downloader Middleware 的 process_request() 和 process_exception() 方法就不会被继续调用，每个 Downloader Middleware 的 process_response() 方法转而被依次调用。调用完毕之后，直接将 Response 对象发送给 Spider 来处理。</li>
                    <li>当返回为 Request 对象时，更低优先级的 Downloader Middleware 的 process_request() 方法会停止执行。这个 Request 会重新放到调度队列里，其实它就是一个全新的 Request，等待被调度。如果被 Scheduler 调度了，那么所有的 Downloader Middleware 的 process_request() 方法会被重新按照顺序执行。</li>
                    <li>如果 IgnoreRequest 异常抛出，则所有的 Downloader Middleware 的 process_exception() 方法会依次执行。如果没有一个方法处理这个异常，那么 Request 的 errorback() 方法就会回调。如果该异常还没有被处理，那么它便会被忽略。</li>
                  </ul>
                  <h4 id="process-response-request-response-spider"><a href="#process-response-request-response-spider" class="headerlink" title="process_response(request, response, spider)"></a>process_response(request, response, spider)</h4>
                  <p>Downloader 执行 Request 下载之后，会得到对应的 Response。Scrapy 引擎便会将 Response 发送给 Spider 进行解析。在发送之前，我们都可以用 process_response() 方法来对 Response 进行处理。方法的返回值必须为 Request 对象、Response 对象之一，或者抛出 IgnoreRequest 异常。 process_response() 方法的参数有如下三个。</p>
                  <ul>
                    <li>request，是 Request 对象，即此 Response 对应的 Request。</li>
                    <li>response，是 Response 对象，即此被处理的 Response。</li>
                    <li>spider，是 Spider 对象，即此 Response 对应的 Spider。</li>
                  </ul>
                  <p>下面对不同的返回情况做一下归纳：</p>
                  <ul>
                    <li>当返回为 Request 对象时，更低优先级的 Downloader Middleware 的 process_response() 方法不会继续调用。该 Request 对象会重新放到调度队列里等待被调度，它相当于一个全新的 Request。然后，该 Request 会被 process_request() 方法顺次处理。</li>
                    <li>当返回为 Response 对象时，更低优先级的 Downloader Middleware 的 process_response() 方法会继续调用，继续对该 Response 对象进行处理。</li>
                    <li>如果 IgnoreRequest 异常抛出，则 Request 的 errorback() 方法会回调。如果该异常还没有被处理，那么它便会被忽略。</li>
                  </ul>
                  <h4 id="process-exception-request-exception-spider"><a href="#process-exception-request-exception-spider" class="headerlink" title="process_exception(request, exception, spider)"></a>process_exception(request, exception, spider)</h4>
                  <p>当 Downloader 或 process_request() 方法抛出异常时，例如抛出 IgnoreRequest 异常，process_exception() 方法就会被调用。方法的返回值必须为 None、Response 对象、Request 对象之一。 process_exception() 方法的参数有如下三个。</p>
                  <ul>
                    <li>request，即 Request 对象，即产生异常的 Request</li>
                    <li>exception，即 Exception 对象，即抛出的异常</li>
                    <li>spdier，即 Spider 对象，即 Request 对应的 Spider</li>
                  </ul>
                  <p>下面归纳一下不同的返回值。</p>
                  <ul>
                    <li>当返回为 None 时，更低优先级的 Downloader Middleware 的 process_exception() 会被继续顺次调用，直到所有的方法都被调度完毕。</li>
                    <li>当返回为 Response 对象时，更低优先级的 Downloader Middleware 的 process_exception() 方法不再被继续调用，每个 Downloader Middleware 的 process_response() 方法转而被依次调用。</li>
                    <li>当返回为 Request 对象时，更低优先级的 Downloader Middleware 的 process_exception() 也不再被继续调用，该 Request 对象会重新放到调度队列里面等待被调度，它相当于一个全新的 Request。然后，该 Request 又会被 process_request() 方法顺次处理。</li>
                  </ul>
                  <p>以上内容便是这三个方法的详细使用逻辑。在使用它们之前，请先对这三个方法的返回值的处理情况有一个清晰的认识。在自定义 Downloader Middleware 的时候，也一定要注意每个方法的返回类型。 下面我们用一个案例实战来加深一下对 Downloader Middleware 用法的理解。</p>
                  <h3 id="3-项目实战"><a href="#3-项目实战" class="headerlink" title="3. 项目实战"></a>3. 项目实战</h3>
                  <p>新建一个项目，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject scrapydownloadertest</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建了一个 Scrapy 项目，名为 scrapydownloadertest。进入项目，新建一个 Spider，命令如下所示：</p>
                  <figure class="highlight avrasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy genspider httpbin httpbin<span class="meta">.org</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建了一个 Spider，名为 httpbin，源代码如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HttpbinSpider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'httpbin'</span></span><br><span class="line">    allowed_domains = [<span class="string">'httpbin.org'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://httpbin.org/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        pass</span><br><span class="line"><span class="string">``</span><span class="string">`接下来我们修改 start_urls 为：`</span>[<span class="string">'http://httpbin.org/'</span>]<span class="string">`。随后将 parse() 方法添加一行日志输出，将 response 变量的 text 属性输出出来，这样我们便可以看到 Scrapy 发送的 Request 信息了。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">修改 Spider 内容如下所示：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">`</span><span class="string">``</span>python</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HttpbinSpider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'httpbin'</span></span><br><span class="line">    allowed_domains = [<span class="string">'httpbin.org'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://httpbin.org/get'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.logger.debug(response.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来运行此 Spider，执行如下命令：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl httpbin</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Scrapy 运行结果包含 Scrapy 发送的 Request 信息，内容如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"args"</span>: &#123;&#125;, </span><br><span class="line">  <span class="attr">"headers"</span>: &#123;</span><br><span class="line">    <span class="attr">"Accept"</span>: <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"</span>, </span><br><span class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip,deflate,br"</span>, </span><br><span class="line">    <span class="attr">"Accept-Language"</span>: <span class="string">"en"</span>, </span><br><span class="line">    <span class="attr">"Connection"</span>: <span class="string">"close"</span>, </span><br><span class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>, </span><br><span class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"Scrapy/1.4.0 (+http://scrapy.org)"</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="attr">"origin"</span>: <span class="string">"60.207.237.85"</span>, </span><br><span class="line">  <span class="attr">"url"</span>: <span class="string">"http://httpbin.org/get"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们观察一下 Headers，Scrapy 发送的 Request 使用的 User-Agent 是 Scrapy/1.4.0(+<a href="http://scrapy.org" target="_blank" rel="noopener">http://scrapy.org)，这其实是由</a>，这其实是由) Scrapy 内置的 UserAgentMiddleware 设置的，UserAgentMiddleware 的源码如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy import signals</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserAgentMiddleware</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, user_agent=<span class="string">'Scrapy'</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.user_agent = user_agent</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        o = cls(crawler.settings[<span class="string">'USER_AGENT'</span>])</span><br><span class="line">        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> o</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.user_agent = getattr(spider, <span class="string">'user_agent'</span>, <span class="keyword">self</span>.user_agent)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(<span class="keyword">self</span>, request, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">user_agent:</span></span><br><span class="line">            request.headers.setdefault(b<span class="string">'User-Agent'</span>, <span class="keyword">self</span>.user_agent)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 from_crawler() 方法中，首先尝试获取 settings 里面 USER_AGENT，然后把 USER_AGENT 传递给<strong>init</strong>() 方法进行初始化，其参数就是 user_agent。如果没有传递 USER_AGENT 参数就默认设置为 Scrapy 字符串。我们新建的项目没有设置 USER_AGENT，所以这里的 user_agent 变量就是 Scrapy。接下来，在 process_request() 方法中，将 user-agent 变量设置为 headers 变量的一个属性，这样就成功设置了 User-Agent。因此，User-Agent 就是通过此 Downloader Middleware 的 process_request() 方法设置的。 修改请求时的 User-Agent 可以有两种方式：一是修改 settings 里面的 USER_AGENT 变量；二是通过 Downloader Middleware 的 process_request() 方法来修改。 第一种方法非常简单，我们只需要在 setting.py 里面加一行 USER_AGENT 的定义即可：</p>
                  <figure class="highlight lsl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">USER_AGENT = 'Mozilla/<span class="number">5.0</span> (Macintosh; Intel Mac OS X <span class="number">10</span>_12_6) AppleWebKit/<span class="number">537.36</span> (KHTML, like Gecko) Chrome/<span class="number">59.0</span><span class="number">.3071</span><span class="number">.115</span> Safari/<span class="number">537.36</span>'</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>一般推荐使用此方法来设置。但是如果想设置得更灵活，比如设置随机的 User-Agent，那就需要借助 Downloader Middleware 了。所以接下来我们用 Downloader Middleware 实现一个随机 User-Agent 的设置。 在 middlewares.py 里面添加一个 RandomUserAgentMiddleware 的类，如下所示：</p>
                  <figure class="highlight haskell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">RandomUserAgentMiddleware</span>():</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        self.user_agents = ['<span class="type">Mozilla</span>/5.0 (<span class="type">Windows</span>; <span class="type">U</span>; <span class="type">MSIE</span> 9.0; <span class="type">Windows</span> <span class="type">NT</span> 9.0; <span class="title">en</span>-<span class="type">US</span>)',</span></span><br><span class="line"><span class="class">            '<span class="type">Mozilla</span>/5.0 (<span class="type">Windows</span> <span class="type">NT</span> 6.1) <span class="type">AppleWebKit</span>/537.2 (<span class="type">KHTML</span>, <span class="title">like</span> <span class="type">Gecko</span>) <span class="type">Chrome</span>/22.0.1216.0 <span class="type">Safari</span>/537.2',</span></span><br><span class="line"><span class="class">            '<span class="type">Mozilla</span>/5.0 (<span class="type">X11</span>; <span class="type">Ubuntu</span>; <span class="type">Linux</span> <span class="title">i686</span>; <span class="title">rv</span>:15.0) <span class="type">Gecko</span>/20100101 <span class="type">Firefox</span>/15.0.1'</span></span><br><span class="line"><span class="class">        ]</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def process_request(<span class="title">self</span>, <span class="title">request</span>, <span class="title">spider</span>):</span></span><br><span class="line"><span class="class">        request.headers['<span class="type">User</span>-<span class="type">Agent'</span>] = random.choice(<span class="title">self</span>.<span class="title">user_agents</span>)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们首先在类的 <strong>init</strong>() 方法中定义了三个不同的 User-Agent，并用一个列表来表示。接下来实现了 process_request() 方法，它有一个参数 request，我们直接修改 request 的属性即可。在这里我们直接设置了 request 对象的 headers 属性的 User-Agent，设置内容是随机选择的 User-Agent，这样一个 Downloader Middleware 就写好了。 不过，要使之生效我们还需要再去调用这个 Downloader Middleware。在 settings.py 中，将 DOWNLOADER_MIDDLEWARES 取消注释，并设置成如下内容：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">DOWNLOADER_MIDDLEWARES</span> = &#123;<span class="string">'scrapydownloadertest.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">543</span>,&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来我们重新运行 Spider，就可以看到 User-Agent 被成功修改为列表中所定义的随机的一个 User-Agent 了：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"args"</span>: &#123;&#125;, </span><br><span class="line">  <span class="attr">"headers"</span>: &#123;</span><br><span class="line">    <span class="attr">"Accept"</span>: <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"</span>, </span><br><span class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip,deflate,br"</span>, </span><br><span class="line">    <span class="attr">"Accept-Language"</span>: <span class="string">"en"</span>, </span><br><span class="line">    <span class="attr">"Connection"</span>: <span class="string">"close"</span>, </span><br><span class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>, </span><br><span class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)"</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="attr">"origin"</span>: <span class="string">"60.207.237.85"</span>, </span><br><span class="line">  <span class="attr">"url"</span>: <span class="string">"http://httpbin.org/get"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们就通过实现 Downloader Middleware 并利用 process_request() 方法成功设置了随机的 User-Agent。 另外，Downloader Middleware 还有 process_response() 方法。Downloader 对 Request 执行下载之后会得到 Response，随后 Scrapy 引擎会将 Response 发送回 Spider 进行处理。但是在 Response 被发送给 Spider 之前，我们同样可以使用 process_response() 方法对 Response 进行处理。比如这里修改一下 Response 的状态码，在 RandomUserAgentMiddleware 添加如下代码：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(<span class="keyword">self</span>, request, response, spider)</span></span><span class="symbol">:</span></span><br><span class="line">    response.status = <span class="number">201</span></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们将 response 对象的 status 属性修改为 201，随后将 response 返回，这个被修改后的 Response 就会被发送到 Spider。 我们再在 Spider 里面输出修改后的状态码，在 parse() 方法中添加如下的输出语句：</p>
                  <figure class="highlight gauss">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">self.logger.<span class="keyword">debug</span>('Status <span class="built_in">Code</span>: ' + <span class="built_in">str</span>(response.status))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>重新运行之后，控制台输出了如下内容：</p>
                  <figure class="highlight gauss">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[httpbin] <span class="keyword">DEBUG</span>: Status <span class="built_in">Code</span>: <span class="number">201</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，Response 的状态码成功修改了。 因此要想对 Response 进行后处理，就可以借助于 process_response() 方法。 另外还有一个 process_exception() 方法，它是用来处理异常的方法。如果需要异常处理的话，我们可以调用此方法。不过这个方法的使用频率相对低一些，在此不用实例演示。</p>
                  <h3 id="4-本节代码"><a href="#4-本节代码" class="headerlink" title="4. 本节代码"></a>4. 本节代码</h3>
                  <p>本节源代码为：<a href="https://github.com/Python3WebSpider/ScrapyDownloaderTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyDownloaderTest</a>。</p>
                  <h3 id="5-结语"><a href="#5-结语" class="headerlink" title="5. 结语"></a>5. 结语</h3>
                  <p>本节讲解了 Downloader Middleware 的基本用法。此组件非常重要，是做异常处理和应对反爬处理的核心。后面我们会在实战中应用此组件来处理代理、Cookies 等内容。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-04 15:11:33" itemprop="dateCreated datePublished" datetime="2019-12-04T15:11:33+08:00">2019-12-04</time>
                </span>
                <span id="/8381.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.5–Downloader Middleware 的用法" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>9.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>9 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8364.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8364.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.1–Scrapy 框架介绍</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-1-Scrapy-框架介绍"><a href="#13-1-Scrapy-框架介绍" class="headerlink" title="13.1 Scrapy 框架介绍"></a>13.1 Scrapy 框架介绍</h1>
                  <p>Scrapy 是一个基于 Twisted 的异步处理框架，是纯 Python 实现的爬虫框架，其架构清晰，模块之间的耦合程度低，可扩展性极强，可以灵活完成各种需求。我们只需要定制开发几个模块就可以轻松实现一个爬虫。</p>
                  <h3 id="1-架构介绍"><a href="#1-架构介绍" class="headerlink" title="1. 架构介绍"></a>1. 架构介绍</h3>
                  <p>首先我们来看下 Scrapy 框架的架构，如图 13-1 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-033839.jpg" alt=""> 图 13-1 Scrapy 架构 它可以分为如下的几个部分。</p>
                  <ul>
                    <li>Engine，引擎，用来处理整个系统的数据流处理，触发事务，是整个框架的核心。</li>
                    <li>Item，项目，它定义了爬取结果的数据结构，爬取的数据会被赋值成该对象。</li>
                    <li>Scheduler， 调度器，用来接受引擎发过来的请求并加入队列中，并在引擎再次请求的时候提供给引擎。</li>
                    <li>Downloader，下载器，用于下载网页内容，并将网页内容返回给蜘蛛。</li>
                    <li>Spiders，蜘蛛，其内定义了爬取的逻辑和网页的解析规则，它主要负责解析响应并生成提取结果和新的请求。</li>
                    <li>Item Pipeline，项目管道，负责处理由蜘蛛从网页中抽取的项目，它的主要任务是清洗、验证和存储数据。</li>
                    <li>Downloader Middlewares，下载器中间件，位于引擎和下载器之间的钩子框架，主要是处理引擎与下载器之间的请求及响应。</li>
                    <li>Spider Middlewares， 蜘蛛中间件，位于引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛输入的响应和输出的结果及新的请求。</li>
                  </ul>
                  <h3 id="2-数据流"><a href="#2-数据流" class="headerlink" title="2. 数据流"></a>2. 数据流</h3>
                  <p>Scrapy 中的数据流由引擎控制，其过程如下:</p>
                  <ul>
                    <li>Engine 首先打开一个网站，找到处理该网站的 Spider 并向该 Spider 请求第一个要爬取的 URL。</li>
                    <li>Engine 从 Spider 中获取到第一个要爬取的 URL 并通过 Scheduler 以 Request 的形式调度。</li>
                    <li>Engine 向 Scheduler 请求下一个要爬取的 URL。</li>
                    <li>Scheduler 返回下一个要爬取的 URL 给 Engine，Engine 将 URL 通过 Downloader Middlewares 转发给 Downloader 下载。</li>
                    <li>一旦页面下载完毕， Downloader 生成一个该页面的 Response，并将其通过 Downloader Middlewares 发送给 Engine。</li>
                    <li>Engine 从下载器中接收到 Response 并通过 Spider Middlewares 发送给 Spider 处理。</li>
                    <li>Spider 处理 Response 并返回爬取到的 Item 及新的 Request 给 Engine。</li>
                    <li>Engine 将 Spider 返回的 Item 给 Item Pipeline，将新的 Request 给 Scheduler。</li>
                    <li>重复第二步到最后一步，直到 Scheduler 中没有更多的 Request，Engine 关闭该网站，爬取结束。</li>
                  </ul>
                  <p>通过多个组件的相互协作、不同组件完成工作的不同、组件对异步处理的支持，Scrapy 最大限度地利用了网络带宽，大大提高了数据爬取和处理的效率。</p>
                  <h3 id="3-项目结构"><a href="#3-项目结构" class="headerlink" title="3. 项目结构"></a>3. 项目结构</h3>
                  <p>Scrapy 框架和 pyspider 不同，它是通过命令行来创建项目的，代码的编写还是需要 IDE。项目创建之后，项目文件结构如下所示：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy.cfg</span><br><span class="line">project/</span><br><span class="line">    __init__.py</span><br><span class="line">    items.py</span><br><span class="line">    pipelines.py</span><br><span class="line">    settings.py</span><br><span class="line">    middlewares.py</span><br><span class="line">    spiders/</span><br><span class="line">        __init__.py</span><br><span class="line">        spider1.py</span><br><span class="line">        spider2.py</span><br><span class="line">        ...</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在此要将各个文件的功能描述如下：</p>
                  <ul>
                    <li>scrapy.cfg：它是 Scrapy 项目的配置文件，其内定义了项目的配置文件路径、部署相关信息等内容。</li>
                    <li>items.py：它定义 Item 数据结构，所有的 Item 的定义都可以放这里。</li>
                    <li>pipelines.py：它定义 Item Pipeline 的实现，所有的 Item Pipeline 的实现都可以放这里。</li>
                    <li>settings.py：它定义项目的全局配置。</li>
                    <li>middlewares.py：它定义 Spider Middlewares 和 Downloader Middlewares 的实现。</li>
                    <li>spiders：其内包含一个个 Spider 的实现，每个 Spider 都有一个文件。</li>
                  </ul>
                  <h3 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h3>
                  <p>本节介绍了 Scrapy 框架的基本架构、数据流过程以及项目结构。后面我们会详细了解 Scrapy 的用法，感受它的强大。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-03 11:42:17" itemprop="dateCreated datePublished" datetime="2019-12-03T11:42:17+08:00">2019-12-03</time>
                </span>
                <span id="/8364.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.1–Scrapy 框架介绍" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8361.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8361.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 9.4–ADSL 拨号代理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="9-4-ADSL-拨号代理"><a href="#9-4-ADSL-拨号代理" class="headerlink" title="9.4 ADSL 拨号代理"></a>9.4 ADSL 拨号代理</h1>
                  <p>我们尝试维护过一个代理池。代理池可以挑选出许多可用代理，但是常常其稳定性不高、响应速度慢，而且这些代理通常是公共代理，可能不止一人同时使用，其 IP 被封的概率很大。另外，这些代理可能有效时间比较短，虽然代理池一直在筛选，但如果没有及时更新状态，也有可能获取到不可用的代理。 如果要追求更加稳定的代理，就需要购买专有代理或者自己搭建代理服务器。但是服务器一般都是固定的 IP，我们总不能搭建 100 个代理就用 100 台服务器吧，这显然是不现实的。 所以，ADSL 动态拨号主机就派上用场了。下面我们来了解一下 ADSL 拨号代理服务器的相关设置。</p>
                  <h3 id="1-什么是-ADSL"><a href="#1-什么是-ADSL" class="headerlink" title="1. 什么是 ADSL"></a>1. 什么是 ADSL</h3>
                  <p>ADSL（Asymmetric Digital Subscriber Line，非对称数字用户环路），它的上行和下行带宽不对称，它采用频分复用技术把普通的电话线分成了电话、上行和下行 3 个相对独立的信道，从而避免了相互之间的干扰。 ADSL 通过拨号的方式上网，需要输入 ADSL 账号和密码，每次拨号就更换一个 IP。IP 分布在多个 A 段，如果 IP 都能使用，则意味着 IP 量级可达千万。如果我们将 ADSL 主机作为代理，每隔一段时间主机拨号就换一个 IP，这样可以有效防止 IP 被封禁。另外，主机的稳定性很好，代理响应速度很快。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>首先需要成功安装 Redis 数据库并启动服务，另外还需要安装 requests、redis-py、Tornado 库。如果没有安装，读者可以参考第一章的安装说明。</p>
                  <h3 id="3-购买主机"><a href="#3-购买主机" class="headerlink" title="3. 购买主机"></a>3. 购买主机</h3>
                  <p>我们先购买一台动态拨号 VPS 主机，这样的主机服务商相当多。在这里使用了云立方，官方网站：<a href="http://www.yunlifang.cn/dynamicvps.asp" target="_blank" rel="noopener">http://www.yunlifang.cn/dynamicvps.asp</a>。 建议选择电信线路。可以自行选择主机配置，主要考虑带宽是否满足需求。 然后进入拨号主机的后台，预装一个操作系统，如图 9-10 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-052822.jpg" alt=""> 图 9-10 预装操作系统 推荐安装 CentOS 7 系统。 然后找到远程管理面板  远程连接的用户名和密码，也就是 SSH 远程连接服务器的信息。比如我使用的 IP 和端口是 153.36.65.214:20063，用户名是 root。命令行下输入如下内容：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ssh <span class="symbol">root@</span><span class="number">153.36</span><span class="number">.65</span><span class="number">.214</span> -p <span class="number">20063</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输入管理密码，就可以连接上远程服务器了。 进入之后，我们发现一个可用的脚本文件 ppp.sh，这是拨号初始化的脚本。运行此脚本会提示输入拨号的用户名和密码，然后它就开始各种拨号配置。一次配置成功，后面拨号就不需要重复输入用户名和密码。 运行 ppp.sh 脚本，输入用户名、密码等待它的配置完成，如图 9-11 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-052827.jpg" alt=""> 图 9-11 配置页面 提示成功之后就可以进行拨号了。注意，在拨号之前测试 ping 任何网站都是不通的，因为当前网络还没联通。输入如下拨号命令：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">adsl-<span class="literal">start</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>拨号命令成功运行，没有报错信息，耗时约几秒。接下来再去 ping 外网就可以通了。 如果要停止拨号，可以输入如下命令：</p>
                  <figure class="highlight arduino">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">adsl-<span class="built_in">stop</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>之后，可以发现又连不通网络了，如图 9-12 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-052841.jpg" alt=""> 图 9-12 拨号建立连接 断线重播的命令就是二者组合起来，先执行 adsl-stop，再执行 adsl-start。每次拨号，ifconfig 命令观察主机的 IP，发现主机的 IP 一直在变化，网卡名称叫作 ppp0，如图 9-13 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-052851.jpg" alt=""> 图 9-13 网络设备信息 接下来，我们要做两件事：一是怎样将主机设置为代理服务器，二是怎样实时获取拨号主机的 IP。</p>
                  <h3 id="4-设置代理服务器"><a href="#4-设置代理服务器" class="headerlink" title="4. 设置代理服务器"></a>4. 设置代理服务器</h3>
                  <p>在 Linux 下搭建 HTTP 代理服务器，推荐 TinyProxy 和 Squid，配置都非常简单。在这里我们以 TinyProxy 为例来讲解一下怎样搭建代理服务器。</p>
                  <h4 id="安装-TinyProxy"><a href="#安装-TinyProxy" class="headerlink" title="安装 TinyProxy"></a>安装 TinyProxy</h4>
                  <p>第一步就是安装 TinyProxy 软件。在这里我使用的系统是 CentOS，所以使用 yum 来安装。如果是其他系统，如 Ubuntu，可以选择 apt-get 等命令安装。 命令行执行 yum 安装指令：</p>
                  <figure class="highlight sql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">yum <span class="keyword">install</span> -y epel-<span class="keyword">release</span></span><br><span class="line">yum <span class="keyword">update</span> -y</span><br><span class="line">yum <span class="keyword">install</span> -y tinyproxy</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行完成之后就可以完成 tinyproxy 的安装了。</p>
                  <h4 id="配置-TinyProxy"><a href="#配置-TinyProxy" class="headerlink" title="配置 TinyProxy"></a>配置 TinyProxy</h4>
                  <p>TinyProxy 安装完成之后还要配置一下才可以用作代理服务器。我们需要编辑配置文件，此文件一般的路径是 /etc/tinyproxy/tinyproxy.conf。 可以看到有一行</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Port <span class="number">8888</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里可以设置代理的端口，默认是 8888。 继续向下找到如下代码：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Allow <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这行代码表示被允许连接的主机 IP。如果希望连接任何主机，那就直接将这行代码注释即可。在这里我们选择直接注释，也就是任何主机都可以使用这台主机作为代理服务器。 修改为如下代码：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"># Allow <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>设置完成之后重启 TinyProxy 即可：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">systemctl</span> <span class="selector-tag">enable</span> <span class="selector-tag">tinyproxy</span><span class="selector-class">.service</span></span><br><span class="line"><span class="selector-tag">systemctl</span> <span class="selector-tag">restart</span> <span class="selector-tag">tinyproxy</span><span class="selector-class">.service</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>防火墙开放该端口：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">iptables -I <span class="keyword">INPUT</span> -p tcp <span class="comment">--dport 8888 -j ACCEPT</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然如果想直接关闭防火墙也可以：</p>
                  <figure class="highlight arduino">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">systemctl <span class="built_in">stop</span> firewalld.service</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就完成了 TinyProxy 的配置了。</p>
                  <h4 id="验证-TinyProxy"><a href="#验证-TinyProxy" class="headerlink" title="验证 TinyProxy"></a>验证 TinyProxy</h4>
                  <p>首先，用 ifconfig 查看当前主机的 IP。比如，当前我的主机拨号 IP 为 112.84.118.216，在其他的主机运行测试一下。 用 curl 命令设置代理请求 httpbin，检测代理是否生效。</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl -x <span class="number">112.84</span><span class="number">.118</span><span class="number">.216</span>:<span class="number">8888</span> httpbin.org/<span class="keyword">get</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如图 9-14 所示： <img src="https://cdn.cuiqingcai.com/2019-10-20-052902.jpg" alt=""> 图 9-14 运行结果 如果有正常的结果输出，并且 origin 的值为代理 IP 的地址，就证明 TinyProxy 配置成功了。</p>
                  <h3 id="5-动态获取-IP"><a href="#5-动态获取-IP" class="headerlink" title="5. 动态获取 IP"></a>5. 动态获取 IP</h3>
                  <p>现在可以执行命令让主机动态切换 IP，也在主机上搭建了代理服务器。我们只需要知道拨号后的 IP 就可以使用代理。 我们考虑到，在一台主机拨号切换 IP 的间隙代理是不可用的，在这拨号的几秒时间内如果有第二台主机顶替第一台主机，那就可以解决拨号间隙代理无法使用的问题了。所以我们要设计的架构必须要考虑支持多主机的问题。 假如有 10 台拨号主机同时需要维护，而爬虫需要使用这 10 台主机的代理，那么在爬虫端维护的开销是非常大的。如果爬虫在不同的机器上运行，那么每个爬虫必须要获得这 10 台拨号主机的配置，这显然是不理想的。 为了更加方便地使用代理，我们可以像上文的代理池一样定义一个统一的代理接口，爬虫端只需要配置代理接口即可获取可用代理。要搭建一个接口，就势必需要一台服务器，而接口的数据从哪里获得呢，当然最理想的还是选择数据库。 比如我们需要同时维护 10 台拨号主机，每台拨号主机都会定时拨号，那这样每台主机在某个时刻可用的代理只有一个，所以我们没有必要存储之前的拨号代理，因为重新拨号之后之前的代理已经不能用了，所以只需要将之前的代理更新其内容就好了。数据库要做的就是定时对每台主机的代理进行更新，而更新时又需要拨号主机的唯一标识，根据主机标识查出这条数据，然后将这条数据对应的代理更新。 所以数据库端就需要存储一个主机标识到代理的映射关系。那么很自然地我们就会想到关系型数据库，如 MySQL 或者 Redis 的 Hash 存储，只需存储一个映射关系，不需要很多字段，而且 Redis 比 MySQL 效率更高、使用更方便，所以最终选定的存储方式就是 Redis 的 Hash。</p>
                  <h3 id="6-存储模块"><a href="#6-存储模块" class="headerlink" title="6. 存储模块"></a>6. 存储模块</h3>
                  <p>那么接下来我们要做可被远程访问的 Redis 数据库，各个拨号机器只需要将各自的主机标识和当前 IP 和端口（也就是代理）发送给数据库就好了。 先定义一个操作 Redis 数据库的类，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis 数据库 IP</span></span><br><span class="line">REDIS_HOST = <span class="string">'remoteaddress'</span></span><br><span class="line"><span class="comment"># Redis 数据库密码，如无则填 None</span></span><br><span class="line">REDIS_PASSWORD = <span class="string">'foobared'</span></span><br><span class="line"><span class="comment"># Redis 数据库端口</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br><span class="line"><span class="comment"># 代理池键名</span></span><br><span class="line">PROXY_KEY = <span class="string">'adsl'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisClient</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, proxy_key=PROXY_KEY)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化 Redis 连接</span></span><br><span class="line"><span class="string">        :param host: Redis 地址</span></span><br><span class="line"><span class="string">        :param port: Redis 端口</span></span><br><span class="line"><span class="string">        :param password: Redis 密码</span></span><br><span class="line"><span class="string">        :param proxy_key: Redis 哈希表名</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.db = redis.StrictRedis(host=host, port=port, password=password, decode_responses=<span class="literal">True</span>)</span><br><span class="line">        self.proxy_key = proxy_key</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set</span><span class="params">(self, name, proxy)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        设置代理</span></span><br><span class="line"><span class="string">        :param name: 主机名称</span></span><br><span class="line"><span class="string">        :param proxy: 代理</span></span><br><span class="line"><span class="string">        :return: 设置结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hset(self.proxy_key, name, proxy)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取代理</span></span><br><span class="line"><span class="string">        :param name: 主机名称</span></span><br><span class="line"><span class="string">        :return: 代理</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hget(self.proxy_key, name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取代理总数</span></span><br><span class="line"><span class="string">        :return: 代理总数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hlen(self.proxy_key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        删除代理</span></span><br><span class="line"><span class="string">        :param name: 主机名称</span></span><br><span class="line"><span class="string">        :return: 删除结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hdel(self.proxy_key, name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">names</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取主机名称列表</span></span><br><span class="line"><span class="string">        :return: 获取主机名称列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hkeys(self.proxy_key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">proxies</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取代理列表</span></span><br><span class="line"><span class="string">        :return: 代理列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hvals(self.proxy_key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        随机获取代理</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        proxies = self.proxies()</span><br><span class="line">        <span class="keyword">return</span> random.choice(proxies)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">all</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取字典</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span><span class="keyword">return</span> self.db.hgetall(self.proxy_key)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了一个 RedisClient 类，在<strong>init</strong>() 方法中初始化了 Redis 连接，其中 REDIS_HOST 就是远程 Redis 的地址，REDIS_PASSWORD 是密码，REDIS_PORT 是端口，PROXY_KEY 是存储代理的散列表的键名。 接下来定义了一个 set() 方法，这个方法用来向散列表添加映射关系。映射是从主机标识到代理的映射，比如一台主机的标识为 adsl1，当前的代理为 118.119.111.172:8888，那么散列表中就会存储一个 key 为 adsl1、value 为 118.119.111.172:8888 的映射，Hash 结构如图 9-15 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-052948.jpg" alt=""> 图 9-15 Hash 结构 如果有多台主机，只需要向 Hash 中添加映射即可。 另外，get() 方法就是从散列表中取出某台主机对应的代理。remove() 方法则是从散列表中移除对应的主机的代理。还有 names()、proxies()、all() 方法则是分别获取散列表中的主机列表、代理列表及所有主机代理映射。count() 方法则是返回当前散列表的大小，也就是可用代理的数目。 最后还有一个比较重要的方法 random()，它随机从散列表中取出一个可用代理，类似前面代理池的思想，确保每个代理都能被取到。 如果要对数据库进行操作，只需要初始化 RedisClient 对象，然后调用它的 set() 或者 remove() 方法，即可对散列表进行设置和删除。</p>
                  <h3 id="7-拨号模块"><a href="#7-拨号模块" class="headerlink" title="7. 拨号模块"></a>7. 拨号模块</h3>
                  <p>接下来要做的就是拨号，并把新的 IP 保存到 Redis 散列表里。 首先是拨号定时，它分为定时拨号和非定时拨号两种选择。 非定时拨号：最好的方法就是向该主机发送一个信号，然后主机就启动拨号，但这样做的话，我们首先要搭建一个重新拨号的接口，如搭建一个 Web 接口，请求该接口即进行拨号，但开始拨号之后，此时主机的状态就从在线转为离线，而此时的 Web 接口也就相应失效了，拨号过程无法再连接，拨号之后接口的 IP 也变了，所以我们无法通过接口来方便地控制拨号过程和获取拨号结果，下次拨号还得改变拨号请求接口，所以非定时拨号的开销还是比较大的。 定时拨号：我们只需要在拨号主机上运行定时脚本即可，每隔一段时间拨号一次，更新 IP，然后将 IP 在 Redis 散列表中更新即可，非常简单易用，另外可以适当将拨号频率调高一点，减少短时间内 IP 被封的可能性。 在这里选择定时拨号。 接下来就是获取 IP。获取拨号后的 IP 非常简单，只需要调用 ifconfig 命令，然后解析出对应网卡的 IP 即可。 获取了 IP 之后，我们还需要进行有效性检测。拨号主机可以自己检测，比如可以利用 requests 设置自身的代理请求外网，如果成功，那么证明代理可用，然后再修改 Redis 散列表，更新代理。 需要注意，由于在拨号的间隙拨号主机是离线状态，而此时 Redis 散列表中还存留了上次的代理，一旦这个代理被取用了，该代理是无法使用的。为了避免这个情况，每台主机在拨号之前还需要将自身的代理从 Redis 散列表中移除。 这样基本的流程就理顺了，我们用如下代码实现：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import re</span><br><span class="line">import time</span><br><span class="line">import requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions import ConnectionError, ReadTimeout</span><br><span class="line"><span class="keyword">from</span> db import RedisClient</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拨号网卡</span></span><br><span class="line">ADSL_IFNAME = <span class="string">'ppp0'</span></span><br><span class="line"><span class="comment"># 测试 URL</span></span><br><span class="line">TEST_URL = <span class="string">'http://www.baidu.com'</span></span><br><span class="line"><span class="comment"># 测试超时时间</span></span><br><span class="line">TEST_TIMEOUT = 20</span><br><span class="line"><span class="comment"># 拨号间隔</span></span><br><span class="line">ADSL_CYCLE = 100</span><br><span class="line"><span class="comment"># 拨号出错重试间隔</span></span><br><span class="line">ADSL_ERROR_CYCLE = 5</span><br><span class="line"><span class="comment"># ADSL 命令</span></span><br><span class="line">ADSL_BASH = <span class="string">'adsl-stop;adsl-start'</span></span><br><span class="line"><span class="comment"># 代理运行端口</span></span><br><span class="line">PROXY_PORT = 8888</span><br><span class="line"><span class="comment"># 客户端唯一标识</span></span><br><span class="line">CLIENT_NAME = <span class="string">'adsl1'</span></span><br><span class="line"></span><br><span class="line">class Sender():</span><br><span class="line">    def get_ip(self, <span class="attribute">ifname</span>=ADSL_IFNAME):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        获取本机 IP</span></span><br><span class="line"><span class="string">        :param ifname: 网卡名称</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        (status, output) = subprocess.getstatusoutput(<span class="string">'ifconfig'</span>)</span><br><span class="line">        <span class="keyword">if</span> status == 0:</span><br><span class="line">            pattern = re.compile(ifname + <span class="string">'.*?inet.*?(d+.d+.d+.d+).*?netmask'</span>, re.S)</span><br><span class="line">            result = re.search(pattern, output)</span><br><span class="line">            <span class="keyword">if</span> result:</span><br><span class="line">               <span class="built_in"> ip </span>= result.group(1)</span><br><span class="line">                return ip</span><br><span class="line"></span><br><span class="line">    def test_proxy(self, proxy):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        测试代理</span></span><br><span class="line"><span class="string">        :param proxy: 代理</span></span><br><span class="line"><span class="string">        :return: 测试结果</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        try:</span><br><span class="line">            response = requests.<span class="builtin-name">get</span>(TEST_URL, proxies=&#123;</span><br><span class="line">                <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">                <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">            &#125;, <span class="attribute">timeout</span>=TEST_TIMEOUT)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == 200:</span><br><span class="line">                return <span class="literal">True</span></span><br><span class="line">        except (ConnectionError, ReadTimeout):</span><br><span class="line">            return <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    def remove_proxy(self):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        移除代理</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        self.redis = RedisClient()</span><br><span class="line">        self.redis.<span class="builtin-name">remove</span>(CLIENT_NAME)</span><br><span class="line">        <span class="builtin-name">print</span>(<span class="string">'Successfully Removed Proxy'</span>)</span><br><span class="line"></span><br><span class="line">    def set_proxy(self, proxy):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        设置代理</span></span><br><span class="line"><span class="string">        :param proxy: 代理</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        self.redis = RedisClient()</span><br><span class="line">        <span class="keyword">if</span> self.redis.<span class="builtin-name">set</span>(CLIENT_NAME, proxy):</span><br><span class="line">            <span class="builtin-name">print</span>(<span class="string">'Successfully Set Proxy'</span>, proxy)</span><br><span class="line"></span><br><span class="line">    def adsl(self):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        拨号主进程</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="builtin-name">print</span>(<span class="string">'ADSL Start, Remove Proxy, Please wait'</span>)</span><br><span class="line">            self.remove_proxy()</span><br><span class="line">            (status, output) = subprocess.getstatusoutput(ADSL_BASH)</span><br><span class="line">            <span class="keyword">if</span> status == 0:</span><br><span class="line">                <span class="builtin-name">print</span>(<span class="string">'ADSL Successfully'</span>)</span><br><span class="line">               <span class="built_in"> ip </span>= self.get_ip()</span><br><span class="line">                <span class="keyword">if</span> ip:</span><br><span class="line">                    <span class="builtin-name">print</span>(<span class="string">'Now IP'</span>, ip)</span><br><span class="line">                    <span class="builtin-name">print</span>(<span class="string">'Testing Proxy, Please Wait'</span>)</span><br><span class="line">                   <span class="built_in"> proxy </span>= <span class="string">'&#123;ip&#125;:&#123;port&#125;'</span>.format(<span class="attribute">ip</span>=ip, <span class="attribute">port</span>=PROXY_PORT)</span><br><span class="line">                    <span class="keyword">if</span> self.test_proxy(proxy):</span><br><span class="line">                        <span class="builtin-name">print</span>(<span class="string">'Valid Proxy'</span>)</span><br><span class="line">                        self.set_proxy(proxy)</span><br><span class="line">                        <span class="builtin-name">print</span>(<span class="string">'Sleeping'</span>)</span><br><span class="line">                        time.sleep(ADSL_CYCLE)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="builtin-name">print</span>(<span class="string">'Invalid Proxy'</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="builtin-name">print</span>(<span class="string">'Get IP Failed, Re Dialing'</span>)</span><br><span class="line">                    time.sleep(ADSL_ERROR_CYCLE)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="builtin-name">print</span>(<span class="string">'ADSL Failed, Please Check'</span>)</span><br><span class="line">                time.sleep(ADSL_ERROR_CYCLE)</span><br><span class="line">def <span class="builtin-name">run</span>():</span><br><span class="line">    sender = Sender()</span><br><span class="line">    sender.adsl()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里定义了一个 Sender 类，它的主要作用是执行定时拨号，并将新的 IP 测试通过之后更新到远程 Redis 散列表里。 主方法是 adsl() 方法，它首先是一个无限循环，循环体内就是拨号的逻辑。 adsl() 方法首先调用了 remove_proxy() 方法，将远程 Redis 散列表中本机对应的代理移除，避免拨号时本主机的残留代理被取到。 接下来利用 subprocess 模块来执行拨号脚本，拨号脚本很简单，就是 stop 之后再 start，这里将拨号的命令直接定义成了 ADSL_BASH。 随后程序又调用 get_ip() 方法，通过 subprocess 模块执行获取 IP 的命令 ifconfig，然后根据网卡名称获取了当前拨号网卡的 IP 地址，即拨号后的 IP。 再接下来就需要测试代理有效性了。程序首先调用了 test_proxy() 方法，将自身的代理设置好，使用 requests 库来用代理连接 TEST_URL。在此 TEST_URL 设置为百度，如果请求成功，则证明代理有效。 如果代理有效，再调用 set_proxy() 方法将 Redis 散列表中本机对应的代理更新，设置时需要指定本机唯一标识和本机当前代理。本机唯一标识可随意配置，其对应的变量为 CLIENT_NAME，保证各台拨号主机不冲突即可。本机当前代理则由拨号后的新 IP 加端口组合而成。通过调用 RedisClient 的 set() 方法，参数 name 为本机唯一标识，proxy 为拨号后的新代理，执行之后便可以更新散列表中的本机代理了。 建议至少配置两台主机，这样在一台主机的拨号间隙还有另一台主机的代理可用。拨号主机的数量不限，越多越好。 在拨号主机上执行拨号脚本，示例输出如图 9-16 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-052912.jpg" alt=""> 图 9-16 示例输出 首先移除了代理，再进行拨号，拨号完成之后获取新的 IP，代理检测成功之后就设置到 Redis 散列表中，然后等待一段时间再重新进行拨号。 我们添加了多台拨号主机，这样就有多个稳定的定时更新的代理可用了。Redis 散列表会实时更新各台拨号主机的代理，如图 9-17 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-053042.jpg" alt=""> 图 9-17 Hash 结构 图中所示是四台 ADSL 拨号主机配置并运行后的散列表的内容，表中的代理都是可用的。</p>
                  <h3 id="8-接口模块"><a href="#8-接口模块" class="headerlink" title="8. 接口模块"></a>8. 接口模块</h3>
                  <p>目前为止，我们已经成功实时更新拨号主机的代理。不过还缺少一个模块，那就是接口模块。像之前的代理池一样，我们也定义一些接口来获取代理，如 random 获取随机代理、count 获取代理个数等。 我们选用 Tornado 来实现，利用 Tornado 的 Server 模块搭建 Web 接口服务，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> tornado.ioloop</span><br><span class="line"><span class="keyword">import</span> tornado.web</span><br><span class="line"><span class="keyword">from</span> tornado.web <span class="keyword">import</span> RequestHandler, Application</span><br><span class="line"></span><br><span class="line"><span class="comment"># API 端口</span></span><br><span class="line">API_PORT = <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainHandler</span><span class="params">(RequestHandler)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self, redis)</span>:</span></span><br><span class="line">        self.redis = redis</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, api=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> api:</span><br><span class="line">            links = [<span class="string">'random'</span>, <span class="string">'proxies'</span>, <span class="string">'names'</span>, <span class="string">'all'</span>, <span class="string">'count'</span>]</span><br><span class="line">            self.write(<span class="string">'&lt;h4&gt;Welcome to ADSL Proxy API&lt;/h4&gt;'</span>)</span><br><span class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">                self.write(<span class="string">'&lt;a href='</span> + link + <span class="string">'&gt;'</span> + link + <span class="string">'&lt;/a&gt;&lt;br&gt;'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> api == <span class="string">'random'</span>:</span><br><span class="line">            result = self.redis.random()</span><br><span class="line">            <span class="keyword">if</span> result:</span><br><span class="line">                self.write(result)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> api == <span class="string">'names'</span>:</span><br><span class="line">            result = self.redis.names()</span><br><span class="line">            <span class="keyword">if</span> result:</span><br><span class="line">                self.write(json.dumps(result))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> api == <span class="string">'proxies'</span>:</span><br><span class="line">            result = self.redis.proxies()</span><br><span class="line">            <span class="keyword">if</span> result:</span><br><span class="line">                self.write(json.dumps(result))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> api == <span class="string">'all'</span>:</span><br><span class="line">            result = self.redis.all()</span><br><span class="line">            <span class="keyword">if</span> result:</span><br><span class="line">                self.write(json.dumps(result))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> api == <span class="string">'count'</span>:</span><br><span class="line">            self.write(str(self.redis.count()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">server</span><span class="params">(redis, port=API_PORT, address=<span class="string">''</span>)</span>:</span></span><br><span class="line">    application = Application([(<span class="string">r'/'</span>, MainHandler, dict(redis=redis)),</span><br><span class="line">        (<span class="string">r'/(.*)'</span>, MainHandler, dict(redis=redis)),</span><br><span class="line">    ])</span><br><span class="line">    application.listen(port, address=address)</span><br><span class="line">    print(<span class="string">'ADSL API Listening on'</span>, port)</span><br><span class="line">    tornado.ioloop.IOLoop.instance().start()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了 5 个接口，random 获取随机代理，names 获取主机列表，proxies 获取代理列表，all 获取代理映射，count 获取代理数量。 程序启动之后便会在 API_PORT 端口上运行 Web 服务，主页面如图 9-18 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-053047.jpg" alt=""> 图 9-18 主页面 访问 proxies 接口可以获得所有代理列表，如图 9-19 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-053052.jpg" alt=""> 图 9-19 代理列表 访问 random 接口可以获取随机可用代理，如图 9-20 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-053057.jpg" alt=""> 图 9-20 随机代理 我们只需将接口部署到服务器上，即可通过 Web 接口获取可用代理，获取方式和代理池类似。</p>
                  <h3 id="9-本节代码"><a href="#9-本节代码" class="headerlink" title="9. 本节代码"></a>9. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/AdslProxy" target="_blank" rel="noopener">https://github.com/Python3WebSpider/AdslProxy</a>。</p>
                  <h3 id="10-结语"><a href="#10-结语" class="headerlink" title="10. 结语"></a>10. 结语</h3>
                  <p>本节介绍了 ADSL 拨号代理的搭建过程。通过这种代理，我们可以无限次更换 IP，而且线路非常稳定，抓取效果好很多。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-03 11:07:04" itemprop="dateCreated datePublished" datetime="2019-12-03T11:07:04+08:00">2019-12-03</time>
                </span>
                <span id="/8361.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 9.4–ADSL 拨号代理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>10k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>9 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8353.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8353.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.4–Spider 的用法</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-4-Spider-的用法"><a href="#13-4-Spider-的用法" class="headerlink" title="13.4 Spider 的用法"></a>13.4 Spider 的用法</h1>
                  <p>在 Scrapy 中，要抓取网站的链接配置、抓取逻辑、解析逻辑里其实都是在 Spider 中配置的。在前一节实例中，我们发现抓取逻辑也是在 Spider 中完成的。本节我们就来专门了解一下 Spider 的基本用法。</p>
                  <h3 id="1-Spider-运行流程"><a href="#1-Spider-运行流程" class="headerlink" title="1. Spider 运行流程"></a>1. Spider 运行流程</h3>
                  <p>在实现 Scrapy 爬虫项目时，最核心的类便是 Spider 类了，它定义了如何爬取某个网站的流程和解析方式。简单来讲，Spider 要做的事就是如下两件。</p>
                  <ul>
                    <li>定义爬取网站的动作</li>
                    <li>分析爬取下来的网页</li>
                  </ul>
                  <p>对于 Spider 类来说，整个爬取循环如下所述。</p>
                  <ul>
                    <li>以初始的 URL 初始化 Request，并设置回调函数。 当该 Request 成功请求并返回时，将生成 Response，并作为参数传给该回调函数。</li>
                    <li>在回调函数内分析返回的网页内容。返回结果可以有两种形式，一种是解析到的有效结果返回字典或 Item 对象。下一步可经过处理后（或直接）保存，另一种是解析得下一个（如下一页）链接，可以利用此链接构造 Request 并设置新的回调函数，返回 Request。</li>
                    <li>如果返回的是字典或 Item 对象，可通过 Feed Exports 等形式存入到文件，如果设置了 Pipeline 的话，可以经由 Pipeline 处理（如过滤、修正等）并保存。</li>
                    <li>如果返回的是 Reqeust，那么 Request 执行成功得到 Response 之后会再次传递给 Request 中定义的回调函数，可以再次使用选择器来分析新得到的网页内容，并根据分析的数据生成 Item。</li>
                  </ul>
                  <p>通过以上几步循环往复进行，便完成了站点的爬取。</p>
                  <h3 id="2-Spider-类分析"><a href="#2-Spider-类分析" class="headerlink" title="2. Spider 类分析"></a>2. Spider 类分析</h3>
                  <p>在上一节的例子中我们定义的 Spider 是继承自 scrapy.spiders.Spider，这个类是最简单最基本的 Spider 类，每个其他的 Spider 必须继承这个类，还有后文要说明的一些特殊 Spider 类也都是继承自它。 这个类里提供了 start_requests() 方法的默认实现，读取并请求 start_urls 属性，并根据返回的结果调用 parse() 方法解析结果。另外它还有一些基础属性，下面对其进行讲解：</p>
                  <ul>
                    <li>name，爬虫名称，是定义 Spider 名字的字符串。Spider 的名字定义了 Scrapy 如何定位并初始化 Spider，所以其必须是唯一的。 不过我们可以生成多个相同的 Spider 实例，这没有任何限制。 name 是 Spider 最重要的属性，而且是必须的。如果该 Spider 爬取单个网站，一个常见的做法是以该网站的域名名称来命名 Spider。 例如，如果 Spider 爬取 mywebsite.com ，该 Spider 通常会被命名为 mywebsite 。</li>
                    <li>allowed_domains，允许爬取的域名，是可选配置，不在此范围的链接不会被跟进爬取。</li>
                    <li>start_urls，起始 URL 列表，当我们没有实现 start_requests() 方法时，默认会从这个列表开始抓取。</li>
                    <li>custom_settings，这是一个字典，是专属于本 Spider 的配置，此设置会覆盖项目全局的设置，而且此设置必须在初始化前被更新，所以它必须定义成类变量。</li>
                    <li>crawler，此属性是由 from_crawler() 方法设置的，代表的是本 Spider 类对应的 Crawler 对象，Crawler 对象中包含了很多项目组件，利用它我们可以获取项目的一些配置信息，如最常见的就是获取项目的设置信息，即 Settings。</li>
                    <li>settings，是一个 Settings 对象，利用它我们可以直接获取项目的全局设置变量。</li>
                  </ul>
                  <p>除了一些基础属性，Spider 还有一些常用的方法，在此介绍如下：</p>
                  <ul>
                    <li>start_requests()，此方法用于生成初始请求，它必须返回一个可迭代对象，此方法会默认使用 start_urls 里面的 URL 来构造 Request，而且 Request 是 GET 请求方式。如果我们想在启动时以 POST 方式访问某个站点，可以直接重写这个方法，发送 POST 请求时我们使用 FormRequest 即可。</li>
                    <li>parse()，当 Response 没有指定回调函数时，该方法会默认被调用，它负责处理 Response，处理返回结果，并从中提取出想要的数据和下一步的请求，然后返回。该方法需要返回一个包含 Request 或 Item 的可迭代对象。</li>
                    <li>closed()，当 Spider 关闭时，该方法会被调用，在这里一般会定义释放资源的一些操作或其他收尾操作。</li>
                  </ul>
                  <h3 id="3-结语"><a href="#3-结语" class="headerlink" title="3. 结语"></a>3. 结语</h3>
                  <p>以上的介绍可能初看起来有点摸不清头脑，不过不用担心，后面我们会有很多实例来使用这些属性和方法，慢慢会熟练掌握的。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-03 09:40:04" itemprop="dateCreated datePublished" datetime="2019-12-03T09:40:04+08:00">2019-12-03</time>
                </span>
                <span id="/8353.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.4–Spider 的用法" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.8k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8350.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8350.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.3–Selector 的用法</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-3-Selector-的用法"><a href="#13-3-Selector-的用法" class="headerlink" title="13.3 Selector 的用法"></a>13.3 Selector 的用法</h1>
                  <p>我们之前介绍了利用 Beautiful Soup、pyquery 以及正则表达式来提取网页数据，这确实非常方便。而 Scrapy 还提供了自己的数据提取方法，即 Selector（选择器）。Selector 是基于 lxml 来构建的，支持 XPath 选择器、CSS 选择器以及正则表达式，功能全面，解析速度和准确度非常高。 本节将介绍 Selector 的用法。</p>
                  <h3 id="1-直接使用"><a href="#1-直接使用" class="headerlink" title="1. 直接使用"></a>1. 直接使用</h3>
                  <p>Selector 是一个可以独立使用的模块。我们可以直接利用 Selector 这个类来构建一个选择器对象，然后调用它的相关方法如 xpath()、css() 等来提取数据。 例如，针对一段 HTML 代码，我们可以用如下方式构建 Selector 对象来提取数据：</p>
                  <figure class="highlight vbnet">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy import Selector</span><br><span class="line"></span><br><span class="line">body = <span class="comment">'<span class="doctag">&lt;html&gt;</span><span class="doctag">&lt;head&gt;</span><span class="doctag">&lt;title&gt;</span>Hello World<span class="doctag">&lt;/title&gt;</span><span class="doctag">&lt;/head&gt;</span><span class="doctag">&lt;body&gt;</span><span class="doctag">&lt;/body&gt;</span><span class="doctag">&lt;/html&gt;</span>'</span></span><br><span class="line">selector = Selector(<span class="keyword">text</span>=body)</span><br><span class="line">title = selector.xpath(<span class="comment">'//title/text()').extract_first()</span></span><br><span class="line">print(title)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">Hello World</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们在这里没有在 Scrapy 框架中运行，而是把 Scrapy 中的 Selector 单独拿出来使用了，构建的时候传入 text 参数，就生成了一个 Selector 选择器对象，然后就可以像前面我们所用的 Scrapy 中的解析方式一样，调用 xpath()、css() 等方法来提取了。 在这里我们查找的是源代码中的 title 中的文本，在 XPath 选择器最后加 text() 方法就可以实现文本的提取了。 以上内容就是 Selector 的直接使用方式。同 Beautiful Soup 等库类似，Selector 其实也是强大的网页解析库。如果方便的话，我们也可以在其他项目中直接使用 Selector 来提取数据。 接下来，我们用实例来详细讲解 Selector 的用法。</p>
                  <h3 id="2-Scrapy-Shell"><a href="#2-Scrapy-Shell" class="headerlink" title="2. Scrapy Shell"></a>2. Scrapy Shell</h3>
                  <p>由于 Selector 主要是与 Scrapy 结合使用，如 Scrapy 的回调函数中的参数 response 直接调用 xpath() 或者 css() 方法来提取数据，所以在这里我们借助 Scrapy shell 来模拟 Scrapy 请求的过程，来讲解相关的提取方法。 我们用官方文档的一个样例页面来做演示：<a href="http://doc.scrapy.org/en/latest/_static/selectors-sample1.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</a>。 开启 Scrapy shell，在命令行输入如下命令：</p>
                  <figure class="highlight awk">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy shell http:<span class="regexp">//</span>doc.scrapy.org<span class="regexp">/en/</span>latest<span class="regexp">/_static/</span>selectors-sample1.html</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们就进入到 Scrapy shell 模式。这个过程其实是，Scrapy 发起了一次请求，请求的 URL 就是刚才命令行下输入的 URL，然后把一些可操作的变量传递给我们，如 request、response 等，如图 13-5 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033928.jpg" alt=""> 图 13-5 Scrapy Shell 我们可以在命令行模式下输入命令调用对象的一些操作方法，回车之后实时显示结果。这与 Python 的命令行交互模式是类似的。 接下来，演示的实例都将页面的源码作为分析目标，页面源码如下所示：</p>
                  <figure class="highlight xml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">base</span> <span class="attr">href</span>=<span class="string">'http://example.com/'</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>Example website<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">'images'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image1.html'</span>&gt;</span>Name: My image 1 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image1_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image2.html'</span>&gt;</span>Name: My image 2 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image2_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image3.html'</span>&gt;</span>Name: My image 3 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image3_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image4.html'</span>&gt;</span>Name: My image 4 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image4_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image5.html'</span>&gt;</span>Name: My image 5 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image5_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="3-XPath-选择器"><a href="#3-XPath-选择器" class="headerlink" title="3. XPath 选择器"></a>3. XPath 选择器</h3>
                  <p>进入 Scrapy shell 之后，我们将主要操作 response 这个变量来进行解析。因为我们解析的是 HTML 代码，Selector 将自动使用 HTML 语法来分析。 response 有一个属性 selector，我们调用 response.selector 返回的内容就相当于用 response 的 text 构造了一个 Selector 对象。通过这个 Selector 对象我们可以调用解析方法如 xpath()、css() 等，通过向方法传入 XPath 或 CSS 选择器参数就可以实现信息的提取。 我们用一个实例感受一下，如下所示：</p>
                  <figure class="highlight vbnet">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; result = response.selector.xpath(<span class="comment">'//a')</span></span><br><span class="line">&gt;&gt;&gt; result</span><br><span class="line">[&lt;Selector xpath=<span class="comment">'//a' data='<span class="doctag">&lt;a href="image1.html"&gt;</span>Name: My image 1 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line"> &lt;Selector xpath=<span class="comment">'//a' data='<span class="doctag">&lt;a href="image2.html"&gt;</span>Name: My image 2 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line"> &lt;Selector xpath=<span class="comment">'//a' data='<span class="doctag">&lt;a href="image3.html"&gt;</span>Name: My image 3 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line"> &lt;Selector xpath=<span class="comment">'//a' data='<span class="doctag">&lt;a href="image4.html"&gt;</span>Name: My image 4 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line"> &lt;Selector xpath=<span class="comment">'//a' data='<span class="doctag">&lt;a href="image5.html"&gt;</span>Name: My image 5 <span class="doctag">&lt;'&gt;</span>]</span></span><br><span class="line">&gt;&gt;&gt; type(result)</span><br><span class="line">scrapy.selector.unified.SelectorList</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>打印结果的形式是 Selector 组成的列表，其实它是 SelectorList 类型，SelectorList 和 Selector 都可以继续调用 xpath() 和 css() 等方法来进一步提取数据。 在上面的例子中，我们提取了 a 节点。接下来，我们尝试继续调用 xpath() 方法来提取 a 节点内包含的 img 节点，如下所示：</p>
                  <figure class="highlight oxygene">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; <span class="keyword">result</span>.xpath(<span class="string">'./img'</span>)</span><br><span class="line">[&lt;<span class="keyword">Selector</span> xpath=<span class="string">'./img'</span> data=<span class="string">'&lt;img src="image1_thumb.jpg"&gt;'</span>&gt;,</span><br><span class="line"> &lt;<span class="keyword">Selector</span> xpath=<span class="string">'./img'</span> data=<span class="string">'&lt;img src="image2_thumb.jpg"&gt;'</span>&gt;,</span><br><span class="line"> &lt;<span class="keyword">Selector</span> xpath=<span class="string">'./img'</span> data=<span class="string">'&lt;img src="image3_thumb.jpg"&gt;'</span>&gt;,</span><br><span class="line"> &lt;<span class="keyword">Selector</span> xpath=<span class="string">'./img'</span> data=<span class="string">'&lt;img src="image4_thumb.jpg"&gt;'</span>&gt;,</span><br><span class="line"> &lt;<span class="keyword">Selector</span> xpath=<span class="string">'./img'</span> data=<span class="string">'&lt;img src="image5_thumb.jpg"&gt;'</span>&gt;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们获得了 a 节点里面的所有 img 节点，结果为 5。 值得注意的是，选择器的最前方加 .（点），这代表提取元素内部的数据，如果没有加点，则代表从根节点开始提取。此处我们用了./img 的提取方式，则代表从 a 节点里进行提取。如果此处我们用 //img，则还是从 html 节点里进行提取。 我们刚才使用了 response.selector.xpath() 方法对数据进行了提取。Scrapy 提供了两个实用的快捷方法，response.xpath() 和 response.css()，它们二者的功能完全等同于 response.selector.xpath() 和 response.selector.css()。方便起见，后面我们统一直接调用 response 的 xpath() 和 css() 方法进行选择。 现在我们得到的是 SelectorList 类型的变量，该变量是由 Selector 对象组成的列表。我们可以用索引单独取出其中某个 Selector 元素，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; result[0]</span><br><span class="line">&lt;Selector <span class="attribute">xpath</span>=<span class="string">'//a'</span> <span class="attribute">data</span>=<span class="string">'&lt;a href="image1.html"&gt;Name: My image 1 &lt;'</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们可以像操作列表一样操作这个 SelectorList。 但是现在获取的内容是 Selector 或者 SelectorList 类型，并不是真正的文本内容。那么具体的内容怎么提取呢？ 比如我们现在想提取出 a 节点元素，就可以利用 extract() 方法，如下所示：</p>
                  <figure class="highlight vbnet">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; result.extract()</span><br><span class="line">[<span class="comment">'<span class="doctag">&lt;a href="image1.html"&gt;</span>Name: My image 1 <span class="doctag">&lt;br&gt;</span><span class="doctag">&lt;img src="image1_thumb.jpg"&gt;</span><span class="doctag">&lt;/a&gt;</span>', '<span class="doctag">&lt;a href="image2.html"&gt;</span>Name: My image 2 <span class="doctag">&lt;br&gt;</span><span class="doctag">&lt;img src="image2_thumb.jpg"&gt;</span><span class="doctag">&lt;/a&gt;</span>', '<span class="doctag">&lt;a href="image3.html"&gt;</span>Name: My image 3 <span class="doctag">&lt;br&gt;</span><span class="doctag">&lt;img src="image3_thumb.jpg"&gt;</span><span class="doctag">&lt;/a&gt;</span>', '<span class="doctag">&lt;a href="image4.html"&gt;</span>Name: My image 4 <span class="doctag">&lt;br&gt;</span><span class="doctag">&lt;img src="image4_thumb.jpg"&gt;</span><span class="doctag">&lt;/a&gt;</span>', '<span class="doctag">&lt;a href="image5.html"&gt;</span>Name: My image 5 <span class="doctag">&lt;br&gt;</span><span class="doctag">&lt;img src="image5_thumb.jpg"&gt;</span><span class="doctag">&lt;/a&gt;</span>']</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里使用了 extract() 方法，我们就可以把真实需要的内容获取下来。 我们还可以改写 XPath 表达式，来选取节点的内部文本和属性，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; response.xpath(<span class="string">'//a/text()'</span>).extract()</span><br><span class="line">[<span class="string">'Name: My image 1 '</span>, <span class="string">'Name: My image 2 '</span>, <span class="string">'Name: My image 3 '</span>, <span class="string">'Name: My image 4 '</span>, <span class="string">'Name: My image 5 '</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.xpath(<span class="string">'//a/@href'</span>).extract()</span><br><span class="line">[<span class="string">'image1.html'</span>, <span class="string">'image2.html'</span>, <span class="string">'image3.html'</span>, <span class="string">'image4.html'</span>, <span class="string">'image5.html'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们只需要再加一层 /text() 就可以获取节点的内部文本，或者加一层 /@href 就可以获取节点的 href 属性。其中，@符号后面内容就是要获取的属性名称。 现在我们可以用一个规则把所有符合要求的节点都获取下来，返回的类型是列表类型。 但是这里有一个问题：如果符合要求的节点只有一个，那么返回的结果会是什么呢？我们再用一个实例来感受一下，如下所示：</p>
                  <figure class="highlight less">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; <span class="selector-tag">response</span><span class="selector-class">.xpath</span>(<span class="string">'//a[@href="image1.html"]/text()'</span>)<span class="selector-class">.extract</span>()</span><br><span class="line"><span class="selector-attr">['Name: My image 1 ']</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们用属性限制了匹配的范围，使 XPath 只可以匹配到一个元素。然后用 extract() 方法提取结果，其结果还是一个列表形式，其文本是列表的第一个元素。但很多情况下，我们其实想要的数据就是第一个元素内容，这里我们通过加一个索引来获取，如下所示： ```python&gt;&gt;&gt; response.xpath(‘//a[@href=”image1.html”]/text()’).extract()[0] ‘Name: My image 1 ‘</p>
                  <figure class="highlight isbl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"> 但是，这个写法很明显是有风险的。一旦 <span class="variable">XPath</span> 有问题，那么 <span class="function"><span class="title">extract</span>() 后的结果可能是一个空列表。如果我们再用索引来获取，那不就会可能导致数组越界吗？</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">所以，另外一个方法可以专门提取单个元素，它叫作 <span class="title">extract_first</span>()。我们可以改写上面的例子如下所示：</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">```<span class="variable">python</span></span></span><br><span class="line"><span class="function">&gt;&gt;&gt; <span class="variable">response.xpath</span>(<span class="string">'//a[@href="image1.html"]/text()'</span>).extract_first()</span></span><br><span class="line"><span class="string">'Name: My image 1 '</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样，我们直接利用 extract_first() 方法将匹配的第一个结果提取出来，同时我们也不用担心数组越界的问题。 另外我们也可以为 extract_first() 方法设置一个默认值参数，这样当 XPath 规则提取不到内容时会直接使用默认值。例如将 XPath 改成一个不存在的规则，重新执行代码，如下所示：</p>
                  <figure class="highlight mathematica">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; response.xpath('//a[@href=<span class="string">"image1"</span>]/text()').extract_first()&gt;&gt;&gt; response.xpath('//a[@href=<span class="string">"image1"</span>]/text()').extract_first('<span class="keyword">Default</span> <span class="keyword">Image</span>')</span><br><span class="line">'<span class="keyword">Default</span> <span class="keyword">Image</span>'</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里，如果 XPath 匹配不到任何元素，调用 extract_first() 会返回空，也不会报错。 在第二行代码中，我们还传递了一个参数当作默认值，如 Default Image。这样如果 XPath 匹配不到结果的话，返回值会使用这个参数来代替，可以看到输出正是如此。 现在为止，我们了解了 Scrapy 中的 XPath 的相关用法，包括嵌套查询、提取内容、提取单个内容、获取文本和属性等。</p>
                  <h3 id="4-CSS-选择器"><a href="#4-CSS-选择器" class="headerlink" title="4. CSS 选择器"></a>4. CSS 选择器</h3>
                  <p>接下来，我们看看 CSS 选择器的用法。 Scrapy 的选择器同时还对接了 CSS 选择器，使用 response.css() 方法可以使用 CSS 选择器来选择对应的元素。 例如在上文我们选取了所有的 a 节点，那么 CSS 选择器同样可以做到，如下所示：</p>
                  <figure class="highlight vbnet">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; response.css(<span class="comment">'a')</span></span><br><span class="line">[&lt;Selector xpath=<span class="comment">'descendant-or-self::a' data='<span class="doctag">&lt;a href="image1.html"&gt;</span>Name: My image 1 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line">&lt;Selector xpath=<span class="comment">'descendant-or-self::a' data='<span class="doctag">&lt;a href="image2.html"&gt;</span>Name: My image 2 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line">&lt;Selector xpath=<span class="comment">'descendant-or-self::a' data='<span class="doctag">&lt;a href="image3.html"&gt;</span>Name: My image 3 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line">&lt;Selector xpath=<span class="comment">'descendant-or-self::a' data='<span class="doctag">&lt;a href="image4.html"&gt;</span>Name: My image 4 <span class="doctag">&lt;'&gt;</span>,</span></span><br><span class="line">&lt;Selector xpath=<span class="comment">'descendant-or-self::a' data='<span class="doctag">&lt;a href="image5.html"&gt;</span>Name: My image 5 <span class="doctag">&lt;'&gt;</span>]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>同样，调用 extract() 方法就可以提取出节点，如下所示： ```python&gt;&gt;&gt; response.css(‘a’).extract() <a href="image1.html">‘[Name: My image 1 <img src="image1_thumb.jpg" alt=""></a> ‘, ‘<a href="image2.html">Name: My image 2 <img src="image2_thumb.jpg" alt=""></a> ‘, ‘<a href="image3.html">Name: My image 3 <img src="image3_thumb.jpg" alt=""></a> ‘, ‘<a href="image4.html">Name: My image 4 <img src="image4_thumb.jpg" alt=""></a> ‘, ‘<a href="image5.html">Name: My image 5 <img src="image5_thumb.jpg" alt=""></a> ‘]</p>
                  <figure class="highlight vbnet">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"> 用法和 XPath 选择是完全一样的。</span><br><span class="line"></span><br><span class="line">另外，我们也可以进行属性选择和嵌套选择，如下所示：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">&gt;&gt;&gt; response.css(<span class="comment">'a[href="image1.html"]').extract()</span></span><br><span class="line">[<span class="comment">'<span class="doctag">&lt;a href="image1.html"&gt;</span>Name: My image 1 <span class="doctag">&lt;br&gt;</span><span class="doctag">&lt;img src="image1_thumb.jpg"&gt;</span><span class="doctag">&lt;/a&gt;</span>']</span></span><br><span class="line">&gt;&gt;&gt; response.css(<span class="comment">'a[href="image1.html"] img').extract()</span></span><br><span class="line">[<span class="comment">'<span class="doctag">&lt;img src="image1_thumb.jpg"&gt;</span>']</span></span><br><span class="line">​```这里用 [href=<span class="string">"image.html"</span>] 限定了 href 属性，可以看到匹配结果就只有一个了。另外如果想查找 a 节点内的 img 节点，只需要再加一个空格和 img 即可。选择器的写法和标准 CSS 选择器写法如出一辙。</span><br><span class="line"></span><br><span class="line">我们也可以使用 extract_first() 方法提取列表的第一个元素，如下所示：</span><br><span class="line"></span><br><span class="line">​```python</span><br><span class="line">&gt;&gt;&gt; response.css(<span class="comment">'a[href="image1.html"] img').extract_first()</span></span><br><span class="line"><span class="comment">'<span class="doctag">&lt;img src="image1_thumb.jpg"&gt;</span>'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来的两个用法不太一样。节点的内部文本和属性的获取是这样实现的，如下所示：</p>
                  <figure class="highlight rust">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; response.css(<span class="symbol">'a</span>[href=<span class="string">"image1.html"</span>]::text').extract_first()</span><br><span class="line"><span class="symbol">'Name</span>: My image <span class="number">1</span> '</span><br><span class="line">&gt;&gt;&gt; response.css(<span class="symbol">'a</span>[href=<span class="string">"image1.html"</span>] img::attr(src)').extract_first()</span><br><span class="line"><span class="symbol">'image1_thumb</span>.jpg'</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>获取文本和属性需要用::text 和::attr() 的写法。而其他库如 Beautiful Soup 或 pyquery 都有单独的方法。 另外，CSS 选择器和 XPath 选择器一样可以嵌套选择。我们可以先用 XPath 选择器选中所有 a 节点，再利用 CSS 选择器选中 img 节点，再用 XPath 选择器获取属性。我们用一个实例来感受一下，如下所示：</p>
                  <figure class="highlight livescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">\&gt;&gt;&gt;</span> response.xpath(<span class="string">'//a'</span>).css(<span class="string">'img'</span>).xpath(<span class="string">'@src'</span>).extract()</span><br><span class="line">[<span class="string">'image1_thumb.jpg'</span>, <span class="string">'image2_thumb.jpg'</span>, <span class="string">'image3_thumb.jpg'</span>, <span class="string">'image4_thumb.jpg'</span>, <span class="string">'image5_thumb.jpg'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们成功获取了所有 img 节点的 src 属性。 因此，我们可以随意使用 xpath() 和 css() 方法二者自由组合实现嵌套查询，二者是完全兼容的。</p>
                  <h3 id="5-正则匹配"><a href="#5-正则匹配" class="headerlink" title="5. 正则匹配"></a>5. 正则匹配</h3>
                  <p>Scrapy 的选择器还支持正则匹配。比如，在示例的 a 节点中的文本类似于 Name: My image 1，现在我们只想把 Name: 后面的内容提取出来，这时就可以借助 re() 方法，实现如下：</p>
                  <figure class="highlight livescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">\&gt;&gt;&gt;</span> response.xpath(<span class="string">'//a/text()'</span>).re(<span class="string">'Name:s(.*)'</span>)</span><br><span class="line">[<span class="string">'My image 1 '</span>, <span class="string">'My image 2 '</span>, <span class="string">'My image 3 '</span>, <span class="string">'My image 4 '</span>, <span class="string">'My image 5 '</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们给 re() 方法传了一个正则表达式，其中 (.*) 就是要匹配的内容，输出的结果就是正则表达式匹配的分组，结果会依次输出。 如果同时存在两个分组，那么结果依然会被按序输出，如下所示：</p>
                  <figure class="highlight livescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">\&gt;&gt;&gt;</span> response.xpath(<span class="string">'//a/text()'</span>).re(<span class="string">'(.*?):s(.*)'</span>)</span><br><span class="line">[<span class="string">'Name'</span>, <span class="string">'My image 1 '</span>, <span class="string">'Name'</span>, <span class="string">'My image 2 '</span>, <span class="string">'Name'</span>, <span class="string">'My image 3 '</span>, <span class="string">'Name'</span>, <span class="string">'My image 4 '</span>, <span class="string">'Name'</span>, <span class="string">'My image 5 '</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>类似 extract_first() 方法，re_first() 方法可以选取列表的第一个元素，用法如下：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; response.xpath(<span class="string">'//a/text()'</span>).re_first(<span class="string">'(.*?):s(.*)'</span>)</span><br><span class="line"><span class="string">'Name'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; response.xpath(<span class="string">'//a/text()'</span>).re_first(<span class="string">'Name:s(.*)'</span>)</span><br><span class="line"><span class="string">'My image 1 '</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>不论正则匹配了几个分组，结果都会等于列表的第一个元素。 值得注意的是，response 对象不能直接调用 re() 和 re_first() 方法。如果想要对全文进行正则匹配，可以先调用 xpath() 方法再正则匹配，如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">\&gt;&gt;&gt; response.re(<span class="string">'Name:s(.*)'</span>)</span><br><span class="line">Traceback (most recent <span class="keyword">call</span> last):</span><br><span class="line">  File "&lt;console&gt;", <span class="type">line</span> <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AttributeError: <span class="string">'HtmlResponse'</span> <span class="keyword">object</span> has <span class="keyword">no</span> <span class="keyword">attribute</span> <span class="string">'re'</span></span><br><span class="line">&gt;&gt;&gt; response.xpath(<span class="string">'.'</span>).re(<span class="string">'Name:s(.*)&lt;br&gt;'</span>)</span><br><span class="line">[<span class="string">'My image 1 '</span>, <span class="string">'My image 2 '</span>, <span class="string">'My image 3 '</span>, <span class="string">'My image 4 '</span>, <span class="string">'My image 5 '</span>]</span><br><span class="line">&gt;&gt;&gt; response.xpath(<span class="string">'.'</span>).re_first(<span class="string">'Name:s(.*)&lt;br&gt;'</span>)</span><br><span class="line"><span class="string">'My image 1 '</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过上面的例子，我们可以看到，直接调用 re() 方法会提示没有 re 属性。但是这里首先调用了 xpath(‘.’) 选中全文，然后调用 re() 和 re_first() 方法，就可以进行正则匹配了。</p>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>以上内容便是 Scrapy 选择器的用法，它包括两个常用选择器和正则匹配功能。熟练掌握 XPath 语法、CSS 选择器语法、正则表达式语法可以大大提高数据提取效率。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-03 09:31:21" itemprop="dateCreated datePublished" datetime="2019-12-03T09:31:21+08:00">2019-12-03</time>
                </span>
                <span id="/8350.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.3–Selector 的用法" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>9.5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>9 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8337.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8337.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.2-Scrapy 入门</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-2-Scrapy-入门"><a href="#13-2-Scrapy-入门" class="headerlink" title="13.2 Scrapy 入门"></a>13.2 Scrapy 入门</h1>
                  <p>接下来介绍一个简单的项目，完成一遍 Scrapy 抓取流程。通过这个过程，我们可以对 Scrapy 的基本用法和原理有大体了解。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本节要完成的任务如下。</p>
                  <ul>
                    <li>创建一个 Scrapy 项目。</li>
                    <li>创建一个 Spider 来抓取站点和处理数据。</li>
                    <li>通过命令行将抓取的内容导出。</li>
                    <li>将抓取的内容保存到 MongoDB 数据库。</li>
                  </ul>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>我们需要安装好 Scrapy 框架、MongoDB 和 PyMongo 库。如果尚未安装，请参照上一节的安装说明。</p>
                  <h3 id="3-创建项目"><a href="#3-创建项目" class="headerlink" title="3. 创建项目"></a>3. 创建项目</h3>
                  <p>创建一个 Scrapy 项目，项目文件可以直接用 scrapy 命令生成，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject tutorial</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个命令可以在任意文件夹运行。如果提示权限问题，可以加 sudo 运行该命令。这个命令将会创建一个名为 tutorial 的文件夹，文件夹结构如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy.cfg     <span class="comment"># Scrapy 部署时的配置文件</span></span><br><span class="line">tutorial         <span class="comment"># 项目的模块，引入的时候需要从这里引入</span></span><br><span class="line">    __init__.py</span><br><span class="line">    <span class="keyword">items</span>.py     <span class="comment"># Items 的定义，定义爬取的数据结构</span></span><br><span class="line">    middlewares.py   <span class="comment"># Middlewares 的定义，定义爬取时的中间件</span></span><br><span class="line">    pipelines.py       <span class="comment"># Pipelines 的定义，定义数据管道</span></span><br><span class="line">    settings.py       <span class="comment"># 配置文件</span></span><br><span class="line">    spiders         <span class="comment"># 放置 Spiders 的文件夹</span></span><br><span class="line">    __init__.py</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="4-创建-Spider"><a href="#4-创建-Spider" class="headerlink" title="4. 创建 Spider"></a>4. 创建 Spider</h3>
                  <p>Spider 是自己定义的类，Scrapy 用它来从网页里抓取内容，并解析抓取的结果。不过这个类必须继承 Scrapy 提供的 Spider 类 scrapy.Spider，还要定义 Spider 的名称和起始请求，以及怎样处理爬取后的结果的方法。 也可以使用命令行创建一个 Spider。比如要生成 Quotes 这个 Spider，可以执行如下命令：</p>
                  <figure class="highlight properties">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">cd</span> <span class="string">tutorial</span></span><br><span class="line"><span class="attr">scrapy</span> <span class="string">genspider quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>进入刚才创建的 tutorial 文件夹，然后执行 genspider 命令。第一个参数是 Spider 的名称，第二个参数是网站域名。执行完毕之后，spiders 文件夹中多了一个 quotes.py，它就是刚刚创建的 Spider，内容如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    allowed_domains = [<span class="string">"quotes.toscrape.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里有三个属性 ——name、allowed_domains 和 start_urls，还有一个方法 parse。</p>
                  <ul>
                    <li>name，它是每个项目唯一的名字，用来区分不同的 Spider。</li>
                    <li>allowed_domains，它是允许爬取的域名，如果初始或后续的请求链接不是这个域名下的，则请求链接会被过滤掉。</li>
                    <li>start_urls，它包含了 Spider 在启动时爬取的 url 列表，初始请求是由它来定义的。</li>
                    <li>parse，它是 Spider 的一个方法。默认情况下，被调用时 start_urls 里面的链接构成的请求完成下载执行后，返回的响应就会作为唯一的参数传递给这个函数。该方法负责解析返回的响应、提取数据或者进一步生成要处理的请求。</li>
                  </ul>
                  <h3 id="5-创建-Item"><a href="#5-创建-Item" class="headerlink" title="5. 创建 Item"></a>5. 创建 Item</h3>
                  <p>Item 是保存爬取数据的容器，它的使用方法和字典类似。不过，相比字典，Item 多了额外的保护机制，可以避免拼写错误或者定义字段错误。 创建 Item 需要继承 scrapy.Item 类，并且定义类型为 scrapy.Field 的字段。观察目标网站，我们可以获取到的内容有 text、author、tags。 定义 Item，此时将 items.py 修改如下：</p>
                  <figure class="highlight haskell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">QuoteItem</span>(<span class="title">scrapy</span>.<span class="type">Item</span>):</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    text = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    author = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    tags = scrapy.<span class="type">Field</span>()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了三个字段，将类的名称修改为 QuoteItem，接下来爬取时我们会使用到这个 Item。</p>
                  <h3 id="6-解析-Response"><a href="#6-解析-Response" class="headerlink" title="6. 解析 Response"></a>6. 解析 Response</h3>
                  <p>前面我们看到，parse() 方法的参数 response 是 start_urls 里面的链接爬取后的结果。所以在 parse() 方法中，我们可以直接对 response 变量包含的内容进行解析，比如浏览请求结果的网页源代码，或者进一步分析源代码内容，或者找出结果中的链接而得到下一个请求。 我们可以看到网页中既有我们想要的结果，又有下一页的链接，这两部分内容我们都要进行处理。 首先看看网页结构，如图 13-2 所示。每一页都有多个 class 为 quote 的区块，每个区块内都包含 text、author、tags。那么我们先找出所有的 quote，然后提取每一个 quote 中的内容。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033901.jpg" alt=""> 图 13-2 页面结构 提取的方式可以是 CSS 选择器或 XPath 选择器。在这里我们使用 CSS 选择器进行选择，parse() 方法的改写如下所示：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse(self, response):</span><br><span class="line">    quotes = response.css('.<span class="literal">quote</span>')</span><br><span class="line">    <span class="keyword">for</span> <span class="literal">quote</span> <span class="keyword">in</span> quotes:</span><br><span class="line">        <span class="built_in">text</span> = <span class="literal">quote</span>.css('.<span class="built_in">text</span>::<span class="built_in">text</span>').extract_first()</span><br><span class="line">        author = <span class="literal">quote</span>.css('.author::<span class="built_in">text</span>').extract_first()</span><br><span class="line">        tags = <span class="literal">quote</span>.css('.tags .tag::<span class="built_in">text</span>').extract()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先利用选择器选取所有的 quote，并将其赋值为 quotes 变量，然后利用 for 循环对每个 quote 遍历，解析每个 quote 的内容。 对 text 来说，观察到它的 class 为 text，所以可以用.text 选择器来选取，这个结果实际上是整个带有标签的节点，要获取它的正文内容，可以加::text 来获取。这时的结果是长度为 1 的列表，所以还需要用 extract_first() 方法来获取第一个元素。而对于 tags 来说，由于我们要获取所有的标签，所以用 extract() 方法获取整个列表即可。 以第一个 quote 的结果为例，各个选择方法及结果的说明如下内容。 源码如下：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"quote"</span> itemscope=<span class="string">""</span>itemtype=<span class="string">"http://schema.org/CreativeWork"</span>&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"text"</span> itemprop=<span class="string">"text"</span>&gt;“The world <span class="keyword">as</span> we have created it is a process <span class="keyword">of</span> our thinking. It cannot be changed without changing our thinking.”&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span&gt;by &lt;small class="author" itemprop="author"&gt;Albert Einstein&lt;/</span>small&gt;</span><br><span class="line">        &lt;a href=<span class="string">"/author/Albert-Einstein"</span>&gt;(about)&lt;<span class="regexp">/a&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>span&gt;</span><br><span class="line">        &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"tags"</span>&gt;</span><br><span class="line">            Tags:</span><br><span class="line">            &lt;meta <span class="class"><span class="keyword">class</span></span>=<span class="string">"keywords"</span> itemprop=<span class="string">"keywords"</span> content=<span class="string">"change,deep-thoughts,thinking,world"</span>&gt;</span><br><span class="line">            &lt;a <span class="class"><span class="keyword">class</span></span>=<span class="string">"tag"</span> href=<span class="string">"/tag/change/page/1/"</span>&gt;change&lt;<span class="regexp">/a&gt;</span></span><br><span class="line"><span class="regexp">            &lt;a class="tag" href="/</span>tag/deep-thoughts/page/<span class="number">1</span>/<span class="string">"&gt;deep-thoughts&lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;a class="</span>tag<span class="string">" href="</span>/tag/thinking/page/<span class="number">1</span>/<span class="string">"&gt;thinking&lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;a class="</span>tag<span class="string">" href="</span>/tag/world/page/<span class="number">1</span>/<span class="string">"&gt;world&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>不同选择器的返回结果如下。</p>
                  <h4 id="quote-css-‘-text’"><a href="#quote-css-‘-text’" class="headerlink" title="quote.css(‘.text’)"></a>quote.css(‘.text’)</h4>
                  <figure class="highlight fsharp">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="meta">[&lt;Selector xpath="descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' text ')]"data='&lt;span class="text"itemprop="text"&gt;“The '&gt;]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="quote-css-‘-text-text’"><a href="#quote-css-‘-text-text’" class="headerlink" title="quote.css(‘.text::text’)"></a>quote.css(‘.text::text’)</h4>
                  <figure class="highlight fsharp">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="meta">[&lt;Selector xpath="descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' text ')]/text()"data='“The world as we have created it is a pr'&gt;]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="quote-css-‘-text’-extract"><a href="#quote-css-‘-text’-extract" class="headerlink" title="quote.css(‘.text’).extract()"></a>quote.css(‘.text’).extract()</h4>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">['&lt;span <span class="built_in">class</span>=<span class="string">"text"</span>itemprop=<span class="string">"text"</span>&gt;“The world <span class="keyword">as</span> we have created <span class="keyword">it</span> <span class="keyword">is</span> a process <span class="keyword">of</span> our thinking. It cannot be changed <span class="keyword">without</span> changing our thinking.”&lt;/span&gt;']</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="quote-css-‘-text-text’-extract"><a href="#quote-css-‘-text-text’-extract" class="headerlink" title="quote.css(‘.text::text’).extract()"></a>quote.css(‘.text::text’).extract()</h4>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">['“The world <span class="keyword">as</span> we have created <span class="keyword">it</span> <span class="keyword">is</span> a process <span class="keyword">of</span> our thinking. It cannot be changed <span class="keyword">without</span> changing our thinking.”']</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="quote-css-‘-text-text’-extract-first"><a href="#quote-css-‘-text-text’-extract-first" class="headerlink" title="quote.css(‘.text::text’).extract_first()"></a>quote.css(‘.text::text’).extract_first()</h4>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">“The world <span class="keyword">as</span> we have created <span class="keyword">it</span> is <span class="keyword">a</span> <span class="built_in">process</span> <span class="keyword">of</span> our thinking. It cannot be changed <span class="keyword">without</span> changing our thinking.”</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>所以，对于 text，获取结果的第一个元素即可，所以使用 extract_first() 方法，对于 tags，要获取所有结果组成的列表，所以使用 extract() 方法。</p>
                  <h3 id="7-使用-Item"><a href="#7-使用-Item" class="headerlink" title="7. 使用 Item"></a>7. 使用 Item</h3>
                  <p>上文定义了 Item，接下来就要使用它了。Item 可以理解为一个字典，不过在声明的时候需要实例化。然后依次用刚才解析的结果赋值 Item 的每一个字段，最后将 Item 返回即可。 QuotesSpider 的改写如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import scrapy</span><br><span class="line"><span class="built_in">from</span> tutorial.<span class="keyword">items</span> import QuoteItem</span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    allowed_domains = [<span class="string">"quotes.toscrape.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        quotes = response.css(<span class="string">'.quote'</span>)</span><br><span class="line">        <span class="keyword">for</span> <span class="literal">quote</span> <span class="keyword">in</span> quotes:</span><br><span class="line">            <span class="keyword">item</span> = QuoteItem()</span><br><span class="line">            <span class="keyword">item</span>[<span class="string">'text'</span>] = <span class="literal">quote</span>.css(<span class="string">'.text::text'</span>).extract_first()</span><br><span class="line">            <span class="keyword">item</span>[<span class="string">'author'</span>] = <span class="literal">quote</span>.css(<span class="string">'.author::text'</span>).extract_first()</span><br><span class="line">            <span class="keyword">item</span>[<span class="string">'tags'</span>] = <span class="literal">quote</span>.css(<span class="string">'.tags .tag::text'</span>).extract()</span><br><span class="line">            yield <span class="keyword">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如此一来，首页的所有内容被解析出来，并被赋值成了一个个 QuoteItem。</p>
                  <h3 id="8-后续-Request"><a href="#8-后续-Request" class="headerlink" title="8. 后续 Request"></a>8. 后续 Request</h3>
                  <p>上面的操作实现了从初始页面抓取内容。那么，下一页的内容该如何抓取？这就需要我们从当前页面中找到信息来生成下一个请求，然后在下一个请求的页面里找到信息再构造再下一个请求。这样循环往复迭代，从而实现整站的爬取。 将刚才的页面拉到最底部，如图 13-3 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033909.jpg" alt=""> 图 13-3 页面底部 有一个 Next 按钮，查看一下源代码，可以发现它的链接是 /page/2/，实际上全链接就是：<a href="http://quotes.toscrape.com/page/2" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2</a>，通过这个链接我们就可以构造下一个请求。 构造请求时需要用到 scrapy.Request。这里我们传递两个参数 ——url 和 callback，这两个参数的说明如下。</p>
                  <ul>
                    <li>url：它是请求链接。</li>
                    <li>callback：它是回调函数。当指定了该回调函数的请求完成之后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。回调函数进行解析或生成下一个请求，回调函数如上文的 parse() 所示。</li>
                  </ul>
                  <p>由于 parse() 就是解析 text、author、tags 的方法，而下一页的结构和刚才已经解析的页面结构是一样的，所以我们可以再次使用 parse() 方法来做页面解析。 接下来我们要做的就是利用选择器得到下一页链接并生成请求，在 parse() 方法后追加如下的代码：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">next = response.css('.pager .next a::attr(href)').extract<span class="constructor">_first()</span></span><br><span class="line">url = response.urljoin(next)</span><br><span class="line">yield scrapy.<span class="constructor">Request(<span class="params">url</span>=<span class="params">url</span>, <span class="params">callback</span>=<span class="params">self</span>.<span class="params">parse</span>)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一句代码首先通过 CSS 选择器获取下一个页面的链接，即要获取 a 超链接中的 href 属性。这里用到了::attr(href) 操作。然后再调用 extract_first() 方法获取内容。 第二句代码调用了 urljoin() 方法，urljoin() 方法可以将相对 URL 构造成一个绝对的 URL。例如，获取到的下一页地址是 /page/2，urljoin() 方法处理后得到的结果就是：<a href="http://quotes.toscrape.com/page/2/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/</a>。 第三句代码通过 url 和 callback 变量构造了一个新的请求，回调函数 callback 依然使用 parse() 方法。这个请求完成后，响应会重新经过 parse 方法处理，得到第二页的解析结果，然后生成第二页的下一页，也就是第三页的请求。这样爬虫就进入了一个循环，直到最后一页。 通过几行代码，我们就轻松实现了一个抓取循环，将每个页面的结果抓取下来了。 现在，改写之后的整个 Spider 类如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import scrapy</span><br><span class="line">from tutorial.items import QuoteItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    allowed_domains = [<span class="string">"quotes.toscrape.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        quotes = response.css(<span class="string">'.quote'</span>)</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> <span class="symbol">quotes:</span></span><br><span class="line">            item = QuoteItem()</span><br><span class="line">            item[<span class="string">'text'</span>] = quote.css(<span class="string">'.text::text'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'author'</span>] = quote.css(<span class="string">'.author::text'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'tags'</span>] = quote.css(<span class="string">'.tags .tag::text'</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="keyword">next</span> = response.css(<span class="string">'.pager .next a::attr("href")'</span>).extract_first()</span><br><span class="line">        url = response.urljoin(<span class="keyword">next</span>)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=url, callback=<span class="keyword">self</span>.parse)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="9-运行"><a href="#9-运行" class="headerlink" title="9. 运行"></a>9. 运行</h3>
                  <p>接下来，进入目录，运行如下命令：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>就可以看到 Scrapy 的运行结果了。</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.utils.log] INFO: Scrapy <span class="number">1.3</span><span class="number">.0</span> started (bot: tutorial)</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.utils.log] INFO: Overridden settings: &#123;<span class="string">'NEWSPIDER_MODULE'</span>: <span class="string">'tutorial.spiders'</span>, <span class="string">'SPIDER_MODULES'</span>: [<span class="string">'tutorial.spiders'</span>], <span class="string">'ROBOTSTXT_OBEY'</span>: <span class="literal">True</span>, <span class="string">'BOT_NAME'</span>: <span class="string">'tutorial'</span>&#125;</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[<span class="string">'scrapy.extensions.logstats.LogStats'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.telnet.TelnetConsole'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.corestats.CoreStats'</span>]</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[<span class="string">'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span>]</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[<span class="string">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span>]</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.core.engine] INFO: Spider opened</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.extensions.logstats] INFO: Crawled <span class="number">0</span> pages (at <span class="number">0</span> pages/min), scraped <span class="number">0</span> items (at <span class="number">0</span> items/min)</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">20</span> [scrapy.extensions.telnet] DEBUG: Telnet console listening on <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6023</span></span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">21</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">404</span>) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: <span class="literal">None</span>)</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">21</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://quotes.toscrape.com/&gt; (referer: <span class="literal">None</span>)</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">21</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://quotes.toscrape.com/&gt;</span><br><span class="line">&#123;<span class="string">'author'</span>: <span class="string">u'Albert Einstein'</span>,</span><br><span class="line"> <span class="string">'tags'</span>: [<span class="string">u'change'</span>, <span class="string">u'deep-thoughts'</span>, <span class="string">u'thinking'</span>, <span class="string">u'world'</span>],</span><br><span class="line"> <span class="string">'text'</span>: <span class="string">u'u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.u201d'</span>&#125;</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">21</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://quotes.toscrape.com/&gt;</span><br><span class="line">&#123;<span class="string">'author'</span>: <span class="string">u'J.K. Rowling'</span>,</span><br><span class="line"> <span class="string">'tags'</span>: [<span class="string">u'abilities'</span>, <span class="string">u'choices'</span>],</span><br><span class="line"> <span class="string">'text'</span>: <span class="string">u'u201cIt is our choices, Harry, that show what we truly are, far more than our abilities.u201d'</span>&#125;</span><br><span class="line">...</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">27</span> [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">27</span> [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;<span class="string">'downloader/request_bytes'</span>: <span class="number">2859</span>,</span><br><span class="line"> <span class="string">'downloader/request_count'</span>: <span class="number">11</span>,</span><br><span class="line"> <span class="string">'downloader/request_method_count/GET'</span>: <span class="number">11</span>,</span><br><span class="line"> <span class="string">'downloader/response_bytes'</span>: <span class="number">24871</span>,</span><br><span class="line"> <span class="string">'downloader/response_count'</span>: <span class="number">11</span>,</span><br><span class="line"> <span class="string">'downloader/response_status_count/200'</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">'downloader/response_status_count/404'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'dupefilter/filtered'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'finish_reason'</span>: <span class="string">'finished'</span>,</span><br><span class="line"> <span class="string">'finish_time'</span>: datetime.datetime(<span class="number">2017</span>, <span class="number">2</span>, <span class="number">19</span>, <span class="number">5</span>, <span class="number">37</span>, <span class="number">27</span>, <span class="number">227438</span>),</span><br><span class="line"> <span class="string">'item_scraped_count'</span>: <span class="number">100</span>,</span><br><span class="line"> <span class="string">'log_count/DEBUG'</span>: <span class="number">113</span>,</span><br><span class="line"> <span class="string">'log_count/INFO'</span>: <span class="number">7</span>,</span><br><span class="line"> <span class="string">'request_depth_max'</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">'response_received_count'</span>: <span class="number">11</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued'</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued/memory'</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued'</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued/memory'</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">'start_time'</span>: datetime.datetime(<span class="number">2017</span>, <span class="number">2</span>, <span class="number">19</span>, <span class="number">5</span>, <span class="number">37</span>, <span class="number">20</span>, <span class="number">321557</span>)&#125;</span><br><span class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-19</span> <span class="number">13</span>:<span class="number">37</span>:<span class="number">27</span> [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里只是部分运行结果，中间一些抓取结果已省略。 首先，Scrapy 输出了当前的版本号以及正在启动的项目名称。接着输出了当前 settings.py 中一些重写后的配置。然后输出了当前所应用的 Middlewares 和 Pipelines。Middlewares 默认是启用的，可以在 settings.py 中修改。Pipelines 默认是空，同样也可以在 settings.py 中配置。后面会对它们进行讲解。 接下来就是输出各个页面的抓取结果了，可以看到爬虫一边解析，一边翻页，直至将所有内容抓取完毕，然后终止。 最后，Scrapy 输出了整个抓取过程的统计信息，如请求的字节数、请求次数、响应次数、完成原因等。 整个 Scrapy 程序成功运行。我们通过非常简单的代码就完成了一个网站内容的爬取，这样相比之前一点点写程序简洁很多。</p>
                  <h3 id="10-保存到文件"><a href="#10-保存到文件" class="headerlink" title="10. 保存到文件"></a>10. 保存到文件</h3>
                  <p>运行完 Scrapy 后，我们只在控制台看到了输出结果。如果想保存结果该怎么办呢？ 要完成这个任务其实不需要任何额外的代码，Scrapy 提供的 Feed Exports 可以轻松将抓取结果输出。例如，我们想将上面的结果保存成 JSON 文件，可以执行如下命令：</p>
                  <figure class="highlight scss">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl <span class="attribute">quotes</span> -o <span class="attribute">quotes</span><span class="selector-class">.json</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>命令运行后，项目内多了一个 quotes.json 文件，文件包含了刚才抓取的所有内容，内容是 JSON 格式。 另外我们还可以每一个 Item 输出一行 JSON，输出后缀为 jl，为 jsonline 的缩写，命令如下所示：</p>
                  <figure class="highlight scss">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl <span class="attribute">quotes</span> -o <span class="attribute">quotes</span><span class="selector-class">.jl</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>或</p>
                  <figure class="highlight scss">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl <span class="attribute">quotes</span> -o <span class="attribute">quotes</span><span class="selector-class">.jsonlines</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出格式还支持很多种，例如 csv、xml、pickle、marshal 等，还支持 ftp、s3 等远程输出，另外还可以通过自定义 ItemExporter 来实现其他的输出。 例如，下面命令对应的输出分别为 csv、xml、pickle、marshal 格式以及 ftp 远程输出：</p>
                  <figure class="highlight scss">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl <span class="attribute">quotes</span> -o <span class="attribute">quotes</span><span class="selector-class">.csv</span></span><br><span class="line">scrapy crawl <span class="attribute">quotes</span> -o <span class="attribute">quotes</span><span class="selector-class">.xml</span></span><br><span class="line">scrapy crawl <span class="attribute">quotes</span> -o <span class="attribute">quotes</span><span class="selector-class">.pickle</span></span><br><span class="line">scrapy crawl <span class="attribute">quotes</span> -o <span class="attribute">quotes</span><span class="selector-class">.marshal</span></span><br><span class="line">scrapy crawl <span class="attribute">quotes</span> -o ftp://user:pass@ftp.example.com/path/to/quotes.csv</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中，ftp 输出需要正确配置用户名、密码、地址、输出路径，否则会报错。 通过 Scrapy 提供的 Feed Exports，我们可以轻松地输出抓取结果到文件。对于一些小型项目来说，这应该足够了。不过如果想要更复杂的输出，如输出到数据库等，我们可以使用 Item Pileline 来完成。</p>
                  <h3 id="11-使用-Item-Pipeline"><a href="#11-使用-Item-Pipeline" class="headerlink" title="11. 使用 Item Pipeline"></a>11. 使用 Item Pipeline</h3>
                  <p>如果想进行更复杂的操作，如将结果保存到 MongoDB 数据库，或者筛选某些有用的 Item，则我们可以定义 Item Pipeline 来实现。 Item Pipeline 为项目管道。当 Item 生成后，它会自动被送到 Item Pipeline 进行处理，我们常用 Item Pipeline 来做如下操作。</p>
                  <ul>
                    <li>清洗 HTML 数据</li>
                    <li>验证爬取数据，检查爬取字段</li>
                    <li>查重并丢弃重复内容</li>
                    <li>将爬取结果储存到数据库</li>
                  </ul>
                  <p>要实现 Item Pipeline 很简单，只需要定义一个类并实现 process_item() 方法即可。启用 Item Pipeline 后，Item Pipeline 会自动调用这个方法。process_item() 方法必须返回包含数据的字典或 Item 对象，或者抛出 DropItem 异常。 process_item() 方法有两个参数。一个参数是 item，每次 Spider 生成的 Item 都会作为参数传递过来。另一个参数是 spider，就是 Spider 的实例。 接下来，我们实现一个 Item Pipeline，筛掉 text 长度大于 50 的 Item，并将结果保存到 MongoDB。 修改项目里的 pipelines.py 文件，之前用命令行自动生成的文件内容可以删掉，增加一个 TextPipeline 类，内容如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.limit = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'text'</span>]<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">if</span> len(item[<span class="string">'text'</span>]) &gt; <span class="keyword">self</span>.<span class="symbol">limit:</span></span><br><span class="line">                item[<span class="string">'text'</span>] = item[<span class="string">'text'</span>][<span class="number">0</span><span class="symbol">:self</span>.limit].rstrip() + <span class="string">'...'</span></span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            <span class="keyword">return</span> DropItem(<span class="string">'Missing Text'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这段代码在构造方法里定义了限制长度为 50，实现了 process_item() 方法，其参数是 item 和 spider。首先该方法判断 item 的 text 属性是否存在，如果不存在，则抛出 DropItem 异常；如果存在，再判断长度是否大于 50，如果大于，那就截断然后拼接省略号，再将 item 返回即可。 接下来，我们将处理后的 item 存入 MongoDB，定义另外一个 Pipeline。同样在 pipelines.py 中，我们实现另一个类 MongoPipeline，内容如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, mongo_uri, mongo_db)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.mongo_uri = mongo_uri</span><br><span class="line">        <span class="keyword">self</span>.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client = pymongo.MongoClient(<span class="keyword">self</span>.mongo_uri)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[<span class="keyword">self</span>.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        name = item.__class_<span class="number">_</span>.__name_<span class="number">_</span></span><br><span class="line">        <span class="keyword">self</span>.db[name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>MongoPipeline 类实现了 API 定义的另外几个方法。</p>
                  <ul>
                    <li>from_crawler，这是一个类方法，用 @classmethod 标识，是一种依赖注入的方式，方法的参数就是 crawler，通过 crawler 这个我们可以拿到全局配置的每个配置信息，在全局配置 settings.py 中我们可以定义 MONGO_URI 和 MONGO_DB 来指定 MongoDB 连接需要的地址和数据库名称，拿到配置信息之后返回类对象即可。所以这个方法的定义主要是用来获取 settings.py 中的配置的。</li>
                    <li>open_spider，当 Spider 被开启时，这个方法被调用。在这里主要进行了一些初始化操作。</li>
                    <li>close_spider，当 Spider 被关闭时，这个方法会调用，在这里将数据库连接关闭。</li>
                  </ul>
                  <p>最主要的 process_item() 方法则执行了数据插入操作。 定义好 TextPipeline 和 MongoPipeline 这两个类后，我们需要在 settings.py 中使用它们。MongoDB 的连接信息还需要定义。 我们在 settings.py 中加入如下内容：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'tutorial.pipelines.TextPipeline'</span>: 300,</span><br><span class="line">   <span class="string">'tutorial.pipelines.MongoPipeline'</span>: 400,</span><br><span class="line">&#125;</span><br><span class="line"><span class="attribute">MONGO_URI</span>=<span class="string">'localhost'</span></span><br><span class="line"><span class="attribute">MONGO_DB</span>=<span class="string">'tutorial'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>赋值 ITEM_PIPELINES 字典，键名是 Pipeline 的类名称，键值是调用优先级，是一个数字，数字越小则对应的 Pipeline 越先被调用。 再重新执行爬取，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>爬取结束后，MongoDB 中创建了一个 tutorial 的数据库、QuoteItem 的表，如图 13-4 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033919.jpg" alt=""> 图 13-4 爬取结果 长的 text 已经被处理并追加了省略号，短的 text 保持不变，author 和 tags 也都相应保存。</p>
                  <h3 id="12-源代码"><a href="#12-源代码" class="headerlink" title="12. 源代码"></a>12. 源代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/ScrapyTutorial" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyTutorial</a>。</p>
                  <h3 id="13-结语"><a href="#13-结语" class="headerlink" title="13. 结语"></a>13. 结语</h3>
                  <p>我们通过抓取 Quotes 网站完成了整个 Scrapy 的简单入门。但这只是冰山一角，还有很多内容等待我们去探索。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-02 11:22:46" itemprop="dateCreated datePublished" datetime="2019-12-02T11:22:46+08:00">2019-12-02</time>
                </span>
                <span id="/8337.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.2-Scrapy 入门" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>14k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>13 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8333.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Paper <i class="label-arrow"></i>
                  </a>
                  <a href="/8333.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.1-Scrapy 框架介绍</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-1-Scrapy-框架介绍"><a href="#13-1-Scrapy-框架介绍" class="headerlink" title="13.1 Scrapy 框架介绍"></a>13.1 Scrapy 框架介绍</h1>
                  <p>Scrapy 是一个基于 Twisted 的异步处理框架，是纯 Python 实现的爬虫框架，其架构清晰，模块之间的耦合程度低，可扩展性极强，可以灵活完成各种需求。我们只需要定制开发几个模块就可以轻松实现一个爬虫。</p>
                  <h3 id="1-架构介绍"><a href="#1-架构介绍" class="headerlink" title="1. 架构介绍"></a>1. 架构介绍</h3>
                  <p>首先我们来看下 Scrapy 框架的架构，如图 13-1 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-033839.jpg" alt=""> 图 13-1 Scrapy 架构 它可以分为如下的几个部分。</p>
                  <ul>
                    <li>Engine，引擎，用来处理整个系统的数据流处理，触发事务，是整个框架的核心。</li>
                    <li>Item，项目，它定义了爬取结果的数据结构，爬取的数据会被赋值成该对象。</li>
                    <li>Scheduler， 调度器，用来接受引擎发过来的请求并加入队列中，并在引擎再次请求的时候提供给引擎。</li>
                    <li>Downloader，下载器，用于下载网页内容，并将网页内容返回给蜘蛛。</li>
                    <li>Spiders，蜘蛛，其内定义了爬取的逻辑和网页的解析规则，它主要负责解析响应并生成提取结果和新的请求。</li>
                    <li>Item Pipeline，项目管道，负责处理由蜘蛛从网页中抽取的项目，它的主要任务是清洗、验证和存储数据。</li>
                    <li>Downloader Middlewares，下载器中间件，位于引擎和下载器之间的钩子框架，主要是处理引擎与下载器之间的请求及响应。</li>
                    <li>Spider Middlewares， 蜘蛛中间件，位于引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛输入的响应和输出的结果及新的请求。</li>
                  </ul>
                  <h3 id="2-数据流"><a href="#2-数据流" class="headerlink" title="2. 数据流"></a>2. 数据流</h3>
                  <p>Scrapy 中的数据流由引擎控制，其过程如下:</p>
                  <ul>
                    <li>Engine 首先打开一个网站，找到处理该网站的 Spider 并向该 Spider 请求第一个要爬取的 URL。</li>
                    <li>Engine 从 Spider 中获取到第一个要爬取的 URL 并通过 Scheduler 以 Request 的形式调度。</li>
                    <li>Engine 向 Scheduler 请求下一个要爬取的 URL。</li>
                    <li>Scheduler 返回下一个要爬取的 URL 给 Engine，Engine 将 URL 通过 Downloader Middlewares 转发给 Downloader 下载。</li>
                    <li>一旦页面下载完毕， Downloader 生成一个该页面的 Response，并将其通过 Downloader Middlewares 发送给 Engine。</li>
                    <li>Engine 从下载器中接收到 Response 并通过 Spider Middlewares 发送给 Spider 处理。</li>
                    <li>Spider 处理 Response 并返回爬取到的 Item 及新的 Request 给 Engine。</li>
                    <li>Engine 将 Spider 返回的 Item 给 Item Pipeline，将新的 Request 给 Scheduler。</li>
                    <li>重复第二步到最后一步，直到 Scheduler 中没有更多的 Request，Engine 关闭该网站，爬取结束。</li>
                  </ul>
                  <p>通过多个组件的相互协作、不同组件完成工作的不同、组件对异步处理的支持，Scrapy 最大限度地利用了网络带宽，大大提高了数据爬取和处理的效率。</p>
                  <h3 id="3-项目结构"><a href="#3-项目结构" class="headerlink" title="3. 项目结构"></a>3. 项目结构</h3>
                  <p>Scrapy 框架和 pyspider 不同，它是通过命令行来创建项目的，代码的编写还是需要 IDE。项目创建之后，项目文件结构如下所示：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy.cfg</span><br><span class="line">project/</span><br><span class="line">    __init__.py</span><br><span class="line">    items.py</span><br><span class="line">    pipelines.py</span><br><span class="line">    settings.py</span><br><span class="line">    middlewares.py</span><br><span class="line">    spiders/</span><br><span class="line">        __init__.py</span><br><span class="line">        spider1.py</span><br><span class="line">        spider2.py</span><br><span class="line">        ...</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在此要将各个文件的功能描述如下：</p>
                  <ul>
                    <li>scrapy.cfg：它是 Scrapy 项目的配置文件，其内定义了项目的配置文件路径、部署相关信息等内容。</li>
                    <li>items.py：它定义 Item 数据结构，所有的 Item 的定义都可以放这里。</li>
                    <li>pipelines.py：它定义 Item Pipeline 的实现，所有的 Item Pipeline 的实现都可以放这里。</li>
                    <li>settings.py：它定义项目的全局配置。</li>
                    <li>middlewares.py：它定义 Spider Middlewares 和 Downloader Middlewares 的实现。</li>
                    <li>spiders：其内包含一个个 Spider 的实现，每个 Spider 都有一个文件。</li>
                  </ul>
                  <h3 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h3>
                  <p>本节介绍了 Scrapy 框架的基本架构、数据流过程以及项目结构。后面我们会详细了解 Scrapy 的用法，感受它的强大。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-02 11:20:01" itemprop="dateCreated datePublished" datetime="2019-12-02T11:20:01+08:00">2019-12-02</time>
                </span>
                <span id="/8333.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.1-Scrapy 框架介绍" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8320.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8320.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 12.3-pyspider 用法详解</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="12-3-pyspider-用法详解"><a href="#12-3-pyspider-用法详解" class="headerlink" title="12.3 pyspider 用法详解"></a>12.3 pyspider 用法详解</h1>
                  <p>前面我们了解了 pyspider 的基本用法，我们通过非常少的代码和便捷的可视化操作就完成了一个爬虫的编写，本节我们来总结一下它的详细用法。</p>
                  <h3 id="1-命令行"><a href="#1-命令行" class="headerlink" title="1. 命令行"></a>1. 命令行</h3>
                  <p>上面的实例通过如下命令启动 pyspider：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pyspider <span class="keyword">all</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>命令行还有很多可配制参数，完整的命令行结构如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">pyspider</span> <span class="selector-attr">[OPTIONS]</span> <span class="selector-tag">COMMAND</span> <span class="selector-attr">[ARGS]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中，OPTIONS 为可选参数，它可以指定如下参数。</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Options:</span><br><span class="line">  -c, <span class="comment">--config FILENAME    指定配置文件名称</span></span><br><span class="line">  <span class="comment">--logging-config TEXT    日志配置文件名称，默认: pyspider/pyspider/logging.conf</span></span><br><span class="line">  <span class="comment">--debug                  开启调试模式</span></span><br><span class="line">  <span class="comment">--queue-maxsize INTEGER  队列的最大长度</span></span><br><span class="line">  <span class="comment">--taskdb TEXT            taskdb 的数据库连接字符串，默认: sqlite</span></span><br><span class="line">  <span class="comment">--projectdb TEXT         projectdb 的数据库连接字符串，默认: sqlite</span></span><br><span class="line">  <span class="comment">--resultdb TEXT          resultdb 的数据库连接字符串，默认: sqlite</span></span><br><span class="line">  <span class="comment">--message-queue TEXT     消息队列连接字符串，默认: multiprocessing.Queue</span></span><br><span class="line">  <span class="comment">--phantomjs-proxy TEXT   PhantomJS 使用的代理，ip:port 的形式</span></span><br><span class="line">  <span class="comment">--data-path TEXT         数据库存放的路径</span></span><br><span class="line">  <span class="comment">--version                pyspider 的版本</span></span><br><span class="line">  <span class="comment">--help                   显示帮助信息</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>例如，-c 可以指定配置文件的名称，这是一个常用的配置，配置文件的样例结构如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"taskdb"</span>: <span class="string">"mysql+taskdb://username:password@host:port/taskdb"</span>,</span><br><span class="line">  <span class="attr">"projectdb"</span>: <span class="string">"mysql+projectdb://username:password@host:port/projectdb"</span>,</span><br><span class="line">  <span class="attr">"resultdb"</span>: <span class="string">"mysql+resultdb://username:password@host:port/resultdb"</span>,</span><br><span class="line">  <span class="attr">"message_queue"</span>: <span class="string">"amqp://username:password@host:port/%2F"</span>,</span><br><span class="line">  <span class="attr">"webui"</span>: &#123;</span><br><span class="line">    <span class="attr">"username"</span>: <span class="string">"some_name"</span>,</span><br><span class="line">    <span class="attr">"password"</span>: <span class="string">"some_passwd"</span>,</span><br><span class="line">    <span class="attr">"need-auth"</span>: <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果要配置 pyspider WebUI 的访问认证，可以新建一个 pyspider.json，内容如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"webui"</span>: &#123;</span><br><span class="line">    <span class="attr">"username"</span>: <span class="string">"root"</span>,</span><br><span class="line">    <span class="attr">"password"</span>: <span class="string">"123456"</span>,</span><br><span class="line">    <span class="attr">"need-auth"</span>: <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们通过在启动时指定配置文件来配置 pyspider WebUI 的访问认证，用户名为 root，密码为 123456，命令如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pyspider -c pyspider.json <span class="keyword">all</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后打开：<a href="http://localhost:5000/" target="_blank" rel="noopener">http://localhost:5000/</a>，页面如 12-26 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-033806.png" alt=""> 图 12-26 运行页面 也可以单独运行 pyspider 的某一个组件。 运行 Scheduler 的命令如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pyspider<span class="built_in"> scheduler </span>[OPTIONS]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行时也可以指定各种配置，参数如下所示：</p>
                  <figure class="highlight brainfuck">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment">Options:</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">xmlrpc</span> <span class="comment">/</span>--<span class="comment">no</span><span class="literal">-</span><span class="comment">xmlrpc</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">xmlrpc</span><span class="literal">-</span><span class="comment">host</span> <span class="comment">TEXT</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">xmlrpc</span><span class="literal">-</span><span class="comment">port</span> <span class="comment">INTEGER</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">inqueue</span><span class="literal">-</span><span class="comment">limit</span> <span class="comment">INTEGER</span>  <span class="comment">任务队列的最大长度，如果满了则新的任务会被忽略</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">delete</span><span class="literal">-</span><span class="comment">time</span> <span class="comment">INTEGER</span>    <span class="comment">设置为</span> <span class="comment">delete</span> <span class="comment">标记之前的删除时间</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">active</span><span class="literal">-</span><span class="comment">tasks</span> <span class="comment">INTEGER</span>   <span class="comment">当前活跃任务数量配置</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">loop</span><span class="literal">-</span><span class="comment">limit</span> <span class="comment">INTEGER</span>     <span class="comment">单轮最多调度的任务数量</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">scheduler</span><span class="literal">-</span><span class="comment">cls</span> <span class="comment">TEXT</span>     <span class="comment">Scheduler</span> <span class="comment">使用的类</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">help</span>                   <span class="comment">显示帮助信息</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行 Fetcher 的命令如下所示：</p>
                  <figure class="highlight apache">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">pyspider</span> fetcher<span class="meta"> [OPTIONS]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数配置如下所示：</p>
                  <figure class="highlight brainfuck">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment">Options:</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">xmlrpc</span> <span class="comment">/</span>--<span class="comment">no</span><span class="literal">-</span><span class="comment">xmlrpc</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">xmlrpc</span><span class="literal">-</span><span class="comment">host</span> <span class="comment">TEXT</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">xmlrpc</span><span class="literal">-</span><span class="comment">port</span> <span class="comment">INTEGER</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">poolsize</span> <span class="comment">INTEGER</span>      <span class="comment">同时请求的个数</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">proxy</span> <span class="comment">TEXT</span>            <span class="comment">使用的代理</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">user</span><span class="literal">-</span><span class="comment">agent</span> <span class="comment">TEXT</span>       <span class="comment">使用的</span> <span class="comment">User</span><span class="literal">-</span><span class="comment">Agent</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">timeout</span> <span class="comment">TEXT</span>          <span class="comment">超时时间</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">fetcher</span><span class="literal">-</span><span class="comment">cls</span> <span class="comment">TEXT</span>      <span class="comment">Fetcher</span> <span class="comment">使用的类</span></span><br><span class="line"><span class="comment"></span>  --<span class="comment">help</span>                  <span class="comment">显示帮助信息</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行 Processer 的命令如下所示：</p>
                  <figure class="highlight apache">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">pyspider</span> processor<span class="meta"> [OPTIONS]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数配置如下所示：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Options:</span><br><span class="line">  <span class="comment">--processor-cls TEXT  Processor 使用的类</span></span><br><span class="line">  <span class="comment">--help                显示帮助信息</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行 WebUI 的命令如下所示：</p>
                  <figure class="highlight apache">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">pyspider</span> webui<span class="meta"> [OPTIONS]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数配置如下所示：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Options:</span><br><span class="line">  <span class="comment">--host TEXT            运行地址</span></span><br><span class="line">  <span class="comment">--port INTEGER         运行端口</span></span><br><span class="line">  <span class="comment">--cdn TEXT             JS 和 CSS 的 CDN 服务器</span></span><br><span class="line">  <span class="comment">--scheduler-rpc TEXT   Scheduler 的 xmlrpc 路径</span></span><br><span class="line">  <span class="comment">--fetcher-rpc TEXT     Fetcher 的 xmlrpc 路径</span></span><br><span class="line">  <span class="comment">--max-rate FLOAT       每个项目最大的 rate 值</span></span><br><span class="line">  <span class="comment">--max-burst FLOAT      每个项目最大的 burst 值</span></span><br><span class="line">  <span class="comment">--username TEXT        Auth 验证的用户名</span></span><br><span class="line">  <span class="comment">--password TEXT        Auth 验证的密码</span></span><br><span class="line">  <span class="comment">--need-auth            是否需要验证</span></span><br><span class="line">  <span class="comment">--webui-instance TEXT  运行时使用的 Flask 应用</span></span><br><span class="line">  <span class="comment">--help                 显示帮助信息</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的配置和前面提到的配置文件参数是相同的。如果想要改变 WebUI 的端口为 5001，单独运行如下命令：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pyspider webui <span class="comment">--port 5001</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>或者可以将端口配置到 JSON 文件中，配置如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"webui"</span>: &#123;<span class="attr">"port"</span>: <span class="number">5001</span>&#125;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>使用如下命令启动同样可以达到相同的效果：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">pyspider</span> <span class="selector-tag">-c</span> <span class="selector-tag">pyspider</span><span class="selector-class">.json</span> <span class="selector-tag">webui</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就可以在 5001 端口上运行 WebUI 了。</p>
                  <h3 id="2-crawl-方法"><a href="#2-crawl-方法" class="headerlink" title="2. crawl() 方法"></a>2. crawl() 方法</h3>
                  <p>在前面的例子中，我们使用 crawl() 方法实现了新请求的生成，但是只指定了 URL 和 Callback。这里将详细介绍一下 crawl() 方法的参数配置。</p>
                  <h4 id="url"><a href="#url" class="headerlink" title="url"></a>url</h4>
                  <p>url 是爬取时的 URL，可以定义为单个 URL 字符串，也可以定义成 URL 列表。</p>
                  <h4 id="callback"><a href="#callback" class="headerlink" title="callback"></a>callback</h4>
                  <p>callback 是回调函数，指定了该 URL 对应的响应内容用哪个方法来解析，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://scrapy.org/'</span>, callback=<span class="keyword">self</span>.index_page)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里指定了 callback 为 index_page，就代表爬取 <a href="http://scrapy.org/" target="_blank" rel="noopener">http://scrapy.org/</a> 链接得到的响应会用 index_page() 方法来解析。 index_page() 方法的第一个参数是响应对象，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">    pass</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>方法中的 response 参数就是请求上述 URL 得到的响应对象，我们可以直接在 index_page() 方法中实现页面的解析。</p>
                  <h4 id="age"><a href="#age" class="headerlink" title="age"></a>age</h4>
                  <p>age 是任务的有效时间。如果某个任务在有效时间内且已经被执行，则它不会重复执行，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://www.example.org/'</span>, callback=<span class="keyword">self</span>.callback,</span><br><span class="line">               age=<span class="number">10</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>或者可以这样设置：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">@config(age=<span class="number">10</span> * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    pass</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>默认的有效时间为 10 天。</p>
                  <h4 id="priority"><a href="#priority" class="headerlink" title="priority"></a>priority</h4>
                  <p>priority 是爬取任务的优先级，其值默认是 0，priority 的数值越大，对应的请求会越优先被调度，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://www.example.org/page.html'</span>, callback=<span class="keyword">self</span>.index_page)</span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://www.example.org/233.html'</span>, callback=<span class="keyword">self</span>.detail_page,</span><br><span class="line">               priority=<span class="number">1</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第二个任务会优先调用，233.html 这个链接优先爬取。</p>
                  <h4 id="exetime"><a href="#exetime" class="headerlink" title="exetime"></a>exetime</h4>
                  <p>exetime 参数可以设置定时任务，其值是时间戳，默认是 0，即代表立即执行，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://www.example.org/'</span>, callback=<span class="keyword">self</span>.callback,</span><br><span class="line">               exetime=time.time()+<span class="number">30</span>*<span class="number">60</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样该任务会在 30 分钟之后执行。</p>
                  <h4 id="retries"><a href="#retries" class="headerlink" title="retries"></a>retries</h4>
                  <p>retries 可以定义重试次数，其值默认是 3。</p>
                  <h4 id="itag"><a href="#itag" class="headerlink" title="itag"></a>itag</h4>
                  <p>itag 参数设置判定网页是否发生变化的节点值，在爬取时会判定次当前节点是否和上次爬取到的节点相同。如果节点相同，则证明页面没有更新，就不会重复爬取，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> response.doc(<span class="string">'.item'</span>).items()<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.crawl(item.find(<span class="string">'a'</span>).attr.url, callback=<span class="keyword">self</span>.detail_page,</span><br><span class="line">                   itag=item.find(<span class="string">'.update-time'</span>).text())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里设置了更新时间这个节点的值为 itag，在下次爬取时就会首先检测这个值有没有发生变化，如果没有变化，则不再重复爬取，否则执行爬取。</p>
                  <h4 id="auto-recrawl"><a href="#auto-recrawl" class="headerlink" title="auto_recrawl"></a>auto_recrawl</h4>
                  <p>当开启时，爬取任务在过期后会重新执行，循环时间即定义的 age 时间长度，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def on_start(self):</span><br><span class="line">    self.crawl(<span class="string">'http://www.example.org/'</span>, <span class="attribute">callback</span>=self.callback,</span><br><span class="line">               <span class="attribute">age</span>=5*60*60, <span class="attribute">auto_recrawl</span>=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了 age 有效期为 5 小时，设置了 auto_recrawl 为 True，这样任务就会每 5 小时执行一次。</p>
                  <h4 id="method"><a href="#method" class="headerlink" title="method"></a>method</h4>
                  <p>method 是 HTTP 请求方式，它默认是 GET。如果想发起 POST 请求，可以将 method 设置为 POST。</p>
                  <h4 id="params"><a href="#params" class="headerlink" title="params"></a>params</h4>
                  <p>我们可以方便地使用 params 来定义 GET 请求参数，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://httpbin.org/get'</span>, callback=<span class="keyword">self</span>.callback,</span><br><span class="line">               params=&#123;<span class="string">'a'</span>: <span class="number">123</span>, <span class="string">'b'</span>: <span class="string">'c'</span>&#125;)</span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://httpbin.org/get?a=123&amp;b=c'</span>, callback=<span class="keyword">self</span>.callback)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里两个爬取任务是等价的。</p>
                  <h4 id="data"><a href="#data" class="headerlink" title="data"></a>data</h4>
                  <p>data 是 POST 表单数据。当请求方式为 POST 时，我们可以通过此参数传递表单数据，如下所示：</p>
                  <figure class="highlight oxygene">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def on_start(<span class="keyword">self</span>):</span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://httpbin.org/post'</span>, callback=<span class="keyword">self</span>.callback,</span><br><span class="line">               <span class="function"><span class="keyword">method</span>='<span class="title">POST</span>', <span class="title">data</span>=<span class="comment">&#123;'a': 123, 'b': 'c'&#125;</span>)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="files"><a href="#files" class="headerlink" title="files"></a>files</h4>
                  <p>files 是上传的文件，需要指定文件名，如下所示：</p>
                  <figure class="highlight oxygene">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def on_start(<span class="keyword">self</span>):</span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://httpbin.org/post'</span>, callback=<span class="keyword">self</span>.callback,</span><br><span class="line">               <span class="function"><span class="keyword">method</span>='<span class="title">POST</span>', <span class="title">files</span>=<span class="comment">&#123;field: &#123;filename: 'content'&#125;</span>&#125;)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="user-agent"><a href="#user-agent" class="headerlink" title="user_agent"></a>user_agent</h4>
                  <p>user_agent 是爬取使用的 User-Agent。</p>
                  <h4 id="headers"><a href="#headers" class="headerlink" title="headers"></a>headers</h4>
                  <p>headers 是爬取时使用的 Headers，即 Request Headers。</p>
                  <h4 id="cookies"><a href="#cookies" class="headerlink" title="cookies"></a>cookies</h4>
                  <p>cookies 是爬取时使用的 Cookies，为字典格式。</p>
                  <h4 id="connect-timeout"><a href="#connect-timeout" class="headerlink" title="connect_timeout"></a>connect_timeout</h4>
                  <p>connect_timeout 是在初始化连接时的最长等待时间，它默认是 20 秒。</p>
                  <h4 id="timeout"><a href="#timeout" class="headerlink" title="timeout"></a>timeout</h4>
                  <p>timeout 是抓取网页时的最长等待时间，它默认是 120 秒。</p>
                  <h4 id="allow-redirects"><a href="#allow-redirects" class="headerlink" title="allow_redirects"></a>allow_redirects</h4>
                  <p>allow_redirects 确定是否自动处理重定向，它默认是 True。</p>
                  <h4 id="validate-cert"><a href="#validate-cert" class="headerlink" title="validate_cert"></a>validate_cert</h4>
                  <p>validate_cert 确定是否验证证书，此选项对 HTTPS 请求有效，默认是 True。</p>
                  <h4 id="proxy"><a href="#proxy" class="headerlink" title="proxy"></a>proxy</h4>
                  <p>proxy 是爬取时使用的代理，它支持用户名密码的配置，格式为 username:password@hostname:port，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://httpbin.org/get'</span>, callback=<span class="keyword">self</span>.callback, proxy=<span class="string">'127.0.0.1:9743'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>也可以设置 craw_config 来实现全局配置，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">class</span> <span class="symbol">Handler</span>(<span class="symbol">BaseHandler</span>):</span><br><span class="line">    <span class="symbol">crawl_config</span> = &#123;<span class="string">'proxy'</span>: <span class="string">'127.0.0.1:9743'</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="fetch-type"><a href="#fetch-type" class="headerlink" title="fetch_type"></a>fetch_type</h4>
                  <p>fetch_type 开启 PhantomJS 渲染。如果遇到 JavaScript 渲染的页面，指定此字段即可实现 PhantomJS 的对接，pyspider 将会使用 PhantomJS 进行网页的抓取，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'https://www.taobao.com'</span>, callback=<span class="keyword">self</span>.index_page, fetch_type=<span class="string">'js'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就可以实现淘宝页面的抓取了，得到的结果就是浏览器中看到的效果。</p>
                  <h4 id="js-script"><a href="#js-script" class="headerlink" title="js_script"></a>js_script</h4>
                  <p>js_script 是页面加载完毕后执行的 JavaScript 脚本，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.crawl(<span class="string">'http://www.example.org/'</span>, callback=self.callback,</span><br><span class="line">               fetch_type=<span class="string">'js'</span>, js_script=<span class="string">'''</span></span><br><span class="line"><span class="string">               function() &#123;window.scrollTo(0,document.body.scrollHeight);</span></span><br><span class="line"><span class="string">                   return 123;</span></span><br><span class="line"><span class="string">               &#125;</span></span><br><span class="line"><span class="string">               '''</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>页面加载成功后将执行页面混动的 JavaScript 代码，页面会下拉到最底部。</p>
                  <h4 id="js-run-at"><a href="#js-run-at" class="headerlink" title="js_run_at"></a>js_run_at</h4>
                  <p>js_run_at 代表 JavaScript 脚本运行的位置，是在页面节点开头还是结尾，默认是结尾，即 document-end。</p>
                  <h4 id="js-viewport-width-js-viewport-height"><a href="#js-viewport-width-js-viewport-height" class="headerlink" title="js_viewport_width/js_viewport_height"></a>js_viewport_width/js_viewport_height</h4>
                  <p>js_viewport_width/js_viewport_height 是 JavaScript 渲染页面时的窗口大小。</p>
                  <h4 id="load-images"><a href="#load-images" class="headerlink" title="load_images"></a>load_images</h4>
                  <p>load_images 在加载 JavaScript 页面时确定是否加载图片，它默认是否。</p>
                  <h4 id="save"><a href="#save" class="headerlink" title="save"></a>save</h4>
                  <p>save 参数非常有用，可以在不同的方法之间传递参数，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://www.example.org/'</span>, callback=<span class="keyword">self</span>.callback,</span><br><span class="line">               save=&#123;<span class="string">'page'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">return</span> response.save[<span class="string">'page'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样，在 on_start() 方法中生成 Request 并传递额外的参数 page，在回调函数里可以通过 response 变量的 save 字段接收到这些参数值。</p>
                  <h4 id="cancel"><a href="#cancel" class="headerlink" title="cancel"></a>cancel</h4>
                  <p>cancel 是取消任务，如果一个任务是 ACTIVE 状态的，则需要将 force_update 设置为 True。</p>
                  <h4 id="force-update"><a href="#force-update" class="headerlink" title="force_update"></a>force_update</h4>
                  <p>即使任务处于 ACTIVE 状态，那也会强制更新状态。 以上便是 crawl() 方法的参数介绍，更加详细的描述可以参考：<a href="http://docs.pyspider.org/en/latest/apis/self.crawl/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/apis/self.crawl/</a>。</p>
                  <h3 id="3-任务区分"><a href="#3-任务区分" class="headerlink" title="3. 任务区分"></a>3. 任务区分</h3>
                  <p>在 pyspider 判断两个任务是否是重复的是使用的是该任务对应的 URL 的 MD5 值作为任务的唯一 ID，如果 ID 相同，那么两个任务就会判定为相同，其中一个就不会爬取了。很多情况下请求的链接可能是同一个，但是 POST 的参数不同。这时可以重写 task_id() 方法，改变这个 ID 的计算方式来实现不同任务的区分，如下所示：</p>
                  <figure class="highlight gradle">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyspider.libs.utils <span class="keyword">import</span> md5string</span><br><span class="line"><span class="keyword">def</span> get_taskid(self, <span class="keyword">task</span>):</span><br><span class="line">    <span class="keyword">return</span> md5string(<span class="keyword">task</span>[<span class="string">'url'</span>]+json.dumps(<span class="keyword">task</span>[<span class="string">'fetch'</span>].get(<span class="string">'data'</span>, <span class="string">''</span>)))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里重写了 get_taskid() 方法，利用 URL 和 POST 的参数来生成 ID。这样一来，即使 URL 相同，但是 POST 的参数不同，两个任务的 ID 就不同，它们就不会被识别成重复任务。</p>
                  <h3 id="4-全局配置"><a href="#4-全局配置" class="headerlink" title="4. 全局配置"></a>4. 全局配置</h3>
                  <p>pyspider 可以使用 crawl_config 来指定全局的配置，配置中的参数会和 crawl() 方法创建任务时的参数合并。如要全局配置一个 Headers，可以定义如下代码：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config = &#123;</span><br><span class="line">        <span class="string">'headers'</span>: &#123;<span class="string">'User-Agent'</span>: <span class="string">'GoogleBot'</span>,&#125;</span><br><span class="line">    &#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="5-定时爬取"><a href="#5-定时爬取" class="headerlink" title="5. 定时爬取"></a>5. 定时爬取</h3>
                  <p>我们可以通过 every 属性来设置爬取的时间间隔，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">@every(minutes=<span class="number">24</span> * <span class="number">60</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> <span class="symbol">urllist:</span></span><br><span class="line">        <span class="keyword">self</span>.crawl(url, callback=<span class="keyword">self</span>.index_page)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里设置了每天执行一次爬取。 在上文中我们提到了任务的有效时间，在有效时间内爬取不会重复。所以要把有效时间设置得比重复时间更短，这样才可以实现定时爬取。 例如，下面的代码就无法做到每天爬取：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">@every(minutes=<span class="number">24</span> * <span class="number">60</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="string">'http://www.example.org/'</span>, callback=<span class="keyword">self</span>.index_page)</span><br><span class="line"></span><br><span class="line">@config(age=<span class="number">10</span> * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    pass</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里任务的过期时间为 10 天，而自动爬取的时间间隔为 1 天。当第二次尝试重新爬取的时候，pyspider 会监测到此任务尚未过期，便不会执行爬取，所以我们需要将 age 设置得小于定时时间。</p>
                  <h3 id="6-项目状态"><a href="#6-项目状态" class="headerlink" title="6. 项目状态"></a>6. 项目状态</h3>
                  <p>每个项目都有 6 个状态，分别是 TODO、STOP、CHECKING、DEBUG、RUNNING、PAUSE。</p>
                  <ul>
                    <li>TODO：它是项目刚刚被创建还未实现时的状态。</li>
                    <li>STOP：如果想停止某项目的抓取，可以将项目的状态设置为 STOP。</li>
                    <li>CHECKING：正在运行的项目被修改后就会变成 CHECKING 状态，项目在中途出错需要调整的时候会遇到这种情况。</li>
                    <li>DEBUG/RUNNING：这两个状态对项目的运行没有影响，状态设置为任意一个，项目都可以运行，但是可以用二者来区分项目是否已经测试通过。</li>
                    <li>PAUSE：当爬取过程中出现连续多次错误时，项目会自动设置为 PAUSE 状态，并等待一定时间后继续爬取。</li>
                  </ul>
                  <h3 id="7-抓取进度"><a href="#7-抓取进度" class="headerlink" title="7. 抓取进度"></a>7. 抓取进度</h3>
                  <p>在抓取时，可以看到抓取的进度，progress 部分会显示 4 个进度条，如图 12-27 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033821.jpg" alt=""> 图 12-27 抓取进度 progress 中的 5m、1h、1d 指的是最近 5 分、1 小时、1 天内的请求情况，all 代表所有的请求情况。 蓝色的请求代表等待被执行的任务，绿色的代表成功的任务，黄色的代表请求失败后等待重试的任务，红色的代表失败次数过多而被忽略的任务，从这里我们可以直观看到爬取的进度和请求情况。</p>
                  <h3 id="8-删除项目"><a href="#8-删除项目" class="headerlink" title="8. 删除项目"></a>8. 删除项目</h3>
                  <p>pyspider 中没有直接删除项目的选项。如要删除任务，那么将项目的状态设置为 STOP，将分组的名称设置为 delete，等待 24 小时，则项目会自动删除。</p>
                  <h3 id="9-结语"><a href="#9-结语" class="headerlink" title="9. 结语"></a>9. 结语</h3>
                  <p>以上内容便是 pyspider 的常用用法。如要了解更多，可以参考 pyspider 的官方文档：<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a>。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-30 09:22:31" itemprop="dateCreated datePublished" datetime="2019-11-30T09:22:31+08:00">2019-11-30</time>
                </span>
                <span id="/8320.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 12.3-pyspider 用法详解" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>8.5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>8 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8317.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8317.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 12.2-pyspider 的基本使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="12-2-pyspider-的基本使用"><a href="#12-2-pyspider-的基本使用" class="headerlink" title="12.2 pyspider 的基本使用"></a>12.2 pyspider 的基本使用</h1>
                  <p>本节用一个实例来讲解 pyspider 的基本用法。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们要爬取的目标是去哪儿网的旅游攻略，链接为 <a href="http://travel.qunar.com/travelbook/list.htm，我们要将所有攻略的作者、标题、出发日期、人均费用、攻略正文等保存下来，存储到" target="_blank" rel="noopener">http://travel.qunar.com/travelbook/list.htm，我们要将所有攻略的作者、标题、出发日期、人均费用、攻略正文等保存下来，存储到</a> MongoDB 中。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保已经安装好了 pyspider 和 PhantomJS，安装好了 MongoDB 并正常运行服务，还需要安装 PyMongo 库，具体安装可以参考第 1 章的说明。</p>
                  <h3 id="3-启动-pyspider"><a href="#3-启动-pyspider" class="headerlink" title="3. 启动 pyspider"></a>3. 启动 pyspider</h3>
                  <p>执行如下命令启动 pyspider：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pyspider <span class="keyword">all</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行效果如图 12-2 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033547.jpg" alt=""> 图 12-2 运行结果 这样可以启动 pyspider 的所有组件，包括 PhantomJS、ResultWorker、Processer、Fetcher、Scheduler、WebUI，这些都是 pyspider 运行必备的组件。最后一行输出提示 WebUI 运行在 5000 端口上。可以打开浏览器，输入链接 <a href="http://localhost:5000，这时我们会看到页面，如图">http://localhost:5000，这时我们会看到页面，如图</a> 12-3 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033552.png" alt=""> 图 12-3 WebUI 页面 此页面便是 pyspider 的 WebUI，我们可以用它来管理项目、编写代码、在线调试、监控任务等。</p>
                  <h3 id="4-创建项目"><a href="#4-创建项目" class="headerlink" title="4. 创建项目"></a>4. 创建项目</h3>
                  <p>新建一个项目，点击右边的 Create 按钮，在弹出的浮窗里输入项目的名称和爬取的链接，再点击 Create 按钮，这样就成功创建了一个项目，如图 12-4 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033557.png" alt=""> 图 12-4 创建项目 接下来会看到 pyspider 的项目编辑和调试页面，如图 12-5 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033600.png" alt=""> 图 12-5 调试页面 左侧就是代码的调试页面，点击左侧右上角的 run 单步调试爬虫程序，在左侧下半部分可以预览当前的爬取页面。右侧是代码编辑页面，我们可以直接编辑代码和保存代码，不需要借助于 IDE。 注意右侧，pyspider 已经帮我们生成了一段代码，代码如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from pyspider.libs.base_handler import *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span>(<span class="title">BaseHandler</span>):</span></span><br><span class="line">    crawl_config = &#123; &#125;</span><br><span class="line"></span><br><span class="line">    @every(minutes=<span class="number">24</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.crawl(<span class="string">'http://travel.qunar.com/travelbook/list.htm'</span>, callback=<span class="keyword">self</span>.index_page)</span><br><span class="line"></span><br><span class="line">    @config(age=<span class="number">10</span> * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.doc(<span class="string">'a[href^="http"]'</span>).items()<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.crawl(each.attr.href, callback=<span class="keyword">self</span>.detail_page)</span><br><span class="line"></span><br><span class="line">    @config(priority=<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_page</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"url"</span>: response.url,</span><br><span class="line">            <span class="string">"title"</span>: response.doc(<span class="string">'title'</span>).text(),&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的 Handler 就是 pyspider 爬虫的主类，我们可以在此处定义爬取、解析、存储的逻辑。整个爬虫的功能只需要一个 Handler 即可完成。 接下来我们可以看到一个 crawl_config 属性。我们可以将本项目的所有爬取配置统一定义到这里，如定义 Headers、设置代理等，配置之后全局生效。 然后，on_start() 方法是爬取入口，初始的爬取请求会在这里产生，该方法通过调用 crawl() 方法即可新建一个爬取请求，第一个参数是爬取的 URL，这里自动替换成我们所定义的 URL。crawl() 方法还有一个参数 callback，它指定了这个页面爬取成功后用哪个方法进行解析，代码中指定为 index_page() 方法，即如果这个 URL 对应的页面爬取成功了，那 Response 将交给 index_page() 方法解析。 index_page() 方法恰好接收这个 Response 参数，Response 对接了 pyquery。我们直接调用 doc() 方法传入相应的 CSS 选择器，就可以像 pyquery 一样解析此页面，代码中默认是 a[href^=”http”]，也就是说该方法解析了页面的所有链接，然后将链接遍历，再次调用了 crawl() 方法生成了新的爬取请求，同时再指定了 callback 为 detail_page，意思是说这些页面爬取成功了就调用 detail_page() 方法解析。这里，index_page() 实现了两个功能，一是将爬取的结果进行解析，二是生成新的爬取请求。 detail_page() 同样接收 Response 作为参数。detail_page() 抓取的就是详情页的信息，就不会生成新的请求，只对 Response 对象做解析，解析之后将结果以字典的形式返回。当然我们也可以进行后续处理，如将结果保存到数据库。 接下来，我们改写一下代码来实现攻略的爬取吧。</p>
                  <h3 id="5-爬取首页"><a href="#5-爬取首页" class="headerlink" title="5. 爬取首页"></a>5. 爬取首页</h3>
                  <p>点击左栏右上角的 run 按钮，即可看到页面下方 follows 便会出现一个标注，其中包含数字 1，这代表有新的爬取请求产生，如图 12-6 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033606.jpg" alt=""> 图 12-6 操作示例 左栏左上角会出现当前 run 的配置文件，这里有一个 callback 为 on_start，这说明点击 run 之后实际是执行了 on_start() 方法。在 on_start() 方法中，我们利用 crawl() 方法生成一个爬取请求，那下方 follows 部分的数字 1 就代表了这一个爬取请求。 点击下方的 follows 按钮，即可看到生成的爬取请求的链接。每个链接的右侧还有一个箭头按钮，如图 12-7 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033611.jpg" alt=""> 图 12-7 操作示例 点击该箭头，我们就可以对此链接进行爬取，也就是爬取攻略的首页内容，如图 12-8 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033616.jpg" alt=""> 图 12-8 爬取结果 上方的 callback 已经变成了 index_page，这就代表当前运行了 index_page() 方法。index_page() 接收到的 response 参数就是刚才生成的第一个爬取请求的 Response 对象。index_page() 方法通过调用 doc() 方法，传入提取所有 a 节点的 CSS 选择器，然后获取 a 节点的属性 href，这样实际上就是获取了第一个爬取页面中的所有链接。然后在 index_page() 方法里遍历了所有链接，同时调用 crawl() 方法，就把这一个个的链接构造成新的爬取请求了。所以最下方 follows 按钮部分有 217 的数字标记，这代表新生成了 217 个爬取请求，同时这些请求的 URL 都呈现在当前页面了。 再点击下方的 web 按钮，即可预览当前爬取结果的页面，如图 12-9 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033620.jpg" alt=""> 图 12-9 预览页面 当前看到的页面结果和浏览器看到的几乎是完全一致的，在这里我们可以方便地查看页面请求的结果。 点击 html 按钮即可查看当前页面的源代码，如图 12-10 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033626.jpg" alt=""> 图 12-10 页面源码 如果需要分析代码的结构，我们可以直接参考页面源码。 我们刚才在 index_page() 方法中提取了所有的链接并生成了新的爬取请求。但是很明显要爬取的肯定不是所有链接，只需要攻略详情的页面链接就够了，所以我们要修改一下当前 index_page() 里提取链接时的 CSS 选择器。 接下来需要另外一个工具。首先切换到 Web 页面，找到攻略的标题，点击下方的 enable css selector helper，点击标题。这时候我们看到标题外多了一个红框，上方出现了一个 CSS 选择器，这就是当前标题对应的 CSS 选择器，如图 12-11 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033631.jpg" alt=""> 图 12-11 CSS 工具 在右侧代码选中要更改的区域，点击左栏的右箭头，此时在上方出现的标题的 CSS 选择器就会被替换到右侧代码中，如图 12-12 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033637.png" alt=""> 图 12-12 操作结果 这样就成功完成了 CSS 选择器的替换，非常便捷。 重新点击左栏右上角的 run 按钮，即可重新执行 index_page() 方法。此时的 follows 就变成了 10 个，也就是说现在我们提取的只有当前页面的 10 个攻略，如图 12-13 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033647.jpg" alt=""> 图 12-13 运行结果 我们现在抓取的只是第一页的内容，还需要抓取后续页面，所以还需要一个爬取链接，即爬取下一页的攻略列表页面。我们再利用 crawl() 方法添加下一页的爬取请求，在 index_page() 方法里面添加如下代码，然后点击 save 保存：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">next</span> = response.doc(<span class="string">'.next'</span>).attr.href</span><br><span class="line"><span class="keyword">self</span>.crawl(<span class="keyword">next</span>, callback=<span class="keyword">self</span>.index_page)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>利用 CSS 选择器选中下一页的链接，获取它的 href 属性，也就获取了页面的 URL。然后将该 URL 传给 crawl() 方法，同时指定回调函数，注意这里回调函数仍然指定为 index_page() 方法，因为下一页的结构与此页相同。 重新点击 run 按钮，这时就可以看到 11 个爬取请求。follows 按钮上会显示 11，这就代表我们成功添加了下一页的爬取请求，如图 12-14 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033657.jpg" alt=""> 图 12-14 运行结果 现在，索引列表页的解析过程我们就完成了。</p>
                  <h3 id="6-爬取详情页"><a href="#6-爬取详情页" class="headerlink" title="6. 爬取详情页"></a>6. 爬取详情页</h3>
                  <p>任意选取一个详情页进入，点击前 10 个爬取请求中的任意一个的右箭头，执行详情页的爬取，如图 12-15 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033702.jpg" alt=""> 图 12-15 运行结果 切换到 Web 页面预览效果，页面下拉之后，头图正文中的一些图片一直显示加载中，如图 12-16 和图 12-17 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033707.jpg" alt=""> 图 12-16 预览结果 <img src="https://cdn.cuiqingcai.com/2019-11-27-033712.jpg" alt=""> 图 12-17 预览结果 查看源代码，我们没有看到 img 节点，如图 12-18 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033717.jpg" alt=""> 图 12-18 源代码 出现此现象的原因是 pyspider 默认发送 HTTP 请求，请求的 HTML 文档本身就不包含 img 节点。但是在浏览器中我们看到了图片，这是因为这张图片是后期经过 JavaScript 出现的。那么，我们该如何获取呢？ 幸运的是，pyspider 内部对接了 PhantomJS，那么我们只需要修改一个参数即可。 我们将 index_page() 中生成抓取详情页的请求方法添加一个参数 fetch_type，改写的 index_page() 变为如下内容：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> response.doc(<span class="string">'li&gt; .tit &gt; a'</span>).items()<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.crawl(each.attr.href, callback=<span class="keyword">self</span>.detail_page, fetch_type=<span class="string">'js'</span>)</span><br><span class="line">    <span class="keyword">next</span> = response.doc(<span class="string">'.next'</span>).attr.href</span><br><span class="line">    <span class="keyword">self</span>.crawl(<span class="keyword">next</span>, callback=<span class="keyword">self</span>.index_page)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来，我们来试试它的抓取效果。 点击左栏上方的左箭头返回，重新调用 index_page() 方法生成新的爬取详情页的 Request，如图 12-19 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033723.jpg" alt=""> 图 12-19 爬取详情 再点击新生成的详情页 Request 的爬取按钮，这时我们便可以看到页面变成了这样子，如图 12-20 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033727.jpg" alt=""> 图 12-20 运行结果 图片被成功渲染出来，这就是启用了 PhantomJS 渲染后的结果。只需要添加一个 fetch_type 参数即可，这非常方便。 最后就是将详情页中需要的信息提取出来，提取过程不再赘述。最终 detail_page() 方法改写如下所示：</p>
                  <figure class="highlight xquery">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def detail_page(self, response):</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'url'</span>: response.url,</span><br><span class="line">        <span class="string">'title'</span>: response<span class="built_in">.doc</span>(<span class="string">'#booktitle'</span>).<span class="type">text</span>(),</span><br><span class="line">        <span class="string">'date'</span>: response<span class="built_in">.doc</span>(<span class="string">'.when .data'</span>).<span class="type">text</span>(),</span><br><span class="line">        <span class="string">'day'</span>: response<span class="built_in">.doc</span>(<span class="string">'.howlong .data'</span>).<span class="type">text</span>(),</span><br><span class="line">        <span class="string">'who'</span>: response<span class="built_in">.doc</span>(<span class="string">'.who .data'</span>).<span class="type">text</span>(),</span><br><span class="line">        <span class="string">'text'</span>: response<span class="built_in">.doc</span>(<span class="string">'#b_panel_schedule'</span>).<span class="type">text</span>(),</span><br><span class="line">        <span class="string">'image'</span>: response<span class="built_in">.doc</span>(<span class="string">'.cover_img'</span>).attr.src</span><br><span class="line">    &#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们分别提取了页面的链接、标题、出行日期、出行天数、人物、攻略正文、头图信息，将这些信息构造成一个字典。 重新运行，即可发现输出结果如图 12-21 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033733.png" alt=""> 图 12-21 输出结果 左栏中输出了最终构造的字典信息，这就是一篇攻略的抓取结果。</p>
                  <h3 id="7-启动爬虫"><a href="#7-启动爬虫" class="headerlink" title="7. 启动爬虫"></a>7. 启动爬虫</h3>
                  <p>返回爬虫的主页面，将爬虫的 status 设置成 DEBUG 或 RUNNING，点击右侧的 Run 按钮即可开始爬取，如图 12-22 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033737.jpg" alt=""> 图 12-22 启动爬虫 在最左侧我们可以定义项目的分组，以方便管理。rate/burst 代表当前的爬取速率，rate 代表 1 秒发出多少个请求，burst 相当于流量控制中的令牌桶算法的令牌数，rate 和 burst 设置的越大，爬取速率越快，当然速率需要考虑本机性能和爬取过快被封的问题。process 中的 5m、1h、1d 指的是最近 5 分、1 小时、1 天内的请求情况，all 代表所有的请求情况。请求由不同颜色表示，蓝色的代表等待被执行的请求，绿色的代表成功的请求，黄色的代表请求失败后等待重试的请求，红色的代表失败次数过多而被忽略的请求，这样可以直观知道爬取的进度和请求情况，如图 12-23 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033743.jpg" alt=""> 图 12-23 爬取情况 点击 Active Tasks，即可查看最近请求的详细状况，如图 12-24 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033748.png" alt=""> 图 12-24 最近请求 点击 Results，即可查看所有的爬取结果，如图 12-25 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033754.png" alt=""> 图 12-25 爬取结果 点击右上角的按钮，即可获取数据的 JSON、CSV 格式。</p>
                  <h3 id="8-本节代码"><a href="#8-本节代码" class="headerlink" title="8. 本节代码"></a>8. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/Qunar" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Qunar</a>。</p>
                  <h3 id="9-结语"><a href="#9-结语" class="headerlink" title="9. 结语"></a>9. 结语</h3>
                  <p>本节介绍了 pyspider 的基本用法，接下来我们会更加深入了解它的详细使用。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-30 09:20:54" itemprop="dateCreated datePublished" datetime="2019-11-30T09:20:54+08:00">2019-11-30</time>
                </span>
                <span id="/8317.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 12.2-pyspider 的基本使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>5.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>5 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8309.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8309.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 12.1-pyspider 框架介绍</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="12-1-pyspider-框架介绍"><a href="#12-1-pyspider-框架介绍" class="headerlink" title="12.1 pyspider 框架介绍"></a>12.1 pyspider 框架介绍</h1>
                  <p>pyspider 是由国人 binux 编写的强大的网络爬虫系统，其 GitHub 地址为 <a href="https://github.com/binux/pyspider，官方文档地址为" target="_blank" rel="noopener">https://github.com/binux/pyspider，官方文档地址为</a> <a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a>。 pyspider 带有强大的 WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器，它支持多种数据库后端、多种消息队列、JavaScript 渲染页面的爬取，使用起来非常方便。</p>
                  <h3 id="1-pyspider-基本功能"><a href="#1-pyspider-基本功能" class="headerlink" title="1. pyspider 基本功能"></a>1. pyspider 基本功能</h3>
                  <p>我们总结了一下，PySpider 的功能有如下几点。</p>
                  <ul>
                    <li>提供方便易用的 WebUI 系统，可以可视化地编写和调试爬虫。</li>
                    <li>提供爬取进度监控、爬取结果查看、爬虫项目管理等功能。</li>
                    <li>支持多种后端数据库，如 MySQL、MongoDB、Redis、SQLite、Elasticsearch、PostgreSQL。</li>
                    <li>支持多种消息队列，如 RabbitMQ、Beanstalk、Redis、Kombu。</li>
                    <li>提供优先级控制、失败重试、定时抓取等功能。</li>
                    <li>对接了 PhantomJS，可以抓取 JavaScript 渲染的页面。</li>
                    <li>支持单机和分布式部署，支持 Docker 部署。</li>
                  </ul>
                  <p>如果想要快速方便地实现一个页面的抓取，使用 pyspider 不失为一个好的选择。</p>
                  <h3 id="2-与-Scrapy-的比较"><a href="#2-与-Scrapy-的比较" class="headerlink" title="2. 与 Scrapy 的比较"></a>2. 与 Scrapy 的比较</h3>
                  <p>后面会介绍另外一个爬虫框架 Scrapy，我们学习完 Scrapy 之后会更容易理解此部分内容。我们先了解一下 pyspider 与 Scrapy 的区别。</p>
                  <ul>
                    <li>pyspider 提供了 WebUI，爬虫的编写、调试都是在 WebUI 中进行的，而 Scrapy 原生是不具备这个功能的，采用的是代码和命令行操作，但可以通过对接 Portia 实现可视化配置。</li>
                    <li>pyspider 调试非常方便，WebUI 操作便捷直观，在 Scrapy 中则是使用 parse 命令进行调试，论方便程度不及 pyspider。</li>
                    <li>pyspider 支持 PhantomJS 来进行 JavaScript 渲染页面的采集，在 Scrapy 中可以对接 ScrapySplash 组件，需要额外配置。</li>
                    <li>PySpide r 中内置了 PyQuery 作为选择器，在 Scrapy 中对接了 XPath、CSS 选择器和正则匹配。</li>
                    <li>pyspider 的可扩展程度不足，可配制化程度不高，在 Scrapy 中可以通过对接 Middleware、Pipeline、Extension 等组件实现非常强大的功能，模块之间的耦合程度低，可扩展程度极高。</li>
                  </ul>
                  <p>如果要快速实现一个页面的抓取，推荐使用 pyspider，开发更加便捷，如快速抓取某个普通新闻网站的新闻内容。如果要应对反爬程度很强、超大规模的抓取，推荐使用 Scrapy，如抓取封 IP、封账号、高频验证的网站的大规模数据采集。</p>
                  <h3 id="3-pyspider-的架构"><a href="#3-pyspider-的架构" class="headerlink" title="3. pyspider 的架构"></a>3. pyspider 的架构</h3>
                  <p>pyspider 的架构主要分为 Scheduler（调度器）、Fetcher（抓取器）、Processer（处理器）三个部分，整个爬取过程受到 Monitor（监控器）的监控，抓取的结果被 Result Worker（结果处理器）处理，如图 12-1 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-033539.jpg" alt=""> 图 12-1 pyspider 架构图 Scheduler 发起任务调度，Fetcher 负责抓取网页内容，Processer 负责解析网页内容，然后将新生成的 Request 发给 Scheduler 进行调度，将生成的提取结果输出保存。 pyspider 的任务执行流程的逻辑很清晰，具体过程如下所示。</p>
                  <ul>
                    <li>每个 pyspider 的项目对应一个 Python 脚本，该脚本中定义了一个 Handler 类，它有一个 on_start() 方法。爬取首先调用 on_start() 方法生成最初的抓取任务，然后发送给 Scheduler 进行调度。</li>
                    <li>Scheduler 将抓取任务分发给 Fetcher 进行抓取，Fetcher 执行并得到响应，随后将响应发送给 Processer。</li>
                    <li>Processer 处理响应并提取出新的 URL 生成新的抓取任务，然后通过消息队列的方式通知 Schduler 当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待 Result Worker 处理。</li>
                    <li>Scheduler 接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回 Fetcher 进行抓取。</li>
                    <li>不断重复以上工作，直到所有的任务都执行完毕，抓取结束。</li>
                    <li>抓取结束后，程序会回调 on_finished() 方法，这里可以定义后处理过程。</li>
                  </ul>
                  <h3 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h3>
                  <p>本节我们主要了解了 pyspider 的基本功能和架构。接下来我们会用实例来体验一下 pyspider 的抓取操作，然后总结它的各种用法。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-29 09:43:40" itemprop="dateCreated datePublished" datetime="2019-11-29T09:43:40+08:00">2019-11-29</time>
                </span>
                <span id="/8309.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 12.1-pyspider 框架介绍" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8306.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8306.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 11.6-Appium+mitmdump 爬取京东商品</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="11-6-Appium-mitmdump-爬取京东商品"><a href="#11-6-Appium-mitmdump-爬取京东商品" class="headerlink" title="11.6 Appium+mitmdump 爬取京东商品"></a>11.6 Appium+mitmdump 爬取京东商品</h1>
                  <p>在前文中，我们曾经用 Charles 分析过京东商品的评论数据，但是可以发现其参数相当复杂，Form 表单有很多加密参数。如果我们只用 Charles 探测到这个接口链接和参数，还是无法直接构造请求的参数，构造的过程涉及一些加密算法，也就无法直接还原抓取过程。</p>
                  <p>我们了解了 mitmproxy 的用法，利用它的 mitmdump 组件，可以直接对接 Python 脚本对抓取的数据包进行处理，用 Python 脚本对请求和响应直接进行处理。这样我们可以绕过请求的参数构造过程，直接监听响应进行处理即可。但是这个过程并不是自动化的，抓取 App 的时候实际是人工模拟了这个拖动过程。如果这个操作可以用程序来实现就更好了。</p>
                  <p>我们又了解了 Appium 的用法，它可以指定自动化脚本模拟实现 App 的一系列动作，如点击、拖动等，也可以提取 App 中呈现的信息。经过上节爬取微信朋友圈的实例，我们知道解析过程比较烦琐，而且速度要加以限制。如果内容没有显示出来解析就会失败，而且还会导致重复提取的问题。更重要的是，它只可以获取在 App 中看到的信息，无法直接提取接口获取的真实数据，而接口的数据往往是最易提取且信息量最全的。</p>
                  <p>综合以上几点，我们就可以确定出一个解决方案了。如果我们用 mitmdump 去监听接口数据，用 Appium 去模拟 App 的操作，就可以绕过复杂的接口参数又可以实现自动化抓取了！这种方式应是抓取 App 数据的最佳方式。某些特殊情况除外，如微信朋友圈数据又经过了一次加密无法解析，而只能用 Appium 提取。但是对于大多数 App 来说，此种方法是奏效的。本节我们用一个实例感受一下这种抓取方式的便捷之处。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>以抓取京东 App 的商品信息和评论为例，实现 Appium 和 mitmdump 二者结合的抓取。抓取的数据分为两部分：一部分是商品信息，我们需要获取商品的 ID、名称和图片，将它们组成一条商品数据；另一部分是商品的评论信息，我们将评论人的昵称、评论正文、评论日期、发表图片都提取，然后加入商品 ID 字段，将它们组成一条评论数据。最后数据保存到 MongoDB 数据库。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保 PC 已经安装好 Charles、mitmdump、Appium、Android 开发环境，以及 Python 版本的 Appium API。Android 手机安装好京东 App。另外，安装好 MongoDB 并运行其服务，安装 PyMongo 库。具体的配置过程可以参考第 1 章。</p>
                  <h3 id="3-Charles-抓包分析"><a href="#3-Charles-抓包分析" class="headerlink" title="3. Charles 抓包分析"></a>3. Charles 抓包分析</h3>
                  <p>首先，我们将手机代理设置到 Charles 上，用 Charles 抓包分析获取商品详情和商品评论的接口。</p>
                  <p>获取商品详情的接口，这里提取到的接口是来自 cdnware.m.jd.com 的链接，返回结果是一个 JSON 字符串，里面包含了商品的 ID 和商品名称，如图 11-47 和图 11-48 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033508.jpg" alt=""></p>
                  <p>图 11-47 请求概览</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033512.jpg" alt=""></p>
                  <p>图 11-48 响应结果</p>
                  <p>再获取商品评论的接口，这个过程在前文已提到，在此不再赘述。这个接口来自 api.m.jd.com，返回结果也是 JSON 字符串，里面包含了商品的数条评论信息。</p>
                  <p>之后我们可以用 mitmdump 对接一个 Python 脚本来实现数据的抓取。</p>
                  <h3 id="4-mitmdump-抓取"><a href="#4-mitmdump-抓取" class="headerlink" title="4. mitmdump 抓取"></a>4. mitmdump 抓取</h3>
                  <p>新建一个脚本文件，然后实现这个脚本以提取这两个接口的数据。首先提取商品的信息，代码如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def response(flow):</span><br><span class="line">    url = <span class="string">'cdnware.m.jd.com'</span></span><br><span class="line">    <span class="keyword">if</span> url <span class="keyword">in</span> flow.request.url:</span><br><span class="line">        text = flow.response.text</span><br><span class="line">        data = json.loads(text)</span><br><span class="line">        <span class="keyword">if</span> data.<span class="builtin-name">get</span>(<span class="string">'wareInfo'</span>) <span class="keyword">and</span> data.<span class="builtin-name">get</span>(<span class="string">'wareInfo'</span>).<span class="builtin-name">get</span>(<span class="string">'basicInfo'</span>):</span><br><span class="line">            <span class="builtin-name">info</span> = data.<span class="builtin-name">get</span>(<span class="string">'wareInfo'</span>).<span class="builtin-name">get</span>(<span class="string">'basicInfo'</span>)</span><br><span class="line">            id = info.<span class="builtin-name">get</span>(<span class="string">'wareId'</span>)</span><br><span class="line">            name = info.<span class="builtin-name">get</span>(<span class="string">'name'</span>)</span><br><span class="line">            images = info.<span class="builtin-name">get</span>(<span class="string">'wareImage'</span>)</span><br><span class="line">            <span class="builtin-name">print</span>(id, name, images)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里声明了接口的部分链接内容，然后与请求的 URL 作比较。如果该链接出现在当前的 URL 中，那就证明当前的响应就是商品详情的响应，然后提取对应的 JSON 信息即可。在这里我们将商品的 ID、名称和图片提取出来，这就是一条商品数据。</p>
                  <p>再提取评论的数据，代码实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 提取评论数据</span></span><br><span class="line">url = <span class="string">'api.m.jd.com/client.action'</span></span><br><span class="line"><span class="keyword">if</span> url <span class="keyword">in</span> flow.request.url:</span><br><span class="line">    pattern = re.compile(<span class="string">'sku".*?"(d+)"'</span>)</span><br><span class="line">    # Request 请求参数中包含商品 ID</span><br><span class="line">    body = unquote(flow.request.text)</span><br><span class="line">    # 提取商品 ID</span><br><span class="line">    id = re.search(pattern, body).group(1) <span class="keyword">if</span> re.search(pattern, body) <span class="keyword">else</span> None</span><br><span class="line">    # 提取 Response Body</span><br><span class="line">    text = flow.response.text</span><br><span class="line">    data = json.loads(text)</span><br><span class="line">    comments = data.<span class="builtin-name">get</span>(<span class="string">'commentInfoList'</span>) <span class="keyword">or</span> []</span><br><span class="line">    # 提取评论数据</span><br><span class="line">    <span class="keyword">for</span> comment <span class="keyword">in</span> comments:</span><br><span class="line">        <span class="keyword">if</span> comment.<span class="builtin-name">get</span>(<span class="string">'commentInfo'</span>) <span class="keyword">and</span> comment.<span class="builtin-name">get</span>(<span class="string">'commentInfo'</span>).<span class="builtin-name">get</span>(<span class="string">'commentData'</span>):</span><br><span class="line">            <span class="builtin-name">info</span> = comment.<span class="builtin-name">get</span>(<span class="string">'commentInfo'</span>)</span><br><span class="line">            text = info.<span class="builtin-name">get</span>(<span class="string">'commentData'</span>)</span><br><span class="line">            date = info.<span class="builtin-name">get</span>(<span class="string">'commentDate'</span>)</span><br><span class="line">            nickname = info.<span class="builtin-name">get</span>(<span class="string">'userNickName'</span>)</span><br><span class="line">            pictures = info.<span class="builtin-name">get</span>(<span class="string">'pictureInfoList'</span>)</span><br><span class="line">            <span class="builtin-name">print</span>(id, nickname, text, date, pictures)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里指定了接口的部分链接内容，以判断当前请求的 URL 是不是获取评论的 URL。如果满足条件，那么就提取商品的 ID 和评论信息。</p>
                  <p>商品的 ID 实际上隐藏在请求中，我们需要提取请求的表单内容来提取商品的 ID，这里直接用了正则表达式。</p>
                  <p>商品的评论信息在响应中，我们像刚才一样提取了响应的内容，然后对 JSON 进行解析，最后提取出商品评论人的昵称、评论正文、评论日期和图片信息。这些信息和商品的 ID 组合起来，形成一条评论数据。</p>
                  <p>最后用 MongoDB 将两部分数据分开保存到两个 Collection，在此不再赘述。</p>
                  <p>运行此脚本，命令如下所示：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">mitmdump -s <span class="keyword">script</span>.py</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>手机的代理设置到 mitmdump 上。我们在京东 App 中打开某个商品，下拉商品评论部分，即可看到控制台输出两部分的抓取结果，结果成功保存到 MongoDB 数据库，如图 11-49 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033523.jpg" alt=""></p>
                  <p>图 11-49 保存结果</p>
                  <p>如果我们手动操作京东 App 就可以做到京东商品评论的抓取了，下一步要做的就是实现自动滚动刷新。</p>
                  <h3 id="5-Appium-自动化"><a href="#5-Appium-自动化" class="headerlink" title="5. Appium 自动化"></a>5. Appium 自动化</h3>
                  <p>将 Appium 对接到手机上，用 Appium 驱动 App 完成一系列动作。进入 App 后，我们需要做的操作有点击搜索框、输入搜索的商品名称、点击进入商品详情、进入评论页面、自动滚动刷新，基本的操作逻辑和爬取微信朋友圈的相同。</p>
                  <p>京东 App 的 Desired Capabilities 配置如下所示：</p>
                  <figure class="highlight sml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="symbol">'platformName'</span>: <span class="symbol">'Android'</span>,</span><br><span class="line">    <span class="symbol">'deviceName'</span>: <span class="symbol">'MI_NOTE_Pro'</span>,</span><br><span class="line">    <span class="symbol">'appPackage'</span>: <span class="symbol">'com</span>.jingdong.app.mall',</span><br><span class="line">    <span class="symbol">'appActivity'</span>: <span class="symbol">'main</span>.<span class="type">MainActivity'</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先用 Appium 内置的驱动打开京东 App，如图 11-50 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033528.jpg" alt=""></p>
                  <p>图 11-50 调试界面</p>
                  <p>这里进行一系动作操作并录制下来，找到各个页面的组件的 ID 并做好记录，最后再改写成完整的代码。参考代码实现如下所示：</p>
                  <figure class="highlight haskell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="title">from</span> appium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="title">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="title">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="title">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="title">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Action</span>():</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        # 驱动配置</span></span><br><span class="line"><span class="class">        self.desired_caps = &#123;</span></span><br><span class="line"><span class="class">            'platformName': <span class="type">PLATFORM</span>,</span></span><br><span class="line"><span class="class">            'deviceName': <span class="type">DEVICE_NAME</span>,</span></span><br><span class="line"><span class="class">            'appPackage': 'com.jingdong.app.mall',</span></span><br><span class="line"><span class="class">            'appActivity': 'main.<span class="type">MainActivity'</span></span></span><br><span class="line"><span class="class">        &#125;</span></span><br><span class="line"><span class="class">        self.driver = webdriver.<span class="type">Remote</span>(<span class="type">DRIVER_SERVER</span>, <span class="title">self</span>.<span class="title">desired_caps</span>)</span></span><br><span class="line"><span class="class">        self.wait = <span class="type">WebDriverWait</span>(<span class="title">self</span>.<span class="title">driver</span>, <span class="type">TIMEOUT</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def comments(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        # 点击进入搜索页面</span></span><br><span class="line"><span class="class">        search = self.wait.until(<span class="type">EC</span>.<span class="title">presence_of_element_located</span>((<span class="type">By</span>.<span class="type">ID</span>, '<span class="title">com</span>.<span class="title">jingdong</span>.<span class="title">app</span>.<span class="title">mall</span>:<span class="title">id</span>/<span class="title">mp'</span>)))</span></span><br><span class="line"><span class="class">        search.click()</span></span><br><span class="line"><span class="class">        # 输入搜索文本</span></span><br><span class="line"><span class="class">        box = self.wait.until(<span class="type">EC</span>.<span class="title">presence_of_element_located</span>((<span class="type">By</span>.<span class="type">ID</span>, '<span class="title">com</span>.<span class="title">jd</span>.<span class="title">lib</span>.<span class="title">search</span>:<span class="title">id</span>/<span class="title">search_box_layout'</span>)))</span></span><br><span class="line"><span class="class">        box.set_text(<span class="type">KEYWORD</span>)</span></span><br><span class="line"><span class="class">        # 点击搜索按钮</span></span><br><span class="line"><span class="class">        button = self.wait.until(<span class="type">EC</span>.<span class="title">presence_of_element_located</span>((<span class="type">By</span>.<span class="type">ID</span>, '<span class="title">com</span>.<span class="title">jd</span>.<span class="title">lib</span>.<span class="title">search</span>:<span class="title">id</span>/<span class="title">search_btn'</span>)))</span></span><br><span class="line"><span class="class">        button.click()</span></span><br><span class="line"><span class="class">        # 点击进入商品详情</span></span><br><span class="line"><span class="class">        view = self.wait.until(<span class="type">EC</span>.<span class="title">presence_of_element_located</span>((<span class="type">By</span>.<span class="type">ID</span>, '<span class="title">com</span>.<span class="title">jd</span>.<span class="title">lib</span>.<span class="title">search</span>:<span class="title">id</span>/<span class="title">product_list_item'</span>)))</span></span><br><span class="line"><span class="class">        view.click()</span></span><br><span class="line"><span class="class">        # 进入评论详情</span></span><br><span class="line"><span class="class">        tab = self.wait.until(<span class="type">EC</span>.<span class="title">presence_of_element_located</span>((<span class="type">By</span>.<span class="type">ID</span>, '<span class="title">com</span>.<span class="title">jd</span>.<span class="title">lib</span>.<span class="title">productdetail</span>:<span class="title">id</span>/<span class="title">pd_tab3'</span>)))</span></span><br><span class="line"><span class="class">        tab.click()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def scroll(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        while <span class="type">True</span>:</span></span><br><span class="line"><span class="class">            # 模拟拖动</span></span><br><span class="line"><span class="class">            self.driver.swipe(<span class="type">FLICK_START_X</span>, <span class="type">FLICK_START_Y</span> + <span class="type">FLICK_DISTANCE</span>, <span class="type">FLICK_START_X</span>, <span class="type">FLICK_START_Y</span>)</span></span><br><span class="line"><span class="class">            sleep(<span class="type">SCROLL_SLEEP_TIME</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def main(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        self.comments()</span></span><br><span class="line"><span class="class">        self.scroll()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">if __name__ == '__main__':</span></span><br><span class="line"><span class="class">    action = <span class="type">Action</span>()</span></span><br><span class="line"><span class="class">    action.main()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>代码实现比较简单，逻辑与上一节微信朋友圈的抓取类似。注意，由于 App 版本更新的原因，交互流程和元素 ID 可能有更改，这里的代码仅做参考。</p>
                  <p>下拉过程已经省去了用 Appium 提取数据的过程，因为这个过程我们已经用 mitmdump 帮助实现了。</p>
                  <p>代码运行之后便会启动京东 App，进入商品的详情页，然后进入评论页再无限滚动，这样就代替了人工操作。Appium 实现模拟滚动，mitmdump 进行抓取，这样 App 的数据就会保存到数据库中。</p>
                  <h3 id="6-本节代码"><a href="#6-本节代码" class="headerlink" title="6. 本节代码"></a>6. 本节代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/MitmAppiumJD" target="_blank" rel="noopener"></a><a href="https://github.com/Python3WebSpider/MitmAppiumJD" target="_blank" rel="noopener">https://github.com/Python3WebSpider/MitmAppiumJD</a>。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>以上内容便是 Appium 和 mitmdump 抓取京东 App 数据的过程。有了两者的配合，我们既可以做到实时数据处理，又可以实现自动化爬取，这样就可以完成绝大多数 App 的爬取了。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-29 09:41:33" itemprop="dateCreated datePublished" datetime="2019-11-29T09:41:33+08:00">2019-11-29</time>
                </span>
                <span id="/8306.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 11.6-Appium+mitmdump 爬取京东商品" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>5.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>5 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8293.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8293.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 11.5-Appium 爬取微信朋友圈</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="11-5-Appium-爬取微信朋友圈"><a href="#11-5-Appium-爬取微信朋友圈" class="headerlink" title="11.5 Appium 爬取微信朋友圈"></a>11.5 Appium 爬取微信朋友圈</h1>
                  <p>接下来，我们将实现微信朋友圈的爬取。</p>
                  <p>如果直接用 Charles 或 mitmproxy 来监听微信朋友圈的接口数据，这是无法实现爬取的，因为数据都是被加密的。而 Appium 不同，Appium 作为一个自动化测试工具可以直接模拟 App 的操作并可以获取当前所见的内容。所以只要 App 显示了内容，我们就可以用 Appium 抓取下来。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本节我们以 Android 平台为例，实现抓取微信朋友圈的动态信息。动态信息包括好友昵称、正文、发布日期。其中发布日期还需要进行转换，如日期显示为 1 小时前，则时间转换为今天，最后动态信息保存到 MongoDB。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保 PC 已经安装好 Appium、Android 开发环境和 Python 版本的 Appium API。Android 手机安装好微信 App、PyMongo 库，安装 MongoDB 并运行其服务，安装方法可以参考第 1 章。</p>
                  <h3 id="3-初始化"><a href="#3-初始化" class="headerlink" title="3. 初始化"></a>3. 初始化</h3>
                  <p>首先新建一个 Moments 类，进行一些初始化配置，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">PLATFORM = <span class="string">'Android'</span></span><br><span class="line">DEVICE_NAME = <span class="string">'MI_NOTE_Pro'</span></span><br><span class="line">APP_PACKAGE = <span class="string">'com.tencent.mm'</span></span><br><span class="line">APP_ACTIVITY = <span class="string">'.ui.LauncherUI'</span></span><br><span class="line">DRIVER_SERVER = <span class="string">'http://localhost:4723/wd/hub'</span></span><br><span class="line">TIMEOUT = <span class="number">300</span></span><br><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'moments'</span></span><br><span class="line">MONGO_COLLECTION = <span class="string">'moments'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Moments</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="string">""</span><span class="string">"初始化"</span><span class="string">""</span></span><br><span class="line">        <span class="comment"># 驱动配置</span></span><br><span class="line">        <span class="keyword">self</span>.desired_caps = &#123;</span><br><span class="line">            <span class="string">'platformName'</span>: PLATFORM,</span><br><span class="line">            <span class="string">'deviceName'</span>: DEVICE_NAME,</span><br><span class="line">            <span class="string">'appPackage'</span>: APP_PACKAGE,</span><br><span class="line">            <span class="string">'appActivity'</span>: APP_ACTIVITY</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">self</span>.driver = webdriver.Remote(DRIVER_SERVER, <span class="keyword">self</span>.desired_caps)</span><br><span class="line">        <span class="keyword">self</span>.wait = WebDriverWait(<span class="keyword">self</span>.driver, TIMEOUT)</span><br><span class="line">        <span class="keyword">self</span>.client = MongoClient(MONGO_URL)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[MONGO_DB]</span><br><span class="line">        <span class="keyword">self</span>.collection = <span class="keyword">self</span>.db[MONGO_COLLECTION]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里实现了一些初始化配置，如驱动的配置、延时等待配置、MongoDB 连接配置等。</p>
                  <h3 id="4-模拟登录"><a href="#4-模拟登录" class="headerlink" title="4. 模拟登录"></a>4. 模拟登录</h3>
                  <p>接下来要做的就是登录微信。点击登录按钮，输入用户名、密码，提交登录即可。实现样例如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def login(self):</span><br><span class="line">    # 登录按钮</span><br><span class="line">    login = self.wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_element_located((By.ID, '<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">cjk</span>')</span>))</span><br><span class="line">    login.click<span class="literal">()</span></span><br><span class="line">    # 手机输入</span><br><span class="line">    phone = self.wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_element_located((By.ID, '<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">h2</span>')</span>))</span><br><span class="line">    phone.set<span class="constructor">_text(USERNAME)</span></span><br><span class="line">    # 下一步</span><br><span class="line">    next = self.wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>element<span class="constructor">_to_be_clickable((By.ID, '<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">adj</span>')</span>))</span><br><span class="line">    next.click<span class="literal">()</span></span><br><span class="line">    # 密码</span><br><span class="line">    password = self.wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_element_located((By.XPATH, '<span class="operator">/</span><span class="operator">/</span><span class="operator">*</span>[@<span class="params">resource</span>-<span class="params">id</span>=<span class="string">"com.tencent.mm:id/h2"</span>][1]')</span>))</span><br><span class="line">    password.set<span class="constructor">_text(PASSWORD)</span></span><br><span class="line">    # 提交</span><br><span class="line">    submit = self.wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>element<span class="constructor">_to_be_clickable((By.ID, '<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">adj</span>')</span>))</span><br><span class="line">    submit.click<span class="literal">()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里依次实现了一些点击和输入操作，思路比较简单。对于不同的平台和版本来说，流程可能不太一致，这里仅作参考。</p>
                  <p>登录完成之后，进入朋友圈的页面。选中朋友圈所在的选项卡，点击朋友圈按钮，即可进入朋友圈，代码实现如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def enter(self):</span><br><span class="line">    # 选项卡</span><br><span class="line">    tab = self.wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_element_located((By.XPATH, '<span class="operator">/</span><span class="operator">/</span><span class="operator">*</span>[@<span class="params">resource</span>-<span class="params">id</span>=<span class="string">"com.tencent.mm:id/bw3"</span>][3]')</span>))</span><br><span class="line">    tab.click<span class="literal">()</span></span><br><span class="line">    # 朋友圈</span><br><span class="line">    moments = self.wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_element_located((By.ID, '<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">atz</span>')</span>))</span><br><span class="line">    moments.click<span class="literal">()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>抓取工作正式开始。</p>
                  <h3 id="5-抓取动态"><a href="#5-抓取动态" class="headerlink" title="5. 抓取动态"></a>5. 抓取动态</h3>
                  <p>我们知道朋友圈可以一直拖动、不断刷新，所以这里需要模拟一个无限拖动的操作，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 滑动点</span></span><br><span class="line">FLICK_START_X = <span class="number">300</span></span><br><span class="line">FLICK_START_Y = <span class="number">300</span></span><br><span class="line">FLICK_DISTANCE = <span class="number">700</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="symbol">True:</span></span><br><span class="line">        <span class="comment"># 上滑</span></span><br><span class="line">        <span class="keyword">self</span>.driver.swipe(FLICK_START_X, FLICK_START_Y + FLICK_DISTANCE, FLICK_START_X, FLICK_START_Y)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们利用 swipe() 方法，传入起始和终止点实现拖动，加入无限循环实现无限拖动。</p>
                  <p>获取当前显示的朋友圈的每条状态对应的区块元素，遍历每个区块元素，再获取内部显示的用户名、正文和发布时间，代码实现如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"># 当前页面显示的所有状态</span><br><span class="line">items = self.wait.until(</span><br><span class="line">    <span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_all_elements_located((By.XPATH, '<span class="operator">/</span><span class="operator">/</span><span class="operator">*</span>[@<span class="params">resource</span>-<span class="params">id</span>=<span class="string">"com.tencent.mm:id/cve"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">android</span>.<span class="params">widget</span>.FrameLayout')</span>))</span><br><span class="line"># 遍历每条状态</span><br><span class="line">for item <span class="keyword">in</span> items:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        # 昵称</span><br><span class="line">        nickname = item.find<span class="constructor">_element_by_id('<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">aig</span>')</span>.get<span class="constructor">_attribute('<span class="params">text</span>')</span></span><br><span class="line">        # 正文</span><br><span class="line">        content = item.find<span class="constructor">_element_by_id('<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">cwm</span>')</span>.get<span class="constructor">_attribute('<span class="params">text</span>')</span></span><br><span class="line">        # 日期</span><br><span class="line">        date = item.find<span class="constructor">_element_by_id('<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">crh</span>')</span>.get<span class="constructor">_attribute('<span class="params">text</span>')</span></span><br><span class="line">        # 处理日期</span><br><span class="line">        date = self.processor.date(date)</span><br><span class="line">        print(nickname, content, date)</span><br><span class="line">        data = &#123;</span><br><span class="line">            'nickname': nickname,</span><br><span class="line">            'content': content,</span><br><span class="line">            'date': date,</span><br><span class="line">        &#125;</span><br><span class="line">    except NoSuchElementException:</span><br><span class="line">        pass</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里遍历每条状态，再调用 find_element_by_id() 方法获取昵称、正文、发布日期对应的元素，然后通过 get_attribute() 方法获取内容。这样我们就成功获取到朋友圈的每条动态信息。</p>
                  <p>针对日期的处理，我们调用了一个 Processor 类的 date() 处理方法，该方法实现如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def <span class="type">date</span>(self, datetime):</span><br><span class="line">    """</span><br><span class="line">    处理时间</span><br><span class="line">    :param datetime: 原始时间</span><br><span class="line">    :return: 处理后时间</span><br><span class="line">    """</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d + 分钟前 '</span>, datetime):</span><br><span class="line">        minute = re.match(<span class="string">'(d+)'</span>, datetime).<span class="keyword">group</span>(<span class="number">1</span>)</span><br><span class="line">        datetime = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time() - <span class="type">float</span>(minute) * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d + 小时前 '</span>, datetime):</span><br><span class="line">        hour = re.match(<span class="string">'(d+)'</span>, datetime).<span class="keyword">group</span>(<span class="number">1</span>)</span><br><span class="line">        datetime = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time() - <span class="type">float</span>(hour) * <span class="number">60</span> * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">' 昨天 '</span>, datetime):</span><br><span class="line">        datetime = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time() - <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d + 天前 '</span>, datetime):</span><br><span class="line">        day = re.match(<span class="string">'(d+)'</span>, datetime).<span class="keyword">group</span>(<span class="number">1</span>)</span><br><span class="line">        datetime = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time()) - <span class="type">float</span>(day) * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="keyword">return</span> datetime</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个方法使用了正则匹配的方法来提取时间中的具体数值，再利用时间转换函数实现时间的转换。例如时间是 5 分钟前，这个方法先将 5 提取出来，用当前时间戳减去 300 即可得到发布时间的时间戳，然后再转化为标准时间即可。</p>
                  <p>最后调用 MongoDB 的 API 来实现爬取结果的存储。为了去除重复，这里调用了 update() 方法，实现如下所示：</p>
                  <figure class="highlight lasso">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="built_in">self</span>.collection.update(&#123;<span class="string">'nickname'</span>: nickname, <span class="string">'content'</span>: content&#125;, &#123;<span class="string">'$set'</span>: <span class="built_in">data</span>&#125;, <span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先根据昵称和正文来查询信息，如果信息不存在，则插入数据，否则更新数据。这个操作的关键点是第三个参数 True，此参数设置为 True，这可以实现存在即更新、不存在则插入的操作。</p>
                  <p>最后实现一个入口方法调用以上的几个方法。调用此方法即可开始爬取，代码实现如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="comment"># 登录</span></span><br><span class="line">    <span class="keyword">self</span>.login()</span><br><span class="line">    <span class="comment"># 进入朋友圈</span></span><br><span class="line">    <span class="keyword">self</span>.enter()</span><br><span class="line">    <span class="comment"># 爬取</span></span><br><span class="line">    <span class="keyword">self</span>.crawl()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就完成了整个朋友圈的爬虫。代码运行之后，手机微信便会启动，并且可以成功进入到朋友圈然后一直不断执行拖动过程。控制台输出相应的爬取结果，结果被成功保存到 MongoDB 数据库中。</p>
                  <h3 id="6-结果查看"><a href="#6-结果查看" class="headerlink" title="6. 结果查看"></a>6. 结果查看</h3>
                  <p>我们到 MongoDB 中查看爬取结果，如图 11-46 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033459.jpg" alt=""></p>
                  <p>可以看到朋友圈的数据就成功保存到了数据库。</p>
                  <h3 id="7-本节代码"><a href="#7-本节代码" class="headerlink" title="7. 本节代码"></a>7. 本节代码</h3>
                  <p>本节源代码地址为：<a href="https://github.com/Python3WebSpider/Moments" target="_blank" rel="noopener"></a><a href="https://github.com/Python3WebSpider/Moments" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Moments</a>。</p>
                  <h3 id="8-结语"><a href="#8-结语" class="headerlink" title="8. 结语"></a>8. 结语</h3>
                  <p>以上内容是利用 Appium 爬取微信朋友圈的过程。利用 Appium，我们可以做到 App 的可见即可爬，也可以实现自动化驱动和数据爬取。但是实际运行之后，Appium 的解析比较烦琐，而且容易发生重复和中断。如果我们可以用前文所说的 mitmdump 来监听 App 数据实时处理，而 Appium 只负责自动化驱动，它们各负其责，那么整个爬取效率和解析效率就会高很多。所以下一节我们会了解，将 mitmdump 和 Appium 结合起来爬取京东商品的过程。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-28 09:29:16" itemprop="dateCreated datePublished" datetime="2019-11-28T09:29:16+08:00">2019-11-28</time>
                </span>
                <span id="/8293.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 11.5-Appium 爬取微信朋友圈" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.8k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8290.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8290.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 11.4-Appium 的基本使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="11-4-Appium-的基本使用"><a href="#11-4-Appium-的基本使用" class="headerlink" title="11.4 Appium 的基本使用"></a>11.4 Appium 的基本使用</h1>
                  <p>Appium 是一个跨平台移动端自动化测试工具，可以非常便捷地为 iOS 和 Android 平台创建自动化测试用例。它可以模拟 App 内部的各种操作，如点击、滑动、文本输入等，只要我们手工操作的动作 Appium 都可以完成。在前面我们了解过 Selenium，它是一个网页端的自动化测试工具。Appium 实际上继承了 Selenium，Appium 也是利用 WebDriver 来实现 App 的自动化测试。对 iOS 设备来说，Appium 使用 UIAutomation 来实现驱动。对于 Android 来说，它使用 UiAutomator 和 Selendroid 来实现驱动。</p>
                  <p>Appium 相当于一个服务器，我们可以向 Appium 发送一些操作指令，Appium 就会根据不同的指令对移动设备进行驱动，完成不同的动作。</p>
                  <p>对于爬虫来说，我们用 Selenium 来抓取 JavaScript 渲染的页面，可见即可爬。Appium 同样也可以，用 Appium 来做 App 爬虫不失为一个好的选择。</p>
                  <p>下面我们来了解 Appium 的基本使用方法。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们以 Android 平台的微信为例来演示 Appium 启动和操作 App 的方法，主要目的是了解利用 Appium 进行自动化测试的流程以及相关 API 的用法。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保 PC 已经安装好 Appium、Android 开发环境和 Python 版本的 Appium API，安装方法可以参考第 1 章。另外，Android 手机安装好微信 App。</p>
                  <h3 id="3-启动-APP"><a href="#3-启动-APP" class="headerlink" title="3. 启动 APP"></a>3. 启动 APP</h3>
                  <p>Appium 启动 App 的方式有两种：一种是用 Appium 内置的驱动器来打开 App，另一种是利用 Python 程序实现此操作。下面我们分别进行说明。</p>
                  <p>首先打开 Appium，启动界面如图 11-37 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033339.png" alt=""></p>
                  <p>图 11-37 Appium 启动界面</p>
                  <p>直接点击 Start Server 按钮即可启动 Appium 的服务，相当于开启了一个 Appium 服务器。我们可以通过 Appium 内置的驱动或 Python 代码向 Appium 的服务器发送一系列操作指令，Appium 就会根据不同的指令对移动设备进行驱动，完成不同的动作。启动后运行界面如图 11-38 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033346.jpg" alt=""></p>
                  <p>图 11-38 Server 运行界面</p>
                  <p>Appium 运行之后正在监听 4723 端口。我们可以向此端口对应的服务接口发送操作指令，此页面就会显示这个过程的操作日志。</p>
                  <p>将 Android 手机通过数据线和运行 Appium 的 PC 相连，同时打开 USB 调试功能，确保 PC 可以连接到手机。</p>
                  <p>可以输入 adb 命令来测试连接情况，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">adb devices -l</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果出现类似如下结果，这就说明 PC 已经正确连接手机。</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">List</span> <span class="selector-tag">of</span> <span class="selector-tag">devices</span> <span class="selector-tag">attached</span></span><br><span class="line">2<span class="selector-tag">da42ac0</span> <span class="selector-tag">device</span> <span class="selector-tag">usb</span><span class="selector-pseudo">:336592896X</span> <span class="selector-tag">product</span><span class="selector-pseudo">:leo</span> <span class="selector-tag">model</span><span class="selector-pseudo">:MI_NOTE_Pro</span> <span class="selector-tag">device</span><span class="selector-pseudo">:leo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>model 是设备的名称，就是后文需要用到的 deviceName 变量。我使用的是小米 Note 顶配版，所以此处名称为 MI_NOTE_Pro。</p>
                  <p>如果提示找不到 adb 命令，请检查 Android 开发环境和环境变量是否配置成功。如果可以成功调用 adb 命令但不显示设备信息，请检查手机和 PC 的连接情况。</p>
                  <p>接下来用 Appium 内置的驱动器打开 App，点击 Appium 中的 Start New Session 按钮，如图 11-39 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033354.jpg" alt=""></p>
                  <p>图 11-39 操作示例</p>
                  <p>这时会出现一个配置页面，如图 11-40 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033359.jpg" alt=""></p>
                  <p>图 11-40 配置页面</p>
                  <p>需要配置启动 App 时的 Desired Capabilities 参数，它们分别是 platformName、deviceName、appPackage、appActivity。</p>
                  <ul>
                    <li>platformName，平台名称，需要区分是 Android 还是 iOS，此处填写 Android。</li>
                    <li>deviceName，设备名称，是手机的具体类型。</li>
                    <li>appPackage，APP 程序包名。</li>
                    <li>appActivity，入口 Activity 名，这里通常需要以。开头。</li>
                  </ul>
                  <p>在当前配置页面的左下角也有配置参数的相关说明，链接是 <a href="https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md" target="_blank" rel="noopener"></a><a href="https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md" target="_blank" rel="noopener">https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md</a>。</p>
                  <p>我们在 Appium 中加入上面 4 个配置，如图 11-41 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033406.jpg" alt=""></p>
                  <p>图 11-41 配置信息</p>
                  <p>点击保存按钮，保存下来，我们以后可以继续使用这个配置。</p>
                  <p>点击右下角的 Start Session 按钮，即可启动 Android 手机上的微信 App 并进入到启动页面。同时 PC 上会弹出一个调试窗口，从这个窗口我们可以预览当前手机页面，并可以查看页面的源码，如图 11-42 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033409.jpg" alt=""></p>
                  <p>图 11-42 调试窗口</p>
                  <p>点击左栏中屏幕的某个元素，如选中登录按钮，它就会高亮显示。这时中间栏就显示了当前选中的按钮对应的源代码，右栏则显示了该元素的基本信息，如元素的 id、class、text 等，以及可以执行的操作，如 Tap、Send Keys、Clear，如图 11-43 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033414.jpg" alt=""></p>
                  <p>图 11-43 操作选项</p>
                  <p>点击中间栏最上方的第三个录制按钮，Appium 会开始录制操作动作，这时我们在窗口中操作 App 的行为都会被记录下来，Recorder 处可以自动生成对应语言的代码。例如，我们点击录制按钮，然后选中 App 中的登录按钮，点击 Tap 操作，即模拟了按钮点击功能，这时手机和窗口的 App 都会跳转到登录页面，同时中间栏会显示此动作对应的代码，如图 11-44 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033421.jpg" alt=""></p>
                  <p>图 11-44 录制动作</p>
                  <p>接下来选中左侧的手机号文本框，点击 Send Keys，对话框就会弹出。输入手机号，点击 Send Keys，即可完成文本的输入，如图 11-45 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-033426.jpg" alt=""></p>
                  <p>图 11-45 文本输入</p>
                  <p>我们可以在此页面点击不同的动作按钮，即可实现对 App 的控制，同时 Recorder 部分也可以生成对应的 Python 代码。</p>
                  <p>下面我们看看使用 Python 代码驱动 App 的方法。首先需要在代码中指定一个 Appium Server，而这个 Server 在刚才打开 Appium 的时候就已经开启了，是在 4723 端口上运行的，配置如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">server</span> = <span class="string">'http://localhost:4723/wd/hub'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>用字典来配置 Desired Capabilities 参数，代码如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">desired_caps</span> = &#123;</span><br><span class="line">    <span class="string">'platformName'</span>: <span class="string">'Android'</span>,</span><br><span class="line">    <span class="string">'deviceName'</span>: <span class="string">'MI_NOTE_Pro'</span>,</span><br><span class="line">    <span class="string">'appPackage'</span>: <span class="string">'com.tencent.mm'</span>,</span><br><span class="line">    <span class="string">'appActivity'</span>: <span class="string">'.ui.LauncherUI'</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建一个 Session，这类似点击 Appium 内置驱动的 Start Session 按钮相同的功能，代码实现如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> appium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"></span><br><span class="line">driver = webdriver.Remote(<span class="keyword">server</span>, desired_caps)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>配置完成后运行，就可以启动微信 App 了。但是现在仅仅是可以启动 App，还没有做任何动作。</p>
                  <p>再用代码来模拟刚才演示的两个动作：一个是点击 “登录” 按钮，一个是输入手机号。</p>
                  <p>看看刚才 Appium 内置驱动器内的 Recorder 录制生成的 Python 代码，自动生成的代码非常累赘，例如点击 “登录” 按钮的代码如下所示：</p>
                  <figure class="highlight abnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">el1</span> = driver.find_element_by_xpath(<span class="string">"/hierarchy/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.view.View/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.RelativeLayout/android.widget.RelativeLayout/android.widget.Button[1]"</span>)</span><br><span class="line">el1.click()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这段代码的 XPath 选择器路径太长，选择方式没有那么科学，获取元素时也没有设置等待，很可能会有超时异常。所以我们修改一下，将其修改为通过 ID 查找元素，设置延时等待，两次操作的代码改写如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">wait = <span class="constructor">WebDriverWait(<span class="params">driver</span>, 30)</span></span><br><span class="line">login = wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_element_located((By.ID, '<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">cjk</span>')</span>))</span><br><span class="line">login.click<span class="literal">()</span></span><br><span class="line">phone = wait.until(<span class="module-access"><span class="module"><span class="identifier">EC</span>.</span></span>presence<span class="constructor">_of_element_located((By.ID, '<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">h2</span>')</span>))</span><br><span class="line">phone.set<span class="constructor">_text('18888888888')</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>综上所述，完整的代码如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> appium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.<span class="keyword">by</span> <span class="keyword">import</span> <span class="keyword">By</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"></span><br><span class="line"><span class="keyword">server</span> = <span class="string">'http://localhost:4723/wd/hub'</span></span><br><span class="line">desired_caps = &#123;</span><br><span class="line">    <span class="string">'platformName'</span>: <span class="string">'Android'</span>,</span><br><span class="line">    <span class="string">'deviceName'</span>: <span class="string">'MI_NOTE_Pro'</span>,</span><br><span class="line">    <span class="string">'appPackage'</span>: <span class="string">'com.tencent.mm'</span>,</span><br><span class="line">    <span class="string">'appActivity'</span>: <span class="string">'.ui.LauncherUI'</span></span><br><span class="line">&#125;</span><br><span class="line">driver = webdriver.Remote(<span class="keyword">server</span>, desired_caps)</span><br><span class="line">wait = WebDriverWait(driver, <span class="number">30</span>)</span><br><span class="line"><span class="keyword">login</span> = wait.<span class="keyword">until</span>(EC.presence_of_element_located((<span class="keyword">By</span>.ID, <span class="string">'com.tencent.mm:id/cjk'</span>)))</span><br><span class="line"><span class="keyword">login</span>.click()</span><br><span class="line">phone = wait.<span class="keyword">until</span>(EC.presence_of_element_located((<span class="keyword">By</span>.ID, <span class="string">'com.tencent.mm:id/h2'</span>)))</span><br><span class="line">phone.set_text(<span class="string">'18888888888'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>一定要重新连接手机，再运行此代码，这时即可观察到手机上首先弹出了微信欢迎页面，然后模拟点击登录按钮、输入手机号，操作完成。这样我们就成功使用 Python 代码实现了 App 的操作。</p>
                  <h3 id="4-API"><a href="#4-API" class="headerlink" title="4. API"></a>4. API</h3>
                  <p>接下来看看使用代码如何操作 App、总结相关 API 的用法。这里使用的 Python 库为 AppiumPythonClient，其 GitHub 地址为 <a href="https://github.com/appium/python-client，此库继承自" target="_blank" rel="noopener">https://github.com/appium/python-client，此库继承自</a> Selenium，使用方法与 Selenium 有很多共同之处。</p>
                  <h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4>
                  <p>需要配置 Desired Capabilities 参数，完整的配置说明可以参考 <a href="https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md" target="_blank" rel="noopener"></a><a href="https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md" target="_blank" rel="noopener">https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md</a>，一般来说我们我们配置几个基本参数即可：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> appium import webdriver</span><br><span class="line"></span><br><span class="line">server = <span class="string">'http://localhost:4723/wd/hub'</span></span><br><span class="line">desired_caps = &#123;</span><br><span class="line">    <span class="string">'platformName'</span>: <span class="string">'Android'</span>,</span><br><span class="line">    <span class="string">'deviceName'</span>: <span class="string">'MI_NOTE_Pro'</span>,</span><br><span class="line">    <span class="string">'appPackage'</span>: <span class="string">'com.tencent.mm'</span>,</span><br><span class="line">    <span class="string">'appActivity'</span>: <span class="string">'.ui.LauncherUI'</span></span><br><span class="line">&#125;</span><br><span class="line">driver = webdriver.Remote(server, desired_caps)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里配置了启动微信 App 的 Desired Capabilities，这样 Appnium 就会自动查找手机上的包名和入口类，然后将其启动。包名和入口类的名称可以在安装包中的 AndroidManifest.xml 文件获取。</p>
                  <p>如果要打开的 App 没有事先在手机上安装，我们可以直接指定 App 参数为安装包所在路径，这样程序启动时就会自动向手机安装并启动 App，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> appium import webdriver</span><br><span class="line"></span><br><span class="line">server = <span class="string">'http://localhost:4723/wd/hub'</span></span><br><span class="line">desired_caps = &#123;</span><br><span class="line">    <span class="string">'platformName'</span>: <span class="string">'Android'</span>,</span><br><span class="line">    <span class="string">'deviceName'</span>: <span class="string">'MI_NOTE_Pro'</span>,</span><br><span class="line">    <span class="string">'app'</span>: <span class="string">'./weixin.apk'</span></span><br><span class="line">&#125;</span><br><span class="line">driver = webdriver.Remote(server, desired_caps)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>程序启动的时候就会寻找 PC 当前路径下的 APK 安装包，然后将其安装到手机中并启动。</p>
                  <h4 id="查找元素"><a href="#查找元素" class="headerlink" title="查找元素"></a>查找元素</h4>
                  <p>我们可以使用 Selenium 中通用的查找方法来实现元素的查找，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">el = driver.find<span class="constructor">_element_by_id('<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">cjk</span>')</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 Selenium 中，其他查找元素的方法同样适用，在此不再赘述。</p>
                  <p>在 Android 平台上，我们还可以使用 UIAutomator 来进行元素选择，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">el</span> = self.driver.find_element_by_android_uiautomator(<span class="string">'new UiSelector().description("Animation")'</span>)</span><br><span class="line"><span class="attr">els</span> = self.driver.find_elements_by_android_uiautomator(<span class="string">'new UiSelector().clickable(true)'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 iOS 平台上，我们可以使用 UIAutomation 来进行元素选择，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">el</span> = self.driver.find_element_by_ios_uiautomation(<span class="string">'.elements()[0]'</span>)</span><br><span class="line"><span class="attr">els</span> = self.driver.find_elements_by_ios_uiautomation(<span class="string">'.elements()'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>还可以使用 iOS Predicates 来进行元素选择，如下所示：</p>
                  <figure class="highlight nix">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">el</span> = self.driver.find_element_by_ios_predicate('<span class="attr">wdName</span> == <span class="string">"Buttons"</span>')</span><br><span class="line"><span class="attr">els</span> = self.driver.find_elements_by_ios_predicate('<span class="attr">wdValue</span> == <span class="string">"SearchBar"</span> AND <span class="attr">isWDDivisible</span> == <span class="number">1</span>')</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>也可以使用 iOS Class Chain 来进行选择，如下所示：</p>
                  <figure class="highlight ceylon">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">el = self.driver.find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_</span>ios<span class="number">_</span><span class="keyword">class</span><span class="number">_</span>chain(<span class="string">'XCUIElementTypeWindow/XCUIElementTypeButton[3]'</span>)</span><br><span class="line">els = self.driver.find<span class="number">_</span>elements<span class="number">_</span><span class="meta">by</span><span class="number">_</span>ios<span class="number">_</span><span class="keyword">class</span><span class="number">_</span>chain(<span class="string">'XCUIElementTypeWindow/XCUIElementTypeButton'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>但是此种方法只适用于 XCUITest 驱动，具体可以参考：<a href="https://github.com/appium/appium-xcuitest" target="_blank" rel="noopener">https://github.com/appium/appium-xcuitest</a>-<br>driver。</p>
                  <h4 id="点击"><a href="#点击" class="headerlink" title="点击"></a>点击</h4>
                  <p>点击可以使用 tap() 方法，该方法可以模拟手指点击（最多五个手指），可设置按时长短（毫秒），代码如下所示：</p>
                  <figure class="highlight lasso">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">tap(<span class="built_in">self</span>, positions, <span class="built_in">duration</span>=<span class="literal">None</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数：</p>
                  <ul>
                    <li>positions，点击的位置组成的列表。</li>
                    <li>duration，点击持续时间。</li>
                  </ul>
                  <p>实例如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">driver.tap([(<span class="number">100</span>, <span class="number">20</span>), (<span class="number">100</span>, <span class="number">60</span>), (<span class="number">100</span>, <span class="number">100</span>)], <span class="number">500</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就可以模拟点击屏幕的某几个点。</p>
                  <p>另外对于某个元素如按钮来说，我们可以直接调用 cilck() 方法实现模拟点击，实例如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">button = find<span class="constructor">_element_by_id('<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">btn</span>')</span></span><br><span class="line">button.click<span class="literal">()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样获取元素之后，然后调用 click() 方法即可实现该元素的模拟点击。</p>
                  <h4 id="屏幕拖动"><a href="#屏幕拖动" class="headerlink" title="屏幕拖动"></a>屏幕拖动</h4>
                  <p>可以使用 scroll() 方法模拟屏幕滚动，用法如下所示：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="title">scroll</span><span class="params">(self, origin_el, destination_el)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以实现从元素 origin_el 滚动至元素 destination_el。</p>
                  <p>参数：</p>
                  <ul>
                    <li>original_el，被操作的元素</li>
                    <li>destination_el，目标元素</li>
                  </ul>
                  <p>实例如下：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">driver</span><span class="selector-class">.scroll</span>(<span class="selector-tag">el1</span>,<span class="selector-tag">el2</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们还可以使用 swipe() 模拟从 A 点滑动到 B 点，用法如下：</p>
                  <figure class="highlight lasso">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">swipe(<span class="built_in">self</span>, start_x, start_y, end_x, end_y, <span class="built_in">duration</span>=<span class="literal">None</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数：</p>
                  <ul>
                    <li>start_x，开始位置的横坐标</li>
                    <li>start_y，开始位置的纵坐标</li>
                    <li>end_x，终止位置的横坐标</li>
                    <li>end_y，终止位置的纵坐标</li>
                    <li>duration，持续时间，毫秒</li>
                  </ul>
                  <p>实例如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">driver.swipe(<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">400</span>, <span class="number">5000</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样可以实现在 5s 由 (100, 100) 滑动到 (100, 400)。</p>
                  <p>另外可以使用 flick() 方法模拟从 A 点快速滑动到 B 点，用法如下：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="title">flick</span><span class="params">(self, start_x, start_y, end_x, end_y)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数：</p>
                  <ul>
                    <li>start_x，开始位置的横坐标</li>
                    <li>start_y，开始位置的纵坐标</li>
                    <li>end_x，终止位置的横坐标</li>
                    <li>end_y，终止位置的纵坐标</li>
                  </ul>
                  <p>实例如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">driver.flick(<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">400</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="拖拽"><a href="#拖拽" class="headerlink" title="拖拽"></a>拖拽</h4>
                  <p>可以使用 drag_and_drop() 实现某个元素拖动到另一个目标元素上。</p>
                  <p>用法如下：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">drag<span class="constructor">_and_drop(<span class="params">self</span>, <span class="params">origin_el</span>, <span class="params">destination_el</span>)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以实现元素 origin_el 拖拽至元素 destination_el。</p>
                  <p>参数：</p>
                  <ul>
                    <li>original_el，被拖拽的元素</li>
                    <li>destination_el，目标元素</li>
                  </ul>
                  <p>实例如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">driver.drag<span class="constructor">_and_drop(<span class="params">el1</span>, <span class="params">el2</span>)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="文本输入"><a href="#文本输入" class="headerlink" title="文本输入"></a>文本输入</h4>
                  <p>可以使用 set_text() 方法实现文本输入，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">el = find<span class="constructor">_element_by_id('<span class="params">com</span>.<span class="params">tencent</span>.<span class="params">mm</span>:<span class="params">id</span><span class="operator">/</span><span class="params">cjk</span>')</span></span><br><span class="line">el.set<span class="constructor">_text('Hello')</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们选中一个文本框元素之后，然后调用 set_text() 方法即可实现文本输入。</p>
                  <h4 id="动作链"><a href="#动作链" class="headerlink" title="动作链"></a>动作链</h4>
                  <p>与 Selenium 中的 ActionChains 类似，Appium 中的 TouchAction 可支持的方法有 tap()、press()、long_press()、release()、move_to()、wait()、cancel() 等，实例如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">el = self.driver.find_element_by_accessibility_id(<span class="string">'Animation'</span>)</span><br><span class="line">action = TouchAction(self.driver)</span><br><span class="line">action.tap(el).<span class="keyword">perform</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先选中一个元素，然后利用 TouchAction 实现点击操作。</p>
                  <p>如果想要实现拖动操作，可以用如下方式：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">els = self.driver.find<span class="constructor">_elements_by_class_name('<span class="params">listView</span>')</span></span><br><span class="line">a1 = <span class="constructor">TouchAction()</span></span><br><span class="line">a1.press(els<span class="literal">[<span class="number">0</span>]</span>).move<span class="constructor">_to(<span class="params">x</span>=10, <span class="params">y</span>=0)</span>.move<span class="constructor">_to(<span class="params">x</span>=10, <span class="params">y</span>=-75)</span>.move<span class="constructor">_to(<span class="params">x</span>=10, <span class="params">y</span>=-600)</span>.release<span class="literal">()</span></span><br><span class="line">a2 = <span class="constructor">TouchAction()</span></span><br><span class="line">a2.press(els<span class="literal">[<span class="number">1</span>]</span>).move<span class="constructor">_to(<span class="params">x</span>=10, <span class="params">y</span>=10)</span>.move<span class="constructor">_to(<span class="params">x</span>=10, <span class="params">y</span>=-300)</span>.move<span class="constructor">_to(<span class="params">x</span>=10, <span class="params">y</span>=-600)</span>.release<span class="literal">()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>利用以上 API，我们就可以完成绝大部分操作。更多的 API 操作可以参考 <a href="https://testerhome.com/topics/3711" target="_blank" rel="noopener"></a><a href="https://testerhome.com/topics/3711" target="_blank" rel="noopener">https://testerhome.com/topics/3711</a>。</p>
                  <h3 id="5-结语"><a href="#5-结语" class="headerlink" title="5. 结语"></a>5. 结语</h3>
                  <p>本节中，我们主要了解了 Appium 的操作 App 的基本用法，以及常用 API 的用法。在下一节我们会用一个实例来演示 Appium 的使用方法。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-28 09:24:05" itemprop="dateCreated datePublished" datetime="2019-11-28T09:24:05+08:00">2019-11-28</time>
                </span>
                <span id="/8290.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 11.4-Appium 的基本使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>8.4k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>8 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8272.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8272.html" class="post-title-link" itemprop="url">X-Forward-For 看破红尘，代理 IP 无所遁形！</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p><img src="http://q0revehsm.bkt.clouddn.com/sfhfpc/20191124150957.png" alt=""> 在开始了解 X-Forward-For 之前，我们先来假设一个场景。你是一名爬虫工程师，现在要爬取目标网站 xxx.com 上面的内容。在编码的时候，你发现单位时间内请求频率过高时会被限制，猜测应该是<strong>目标网站针对 IP 地址做了限制</strong>。现在你有两种选择：</p>
                  <ul>
                    <li><strong>单机，用 IP 代理解决频率高被限制的问题。</strong></li>
                    <li><strong>多机，用分布式爬虫解决单机 IP 被限制的问题。</strong></li>
                  </ul>
                  <p>由于目标网站只需要爬取一次，<strong>单机+IP 代理</strong>这种组合的成本更低，所以你选择了它。从 IP 代理服务商 xx 处购买了代理服务后，你进行了新一轮的测试，代码片段 Forwarded-Test 为测试代码。</p>
                  <figure class="highlight nix">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="built_in">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求地址</span></span><br><span class="line"><span class="attr">targetUrl</span> = <span class="string">"http://111.231.93.117/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 代理服务器</span></span><br><span class="line"><span class="attr">proxyHost</span> = <span class="string">"220.185.128.170"</span></span><br><span class="line"><span class="attr">proxyPort</span> = <span class="string">"9999"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">proxyMeta</span> = <span class="string">"http://%(host)s:%(port)s"</span> % &#123;</span><br><span class="line"></span><br><span class="line">    <span class="string">"host"</span>: proxyHost,</span><br><span class="line">    <span class="string">"port"</span>: proxyPort,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">proxies</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="string">"http"</span>: proxyMeta,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 设定一个 Referer</span></span><br><span class="line"><span class="attr">header</span> = &#123;</span><br><span class="line">    <span class="string">"Referer"</span>: <span class="string">"http://www.sfhfpc.com"</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="attr">resp</span> = requests.get(targetUrl, <span class="attr">proxies=proxies,</span> <span class="attr">headers=header)</span></span><br><span class="line">print(resp.status_code)</span><br><span class="line">print(resp.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>代码片段 Forwarded-Test 代码运行后，你发现你仍然被限制！ 顿时感到头大，于是在各大搜索引擎寻找相关资料，例如：</p>
                  <blockquote>
                    <p>ip 代理无效 识别 ip 代理 ip 代理被发现</p>
                  </blockquote>
                  <p>你发现很多文章中都提到一个东西 X-Forward-For，大家都说它能够<strong>看破</strong> IP 代理。 <img src="http://q0revehsm.bkt.clouddn.com/sfhfpc/20191127160630.jpg" alt=""> 那么问题来了：</p>
                  <ul>
                    <li>X-Forward-For 到底是什么呢？</li>
                    <li>为什么 X-Forward-For 能够发现我们使用了 <strong>IP 代理</strong></li>
                    <li>它怎么能找到<strong>原始 IP</strong> 呢？</li>
                    <li>有什么方法可以骗过 X-Forward-For 呢？</li>
                  </ul>
                  <p>带着这些问题，我们就来研究一下 X-Forward-For。</p>
                  <h2 id="X-Forward-For-是什么"><a href="#X-Forward-For-是什么" class="headerlink" title="X-Forward-For 是什么"></a>X-Forward-For 是什么</h2>
                  <p>X-Forward-For 跟 Referer 和 User-Agent 一样，都是 HTTP 中的头域。HTTP/1.1 的 <strong>RFC</strong> 文档编号为 2616，在 2616 中并未提及 X-Forward-For，也就是说 HTTP/1.1 出现的时候 X-Forward-For 还没出生。真正提出 X-Forward-For 的是2014 年的 RFC7239（详见 <a href="https://www.rfc-editor.org/rfc/rfc7239.txt），这时候" target="_blank" rel="noopener">https://www.rfc-editor.org/rfc/rfc7239.txt），这时候</a> X-Forward-For 作为<strong>HTTP 扩展</strong>出现。 <sup><a href="#fn_RFC" id="reffn_RFC">RFC</a></sup>: 全称 Request For Comments，是一系列以编号排定的文件。它收集了互联网相关的协议信息，你可以抽象地将 RFC2616 理解为 HTTP/1.1 的协议规范。Websocket 协议规范的详细解读可参考《Python3 反爬虫原理与绕过实战》一书。 关于 X-Forward-For 的所有正确描述都写在了 RFC7239 中，所有符合规范的 HTTP 也会遵守 RFC7239。当然，你也可以选择<strong>不遵守</strong>。 <sup><a href="#fn_不遵守" id="reffn_不遵守">不遵守</a></sup>: 实际上，RFC 只是一种规范、约定，作为大家统一行径的参考，并未强制实现。很多反爬虫手段就是另辟蹊径，采用了与 RFC 约定不同的策略，具体反爬虫思路和案例可参考《Python3 反爬虫原理与绕过实战》一书。 RFC7239 很长，我们不必逐一阅读。实际上跟我们相关的只有几个部分，例如：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">1.</span>Abstract</span><br><span class="line"><span class="number">7.5</span>. Example Usage</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Abstract 是本文章的摘要，它描述了 RFC7239 的作用：</p>
                  <blockquote>
                    <p>This document defines an HTTP extension header field that allows proxy components to disclose information lost in the proxying process, for example, the originating IP address of a request or IP address of the proxy on the user-agent-facing interface. In a path of proxying components, this makes it possible to arrange it so that each subsequent component will have access to, for example, all IP addresses used in the chain of proxied HTTP requests. This document also specifies guidelines for a proxy administrator to anonymize the origin of a request.</p>
                  </blockquote>
                  <p>大体意思为本文的定义（扩展）了一个 HTTP 头域，这个字段允许代理组件披露原始 IP 地址。 从这里我们了解到 X-Forward-For 的正向用途是便于服务端识别原始 IP，并根据原始 IP 作出动态处理。例如服务端按照 IP 地址进行负载均衡时，如果能够<strong>看破</strong> IP 代理，取得原始 IP 地址，那么就能够作出<strong>有效</strong>的负载。否则有可能造成资源分配不均，导致<strong>假负载均衡</strong>的情况出现。 Example Usage 给出了 X-Forward-For 的使用示例：</p>
                  <blockquote>
                    <p>A request from a client with IP address 192.0.2.43 passes through a proxy with IP address 198.51.100.17, then through another proxy with IP address 203.0.113.60 before reaching an origin server. This could, for example, be an office client behind a corporate malware filter talking to a origin server through a reverse proxy. o The HTTP request between the client and the first proxy has no “Forwarded” header field. o The HTTP request between the first and second proxy has a “Forwarded: for=192.0.2.43” header field. o The HTTP request between the second proxy and the origin server has a “Forwarded: for=192.0.2.43, for=198.51.100.17;by=203.0.113.60;proto=http;host=example.com” header field.</p>
                  </blockquote>
                  <p>假设原始 IP 为192.0.2.43，它的请求使用了地址为 198.51.100.17 的代理，在到达目标服务器 203.0.113.60 之前还使用了另外一个代理（文章假设另外一个代理为 222.111.222.111）。 这种情况下</p>
                  <ul>
                    <li>客户端和第一个代理之间的 HTTP 请求中没有 Forwarded 头域。</li>
                    <li>第一个代理和第二个代理之间的 HTTP 请求中有 Forwarded 头域，头域及值为 Forwarded: for=192.0.2.43 。</li>
                    <li>第二个代理和服务器之间的 HTTP 请求中有 Forwarded 头域，头域及值为 Forwarded: for=192.0.2.43, for=198.51.100.17;by=203.0.113.60;proto=http;host=example.com”</li>
                  </ul>
                  <p>图 forwarded-client-server 描述了上述情景。 <img src="http://q0revehsm.bkt.clouddn.com/sfhfpc/20191124123035.jpg" alt=""> 图 forwarded-client-server 由于客户端到代理 1 的请求没有使用代理，所以值为空或短横线。到代理 2 时，中间经过了代理 1，所以值为原始 IP。到服务端时，中间经过了代理 1 和代理2 ，所以值为原始 IP 和代理 1 IP。 上面就是关于 RFC7239 中部分内容的解读。看到这里，想必你已有丝丝头绪，接下来我们再捋一捋。</p>
                  <h2 id="IP-代理实验"><a href="#IP-代理实验" class="headerlink" title="IP 代理实验"></a>IP 代理实验</h2>
                  <p>首先我在自己的测试服务器上安装并启动了 Nginx，它的默认日志格式如下：</p>
                  <figure class="highlight nginx">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">log_format</span>  main  </span><br><span class="line"><span class="string">'<span class="variable">$remote_addr</span> - <span class="variable">$remote_user</span> [<span class="variable">$time_local</span>] "<span class="variable">$request</span>" '</span></span><br><span class="line"><span class="string">'<span class="variable">$status</span> <span class="variable">$body_bytes_sent</span> "<span class="variable">$http_referer</span>" '</span></span><br><span class="line"><span class="string">'"<span class="variable">$http_user_agent</span>" "<span class="variable">$http_x_forwarded_for</span>"'</span>;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>即 access.log 文件中会记录客户端 IP 地址、客户端时间、请求方式、响应状态码、响应正文大小、Referer、User-Agent 和代理清单。</p>
                  <blockquote>
                    <p>提示：Nginx 中 $http_x_forwarded_for 对应的值这里称为代理清单，它与 RFC7239 中的 Forwarded 含义相同。</p>
                  </blockquote>
                  <p>当我使用计算机终端浏览器访问测试服务器地址时，对应的日志记录如下：</p>
                  <figure class="highlight accesslog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">180.137.156.168</span> - - <span class="string">[24/Nov/2019:12:41:19 +0800]</span> <span class="string">"<span class="keyword">GET</span> / HTTP/1.1"</span> <span class="number">200</span> <span class="number">612</span> <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Safari/605.1.15"</span> <span class="string">"-"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>服务器记录到的信息含义如下：</p>
                  <ul>
                    <li>客户端 IP 为 180.137.156.168</li>
                    <li>客户端时间为 [24/Nov/2019:12:41:19 +0800]</li>
                    <li>请求方式为 GET / HTTP/1.1</li>
                    <li>响应状态码为 200</li>
                    <li>响应正文大小为 612</li>
                    <li>Referer 为短横线，即为空</li>
                    <li>User-Agent 显示浏览器品牌为 Safari</li>
                    <li>代理清单为短横线，即为空。</li>
                  </ul>
                  <p>由于本次并未使用 IP 代理，那么代理清单自然就是短横线。接着我们用 Python 代码测试一下，代码片段 Python-Request 为测试代码。</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import requests</span><br><span class="line">resp = requests.<span class="builtin-name">get</span>(<span class="string">"http://111.231.93.117/"</span>)</span><br><span class="line"><span class="builtin-name">print</span>(resp.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>代码片段 Python-Request 代码运行结果为 200，即目标服务器正确响应了本次请求。对应的日志记录如下：</p>
                  <figure class="highlight accesslog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">180.137.156.168</span> - - <span class="string">[24/Nov/2019:12:49:41 +0800]</span> <span class="string">"<span class="keyword">GET</span> / HTTP/1.1"</span> <span class="number">200</span> <span class="number">612</span> <span class="string">"-"</span> <span class="string">"python-requests/2.21.0"</span> <span class="string">"-"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次也没有使用 IP 代理，所以代理清单依旧是短横线。现在用代理 IP 测试一下，代码片段 Forwarded-Test 中使用了 IP 代理，我们就用它进行测试即可。这里的代理服务器 IP 地址为 220.185.128.170，根据之前对 RFC7239 的了解，猜测本次请求对应的 Forwarded 记录的会是原始 IP，而客户端 IP 则是代理服务器的 IP。 代码运行后，服务器记录到对应的日志信息如下：</p>
                  <figure class="highlight accesslog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">220.185.128.170</span> - - <span class="string">[24/Nov/2019:12:52:58 +0800]</span> <span class="string">"<span class="keyword">GET</span> / HTTP/1.1"</span> <span class="number">200</span> <span class="number">612</span> <span class="string">"http://www.sfhfpc.com"</span> <span class="string">"python-requests/2.21.0"</span> <span class="string">"180.137.156.168"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>果然，记录中客户端 IP 对应的是 220.185.128.170，即代理服务器的 IP。Forwarded 中记录的 180.137.156.168 是 Python 程序所在的计算机 IP 地址，即原始 IP。 这与 RFC7239 的描述完全相符，服务端可以通过 Forwarded 找到原始 IP，甚至是使用过的代理服务器 IP。</p>
                  <h2 id="调皮的-IP-代理商"><a href="#调皮的-IP-代理商" class="headerlink" title="调皮的 IP 代理商"></a>调皮的 IP 代理商</h2>
                  <p>刚才我们用的是普通 IP 代理，由于它很容易被识别，达不到<strong>隐匿</strong>的目的，所以 IP 代理商又推出了<strong>高匿代理</strong>。 <sup><a href="#fn_高匿代理" id="reffn_高匿代理">高匿代理</a></sup>: 相对于普通 IP 代理而言，使用高匿代理后，原始 IP 会被隐藏得更好，服务端更难发现。 这里我使用了 <a href="http://h.zhimaruanjian.com/?utm-source=ggzz&amp;utm-keyword=?01" target="_blank" rel="noopener">芝麻代理</a> 服务商提供的免费高匿 IP，注册后就可以领取免费 IP，简直就是开箱即用。 <img src="http://q0revehsm.bkt.clouddn.com/sfhfpc/20191127155616.png" alt=""> 将代码片段 Forwarded-Test 中用于设置代理服务器 IP 和端口号的字段值改为高匿 IP 及对应的端口号即可，例如：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 代理服务器</span></span><br><span class="line"><span class="attr">proxyHost</span> = <span class="string">"58.218.92.132"</span>  <span class="comment"># "220.185.128.170"</span></span><br><span class="line"><span class="attr">proxyPort</span> = <span class="string">"2390"</span>  <span class="comment"># "9999"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>保存更改后运行代码，对应的日志记录如下：</p>
                  <figure class="highlight accesslog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">125.82.188.4</span> - - <span class="string">[24/Nov/2019:13:05:07 +0800]</span> <span class="string">"<span class="keyword">GET</span> / HTTP/1.1"</span> <span class="number">200</span> <span class="number">612</span> <span class="string">"http://www.sfhfpc.com"</span> <span class="string">"python-requests/2.21.0"</span> <span class="string">"-"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>原始 IP 为 125.82.188.4，代理清单为短横线。细心的你可能会有疑问，为什么填写的代理 IP 是 58.218.92.132，而日志中的却不是呢？ 这是代理服务商做了多一层的转移，58.218.92.132 是给用户的入口，代理商的服务端会将入口为 58.218.92.132 的请求转给地址为 125.82.188.4。其中过程我们不用深究，高匿代理和普通代理的原理会再开一篇文章进行讨论。 日志记录说明高匿 IP 能够帮助我们实现<strong>隐匿</strong>的目的。说到这里不得不提一下，芝麻代理高匿 IP 的质量真的好，听说他们的 IP 还支持高并发调用，有需求的朋友不妨去试试。</p>
                  <h2 id="机智的你和想当然的开发者"><a href="#机智的你和想当然的开发者" class="headerlink" title="机智的你和想当然的开发者"></a>机智的你和想当然的开发者</h2>
                  <p>难道普通代理就一定会被 X-Forward-For 发现吗？ 办法总是会有的，翻一下 <a href="http://www.sfhfpc.com" target="_blank" rel="noopener">http://www.sfhfpc.com</a> 或者公众号<strong>韦世东学算法和反爬虫</strong>说不定灵感就来了！在解读 RFC7239 - Example Usage 时，我们了解到 X-Forward-For 会记录原始 IP，在使用多层 IP 代理的情况下记录的是上层 IP。利用这个特点，是不是可以<strong>伪造</strong>一下呢？ 既然 X-Forward-For 和 Referer 一样是头域，那么就说明它可以被人为改变。我们只需要在请求时加上 X-Forward-For 请求头和对应的值即可。代码片段 Python-Request-CustomHeader 实现了这样的需求。</p>
                  <figure class="highlight nix">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="built_in">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求地址</span></span><br><span class="line"><span class="attr">targetUrl</span> = <span class="string">"http://111.231.93.117/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 代理服务器</span></span><br><span class="line"><span class="attr">proxyHost</span> = <span class="string">"220.185.128.170"</span></span><br><span class="line"><span class="attr">proxyPort</span> = <span class="string">"9999"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">proxyMeta</span> = <span class="string">"http://%(host)s:%(port)s"</span> % &#123;</span><br><span class="line"></span><br><span class="line">    <span class="string">"host"</span>: proxyHost,</span><br><span class="line">    <span class="string">"port"</span>: proxyPort,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">proxies</span> = &#123;</span><br><span class="line">    <span class="string">"http"</span>: proxyMeta,</span><br><span class="line">&#125;</span><br><span class="line"><span class="attr">header</span> = &#123;</span><br><span class="line">    <span class="string">"Referer"</span>: <span class="string">"http://www.sfhfpc.com"</span>,</span><br><span class="line">    <span class="string">"X-Forwarded-For"</span>: <span class="string">"_"</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="attr">resp</span> = requests.get(targetUrl, <span class="attr">proxies=proxies,</span> <span class="attr">headers=header)</span></span><br><span class="line">print(resp.status_code)</span><br><span class="line">print(resp.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>代码片段 Python-Request-CustomHeader 代码运行后，控制台结果如下：</p>
                  <figure class="highlight xml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">200</span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>Welcome to nginx!<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span></span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Welcome to nginx!<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>For online documentation and support please refer to</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://nginx.org/"</span>&gt;</span>nginx.org<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">Commercial support is available at</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://nginx.com/"</span>&gt;</span>nginx.com<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">em</span>&gt;</span>Thank you for using nginx.<span class="tag">&lt;/<span class="name">em</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>响应状态码是 200，并且返回了 Welcome to nginx 等字样，这说明请求成功。对应的日志记录为：</p>
                  <figure class="highlight accesslog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">220.185.128.170</span> - - <span class="string">[24/Nov/2019:14:13:24 +0800]</span> <span class="string">"<span class="keyword">GET</span> / HTTP/1.1"</span> <span class="number">200</span> <span class="number">612</span> <span class="string">"http://www.sfhfpc.com"</span> <span class="string">"python-requests/2.21.0"</span> <span class="string">"_, 180.137.156.168"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>记录显示，原始 IP 为 220.185.128.170、代理清单为 “_, 180.137.156.168”。实际上原始 IP 是 180.137.156.168，而代理服务器的 IP 是 220.185.128.170。代理清单中多出来的短横线是我们在代码中加上的，这里居然也显示了。这说明我们只需要在请求时附带上 X-Forward-For 头域就可以达到<strong>伪造</strong>的目的。 如果我想让服务端认为原始 IP 为 112.113.115.116，那么只需要将代码片段 Python-Request-CustomHeader 中 header 对象中 X-Forwarded-For 键对应的值设置为 112.113.115.116 即可。 保存后运行代码，对应的日志记录如下：</p>
                  <figure class="highlight accesslog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">220.185.128.170</span> - - <span class="string">[24/Nov/2019:14:28:08 +0800]</span> <span class="string">"<span class="keyword">GET</span> / HTTP/1.1"</span> <span class="number">200</span> <span class="number">612</span> <span class="string">"http://www.sfhfpc.com"</span> <span class="string">"python-requests/2.21.0"</span> <span class="string">"112.113.115.116, 180.137.156.168"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>根据 RFC7239 - Example Usage，开发者会认为代理清单中的第一组 IP 地址是原始 IP，殊不知这是我们特意为他准备的。</p>
                  <h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2>
                  <p>X-Forward-For 是 HTTP 协议扩展的一个头域，它可以识别出经过多层代理后的原始 IP。捣蛋的人向来不喜欢遵守约定和规范，来了个鱼目混珠。更多关于 RFC 协议解读和通过违反约定实现的反爬虫措施可翻阅《Python3 反爬虫原理与绕过实战》一书。 <strong>提示：点击链接「<a href="http://h.zhimaruanjian.com/?utm-source=ggzz&amp;utm-keyword=?01" target="_blank" rel="noopener">免费领 IP</a>」可前往芝麻代理领取免费 IP。</strong> <strong>版权声明</strong> 作者：韦世东 链接：<a href="http://www.sfhfpc.com" target="_blank" rel="noopener">http://www.sfhfpc.com</a> 来源：算法和反爬虫 著作权归作者所有，非商业转载请注明出处，禁止商业转载。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/韦世东学算法和反爬虫" class="author" itemprop="url" rel="index">韦世东学算法和反爬虫</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-27 20:06:12" itemprop="dateCreated datePublished" datetime="2019-11-27T20:06:12+08:00">2019-11-27</time>
                </span>
                <span id="/8272.html" class="post-meta-item leancloud_visitors" data-flag-title="X-Forward-For 看破红尘，代理 IP 无所遁形！" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>7.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>7 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8263.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8263.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 11.3-mitmdump 爬取 “得到” App 电子书信息</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="11-3-mitmdump-爬取-“得到”-App-电子书信息"><a href="#11-3-mitmdump-爬取-“得到”-App-电子书信息" class="headerlink" title="11.3 mitmdump 爬取 “得到” App 电子书信息"></a>11.3 mitmdump 爬取 “得到” App 电子书信息</h1>
                  <p>“得到” App 是罗辑思维出品的一款碎片时间学习的 App，其官方网站为 <a href="https://www.igetget.com，App" target="_blank" rel="noopener">https://www.igetget.com，App</a> 内有很多学习资源。不过 “得到” App 没有对应的网页版，所以信息必须要通过 App 才可以获取。这次我们通过抓取其 App 来练习 mitmdump 的用法。</p>
                  <h3 id="1-爬取目标"><a href="#1-爬取目标" class="headerlink" title="1. 爬取目标"></a>1. 爬取目标</h3>
                  <p>我们的爬取目标是 App 内电子书版块的电子书信息，并将信息保存到 MongoDB，如图 11-30 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-031213.jpg" alt=""></p>
                  <p>我们要把图书的名称、简介、封面、价格爬取下来，不过这次爬取的侧重点还是了解 mitmdump 工具的用法，所以暂不涉及自动化爬取，App 的操作还是手动进行。mitmdump 负责捕捉响应并将数据提取保存。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保已经正确安装好了 mitmproxy 和 mitmdump，手机和 PC 处于同一个局域网下，同时配置好了 mitmproxy 的 CA 证书，安装好 MongoDB 并运行其服务，安装 PyMongo 库，具体的配置可以参考第 1 章的说明。</p>
                  <h3 id="3-抓取分析"><a href="#3-抓取分析" class="headerlink" title="3. 抓取分析"></a>3. 抓取分析</h3>
                  <p>首先探寻一下当前页面的 URL 和返回内容，我们编写一个脚本如下所示：</p>
                  <figure class="highlight mel">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def response(<span class="keyword">flow</span>):</span><br><span class="line">    <span class="keyword">print</span>(<span class="keyword">flow</span>.request.url)</span><br><span class="line">    <span class="keyword">print</span>(<span class="keyword">flow</span>.response.<span class="keyword">text</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里只输出了请求的 URL 和响应的 Body 内容，也就是请求链接和响应内容这两个最关键的部分。脚本保存名称为 script.py。</p>
                  <p>接下来运行 mitmdump，命令如下所示：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">mitmdump -s <span class="keyword">script</span>.py</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>打开 “得到” App 的电子书页面，便可以看到 PC 端控制台有相应输出。接着滑动页面加载更多电子书，控制台新出现的输出内容就是 App 发出的新的加载请求，包含了下一页的电子书内容。控制台输出结果示例如图 11-31 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-031220.png" alt=""></p>
                  <p>图 11-31 控制台输出</p>
                  <p>可以看到 URL 为 <a href="https://dedao.igetget.com/v3/discover/bookList" target="_blank" rel="noopener">https://dedao.igetget.com/v3/discover/bookList</a> 的接口，其后面还加了一个 sign 参数。通过 URL 的名称，可以确定这就是获取电子书列表的接口。在 URL 的下方输出的是响应内容，是一个 JSON 格式的字符串，我们将它格式化，如图 11-32 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-031226.png" alt=""></p>
                  <p>图 11-32 格式化结果</p>
                  <p>格式化后的内容包含一个 c 字段、一个 list 字段，list 的每个元素都包含价格、标题、描述等内容。第一个返回结果是电子书《情人》，而此时 App 的内容也是这本电子书，描述的内容和价格也是完全匹配的，App 页面如图 11-33 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-031231.jpg" alt=""></p>
                  <p>图 11-33 APP 页面</p>
                  <p>这就说明当前接口就是获取电子书信息的接口，我们只需要从这个接口来获取内容就好了。然后解析返回结果，将结果保存到数据库。</p>
                  <h3 id="4-数据抓取"><a href="#4-数据抓取" class="headerlink" title="4. 数据抓取"></a>4. 数据抓取</h3>
                  <p>接下来我们需要对接口做过滤限制，抓取如上分析的接口，再提取结果中的对应字段。</p>
                  <p>这里，我们修改脚本如下所示：</p>
                  <figure class="highlight xl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">from mitmproxy <span class="keyword">import</span> ctx</span><br><span class="line"></span><br><span class="line">def response(flow):</span><br><span class="line">    url = <span class="string">'https://dedao.igetget.com/v3/discover/bookList'</span></span><br><span class="line">    <span class="keyword">if</span> flow.request.url.startswith(url):</span><br><span class="line">        <span class="keyword">text</span> = flow.response.<span class="keyword">text</span></span><br><span class="line">        <span class="keyword">data</span> = json.loads(<span class="keyword">text</span>)</span><br><span class="line">        books = <span class="keyword">data</span>.get(<span class="string">'c'</span>).get(<span class="string">'list'</span>)</span><br><span class="line">        <span class="keyword">for</span> book <span class="built_in">in</span> books:</span><br><span class="line">            ctx.<span class="built_in">log</span>.info(str(book))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>重新滑动电子书页面，在 PC 端控制台观察输出，如图 11-34 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-031239.jpg" alt=""></p>
                  <p>图 11-34 控制台输出</p>
                  <p>现在输出了图书的全部信息，一本图书信息对应一条 JSON 格式的数据。</p>
                  <h3 id="5-提取保存"><a href="#5-提取保存" class="headerlink" title="5. 提取保存"></a>5. 提取保存</h3>
                  <p>接下来我们需要提取信息，再把信息保存到数据库中。方便起见，我们选择 MongoDB 数据库。</p>
                  <p>脚本还可以增加提取信息和保存信息的部分，修改代码如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import json</span><br><span class="line">import pymongo</span><br><span class="line"><span class="keyword">from</span> mitmproxy import ctx</span><br><span class="line"></span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>)</span><br><span class="line">db = client[<span class="string">'igetget'</span>]</span><br><span class="line">collection = db[<span class="string">'books'</span>]</span><br><span class="line"></span><br><span class="line">def response(flow):</span><br><span class="line">    global collection</span><br><span class="line">    url = <span class="string">'https://dedao.igetget.com/v3/discover/bookList'</span></span><br><span class="line">    <span class="keyword">if</span> flow.request.url.startswith(url):</span><br><span class="line">        text = flow.response.text</span><br><span class="line">        data = json.loads(text)</span><br><span class="line">        books = data.<span class="builtin-name">get</span>(<span class="string">'c'</span>).<span class="builtin-name">get</span>(<span class="string">'list'</span>)</span><br><span class="line">        <span class="keyword">for</span> book <span class="keyword">in</span> books:</span><br><span class="line">            data = &#123;<span class="string">'title'</span>: book.<span class="builtin-name">get</span>(<span class="string">'operating_title'</span>),</span><br><span class="line">                <span class="string">'cover'</span>: book.<span class="builtin-name">get</span>(<span class="string">'cover'</span>),</span><br><span class="line">                <span class="string">'summary'</span>: book.<span class="builtin-name">get</span>(<span class="string">'other_share_summary'</span>),</span><br><span class="line">                <span class="string">'price'</span>: book.<span class="builtin-name">get</span>(<span class="string">'price'</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            ctx.log.<span class="builtin-name">info</span>(str(data))</span><br><span class="line">            collection.insert(data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>重新滑动页面，控制台便会输出信息，如图 11-35 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-031246.jpg" alt=""></p>
                  <p>图 11-35 控制台输出</p>
                  <p>现在输出的每一条内容都是经过提取之后的内容，包含了电子书的标题、封面、描述、价格信息。</p>
                  <p>最开始我们声明了 MongoDB 的数据库连接，提取出信息之后调用该对象的 insert() 方法将数据插入到数据库即可。</p>
                  <p>滑动几页，发现所有图书信息都被保存到 MongoDB 中，如图 11-36 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2019-11-27-031250.jpg" alt=""></p>
                  <p>目前为止，我们利用一个非常简单的脚本把 “得到” App 的电子书信息保存下来。</p>
                  <h3 id="6-本节代码"><a href="#6-本节代码" class="headerlink" title="6. 本节代码"></a>6. 本节代码</h3>
                  <p>本节的代码地址是：<a href="https://github.com/Python3WebSpider/IGetGet" target="_blank" rel="noopener"></a><a href="https://github.com/Python3WebSpider/IGetGet" target="_blank" rel="noopener">https://github.com/Python3WebSpider/IGetGet</a>。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>本节主要讲解了 mitmdump 的用法及脚本的编写方法。通过本节的实例，我们可以学习到如何实时将 App 的数据抓取下来。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-27 11:15:45" itemprop="dateCreated datePublished" datetime="2019-11-27T11:15:45+08:00">2019-11-27</time>
                </span>
                <span id="/8263.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 11.3-mitmdump 爬取 “得到” App 电子书信息" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>2.5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8260.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8260.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 11.2-mitmproxy 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="11-2-mitmproxy-的使用"><a href="#11-2-mitmproxy-的使用" class="headerlink" title="11.2 mitmproxy 的使用"></a>11.2 mitmproxy 的使用</h1>
                  <p>mitmproxy 是一个支持 HTTP 和 HTTPS 的抓包程序，有类似 Fiddler、Charles 的功能，只不过它是一个控制台的形式操作。 mitmproxy 还有两个关联组件。一个是 mitmdump，它是 mitmproxy 的命令行接口，利用它我们可以对接 Python 脚本，用 Python 实现监听后的处理。另一个是 mitmweb，它是一个 Web 程序，通过它我们可以清楚观察 mitmproxy 捕获的请求。 下面我们来了解它们的用法。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请确保已经正确安装好了 mitmproxy，并且手机和 PC 处于同一个局域网下，同时配置好了 mitmproxy 的 CA 证书，具体的配置可以参考第 1 章的说明。</p>
                  <h3 id="2-mitmproxy-的功能"><a href="#2-mitmproxy-的功能" class="headerlink" title="2. mitmproxy 的功能"></a>2. mitmproxy 的功能</h3>
                  <p>mitmproxy 有如下几项功能。</p>
                  <ul>
                    <li>拦截 HTTP 和 HTTPS 请求和响应</li>
                    <li>保存 HTTP 会话并进行分析</li>
                    <li>模拟客户端发起请求，模拟服务端返回响应</li>
                    <li>利用反向代理将流量转发给指定的服务器</li>
                    <li>支持 Mac 和 Linux 上的透明代理</li>
                    <li>利用 Python 对 HTTP 请求和响应进行实时处理</li>
                  </ul>
                  <h3 id="3-抓包原理"><a href="#3-抓包原理" class="headerlink" title="3. 抓包原理"></a>3. 抓包原理</h3>
                  <p>和 Charles 一样，mitmproxy 运行于自己的 PC 上，mitmproxy 会在 PC 的 8080 端口运行，然后开启一个代理服务，这个服务实际上是一个 HTTP/HTTPS 的代理。 手机和 PC 在同一个局域网内，设置代理为 mitmproxy 的代理地址，这样手机在访问互联网的时候流量数据包就会流经 mitmproxy，mitmproxy 再去转发这些数据包到真实的服务器，服务器返回数据包时再由 mitmproxy 转发回手机，这样 mitmproxy 就相当于起了中间人的作用，抓取到所有 Request 和 Response，另外这个过程还可以对接 mitmdump，抓取到的 Request 和 Response 的具体内容都可以直接用 Python 来处理，比如得到 Response 之后我们可以直接进行解析，然后存入数据库，这样就完成了数据的解析和存储过程。</p>
                  <h3 id="4-设置代理"><a href="#4-设置代理" class="headerlink" title="4. 设置代理"></a>4. 设置代理</h3>
                  <p>首先，我们需要运行 mitmproxy，命令如下所示： 启动 mitmproxy 的命令如下：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">mitmproxy</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后会在 8080 端口上运行一个代理服务，如图 11-12 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-030652.jpg" alt=""> 图 11-12 mitmproxy 运行结果 右下角会出现当前正在监听的端口。 或者启动 mitmdump，它也会监听 8080 端口，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">mitmdump</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如图 11-13 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030657.jpg" alt=""> 图 11-13 MitmDump 运行结果 将手机和 PC 连接在同一局域网下，设置代理为当前代理。首先看看 PC 的当前局域网 IP。 Windows 上的命令如下所示：</p>
                  <figure class="highlight dos">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="built_in">ipconfig</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Linux 和 Mac 上的命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">ifconfig</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如图 11-14 所示： <img src="https://cdn.cuiqingcai.com/2019-11-27-030701.jpg" alt=""> 图 11-14 查看局域网 IP 一般类似 10.<em>.</em>.<em> 或 172.16.</em>.<em> 或 192.168.1.</em> 这样的 IP 就是当前 PC 的局域网 IP，例如此图中 PC 的 IP 为 192.168.1.28，手机代理设置类似如图 11-15 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030706.jpg" alt=""> 图 11-15 代理设置 这样我们就配置好了 mitmproxy 的的代理。</p>
                  <h3 id="5-mitmproxy-的使用"><a href="#5-mitmproxy-的使用" class="headerlink" title="5. mitmproxy 的使用"></a>5. mitmproxy 的使用</h3>
                  <p>确保 mitmproxy 正常运行，并且手机和 PC 处于同一个局域网内，设置了 mitmproxy 的代理，具体的配置方法可以参考第 1 章。 运行 mitmproxy，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">mitmproxy</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>设置成功之后，我们只需要在手机浏览器上访问任意的网页或浏览任意的 App 即可。例如在手机上打开百度，mitmproxy 页面便会呈现出手机上的所有请求，如图 11-16 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030716.jpg" alt=""> 图 11-16 所有请求 这就相当于之前我们在浏览器开发者工具监听到的浏览器请求，在这里我们借助于 mitmproxy 完成。Charles 完全也可以做到。 这里是刚才手机打开百度页面时的所有请求列表，左下角显示的 2/38 代表一共发生了 38 个请求，当前箭头所指的是第二个请求。 每个请求开头都有一个 GET 或 POST，这是各个请求的请求方式。紧接的是请求的 URL。第二行开头的数字就是请求对应的响应状态码，后面是响应内容的类型，如 text/html 代表网页文档、image/gif 代表图片。再往后是响应体的大小和响应的时间。 当前呈现了所有请求和响应的概览，我们可以通过这个页面观察到所有的请求。 如果想查看某个请求的详情，我们可以敲击回车，进入请求的详情页面，如图 11-17 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030720.png" alt=""> 图 11-17 详情页面 可以看到 Headers 的详细信息，如 Host、Cookies、User-Agent 等。 最上方是一个 Request、Response、Detail 的列表，当前处在 Request 这个选项上。这时我们再点击 TAB 键，即可查看这个请求对应的响应详情，如图 11-18 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030725.jpg" alt=""> 图 11-18 响应详情 最上面是响应头的信息，下拉之后我们可以看到响应体的信息。针对当前请求，响应体就是网页的源代码。 这时再敲击 TAB 键，切换到最后一个选项卡 Detail，即可看到当前请求的详细信息，如服务器的 IP 和端口、HTTP 协议版本、客户端的 IP 和端口等，如图 11-19 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030729.jpg" alt=""> 图 11-19 详细信息 mitmproxy 还提供了命令行式的编辑功能，我们可以在此页面中重新编辑请求。敲击 e 键即可进入编辑功能，这时它会询问你要编辑哪部分内容，如 Cookies、Query、URL 等，每个选项的第一个字母会高亮显示。敲击要编辑内容名称的首字母即可进入该内容的编辑页面，如敲击 m 即可编辑请求的方式，敲击 q 即可修改 GET 请求参数 Query。 这时我们敲击 q，进入到编辑 Query 的页面。由于没有任何参数，我们可以敲击 a 来增加一行，然后就可以输入参数对应的 Key 和 Value，如图 11-20 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030735.jpg" alt=""> 图 11-20 编辑页面 这里我们输入 Key 为 wd，Value 为 NBA。 然后再敲击 esc 键和 q 键，返回之前的页面，再敲击 e 和 p 键修改 Path。和上面一样，敲击 a 增加 Path 的内容，这时我们将 Path 修改为 s，如图 11-21 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030741.jpg" alt=""> 图 11-21 编辑页面 再敲击 esc 和 q 键返回，这时我们可以看到最上面的请求链接变成了 <a href="https://www.baidu.com/s?wd=NBA" target="_blank" rel="noopener">https://www.baidu.com/s?wd=NBA</a>，访问这个页面，可以看到百度搜索 NBA 关键词的搜索结果，如图 11-22 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030745.jpg" alt=""> 图 11-22 请求详情 敲击 a 保存修改，敲击 r 重新发起修改后的请求，即可看到上方请求方式前面多了一个回旋箭头，这说明重新执行了修改后的请求。这时我们再观察响应体内容，即可看到搜索 NBA 的页面结果的源代码，如图 11-23 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030750.jpg" alt=""> 图 11-23 响应结果 以上内容便是 mitmproxy 的简单用法。利用 mitmproxy，我们可以观察到手机上的所有请求，还可以对请求进行修改并重新发起。 Fiddler、Charles 也有这个功能，而且它们的图形界面操作更加方便。那么 mitmproxy 的优势何在？ mitmproxy 的强大之处体现在它的另一个工具 mitmdump，有了它我们可以直接对接 Python 对请求进行处理。下面我们来看看 mitmdump 的用法。</p>
                  <h3 id="6-MitmDump-的使用"><a href="#6-MitmDump-的使用" class="headerlink" title="6. MitmDump 的使用"></a>6. MitmDump 的使用</h3>
                  <p>mitmdump 是 mitmproxy 的命令行接口，同时还可以对接 Python 对请求进行处理，这是相比 Fiddler、Charles 等工具更加方便的地方。有了它我们可以不用手动截获和分析 HTTP 请求和响应，只需写好请求和响应的处理逻辑即可。它还可以实现数据的解析、存储等工作，这些过程都可以通过 Python 实现。</p>
                  <h4 id="实例引入"><a href="#实例引入" class="headerlink" title="实例引入"></a>实例引入</h4>
                  <p>我们可以使用命令启动 mitmproxy，并把截获的数据保存到文件中，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">mitmdump -w outfile</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中 outfile 的名称任意，截获的数据都会被保存到此文件中。 还可以指定一个脚本来处理截获的数据，使用 - s 参数即可：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">mitmdump -s <span class="keyword">script</span>.py</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里指定了当前处理脚本为 script.py，它需要放置在当前命令执行的目录下。 我们可以在脚本里写入如下的代码：</p>
                  <figure class="highlight mel">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def request(<span class="keyword">flow</span>):</span><br><span class="line">    <span class="keyword">flow</span>.request.headers[<span class="string">'User-Agent'</span>] = <span class="string">'MitmProxy'</span></span><br><span class="line">    <span class="keyword">print</span>(<span class="keyword">flow</span>.request.headers)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们定义了一个 request() 方法，参数为 flow，它其实是一个 HTTPFlow 对象，通过 request 属性即可获取到当前请求对象。然后打印输出了请求的请求头，将请求头的 User-Agent 修改成了 MitmProxy。 运行之后我们在手机端访问 <a href="http://httpbin.org/get" target="_blank" rel="noopener">http://httpbin.org/get</a>，就可以看到有如下情况发生。 手机端的页面显示如图 11-24 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030755.jpg" alt=""> 图 11-24 手机端页面 PC 端控制台输出如图 11-25 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030800.png" alt=""> 图 11-25 PC 端控制台 手机端返回结果的 Headers 实际上就是请求的 Headers，User-Agent 被修改成了 mitmproxy。PC 端控制台输出了修改后的 Headers 内容，其 User-Agent 的内容正是 mitmproxy。 所以，通过这三行代码我们就可以完成对请求的改写。print() 方法输出结果可以呈现在 PC 端控制台上，可以方便地进行调试。</p>
                  <h4 id="日志输出"><a href="#日志输出" class="headerlink" title="日志输出"></a>日志输出</h4>
                  <p>mitmdump 提供了专门的日志输出功能，可以设定不同级别以不同颜色输出结果。我们把脚本修改成如下内容：</p>
                  <figure class="highlight mel">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from mitmproxy import ctx</span><br><span class="line"></span><br><span class="line">def request(<span class="keyword">flow</span>):</span><br><span class="line">    <span class="keyword">flow</span>.request.headers[<span class="string">'User-Agent'</span>] = <span class="string">'MitmProxy'</span></span><br><span class="line">    ctx.<span class="keyword">log</span>.info(str(<span class="keyword">flow</span>.request.headers))</span><br><span class="line">    ctx.<span class="keyword">log</span>.warn(str(<span class="keyword">flow</span>.request.headers))</span><br><span class="line">    ctx.<span class="keyword">log</span>.<span class="keyword">error</span>(str(<span class="keyword">flow</span>.request.headers))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里调用了 ctx 模块，它有一个 log 功能，调用不同的输出方法就可以输出不同颜色的结果，以方便我们做调试。例如，info() 方法输出的内容是白色的，warn() 方法输出的内容是黄色的，error() 方法输出的内容是红色的。运行结果如图 11-26 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030807.png" alt=""> 图 11-26 运行结果 不同的颜色对应不同级别的输出，我们可以将不同的结果合理划分级别输出，以更直观方便地查看调试信息。</p>
                  <h4 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h4>
                  <p>最开始我们实现了 request() 方法并且对 Headers 进行了修改。下面我们来看看 Request 还有哪些常用的功能。我们先用一个实例来感受一下。</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> mitmproxy import ctx</span><br><span class="line"></span><br><span class="line">def request(flow):</span><br><span class="line">    request = flow.request</span><br><span class="line">    <span class="builtin-name">info</span> = ctx.log.info</span><br><span class="line">    <span class="builtin-name">info</span>(request.url)</span><br><span class="line">    <span class="builtin-name">info</span>(str(request.headers))</span><br><span class="line">    <span class="builtin-name">info</span>(str(request.cookies))</span><br><span class="line">    <span class="builtin-name">info</span>(request.host)</span><br><span class="line">    <span class="builtin-name">info</span>(request.method)</span><br><span class="line">    <span class="builtin-name">info</span>(str(request.port))</span><br><span class="line">    <span class="builtin-name">info</span>(request.scheme)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们修改脚本，然后在手机上打开百度，即可看到 PC 端控制台输出了一系列的请求，在这里我们找到第一个请求。控制台打印输出了 Request 的一些常见属性，如 URL、Headers、Cookies、Host、Method、Scheme 等。输出结果如图 11-27 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030812.png" alt=""> 图 11-27 输出结果 结果中分别输出了请求链接、请求头、请求 Cookies、请求 Host、请求方法、请求端口、请求协议这些内容。 同时我们还可以对任意属性进行修改，就像最初修改 Headers 一样，直接赋值即可。例如，这里将请求的 URL 修改一下，脚本修改如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">(flow)</span></span><span class="symbol">:</span></span><br><span class="line">    url = <span class="string">'https://httpbin.org/get'</span></span><br><span class="line">    flow.request.url = url</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>手机端得到如下结果，如图 11-28 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030817.jpg" alt=""> 图 11-28 手机端页面 比较有意思的是，浏览器最上方还是呈现百度的 URL，但是页面已经变成了 httpbin.org 的页面了。另外，Cookies 明显还是百度的 Cookies。我们只是用简单的脚本就成功把请求修改为其他的站点。通过这种方式修改和伪造请求就变得轻而易举。 通过这个实例我们知道，有时候 URL 虽然是正确的，但是内容并非是正确的。我们需要进一步提高自己的安全防范意识。 Request 还有很多属性，在此不再一一列举。更多属性可以参考：<a href="http://docs.mitmproxy.org/en/latest/scripting/api.html" target="_blank" rel="noopener">http://docs.mitmproxy.org/en/latest/scripting/api.html</a>。 只要我们了解了基本用法，会很容易地获取和修改 Reqeust 的任意内容，比如可以用修改 Cookies、添加代理等方式来规避反爬。</p>
                  <h4 id="Response"><a href="#Response" class="headerlink" title="Response"></a>Response</h4>
                  <p>对于爬虫来说，我们更加关心的其实是响应的内容，因为 Response Body 才是爬取的结果。对于响应来说，mitmdump 也提供了对应的处理接口，就是 response() 方法。下面我们用一个实例感受一下。</p>
                  <figure class="highlight isbl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="variable">from</span> <span class="variable">mitmproxy</span> <span class="variable">import</span> <span class="variable">ctx</span></span><br><span class="line"></span><br><span class="line"><span class="variable">def</span> <span class="function"><span class="title">response</span>(<span class="variable">flow</span>):</span></span><br><span class="line"><span class="function">    <span class="variable">response</span> = <span class="variable">flow.response</span></span></span><br><span class="line"><span class="function">    <span class="variable">info</span> = <span class="variable">ctx.log.info</span></span></span><br><span class="line"><span class="function">    <span class="title">info</span>(<span class="title">str</span>(<span class="variable">response.status_code</span>))</span></span><br><span class="line">    <span class="function"><span class="title">info</span>(<span class="title">str</span>(<span class="variable">response.headers</span>))</span></span><br><span class="line">    <span class="function"><span class="title">info</span>(<span class="title">str</span>(<span class="variable">response.cookies</span>))</span></span><br><span class="line">    <span class="function"><span class="title">info</span>(<span class="title">str</span>(<span class="variable">response.text</span>))</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>将脚本修改为如上内容，然后手机访问：<a href="http://httpbin.org/get" target="_blank" rel="noopener">http://httpbin.org/get</a>。 这里打印输出了响应的 status_code、headers、cookies、text 这几个属性，其中最主要的 text 属性就是网页的源代码。 PC 端控制台输出如图 11-29 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-27-030826.png" alt=""> 图 11-29 PC 端控制台 控制台输出了响应的状态码、响应头、Cookies、响应体这几部分内容。 我们可以通过 response() 方法获取每个请求的响应内容。接下来再进行响应的信息提取和存储，我们就可以成功完成爬取了。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>本节介绍了 mitmproxy 和 mitmdump 的用法，在下一节我们会利用它们来实现一个 App 的爬取实战。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-27 11:11:52" itemprop="dateCreated datePublished" datetime="2019-11-27T11:11:52+08:00">2019-11-27</time>
                </span>
                <span id="/8260.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 11.2-mitmproxy 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>5.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>5 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8247.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8247.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 11.1-Charles 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="11-1-Charles-的使用"><a href="#11-1-Charles-的使用" class="headerlink" title="11.1 Charles 的使用"></a>11.1 Charles 的使用</h1>
                  <p>Charles 是一个网络抓包工具，我们可以用它来做 App 的抓包分析，得到 App 运行过程中发生的所有网络请求和响应内容，这就和 Web 端浏览器的开发者工具 Network 部分看到的结果一致。 相比 Fiddler 来说，Charles 的功能更强大，而且跨平台支持更好。所以我们选用 Charles 作为主要的移动端抓包工具，用于分析移动 App 的数据包，辅助完成 App 数据抓取工作。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本节我们以京东 App 为例，通过 Charles 抓取 App 运行过程中的网络数据包，然后查看具体的 Request 和 Response 内容，以此来了解 Charles 的用法。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保已经正确安装 Charles 并开启了代理服务，手机和 Charles 处于同一个局域网下，Charles 代理和 CharlesCA 证书设置好，另外需要开启 SSL 监听，具体的配置可以参考第 1 章的说明。</p>
                  <h3 id="3-原理"><a href="#3-原理" class="headerlink" title="3. 原理"></a>3. 原理</h3>
                  <p>首先 Charles 运行在自己的 PC 上，Charles 运行的时候会在 PC 的 8888 端口开启一个代理服务，这个服务实际上是一个 HTTP/HTTPS 的代理。 确保手机和 PC 在同一个局域网内，我们可以使用手机模拟器通过虚拟网络连接，也可以使用手机真机和 PC 通过无线网络连接。 设置手机代理为 Charles 的代理地址，这样手机访问互联网的数据包就会流经 Charles，Charles 再转发这些数据包到真实的服务器，服务器返回的数据包再由 Charles 转发回手机，Charles 就起到中间人的作用，所有流量包都可以捕捉到，因此所有 HTTP 请求和响应都可以捕获到。同时 Charles 还有权力对请求和响应进行修改。</p>
                  <h3 id="4-抓包"><a href="#4-抓包" class="headerlink" title="4. 抓包"></a>4. 抓包</h3>
                  <p>初始状态下 Charles 的运行界面如图 11-1 所示： <img src="https://cdn.cuiqingcai.com/2019-11-26-034750.png" alt=""> 图 11-1 Charles 运行界面 Charles 会一直监听 PC 和手机发生的网络数据包，捕获到的数据包就会显示在左侧，随着时间的推移，捕获的数据包越来越多，左侧列表的内容也会越来越多。 可以看到，图中左侧显示了 Charles 抓取到的请求站点，我们点击任意一个条目便可以查看对应请求的详细信息，其中包括 Request、Response 等内容。 接下来清空 Charles 的抓取结果，点击左侧的扫帚按钮即可清空当前捕获到的所有请求。然后点击第二个监听按钮，确保监听按钮是打开的，这表示 Charles 正在监听 App 的网络数据流，如图 11-2 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034803.png" alt=""> 图 11-2 监听过程 这时打开手机京东，注意一定要提前设置好 Charles 的代理并配置好 CA 证书，否则没有效果。 打开任意一个商品，如 iPhone，然后打开它的商品评论页面，如图 11-3 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034824.png" alt=""> 图 11-3 评论页面 不断上拉加载评论，可以看到 Charles 捕获到这个过程中京东 App 内发生的所有网络请求，如图 11-4 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034830.png" alt=""> 图 11-4 监听结果 左侧列表中会出现一个 api.m.jd.com 链接，而且它在不停闪动，很可能就是当前 App 发出的获取评论数据的请求被 Charles 捕获到了。我们点击将其展开，继续上拉刷新评论。随着上拉的进行，此处又会出现一个个网络请求记录，这时新出现的数据包请求确定就是获取评论的请求。 为了验证其正确性，我们点击查看其中一个条目的详情信息。切换到 Contents 选项卡，这时我们发现一些 JSON 数据，核对一下结果，结果有 commentData 字段，其内容和我们在 App 中看到的评论内容一致，如图 11-5 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034834.png" alt=""> 图 11-5 Json 数据结果 这时可以确定，此请求对应的接口就是获取商品评论的接口。这样我们就成功捕获到了在上拉刷新的过程中发生的请求和响应内容。</p>
                  <h3 id="5-分析"><a href="#5-分析" class="headerlink" title="5. 分析"></a>5. 分析</h3>
                  <p>现在分析一下这个请求和响应的详细信息。首先可以回到 Overview 选项卡，上方显示了请求的接口 URL，接着是响应状态 Status Code、请求方式 Method 等，如图 11-6 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034838.png" alt=""> 图 11-6 监听结果 这个结果和原本在 Web 端用浏览器开发者工具内捕获到的结果形式是类似的。 接下来点击 Contents 选项卡，查看该请求和响应的详情信息。 上半部分显示的是 Request 的信息，下半部分显示的是 Response 的信息。比如针对 Reqeust，我们切换到 Headers 选项卡即可看到该 Request 的 Headers 信息，针对 Response，我们切换到 JSON TEXT 选项卡即可看到该 Response 的 Body 信息，并且该内容已经被格式化，如图 11-7 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034842.png" alt=""> 图 11-7 监听结果 由于这个请求是 POST 请求，所以我们还需要关心的就是 POST 的表单信息，切换到 Form 选项卡即可查看，如图 11-8 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034848.png" alt=""> 图 11-8 监听结果 这样我们就成功抓取 App 中的评论接口的请求和响应，并且可以查看 Response 返回的 JSON 数据。 至于其他 App，我们同样可以使用这样的方式来分析。如果我们可以直接分析得到请求的 URL 和参数的规律，直接用程序模拟即可批量抓取。</p>
                  <h3 id="6-重发"><a href="#6-重发" class="headerlink" title="6. 重发"></a>6. 重发</h3>
                  <p>Charles 还有一个强大功能，它可以将捕获到的请求加以修改并发送修改后的请求。点击上方的修改按钮，左侧列表就多了一个以编辑图标为开头的链接，这就代表此链接对应的请求正在被我们修改，如图 11-9 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034857.png" alt=""> 图 11-9 编辑页面 我们可以将 Form 中的某个字段移除，比如这里将 partner 字段移除，然后点击 Remove。这时我们已经对原来请求携带的 Form Data 做了修改，然后点击下方的 Execute 按钮即可执行修改后的请求，如图 11-10 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034905.png" alt=""> 图 11-10 编辑页面 可以发现左侧列表再次出现了接口的请求结果，内容仍然不变，如图 11-11 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-034910.png" alt=""> 图 11-11 重新请求后结果 删除 Form 表单中的 partner 字段并没有带来什么影响，所以这个字段是无关紧要的。 有了这个功能，我们就可以方便地使用 Charles 来做调试，可以通过修改参数、接口等来测试不同请求的响应状态，就可以知道哪些参数是必要的哪些是不必要的，以及参数分别有什么规律，最后得到一个最简单的接口和参数形式以供程序模拟调用使用。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>以上内容便是通过 Charles 抓包分析 App 请求的过程。通过 Charles，我们成功抓取 App 中流经的网络数据包，捕获原始的数据，还可以修改原始请求和重新发起修改后的请求进行接口测试。 知道了请求和响应的具体信息，如果我们可以分析得到请求的 URL 和参数的规律，直接用程序模拟即可批量抓取，这当然最好不过了。 但是随着技术的发展，App 接口往往会带有密钥，我们并不能直接找到这些规律，那么怎么办呢？接下来，我们将了解利用 Charles 和 mitmdump 直接对接 Python 脚本实时处理抓取到的 Response 的过程。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-26 11:50:41" itemprop="dateCreated datePublished" datetime="2019-11-26T11:50:41+08:00">2019-11-26</time>
                </span>
                <span id="/8247.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 11.1-Charles 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>2.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8243.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8243.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 10.2-Cookies 池的搭建</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="10-2-Cookies-池的搭建"><a href="#10-2-Cookies-池的搭建" class="headerlink" title="10.2 Cookies 池的搭建"></a>10.2 Cookies 池的搭建</h1>
                  <p>很多时候，在爬取没有登录的情况下，我们也可以访问一部分页面或请求一些接口，因为毕竟网站本身需要做 SEO，不会对所有页面都设置登录限制。 但是，不登录直接爬取会有一些弊端，弊端主要有以下两点。</p>
                  <ul>
                    <li>设置了登录限制的页面无法爬取。如某论坛设置了登录才可查看资源，某博客设置了登录才可查看全文等，这些页面都需要登录账号才可以查看和爬取。</li>
                    <li>一些页面和接口虽然可以直接请求，但是请求一旦频繁，访问就容易被限制或者 IP 直接被封，但是登录之后就不会出现这样的问题，因此登录之后被反爬的可能性更低。</li>
                  </ul>
                  <p>下面我们就第二种情况做一个简单的实验。以微博为例，我们先找到一个 Ajax 接口，例如新浪财经官方微博的信息接口 <a href="https://m.weibo.cn/api/container/getIndex?uid=1638782947&amp;luicode=20000174" target="_blank" rel="noopener">https://m.weibo.cn/api/container/getIndex?uid=1638782947&amp;luicode=20000174</a> &amp;type=uid&amp;value=1638782947&amp;containerid=1005051638782947，如果用浏览器直接访问，返回的数据是 JSON 格式，如图 10-7 所示，其中包含了新浪财经官方微博的一些信息，直接解析 JSON 即可提取信息。 <img src="https://cdn.cuiqingcai.com/2019-11-25-144100.png" alt=""> 图 10-7 返回数据 但是，这个接口在没有登录的情况下会有请求频率检测。如果一段时间内访问太过频繁，比如打开这个链接，一直不断刷新，则会看到请求频率过高的提示，如图 10-8 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-25-144106.png" alt=""> 图 10-8 提示页面 如果重新打开一个浏览器窗口，打开 <a href="https://passport.weibo.cn/signin/login?entry=mweibo&amp;r" target="_blank" rel="noopener">https://passport.weibo.cn/signin/login?entry=mweibo&amp;r</a>\= <a href="https://m.weibo.cn/，登录微博账号之后重新打开此链接，则页面正常显示接口的结果，而未登录的页面仍然显示请求过于频繁，如图" target="_blank" rel="noopener">https://m.weibo.cn/，登录微博账号之后重新打开此链接，则页面正常显示接口的结果，而未登录的页面仍然显示请求过于频繁，如图</a> 10-9 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-26-032830.jpg" alt=""> 图 10-9 对比页面 图中左侧是登录了账号之后请求接口的结果，右侧是未登录账号请求接口的结果，二者的接口链接是完全一样的。未登录状态无法正常访问，而登录状态可以正常显示。 因此，登录账号可以降低被封禁的概率。 我们可以尝试登录之后再做爬取，被封禁的几率会小很多，但是也不能完全排除被封禁的风险。如果一直用同一个账号频繁请求，那就有可能遇到请求过于频繁而封号的问题。 如果需要做大规模抓取，我们就需要拥有很多账号，每次请求随机选取一个账号，这样就降低了单个账号的访问频率，被封的概率又会大大降低。 那么如何维护多个账号的登录信息呢？这时就需要用到 Cookies 池了。接下来我们看看 Cookies 池的构建方法。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们以新浪微博为例来实现一个 Cookies 池的搭建过程。Cookies 池中保存了许多新浪微博账号和登录后的 Cookies 信息，并且 Cookies 池还需要定时检测每个 Cookies 的有效性，如果某 Cookies 无效，那就删除该 Cookies 并模拟登录生成新的 Cookies。同时 Cookies 池还需要一个非常重要的接口，即获取随机 Cookies 的接口，Cookies 运行后，我们只需请求该接口，即可随机获得一个 Cookies 并用其爬取。 由此可见，Cookies 池需要有自动生成 Cookies、定时检测 Cookies、提供随机 Cookies 等几大核心功能。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>搭建之前肯定需要一些微博的账号。需要安装好 Redis 数据库并使其正常运行。需要安装 Python 的 redis-py、requests、Selelnium 和 Flask 库。另外，还需要安装 Chrome 浏览器并配置好 ChromeDriver，其流程可以参考第一章的安装说明。</p>
                  <h3 id="3-Cookies-池架构"><a href="#3-Cookies-池架构" class="headerlink" title="3. Cookies 池架构"></a>3. Cookies 池架构</h3>
                  <p>Cookies 的架构和代理池类似，同样是 4 个核心模块，如图 10-10 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-25-144125.jpg" alt=""> 图 10-10 Cookies 池架构 Cookies 池架构的基本模块分为 4 块：存储模块、生成模块、检测模块和接口模块。每个模块的功能如下。</p>
                  <ul>
                    <li>存储模块负责存储每个账号的用户名密码以及每个账号对应的 Cookies 信息，同时还需要提供一些方法来实现方便的存取操作。</li>
                    <li>生成模块负责生成新的 Cookies。此模块会从存储模块逐个拿取账号的用户名和密码，然后模拟登录目标页面，判断登录成功，就将 Cookies 返回并交给存储模块存储。</li>
                    <li>检测模块需要定时检测数据库中的 Cookies。在这里我们需要设置一个检测链接，不同的站点检测链接不同，检测模块会逐个拿取账号对应的 Cookies 去请求链接，如果返回的状态是有效的，那么此 Cookies 没有失效，否则 Cookies 失效并移除。接下来等待生成模块重新生成即可。</li>
                    <li>接口模块需要用 API 来提供对外服务的接口。由于可用的 Cookies 可能有多个，我们可以随机返回 Cookies 的接口，这样保证每个 Cookies 都有可能被取到。Cookies 越多，每个 Cookies 被取到的概率就会越小，从而减少被封号的风险。</li>
                  </ul>
                  <p>以上设计 Cookies 池的基本思路和前面讲的代理池有相似之处。接下来我们设计整体的架构，然后用代码实现该 Cookies 池。</p>
                  <h3 id="4-Cookies-池的实现"><a href="#4-Cookies-池的实现" class="headerlink" title="4. Cookies 池的实现"></a>4. Cookies 池的实现</h3>
                  <p>首先分别了解各个模块的实现过程。</p>
                  <h4 id="存储模块"><a href="#存储模块" class="headerlink" title="存储模块"></a>存储模块</h4>
                  <p>其实，需要存储的内容无非就是账号信息和 Cookies 信息。账号由用户名和密码两部分组成，我们可以存成用户名和密码的映射。Cookies 可以存成 JSON 字符串，但是我们后面得需要根据账号来生成 Cookies。生成的时候我们需要知道哪些账号已经生成了 Cookies，哪些没有生成，所以需要同时保存该 Cookies 对应的用户名信息，其实也是用户名和 Cookies 的映射。这里就是两组映射，我们自然而然想到 Redis 的 Hash，于是就建立两个 Hash，结构分别如图 10-11 和图 10-12 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-25-144149.jpg" alt=""> 图 10-11 用户名密码 Hash 结构 <img src="https://cdn.cuiqingcai.com/2019-11-25-144159.jpg" alt=""> 图 10-12 用户名 Cookies Hash 结构 Hash 的 Key 就是账号，Value 对应着密码或者 Cookies。另外需要注意，由于 Cookies 池需要做到可扩展，存储的账号和 Cookies 不一定单单只有本例中的微博，其他站点同样可以对接此 Cookies 池，所以这里 Hash 的名称可以做二级分类，例如存账号的 Hash 名称可以为 accounts:weibo，Cookies 的 Hash 名称可以为 cookies:weibo。如要扩展知乎的 Cookies 池，我们就可以使用 accounts:zhihu 和 cookies:zhihu，这样比较方便。 好，接下来我们就创建一个存储模块类，用以提供一些 Hash 的基本操作，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisClient</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, type, website, host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化 Redis 连接</span></span><br><span class="line"><span class="string">        :param host: 地址</span></span><br><span class="line"><span class="string">        :param port: 端口</span></span><br><span class="line"><span class="string">        :param password: 密码</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.db = redis.StrictRedis(host=host, port=port, password=password, decode_responses=<span class="literal">True</span>)</span><br><span class="line">        self.type = type</span><br><span class="line">        self.website = website</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取 Hash 的名称</span></span><br><span class="line"><span class="string">        :return: Hash 名称</span></span><br><span class="line"><span class="string">        """</span><span class="keyword">return</span><span class="string">"&#123;type&#125;:&#123;website&#125;"</span>.format(type=self.type, website=self.website)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set</span><span class="params">(self, username, value)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        设置键值对</span></span><br><span class="line"><span class="string">        :param username: 用户名</span></span><br><span class="line"><span class="string">        :param value: 密码或 Cookies</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hset(self.name(), username, value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, username)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        根据键名获取键值</span></span><br><span class="line"><span class="string">        :param username: 用户名</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hget(self.name(), username)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete</span><span class="params">(self, username)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        根据键名删除键值对</span></span><br><span class="line"><span class="string">        :param username: 用户名</span></span><br><span class="line"><span class="string">        :return: 删除结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hdel(self.name(), username)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取数目</span></span><br><span class="line"><span class="string">        :return: 数目</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hlen(self.name())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        随机得到键值，用于随机 Cookies 获取</span></span><br><span class="line"><span class="string">        :return: 随机 Cookies</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> random.choice(self.db.hvals(self.name()))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">usernames</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取所有账户信息</span></span><br><span class="line"><span class="string">        :return: 所有用户名</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.hkeys(self.name())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">all</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取所有键值对</span></span><br><span class="line"><span class="string">        :return: 用户名和密码或 Cookies 的映射表</span></span><br><span class="line"><span class="string">        """</span><span class="keyword">return</span> self.db.hgetall(self.name())```</span><br><span class="line"></span><br><span class="line">这里我们新建了一个 RedisClient 类，初始化__init__() 方法有两个关键参数 type 和 website，分别代表类型和站点名称，它们就是用来拼接 Hash 名称的两个字段。如果这是存储账户的 Hash，那么此处的 type 为 accounts、website 为 weibo，如果是存储 Cookies 的 Hash，那么此处的 type 为 cookies、website 为 weibo。</span><br><span class="line"></span><br><span class="line">接下来还有几个字段代表了 Redis 的连接信息，初始化时获得这些信息后初始化 StrictRedis 对象，建立 Redis 连接。</span><br><span class="line"></span><br><span class="line">name() 方法拼接了 type 和 website，组成 Hash 的名称。set()、get()、delete() 方法分别代表设置、获取、删除 Hash 的某一个键值对，count() 获取 Hash 的长度。</span><br><span class="line"></span><br><span class="line">比较重要的方法是 random()，它主要用于从 Hash 里随机选取一个 Cookies 并返回。每调用一次 random() 方法，就会获得随机的 Cookies，此方法与接口模块对接即可实现请求接口获取随机 Cookies。</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 生成模块</span></span><br><span class="line"></span><br><span class="line">生成模块负责获取各个账号信息并模拟登录，随后生成 Cookies 并保存。我们首先获取两个 Hash 的信息，看看账户的 Hash 比 Cookies 的 Hash 多了哪些还没有生成 Cookies 的账号，然后将剩余的账号遍历，再去生成 Cookies 即可。</span><br><span class="line"></span><br><span class="line">这里主要逻辑就是找出那些还没有对应 Cookies 的账号，然后再逐个获取 Cookies，代码如下：</span><br><span class="line"></span><br><span class="line">​```python</span><br><span class="line"><span class="keyword">for</span> username <span class="keyword">in</span> accounts_usernames:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> username <span class="keyword">in</span> cookies_usernames:</span><br><span class="line">        password = self.accounts_db.get(username)</span><br><span class="line">        print(<span class="string">' 正在生成 Cookies'</span>, <span class="string">' 账号 '</span>, username, <span class="string">' 密码 '</span>, password)</span><br><span class="line">        result = self.new_cookies(username, password)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>因为我们对接的是新浪微博，前面我们已经破解了新浪微博的四宫格验证码，在这里我们直接对接过来即可，不过现在需要加一个获取 Cookies 的方法，并针对不同的情况返回不同的结果，逻辑如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cookies</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">self</span>.browser.get_cookies()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.open()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.password_error()<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'status'</span>: <span class="number">2</span>,</span><br><span class="line">            <span class="string">'content'</span>: <span class="string">' 用户名或密码错误 '</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment"># 如果不需要验证码直接登录成功</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.login_successfully()<span class="symbol">:</span></span><br><span class="line">        cookies = <span class="keyword">self</span>.get_cookies()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'status'</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">'content'</span>: cookies</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment"># 获取验证码图片</span></span><br><span class="line">    image = <span class="keyword">self</span>.get_image(<span class="string">'captcha.png'</span>)</span><br><span class="line">    numbers = <span class="keyword">self</span>.detect_image(image)</span><br><span class="line">    <span class="keyword">self</span>.move(numbers)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.login_successfully()<span class="symbol">:</span></span><br><span class="line">        cookies = <span class="keyword">self</span>.get_cookies()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'status'</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">'content'</span>: cookies</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="symbol">else:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'status'</span>: <span class="number">3</span>,</span><br><span class="line">            <span class="string">'content'</span>: <span class="string">' 登录失败 '</span></span><br><span class="line">        &#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里返回结果的类型是字典，并且附有状态码 status，在生成模块里我们可以根据不同的状态码做不同的处理。例如状态码为 1 的情况，表示成功获取 Cookies，我们只需要将 Cookies 保存到数据库即可。如状态码为 2 的情况，代表用户名或密码错误，那么我们就应该把当前数据库中存储的账号信息删除。如状态码为 3 的情况，则代表登录失败的一些错误，此时不能判断是否用户名或密码错误，也不能成功获取 Cookies，那么简单提示再进行下一个处理即可，类似代码实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = self.new_cookies(username, password)</span><br><span class="line"><span class="comment"># 成功获取</span></span><br><span class="line"><span class="keyword">if</span> result.<span class="builtin-name">get</span>(<span class="string">'status'</span>) == 1:</span><br><span class="line">    cookies = self.process_cookies(result.<span class="builtin-name">get</span>(<span class="string">'content'</span>))</span><br><span class="line">    <span class="builtin-name">print</span>(<span class="string">' 成功获取到 Cookies'</span>, cookies)</span><br><span class="line">    <span class="keyword">if</span> self.cookies_db.<span class="builtin-name">set</span>(username, json.dumps(cookies)):</span><br><span class="line">        <span class="builtin-name">print</span>(<span class="string">' 成功保存 Cookies'</span>)</span><br><span class="line"><span class="comment"># 密码错误，移除账号</span></span><br><span class="line">elif result.<span class="builtin-name">get</span>(<span class="string">'status'</span>) == 2:</span><br><span class="line">    <span class="builtin-name">print</span>(result.<span class="builtin-name">get</span>(<span class="string">'content'</span>))</span><br><span class="line">    <span class="keyword">if</span> self.accounts_db.delete(username):</span><br><span class="line">        <span class="builtin-name">print</span>(<span class="string">' 成功删除账号 '</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="builtin-name">print</span>(result.<span class="builtin-name">get</span>(<span class="string">'content'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果要扩展其他站点，只需要实现 new_cookies() 方法即可，然后按此处理规则返回对应的模拟登录结果，比如 1 代表获取成功，2 代表用户名或密码错误。 代码运行之后就会遍历一次尚未生成 Cookies 的账号，模拟登录生成新的 Cookies。</p>
                  <h4 id="检测模块"><a href="#检测模块" class="headerlink" title="检测模块"></a>检测模块</h4>
                  <p>我们现在可以用生成模块来生成 Cookies，但还是免不了 Cookies 失效的问题，例如时间太长导致 Cookies 失效，或者 Cookies 使用太频繁导致无法正常请求网页。如果遇到这样的 Cookies，我们肯定不能让它继续保存在数据库里。 所以我们还需要增加一个定时检测模块，它负责遍历池中的所有 Cookies，同时设置好对应的检测链接，我们用一个个 Cookies 去请求这个链接。如果请求成功，或者状态码合法，那么该 Cookies 有效；如果请求失败，或者无法获取正常的数据，比如直接跳回登录页面或者跳到验证页面，那么此 Cookies 无效，我们需要将该 Cookies 从数据库中移除。 此 Cookies 移除之后，刚才所说的生成模块就会检测到 Cookies 的 Hash 和账号的 Hash 相比少了此账号的 Cookies，生成模块就会认为这个账号还没生成 Cookies，那么就会用此账号重新登录，此账号的 Cookies 又被重新更新。 检测模块需要做的就是检测 Cookies 失效，然后将其从数据中移除。 为了实现通用可扩展性，我们首先定义一个检测器的父类，声明一些通用组件，实现如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValidTester</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, website=<span class="string">'default'</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.website = website</span><br><span class="line">        <span class="keyword">self</span>.cookies_db = RedisClient(<span class="string">'cookies'</span>, <span class="keyword">self</span>.website)</span><br><span class="line">        <span class="keyword">self</span>.accounts_db = RedisClient(<span class="string">'accounts'</span>, <span class="keyword">self</span>.website)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(<span class="keyword">self</span>, username, cookies)</span></span><span class="symbol">:</span></span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        cookies_groups = <span class="keyword">self</span>.cookies_db.all()</span><br><span class="line">        <span class="keyword">for</span> username, cookies <span class="keyword">in</span> cookies_groups.items()<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.test(username, cookies)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里定义了一个父类叫作 ValidTester，在<strong>init</strong>() 方法里指定好站点的名称 website，另外建立两个存储模块连接对象 cookies_db 和 accounts_db，分别负责操作 Cookies 和账号的 Hash，run() 方法是入口，在这里是遍历了所有的 Cookies，然后调用 test() 方法进行测试，在这里 test() 方法是没有实现的，也就是说我们需要写一个子类来重写这个 test() 方法，每个子类负责各自不同网站的检测，如检测微博的就可以定义为 WeiboValidTester，实现其独有的 test() 方法来检测微博的 Cookies 是否合法，然后做相应的处理，所以在这里我们还需要再加一个子类来继承这个 ValidTester，重写其 test() 方法，实现如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions import ConnectionError</span><br><span class="line"></span><br><span class="line">class WeiboValidTester(ValidTester):</span><br><span class="line">    def __init__(self, <span class="attribute">website</span>=<span class="string">'weibo'</span>):</span><br><span class="line">        ValidTester.__init__(self, website)</span><br><span class="line"></span><br><span class="line">    def test(self, username, cookies):</span><br><span class="line">        <span class="builtin-name">print</span>(<span class="string">' 正在测试 Cookies'</span>, <span class="string">' 用户名 '</span>, username)</span><br><span class="line">        try:</span><br><span class="line">            cookies = json.loads(cookies)</span><br><span class="line">        except TypeError:</span><br><span class="line">            <span class="builtin-name">print</span>(<span class="string">'Cookies 不合法 '</span>, username)</span><br><span class="line">            self.cookies_db.delete(username)</span><br><span class="line">            <span class="builtin-name">print</span>(<span class="string">' 删除 Cookies'</span>, username)</span><br><span class="line">            return</span><br><span class="line">        try:</span><br><span class="line">            test_url = TEST_URL_MAP[self.website]</span><br><span class="line">            response = requests.<span class="builtin-name">get</span>(test_url, <span class="attribute">cookies</span>=cookies, <span class="attribute">timeout</span>=5, <span class="attribute">allow_redirects</span>=<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == 200:</span><br><span class="line">                <span class="builtin-name">print</span>(<span class="string">'Cookies 有效 '</span>, username)</span><br><span class="line">                <span class="builtin-name">print</span>(<span class="string">' 部分测试结果 '</span>, response.text[0:50])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="builtin-name">print</span>(response.status_code, response.headers)</span><br><span class="line">                <span class="builtin-name">print</span>(<span class="string">'Cookies 失效 '</span>, username)</span><br><span class="line">                self.cookies_db.delete(username)</span><br><span class="line">                <span class="builtin-name">print</span>(<span class="string">' 删除 Cookies'</span>, username)</span><br><span class="line">        except ConnectionError as e:</span><br><span class="line">            <span class="builtin-name">print</span>(<span class="string">' 发生异常 '</span>, e.args)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>test() 方法首先将 Cookies 转化为字典，检测 Cookies 的格式，如果格式不正确，直接将其删除，如果格式没问题，那么就拿此 Cookies 请求被检测的 URL。test() 方法在这里检测微博，检测的 URL 可以是某个 Ajax 接口，为了实现可配置化，我们将测试 URL 也定义成字典，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">TEST_URL_MAP</span> = &#123;<span class="string">'weibo'</span>: <span class="string">'https://m.weibo.cn/'</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果要扩展其他站点，我们可以统一在字典里添加。对微博来说，我们用 Cookies 去请求目标站点，同时禁止重定向和设置超时时间，得到响应之后检测其返回状态码。如果直接返回 200 状态码，则 Cookies 有效，否则可能遇到了 302 跳转等情况，一般会跳转到登录页面，则 Cookies 已失效。如果 Cookies 失效，我们将其从 Cookies 的 Hash 里移除即可。</p>
                  <h4 id="接口模块"><a href="#接口模块" class="headerlink" title="接口模块"></a>接口模块</h4>
                  <p>生成模块和检测模块如果定时运行就可以完成 Cookies 实时检测和更新。但是 Cookies 最终还是需要给爬虫来用，同时一个 Cookies 池可供多个爬虫使用，所以我们还需要定义一个 Web 接口，爬虫访问此接口便可以取到随机的 Cookies。我们采用 Flask 来实现接口的搭建，代码如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, g</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment"># 生成模块的配置字典</span></span><br><span class="line">GENERATOR_MAP = &#123;<span class="string">'weibo'</span>: <span class="string">'WeiboCookiesGenerator'</span>&#125;</span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'&lt;h2&gt;Welcome to Cookie Pool System&lt;/h2&gt;'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> website <span class="keyword">in</span> GENERATOR_MAP:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(g, website):</span><br><span class="line">            setattr(g, website + <span class="string">'_cookies'</span>, eval(<span class="string">'RedisClient'</span> + <span class="string">'("cookies", "'</span> + website + <span class="string">'")'</span>))</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/&lt;website&gt;/random')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random</span><span class="params">(website)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取随机的 Cookie, 访问地址如 /weibo/random</span></span><br><span class="line"><span class="string">    :return: 随机 Cookie</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    g = get_conn()</span><br><span class="line">    cookies = getattr(g, website + <span class="string">'_cookies'</span>).random()</span><br><span class="line">    <span class="keyword">return</span> cookies</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们同样需要实现通用的配置来对接不同的站点，所以接口链接的第一个字段定义为站点名称，第二个字段定义为获取的方法，例如，/weibo/random 是获取微博的随机 Cookies，/zhihu/random 是获取知乎的随机 Cookies。</p>
                  <h4 id="调度模块"><a href="#调度模块" class="headerlink" title="调度模块"></a>调度模块</h4>
                  <p>最后，我们再加一个调度模块让这几个模块配合运行起来，主要的工作就是驱动几个模块定时运行，同时各个模块需要在不同进程上运行，实现如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">from</span> cookiespool.api <span class="keyword">import</span> app</span><br><span class="line"><span class="keyword">from</span> cookiespool.config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> cookiespool.generator <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> cookiespool.tester <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scheduler</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">valid_cookie</span><span class="params">(cycle=CYCLE)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">'Cookies 检测进程开始运行 '</span>)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> website, cls <span class="keyword">in</span> TESTER_MAP.items():</span><br><span class="line">                    tester = eval(cls + <span class="string">'(website="'</span> + website + <span class="string">'")'</span>)</span><br><span class="line">                    tester.run()</span><br><span class="line">                    print(<span class="string">'Cookies 检测完成 '</span>)</span><br><span class="line">                    <span class="keyword">del</span> tester</span><br><span class="line">                    time.sleep(cycle)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(e.args)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_cookie</span><span class="params">(cycle=CYCLE)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">'Cookies 生成进程开始运行 '</span>)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> website, cls <span class="keyword">in</span> GENERATOR_MAP.items():</span><br><span class="line">                    generator = eval(cls + <span class="string">'(website="'</span> + website + <span class="string">'")'</span>)</span><br><span class="line">                    generator.run()</span><br><span class="line">                    print(<span class="string">'Cookies 生成完成 '</span>)</span><br><span class="line">                    generator.close()</span><br><span class="line">                    time.sleep(cycle)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(e.args)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">api</span><span class="params">()</span>:</span></span><br><span class="line">        print(<span class="string">'API 接口开始运行 '</span>)</span><br><span class="line">        app.run(host=API_HOST, port=API_PORT)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> API_PROCESS:</span><br><span class="line">            api_process = Process(target=Scheduler.api)</span><br><span class="line">            api_process.start()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> GENERATOR_PROCESS:</span><br><span class="line">            generate_process = Process(target=Scheduler.generate_cookie)</span><br><span class="line">            generate_process.start()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> VALID_PROCESS:</span><br><span class="line">            valid_process = Process(target=Scheduler.valid_cookie)</span><br><span class="line">            valid_process.start()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里用到了两个重要的配置，即产生模块类和测试模块类的字典配置，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 产生模块类，如扩展其他站点，请在此配置</span></span><br><span class="line"><span class="attr">GENERATOR_MAP</span> = &#123;<span class="string">'weibo'</span>: <span class="string">'WeiboCookiesGenerator'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模块类，如扩展其他站点，请在此配置</span></span><br><span class="line"><span class="attr">TESTER_MAP</span> = &#123;<span class="string">'weibo'</span>: <span class="string">'WeiboValidTester'</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的配置是为了方便动态扩展使用的，键名为站点名称，键值为类名。如需要配置其他站点可以在字典中添加，如扩展知乎站点的产生模块，则可以配置成：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">GENERATOR_MAP</span> = &#123;</span><br><span class="line">    <span class="string">'weibo'</span>: <span class="string">'WeiboCookiesGenerator'</span>,</span><br><span class="line">    <span class="string">'zhihu'</span>: <span class="string">'ZhihuCookiesGenerator'</span>,</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Scheduler 里将字典进行遍历，同时利用 eval() 动态新建各个类的对象，调用其入口 run() 方法运行各个模块。同时，各个模块的多进程使用了 multiprocessing 中的 Process 类，调用其 start() 方法即可启动各个进程。 另外，各个模块还设有模块开关，我们可以在配置文件中自由设置开关的开启和关闭，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 产生模块开关</span></span><br><span class="line"><span class="attr">GENERATOR_PROCESS</span> = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 验证模块开关</span></span><br><span class="line"><span class="attr">VALID_PROCESS</span> = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 接口模块开关</span></span><br><span class="line"><span class="attr">API_PROCESS</span> = <span class="literal">True</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>定义为 True 即可开启该模块，定义为 False 即关闭此模块。 至此，我们的 Cookies 就全部完成了。接下来我们将模块同时开启，启动调度器，控制台类似输出如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">API</span> <span class="string">接口开始运行</span></span><br><span class="line"> <span class="string">*</span> <span class="string">Running</span> <span class="string">on</span> <span class="string">http://0.0.0.0:5000/</span> <span class="string">(Press</span> <span class="string">CTRL+C</span> <span class="string">to</span> <span class="string">quit)</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">生成进程开始运行</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">检测进程开始运行</span></span><br><span class="line"><span class="string">正在生成</span> <span class="string">Cookies</span> <span class="string">账号</span> <span class="number">14747223314</span> <span class="string">密码</span> <span class="string">asdf1129</span></span><br><span class="line"><span class="string">正在测试</span> <span class="string">Cookies</span> <span class="string">用户名</span> <span class="number">14747219309</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">有效</span> <span class="number">14747219309</span></span><br><span class="line"><span class="string">正在测试</span> <span class="string">Cookies</span> <span class="string">用户名</span> <span class="number">14740626332</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">有效</span> <span class="number">14740626332</span></span><br><span class="line"><span class="string">正在测试</span> <span class="string">Cookies</span> <span class="string">用户名</span> <span class="number">14740691419</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">有效</span> <span class="number">14740691419</span></span><br><span class="line"><span class="string">正在测试</span> <span class="string">Cookies</span> <span class="string">用户名</span> <span class="number">14740618009</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">有效</span> <span class="number">14740618009</span></span><br><span class="line"><span class="string">正在测试</span> <span class="string">Cookies</span> <span class="string">用户名</span> <span class="number">14740636046</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">有效</span> <span class="number">14740636046</span></span><br><span class="line"><span class="string">正在测试</span> <span class="string">Cookies</span> <span class="string">用户名</span> <span class="number">14747222472</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">有效</span> <span class="number">14747222472</span></span><br><span class="line"><span class="string">Cookies</span> <span class="string">检测完成</span></span><br><span class="line"><span class="string">验证码位置</span> <span class="number">420</span> <span class="number">580</span> <span class="number">384</span> <span class="number">544</span></span><br><span class="line"><span class="string">成功匹配</span></span><br><span class="line"><span class="string">拖动顺序</span> <span class="string">[1,</span> <span class="number">4</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">3</span><span class="string">]</span></span><br><span class="line"><span class="string">成功获取到</span> <span class="string">Cookies</span> <span class="string">&#123;'SUHB':</span> <span class="string">'08J77UIj4w5n_T'</span><span class="string">,</span> <span class="attr">'SCF':</span> <span class="string">'AimcUCUVvHjswSBmTswKh0g4kNj4K7_U9k57YzxbqFt4SFBhXq3Lx4YSNO9VuBV841BMHFIaH4ipnfqZnK7W6Qs.'</span><span class="string">,</span> <span class="attr">'SSOLoginState':</span> <span class="string">'1501439488'</span><span class="string">,</span> <span class="attr">'_T_WM':</span> <span class="string">'99b7d656220aeb9207b5db97743adc02'</span><span class="string">,</span> <span class="attr">'M_WEIBOCN_PARAMS':</span> <span class="string">'uicode%3D20000174'</span><span class="string">,</span> <span class="attr">'SUB':</span> <span class="string">'_2A250elZQDeRhGeBM6VAR8ifEzTuIHXVXhXoYrDV6PUJbkdBeLXTxkW17ZoYhhJ92N_RGCjmHpfv9TB8OJQ..'</span><span class="string">&#125;</span></span><br><span class="line"><span class="string">成功保存</span> <span class="string">Cookies</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上所示是程序运行的控制台输出内容，我们从中可以看到各个模块都正常启动，测试模块逐个测试 Cookies，生成模块获取尚未生成 Cookies 的账号的 Cookies，各个模块并行运行，互不干扰。 我们可以访问接口获取随机的 Cookies，如图 10-13 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-25-144223.jpg" alt=""> 图 10-13 接口页面 爬虫只需要请求该接口就可以实现随机 Cookies 的获取。</p>
                  <h3 id="5-本节代码"><a href="#5-本节代码" class="headerlink" title="5. 本节代码"></a>5. 本节代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/CookiesPool" target="_blank" rel="noopener">https://github.com/Python3WebSpider/CookiesPool</a>。</p>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>以上内容便是 Cookies 池的用法，后文中我们会利用该 Cookies 池和之前所讲的代理池来进行新浪微博的大规模爬取。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-26 11:44:50" itemprop="dateCreated datePublished" datetime="2019-11-26T11:44:50+08:00">2019-11-26</time>
                </span>
                <span id="/8243.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 10.2-Cookies 池的搭建" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>12k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>11 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8229.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8229.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 10.1-模拟登录并爬取 GitHub</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>我们先以一个最简单的实例来了解模拟登录后页面的抓取过程，其原理在于模拟登录后 Cookies 的维护。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本节将讲解以 GitHub 为例来实现模拟登录的过程，同时爬取登录后才可以访问的页面信息，如好友动态、个人信息等内容。 我们应该都听说过 GitHub，如果在我们在 Github 上关注了某些人，在登录之后就会看到他们最近的动态信息，比如他们最近收藏了哪个 Repository，创建了哪个组织，推送了哪些代码。但是退出登录之后，我们就无法再看到这些信息。 如果希望爬取 GitHub 上所关注人的最近动态，我们就需要模拟登录 GitHub。</p>
                  <h3 id="2-环境准备"><a href="#2-环境准备" class="headerlink" title="2. 环境准备"></a>2. 环境准备</h3>
                  <p>请确保已经安装好了 requests 和 lxml 库，如没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="3-分析登录过程"><a href="#3-分析登录过程" class="headerlink" title="3. 分析登录过程"></a>3. 分析登录过程</h3>
                  <p>首先要分析登录的过程，需要探究后台的登录请求是怎样发送的，登录之后又有怎样的处理过程。 如果已经登录 GitHub，先退出登录，同时清除 Cookies。 打开 GitHub 的登录页面，链接为 <a href="https://github.com/login，输入" target="_blank" rel="noopener">https://github.com/login，输入</a> GitHub 的用户名和密码，打开开发者工具，将 Preserve Log 选项勾选上，这表示显示持续日志，如图 10-1 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-24-153921.png" alt=""> 图 10-1 开发者工具设置 点击登录按钮，这时便会看到开发者工具下方显示了各个请求过程，如图 10-2 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-24-154042.jpg" alt=""> 图 10-2 请求过程 点击第一个请求，进入其详情页面，如图 10-3 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-24-154052.jpg" alt=""> 图 10-3 详情页面 可以看到请求的 URL 为 <a href="https://github.com/session，请求方式为" target="_blank" rel="noopener">https://github.com/session，请求方式为</a> POST。再往下看，我们观察到它的 Form Data 和 Headers 这两部分内容，如图 10-4 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-24-154058.jpg" alt=""> 图 10-4 详情页面 Headers 里面包含了 Cookies、Host、Origin、Referer、User-Agent 等信息。Form Data 包含了 5 个字段，commit 是固定的字符串 Sign in，utf8 是一个勾选字符，authenticity_token 较长，其初步判断是一个 Base64 加密的字符串，login 是登录的用户名，password 是登录的密码。 综上所述，我们现在无法直接构造的内容有 Cookies 和 authenticity_token。下面我们再来探寻一下这两部分内容如何获取。 在登录之前我们会访问到一个登录页面，此页面是通过 GET 形式访问的。输入用户名密码，点击登录按钮，浏览器发送这两部分信息，也就是说 Cookies 和 authenticity_token 一定是在访问登录页的时候设置的。 这时再退出登录，回到登录页，同时清空 Cookies，重新访问登录页，截获发生的请求，如图 10-5 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-24-154127.jpg" alt=""> 图 10-5 截获请求 访问登录页面的请求如图所示，Response Headers 有一个 Set-Cookie 字段。这就是设置 Cookies 的过程。 另外，我们发现 Response Headers 没有和 authenticity_token 相关的信息，所以可能 authenticity_token 还隐藏在其他的地方或者是计算出来的。我们再从网页的源码探寻，搜索相关字段，发现源代码里面隐藏着此信息，它是一个隐藏式表单元素，如图 10-6 所示。 <img src="https://cdn.cuiqingcai.com/2019-11-24-154124.jpg" alt=""> 图 10-6 表单元素 现在我们已经获取到所有信息，接下来实现模拟登录。</p>
                  <h3 id="4-代码实战"><a href="#4-代码实战" class="headerlink" title="4. 代码实战"></a>4. 代码实战</h3>
                  <p>首先我们定义一个 Login 类，初始化一些变量：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Login</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.headers = &#123;</span><br><span class="line">            <span class="string">'Referer'</span>: <span class="string">'https://github.com/'</span>,</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'</span>,</span><br><span class="line">            <span class="string">'Host'</span>: <span class="string">'github.com'</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">self</span>.login_url = <span class="string">'https://github.com/login'</span></span><br><span class="line">        <span class="keyword">self</span>.post_url = <span class="string">'https://github.com/session'</span></span><br><span class="line">        <span class="keyword">self</span>.logined_url = <span class="string">'https://github.com/settings/profile'</span></span><br><span class="line">        <span class="keyword">self</span>.session = requests.Session()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里最重要的一个变量就是 requests 库的 Session，它可以帮助我们维持一个会话，而且可以自动处理 Cookies，我们不用再去担心 Cookies 的问题。 接下来，访问登录页面要完成两件事：一是通过此页面获取初始的 Cookies，二是提取出 authenticity_token。 在这里我们实现一个 token() 方法，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">token</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    response = <span class="keyword">self</span>.session.get(<span class="keyword">self</span>.login_url, headers=<span class="keyword">self</span>.headers)</span><br><span class="line">    selector = etree.HTML(response.text)</span><br><span class="line">    token = selector.xpath(<span class="string">'//div/input[2]/@value'</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> token</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们用 Session 对象的 get() 方法访问 GitHub 的登录页面，然后用 XPath 解析出登录所需的 authenticity_token 信息并返回。 现在已经获取初始的 Cookies 和 authenticity_token，开始模拟登录，实现一个 login() 方法，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(<span class="keyword">self</span>, email, password)</span></span><span class="symbol">:</span></span><br><span class="line">    post_data = &#123;</span><br><span class="line">        <span class="string">'commit'</span>: <span class="string">'Sign in'</span>,</span><br><span class="line">        <span class="string">'utf8'</span>: <span class="string">'✓'</span>,</span><br><span class="line">        <span class="string">'authenticity_token'</span>: <span class="keyword">self</span>.token(),</span><br><span class="line">        <span class="string">'login'</span>: email,</span><br><span class="line">        <span class="string">'password'</span>: password</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    response = <span class="keyword">self</span>.session.post(<span class="keyword">self</span>.post_url, data=post_data, headers=<span class="keyword">self</span>.headers)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.dynamics(response.text)</span><br><span class="line"></span><br><span class="line">    response = <span class="keyword">self</span>.session.get(<span class="keyword">self</span>.logined_url, headers=<span class="keyword">self</span>.headers)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.profile(response.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先构造一个表单，复制各个字段，其中 email 和 password 是以变量的形式传递。然后再用 Session 对象的 post() 方法模拟登录即可。由于 requests 自动处理了重定向信息，我们登录成功后就可以直接跳转到首页，首页会显示所关注人的动态信息，得到响应之后我们用 dynamics() 方法来对其进行处理。接下来再用 Session 对象请求个人详情页，然后用 profile() 方法来处理个人详情页信息。 其中，dynamics() 方法和 profile() 方法的实现如下所示：</p>
                  <figure class="highlight julia">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def dynamics(self, html):</span><br><span class="line">    selector = etree.<span class="built_in">HTML</span>(html)</span><br><span class="line">    dynamics = selector.xpath('//div[contains(<span class="meta">@class</span>, <span class="string">"news"</span>)]//div[contains(<span class="meta">@class</span>, <span class="string">"alert"</span>)]')</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> dynamics:</span><br><span class="line">        dynamic = <span class="string">' '</span>.join(item.xpath('.//div[<span class="meta">@class</span>=<span class="string">"title"</span>]//text()')).strip()</span><br><span class="line">        print(dynamic)</span><br><span class="line"></span><br><span class="line">def profile(self, html):</span><br><span class="line">    selector = etree.<span class="built_in">HTML</span>(html)</span><br><span class="line">    name = selector.xpath('//input[<span class="meta">@id</span>=<span class="string">"user_profile_name"</span>]/<span class="meta">@value</span>')[<span class="number">0</span>]</span><br><span class="line">    email = selector.xpath('//select[<span class="meta">@id</span>=<span class="string">"user_profile_email"</span>]/option[<span class="meta">@value</span>!=<span class="string">""</span>]/text()')</span><br><span class="line">    print(name, email)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里，我们仍然使用 XPath 对信息进行提取。在 dynamics() 方法里，我们提取了所有的动态信息，然后将其遍历输出。在 prifile() 方法里，我们提取了个人的昵称和绑定的邮箱，然后将其输出。 这样，整个类的编写就完成了。</p>
                  <h3 id="5-运行"><a href="#5-运行" class="headerlink" title="5. 运行"></a>5. 运行</h3>
                  <p>我们新建一个 Login 对象，然后运行程序，如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">if</span> __name__ == "__main__":</span><br><span class="line">   <span class="keyword">login</span> = <span class="keyword">Login</span>()</span><br><span class="line">   <span class="keyword">login</span>.<span class="keyword">login</span>(email=<span class="string">'cqc@cuiqingcai.com'</span>, <span class="keyword">password</span>=<span class="string">'password'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 login() 方法传入用户名和密码，实现模拟登录。 可以看到控制台有类似如下输出：</p>
                  <figure class="highlight properties">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">GrahamCampbell</span>  <span class="string">starred  nunomaduro/zero-framework</span></span><br><span class="line"><span class="attr">GrahamCampbell</span>  <span class="string">starred  nunomaduro/laravel-zero</span></span><br><span class="line"><span class="attr">happyAnger6</span>  <span class="string">created repository  happyAnger6/nodejs_chatroom</span></span><br><span class="line"><span class="attr">viosey</span>  <span class="string">starred  nitely/Spirit</span></span><br><span class="line"><span class="attr">lbgws2</span>  <span class="string">starred  Germey/TaobaoMM</span></span><br><span class="line"><span class="attr">EasyChris</span>  <span class="string">starred  ageitgey/face_recognition</span></span><br><span class="line"><span class="attr">callmewhy</span>  <span class="string">starred  macmade/GitHubUpdates</span></span><br><span class="line"><span class="attr">sindresorhus</span>  <span class="string">starred  sholladay/squatter</span></span><br><span class="line"><span class="attr">SamyPesse</span>  <span class="string">starred  graphcool/chromeless</span></span><br><span class="line"><span class="attr">wbotelhos</span>  <span class="string">starred  tkadlec/grunt-perfbudget</span></span><br><span class="line"><span class="attr">wbotelhos</span>  <span class="string">created repository  wbotelhos/eggy</span></span><br><span class="line"><span class="attr">leohxj</span>  <span class="string">starred  MacGesture/MacGesture</span></span><br><span class="line"><span class="attr">GrahamCampbell</span>  <span class="string">starred  GrahamCampbell/Analyzer</span></span><br><span class="line"><span class="attr">EasyChris</span>  <span class="string">starred  golang/go</span></span><br><span class="line"><span class="attr">mitulgolakiya</span>  <span class="string">starred  veltman/flubber</span></span><br><span class="line"><span class="attr">liaoyuming</span>  <span class="string">pushed to  student  at  Germey/SecurityCourse</span></span><br><span class="line"><span class="attr">leohxj</span>  <span class="string">starred  jasonslyvia/a-cartoon-intro-to-redux-cn</span></span><br><span class="line"><span class="attr">ruanyf</span>  <span class="string">starred  ericchiang/pup</span></span><br><span class="line"><span class="attr">ruanyf</span>  <span class="string">starred  bpesquet/thejsway</span></span><br><span class="line"><span class="attr">louwailou</span>  <span class="string">forked  Germey/ScrapyTutorial  to  louwailou/ScrapyTutorial</span></span><br><span class="line"><span class="attr">Lving</span>  <span class="string">forked  shadowsocksr-backup/shadowsocksr  to  Lving/shadowsocksr</span></span><br><span class="line"><span class="attr">qifuren1985</span>  <span class="string">starred  Germey/ADSLProxyPool</span></span><br><span class="line"><span class="attr">QWp6t</span>  <span class="string">starred  laravel/framework</span></span><br><span class="line"><span class="attr">Germey</span> <span class="string">['1016903103@qq.com', 'cqc@cuiqingcai.com']</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，我们成功获取到关注的人的动态信息和个人的昵称及绑定邮箱。模拟登录成功！</p>
                  <h3 id="6-本节代码"><a href="#6-本节代码" class="headerlink" title="6. 本节代码"></a>6. 本节代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/GithubLogin" target="_blank" rel="noopener">https://github.com/Python3WebSpider/GithubLogin</a>。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>我们利用 requests 的 Session 实现了模拟登录操作，其中最重要的还是分析思路，只要各个参数都成功获取，那么模拟登录是没有问题的。 登录成功，这就相当于建立了一个 Session 会话，Session 对象维护着 Cookies 的信息，直接请求就会得到模拟登录成功后的页面。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-24 23:45:25" itemprop="dateCreated datePublished" datetime="2019-11-24T23:45:25+08:00">2019-11-24</time>
                </span>
                <span id="/8229.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 10.1-模拟登录并爬取 GitHub" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8155.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8155.html" class="post-title-link" itemprop="url">如何分离 Git 子目录</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>最近工作遇到了一个问题。对我们公司的开发小组来说，整个小组的人员都在一个 Repository 下面协作，这个 Repository 里面的文件夹非常多，而我只负责其中的一个功能的开发，我开发的功能所在的文件夹是可以独立维护的，它不依赖于 Repository 里面的其他的任何一个文件夹。</p>
                  <p>现在我新招到了一位实习生，会跟我一同做这个功能。但很尴尬的是，原则上来说实习生是不能有整个 Repository 的权限的，因为其他的文件夹下可能有包含一些关键信息，那我又怎么把我的这一部分的代码共享给他呢？</p>
                  <p>有的小伙伴可能说可以用软连接，但是用软连接的话实际上是不行的，因为 git 在 commit 软连接的时候会把它当成文件对待的。</p>
                  <p>比如说我有一个文件夹啊，我创建了一个软连接到这个文件夹，创建的链接文件实际上是不能以文件夹的形式提交到 Git 仓库的。</p>
                  <p>那么怎么办呢？硬链接就好了。</p>
                  <p>我使用的是 Mac OS 系统，可选的方案有 hln、bindfs，但前者是不能链接文件夹的。</p>
                  <p>一个比较可行的方案就是使用 bindfs，安装方法如下：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">brew </span><span class="keyword">install </span><span class="keyword">bindfs</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后使用如下命令即可：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">bindfs </span>source target</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的话，比如我大库里面有个文件夹，名字叫做 foo，我就可以在我其他的目录下创建一个对该目录的挂载点 bar。</p>
                  <figure class="highlight awk">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">bindfs <span class="regexp">/var/</span>project1<span class="regexp">/foo /</span>var<span class="regexp">/project2/</span>bar</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我在 project1 下修改 foo 文件夹下的内容，project2 下的 bar 文件夹下的内容也会跟着修改了，我只需要把想要链接的文件夹都放在 project2 下，project2 作为一个独立的 Git 仓库，实习生只能看到我分离出来的内容，看不到大库 project1 下的内容。</p>
                  <p>这样如果实习生更新了 project2 的 bar 文件夹，提交到了 project2 对应的 Git 仓库，我从上面 pull 下代码，这样 project1 里面的 foo 文件夹也会跟着更新了，这样我再把新的改动提交到 project1 即可。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-18 20:32:59" itemprop="dateCreated datePublished" datetime="2019-11-18T20:32:59+08:00">2019-11-18</time>
                </span>
                <span id="/8155.html" class="post-meta-item leancloud_visitors" data-flag-title="如何分离 Git 子目录" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>830</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8063.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8063.html" class="post-title-link" itemprop="url">利用深度学习识别验证码缺口</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>做爬虫的同学肯定或多或少会为验证码苦恼过，在最初的时候，大部分验证码都是图形验证码。但是前几年「极验」验证码横空出世，行为验证码变得越来越流行，其中之一的形式便是滑块验证码。 滑块验证码是怎样的呢？如图所示，验证码是一张矩形图，图片左侧会出现一个滑块，右侧会出现一个缺口，下侧会出现一个滑轨。左侧的滑块会随着滑轨的拖动而移动，如果能将左侧滑块正好滑动到右侧缺口处，就算完成了验证。 <img src="https://cdn.cuiqingcai.com/2019-11-06-183053.png" alt="image-20191107023051548"> 由于这种验证码交互形式比较友好，且安全性、美观度上也会更高，像这种类似的验证码也变得越来越流行。另外不仅仅是「极验」，其他很多验证码服务商也推出了类似的验证码服务，如「网易易盾」等，上图所示的就是「网易易盾」的滑动验证码。 没错，确实这种滑动验证码的出现让很多网站变得更安全。但是做爬虫的可就苦恼了，如果采用自动化的方法来绕过这种滑动验证码，关键部分在于以下两点：</p>
                  <ul>
                    <li>找出目标缺口的位置。</li>
                    <li>模拟人的滑动轨迹将滑块滑动到缺口处。</li>
                  </ul>
                  <p>那么问题来了，第一步怎么做呢？ 我们怎么识别目标缺口到底在图片的哪个地方？大家可能想到的答案有：</p>
                  <ul>
                    <li>直接手工一把梭。</li>
                    <li>利用图像处理算法检测缺口处特征。</li>
                    <li>对接打码平台，获取缺口位置。</li>
                  </ul>
                  <p>另外对于极验来说，之前还有一种方法来识别缺口，那就是对比原图和缺口图的不同之处，通过遍历像素点来找出缺口的位置，但这种方法就比较投机了。如果换家验证码服务商，不给我们原图，我们就无从比较计算了。 总之，我们的目标就是输入一张图，输出缺口的的位置。 上面的方法呢，要么费时费钱、要么准确率不高。那还有没有其他的解决方案呢？ 当然有。 现在深度学习这么火，基于深度学习的图像识别技术已经发展得比较成熟了。那么我们能不能利用它来识别缺口位置呢？ 答案是，没问题，我们只需要将这个问题归结成一个深度学习的「目标检测」问题就好了。 听到这里，现在可能有的同学已经望而却步了，深度学习？我一点基础都没有呀，咋办？ 不用担心，本节介绍的内容全程没有一行代码，不需要任何深度学习基础，我们只需要动动手点一点就能搭建一个识别验证码缺口的深度学习的模型。 这么神奇？是的，那么本节我就带大家来实现一下吧。</p>
                  <h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2>
                  <p>首先在开始之前简单说下目标检测。什么叫目标检测？顾名思义，就是把我们想找的东西找出来。比如给一张「狗」的图片，如图所示： <img src="https://cdn.cuiqingcai.com/2019-11-06-184842.png" alt="image-20191107024841075"> 我们想知道这只狗在哪，它的舌头在哪，找到了就把它们框选出来，这就是目标检测。 经过目标检测算法处理之后，我们期望得到的图片是这样的： <img src="https://cdn.cuiqingcai.com/2019-11-06-185010.png" alt="image-20191107025008947"> 可以看到这只狗和它的舌头就被框选出来了，这就完成了一个不错的目标检测。 现在比较流行的目标检测算法有 R-CNN、Fast R-CNN、Faster R-CNN、SSD、YOLO 等，感兴趣同学的可以了解一下，当然看不懂也没有什么影响。 另外再提一个地方，不懂深度学习的同学可以看看，懂的直接跳过下面一段。 我们既然要搭建一个模型来实现一个目标检测算法，那模型怎么知道我们究竟想识别个什么东西？就比如上图，模型咋知道我们想识别的是狗而不是草，是舌头而不是鼻子。这是因为，既然叫深度学习，那得有学习的东西。所以，搭建一个深度学习模型需要训练数据。啥也不告诉模型，模型从哪里去学习？所以，我们得预先有一些标注好位置的图片供模型去学习（训练），比如准备好多张狗的图片和狗的轮廓标注位置，模型在训练过程中会自动学习到图片和标注位置的关系。模型训练好了之后，我们给模型一个没有见过的类似的狗的图，模型也能找出来目标的位置了。 所以，迁移到验证码缺口识别这个任务上来，我们第一步就是给模型提供一些训练数据，训练数据就包括验证码的图片和缺口的位置标注轮廓信息。 好，既然如此，我们第一步就得准备一批验证码数据供标注和训练了。</p>
                  <h2 id="准备训练数据"><a href="#准备训练数据" class="headerlink" title="准备训练数据"></a>准备训练数据</h2>
                  <p>这里我用的是网易易盾的验证码，链接为：<a href="http://dun.163.com/trial/jigsaw" target="_blank" rel="noopener">http://dun.163.com/trial/jigsaw</a>。 我写爬虫爬下来了一些验证码的图，具体怎么爬的就不再赘述了，简单粗暴直接干就行了。 爬下来的验证码图类似这样子： <img src="https://cdn.cuiqingcai.com/2019-11-06-190724.png" alt="image-20191107030722603"> 我们不需要滑轨的部分，只保留验证码本身的图片和上面的两个缺口就行了，下面是我准备的一些验证码图： <img src="https://cdn.cuiqingcai.com/2019-11-06-190827.png" alt="image-20191107030825681"> 我爬了大约上千张吧，越多越好。当然对于今天的任务来说，其实几十上百张已经就够了。</p>
                  <h2 id="标注缺口位置"><a href="#标注缺口位置" class="headerlink" title="标注缺口位置"></a>标注缺口位置</h2>
                  <p>下一步就是把缺口的位置标注出来了。想一想这一步又不太好办，我难道还得每张图片量一量吗？这费了劲了，那咋整啊？ 很多同学可能到了这一步就望而却步了，更别提后面的搭建模型训练了。 但我们在文章开头说了，我们不需要写一行代码，点一点就能把模型搭建好。怎么做到的呢？我们可以借助于一些平台和工具。 在这里就要请出今天的主角—— ModelArts 了，这是我发现的华为云的一个深度学习平台，借助它我们可以完成数据标注、模型训练、模型部署三个步骤，最重要的是，我们不需要写代码，只需要点来点去就可以完成了。 让我们进入 ModelArts 来看看： <img src="https://cdn.cuiqingcai.com/2019-11-06-191803.png" alt="image-20191107031802815"> 它已经内置了一些深度学习模型，包括图像分类、物体检测、预测分析等等，我们可以直接利用它们来快速搭建属于自己的模型。 在这里我们就切换到「自动学习」的选项卡，创建一个物体检测的项目。 <img src="https://cdn.cuiqingcai.com/2019-11-06-192043.png" alt="image-20191107032040036"> 进入项目里面，可以看到最上面会显示三个步骤：</p>
                  <ul>
                    <li>数据标注</li>
                    <li>模型训练</li>
                    <li>部署上线</li>
                  </ul>
                  <p>也就是说，经过这三步，我们就可以搭建和部署一个深度学习模型。 页面如图所示： <img src="https://cdn.cuiqingcai.com/2019-11-06-192250.png" alt="image-20191107032248156"> 那我们先来第一步——数据标注，这里我把一些验证码的图上传到页面中，在这里我上传了 112 张图： <img src="https://cdn.cuiqingcai.com/2019-11-06-192409.png" alt="image-20191107032407896"> 上传完毕之后我们可以点击每一张图片进行标注了，这个平台提供了非常方便的标注功能，只需要鼠标拖拽个轮廓就完成了，112 张图标注完也就几分钟，标注的时候就框选这么个轮廓就行了，如图所示： <img src="https://cdn.cuiqingcai.com/2019-11-06-192558.png" alt="image-20191107032556453"> 在这里边界需要把整个缺口的图全框选出来，其中上边界和右边界和标注框相切即可，总之确保标注框正好把缺口图框选出来就行，平台会自动保存和记录标注的像素点位置。 标注完一个，它会提示要添加一个名字，我在这里添加的名字叫「边界」，可以随意指定。 等全部标注完毕，点击「保存并返回」按钮即可。</p>
                  <h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2>
                  <p>好，标注完了我们就可以开始训练了。我们在这里不需要写任何的代码，因为平台已经给我们写好了，内置了目标检测的深度学习模型，我们只需要提供数据训练就行了，如图所示： <img src="https://cdn.cuiqingcai.com/2019-11-06-193006.png" alt="image-20191107033005181"> 在这里，我们只需要设置一下「最大训练时长」就好了，这么点图片其实几分钟就能训练完了，「最大训练时长」随意填写即可，最小不小于 0.05，填写完了之后就可以点击「开始训练」按钮训练了。 等几分钟，就会训练完成了，可以看到类似如图的页面： <img src="https://cdn.cuiqingcai.com/2019-11-06-193212.png" alt="image-20191107033211474"> 这里显示了模型的各个参数和指标。 是的，你没看错，我们没有写任何代码，只过了几分钟，模型就已经训练完，并且可以部署上线了。</p>
                  <h2 id="部署测试"><a href="#部署测试" class="headerlink" title="部署测试"></a>部署测试</h2>
                  <p>然后进行下一步，部署上线，直接点击左上角的部署按钮即可： <img src="https://cdn.cuiqingcai.com/2019-11-06-193412.png" alt="image-20191107033411530"> 过一会儿， 部署成功之后便可以看到类似这样的界面： <img src="https://cdn.cuiqingcai.com/2019-11-06-193447.png" alt="image-20191107033446107"> 在这里我们可以上传任意的验证码图片进行测试，比如我随意上传一张没有标注过的验证码图，然后它会给我们展示出预测结果，如图所示： <img src="https://cdn.cuiqingcai.com/2019-11-06-193909.png" alt="image-20191107033907756"> 可以看到，它就把缺口的位置检测出来了，同时在右侧显示了具体的像素值和置信度：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"detection_classes"</span>: [</span><br><span class="line">        <span class="string">"边界"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"detection_boxes"</span>: [</span><br><span class="line">        [</span><br><span class="line">            <span class="number">16.579784393310547</span>,</span><br><span class="line">            <span class="number">331.89569091796875</span>,</span><br><span class="line">            <span class="number">124.46369934082031</span>,</span><br><span class="line">            <span class="number">435.0449523925781</span></span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"detection_scores"</span>: [</span><br><span class="line">        <span class="number">0.9999219179153442</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>是的，检测的结果还是比较准确的。有了这个结果，我们下一步模拟滑动到标注结果的左边界位置就好了！具体的模拟过程这里就不展开讲了。 另外平台同时还提供了模型部署后的 URL 接口和接口调用指南，也就是我们只需要向接口上传任意的验证码图片，就可以得到缺口的位置了！调用方式可以见：<a href="https://support.huaweicloud.com/engineers-modelarts/modelarts_23_0063.html" target="_blank" rel="noopener">https://support.huaweicloud.com/engineers-modelarts/modelarts_23_0063.html</a>。 嗯，就是这样，我们通过非常简单的操作，不需要任何代码，几分钟就搭建了一个深度学习模型，准确率也还不错。 当然这里我们只标注了 100 多张，标注得越多，标注得越精确，模型的准确率也会越高的。 以上便是利用 ModelArts 搭建滑动验证码缺口识别模型的方法，十分简洁高效。大家感兴趣可以了解下 ModelArts：<a href="https://www.huaweicloud.com/product/modelarts.html" target="_blank" rel="noopener">https://www.huaweicloud.com/product/modelarts.html</a>。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-08 18:38:23" itemprop="dateCreated datePublished" datetime="2019-11-08T18:38:23+08:00">2019-11-08</time>
                </span>
                <span id="/8063.html" class="post-meta-item leancloud_visitors" data-flag-title="利用深度学习识别验证码缺口" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.4k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8012.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8012.html" class="post-title-link" itemprop="url">实战！手把手带你搭建图像分类 AI 服务</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>人工智能技术（以下称 AI）是人类优秀的发现和创造之一，它代表着至少几十年的未来。在传统的编程中，工程师将自己的想法和业务变成代码，计算机会根据代码设定的逻辑运行。与之不同的是，AI 使计算机有了「属于自己的思想」，它就像生物一样，能够「看」、「听」、「说」、「动」、「理解」、「分辨」和「思考」。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krv9b06uj309q0a2gma.jpg" alt=""> AI 在图像识别和文本处理方面的效果尤为突出，且已经应用到人类的生活中，例如人脸识别、对话、车牌识别、城市智慧大脑项目中的目标检测和目标分类等。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks0uuxckj30fu09mt97.jpg" alt=""> <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krzm9n5pj30zq0ee0x3.jpg" alt=""> 接下来，我们将了解图像分类的需求、完成任务的前提条件和任务实践。</p>
                  <h2 id="图像分类以及目标检测的需求"><a href="#图像分类以及目标检测的需求" class="headerlink" title="图像分类以及目标检测的需求"></a>图像分类以及目标检测的需求</h2>
                  <p>AI 的能力和应用都非常广泛，这里我们主要讨论的是图像分类。 图像分类，其实是对图像中主要目标的识别和归类。例如在很多张随机图片中分辨出哪一张中有直升飞机、哪一张中有狗。或者给定一张图片，让计算机分辨图像中主要目标的类别。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krb83b8aj31800osth4.jpg" alt=""> 目标检测，指的是检测目标在图片中的位置。例如智慧交通项目中，路面监控摄像头拍摄画面中车辆的位置。目标检测涉及两种技术：分类和定位。也就是说先判定图片中是否存在指定的目标，然后还需要确定目标在图片中的位置。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks2pv2iwj31tq0s8gsj.jpg" alt=""> 这样的技术将会应用在人脸识别打卡、视频监控警报、停车场、高速收费站和城市智慧交通等项目当中。</p>
                  <h2 id="计算机识图的步骤"><a href="#计算机识图的步骤" class="headerlink" title="计算机识图的步骤"></a>计算机识图的步骤</h2>
                  <p>我们可以将计算机的看作是一个小朋友，它在拥有「分辨」的能力之前，必须经历「看」和「认识」这两个步骤，在看过很多图片后，它就会形成自己的「认知」，也就是获得了「分辨」能力。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks3o2wyfj30h209ajsx.jpg" alt=""> 简单来说，AI 工程师必须准备很多张不同的图片，并且将一大部分图片中的目标标注出来，然后让计算机提取每张图片中的特征，最后就会形成「认知」。 想一想，你还小的时候，是如何分辨鸭子和鹅的呢？ <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks4nckjyj30qq08aacn.jpg" alt=""> 是不是根据它们的特征进行判断的？</p>
                  <h2 id="学习和编程实现任务需要的条件"><a href="#学习和编程实现任务需要的条件" class="headerlink" title="学习和编程实现任务需要的条件"></a>学习和编程实现任务需要的条件</h2>
                  <p>了解完需求和步骤之后，我们还需要准备一些条件：</p>
                  <ul>
                    <li>首先，你必须是一名 IT 工程师。</li>
                    <li>然后你有一定的数学和统计学习基础。</li>
                    <li>你还得了解计算机处理图像的方式。</li>
                    <li>如果图片较多，你需要一台拥有较高算力 GPU 的计算机，否则计算机的「学习」速度会非常慢。</li>
                  </ul>
                  <p>具备以上条件后，再通过短时间（几天或一周）的学习，我们就能够完成图像分类的任务。 讨论个额外的话题，人人都能够做 AI 工程师吗？ AI 的门槛是比较高的，首先得具备高等数学、统计学习和编程等基础，然后要有很强的学习能力。对于 IT 工程师来说：</p>
                  <ul>
                    <li>编程基础是没有问题的</li>
                    <li>学习能力看个人，但花时间、下功夫肯定会有进步</li>
                    <li>高等数学基础，得好好补</li>
                    <li>统计学习基础，也得好好补</li>
                    <li>经济上无压力</li>
                  </ul>
                  <p>如果你想要成为一名 AI 工程师，那么「高学历」几乎是必备的。无论是一线互联网企业或者新崛起的 AI 独角兽，它们为 AI 工程师设立的学历门槛都是「硕士」。除非特别优秀的、才华横溢的大专或本科生，否则是不可能有机会进入这样的企业做 AI 工程师的。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks5vlpo0j30ek088gm9.jpg" alt=""> AI 在硬件、软件、数据资料和人才方面都是很费钱的，普通的 IT 工程师也就是学习了解一下，远远达不到产品商用的要求。 普通的中小企业，极少有资质和经济能力吸引高学历且优秀的 AI 工程师，这就导致了资源的聚拢和倾斜。 想要将图像分类技术商用，在让计算机经历「看」、「认识」的步骤并拥有「分辨」能力后，还要将其转换为 Web 服务。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks6xmjhfj316y0godlw.jpg" alt=""> 但我只想将人脸识别或者图像分类的功能集成到我的项目当中，就那么困难吗？ 我只是一个很小的企业，想要在原来普通的视频监控系统中增加「家人识别」、「陌生人警报」、「火灾警报」和「生物闯入提醒」等功能，没有上述的条件和经济投入，就不能实现了吗？ 我好苦恼！ 有什么好办法吗？</p>
                  <h2 id="ModelArts-简介和条件"><a href="#ModelArts-简介和条件" class="headerlink" title="ModelArts 简介和条件"></a>ModelArts 简介和条件</h2>
                  <p>ModelArts 是华为云推出的产品，它是面向开发者的一站式 AI 开发平台。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks7ujxpwj31to0b440n.jpg" alt=""> 它为机器学习与深度学习提供海量数据预处理及半自动化标注、大规模分布式 Training、自动化模型生成，及端-边-云模型按需部署能力，帮助用户快速创建和部署模型，管理全周期 AI 工作流。 它为用户提供了以下可选模式：</p>
                  <ul>
                    <li>零编码经验、零 AI 经验的自动学习模式</li>
                    <li>有 AI 研发经验的全流程开发模式</li>
                  </ul>
                  <p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ksaaafekj31ve0u0gu1.jpg" alt=""> <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks9j6iguj31qe0dsdi2.jpg" alt=""> 同时，它将 AI 开发的整个过程都集成了进来。例如数据标注、模型训练、参数优化、服务部署、开放接口等，这就是「全周期 AI 工作流」。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks8i8yrpj31kv0u0wn2.jpg" alt=""> 还有，平台上的操作都是可视化的。 这些条件对于想要将 AI 技术应用于产品，但无奈条件不佳的个人开发者和企业提供了机会，这很重要！可以说 <a href="[https://www.huaweicloud.com/product/modelarts.html](https://www.huaweicloud.com/product/modelarts.html">ModelArts</a>) 缩短了 AI 商用的时间，降低了对应的经济成本、时间成本和人力成本。 更贴心的是，华为云 <a href="[https://www.huaweicloud.com/product/modelarts.html](https://www.huaweicloud.com/product/modelarts.html">ModelArts</a>) 为用户准备了很多的教程。即使用户没有经验，但只要按照教程指引进行操作，也能够实现自己的 AI 需求。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ks907u59j31m90u0468.jpg" alt=""> 想想就美滋滋，太棒了！ 赶紧体验一下！</p>
                  <h2 id="图像分类服务实践"><a href="#图像分类服务实践" class="headerlink" title="图像分类服务实践"></a>图像分类服务实践</h2>
                  <p>这次我们以零 AI 基础和零编码经验的自动学习模式演示如何搭建一个图像分类的 AI 服务。</p>
                  <h3 id="前期准备和相关设置"><a href="#前期准备和相关设置" class="headerlink" title="前期准备和相关设置"></a>前期准备和相关设置</h3>
                  <p>首先打开华为云官网，将鼠标移动导航栏的「EI 企业智能」菜单上，并在弹出的选项中选择「AI 开发平台 ModelArts」。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kr80qn02j31so0u045b.jpg" alt=""> 进入到 <a href="[https://www.huaweicloud.com/product/modelarts.html](https://www.huaweicloud.com/product/modelarts.html">ModelArts</a>) 主页后，可以浏览一下关于 <a href="[https://www.huaweicloud.com/product/modelarts.html](https://www.huaweicloud.com/product/modelarts.html">ModelArts</a>) 的介绍。 点击 Banner 处的「进入控制台」按钮，页面会跳转到 ModelArts 控制台。控制台大体分为几个区域： <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kr8u1cu5j31g90u0aqf.jpg" alt=""> 区域 2 自动学习模式中有图像分类，将鼠标移动到图标上，并点击弹出的「开始体验」按钮。如果是华为云的新用户，网页会提示我们输入访问密钥和私有访问密钥。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kr948a21j312s0jktaw.jpg" alt=""> 没有密钥的开发者可以点击页面给出的链接并按照指引获取密钥，得到两种密钥后将其填入框中，点击「确定」按钮即可。 此时正式进入项目创建流程中，点击「图像分类」中的「创建项目」按钮（华为云为用户准备了对应的教程，很贴心）。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kr9gpk16j31iu0iyn0j.jpg" alt=""> 在创建项目的页面中，我们需要填两三项配置。要注意的是，项目是按需计费的，这次我们只是体验，也没有训练和存储太多数据，所以费用很低，大家不用担心。 项目名称可以根据需求设定一个容易记的，案例中我将其设定为 ImageCLF-Test-Pro。在训练数据的存储选择处，点击输入框中的文件夹图标，在弹出的选项卡中新建 obs 桶 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kr9o03hfj321o0m4aeb.jpg" alt=""> 并在创建的桶中新建文件夹 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kra1rz5jj32200ogn26.jpg" alt=""> 最后输入描述，并点击页面右下角的「创建项目」按钮即可。</p>
                  <h3 id="上传图片和标注"><a href="#上传图片和标注" class="headerlink" title="上传图片和标注"></a>上传图片和标注</h3>
                  <p>项目创建好之后，我们需要准备用于训练的多张图片，图片尽量清晰、种类超过 2 类、每种分类的图片数量不少于 5 张。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kranz44wj31wm0u0n1w.jpg" alt=""> 当然，数据越多、形态越丰富、标注越准确，那么训练结果就会越好，AI 服务的体验就会越好。 这里我准备了一些直升机、坦克和狗的图片，共 45 张。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krb83b8aj31800osth4.jpg" alt=""> 将其批量导入后勾选同类型的图片，一次性为多张图添加标签。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krbj4ng2j31p90u0qgn.jpg" alt=""> 依次将 3 类图片标注后，左侧图片标注的「未标注」选项卡中的图就会清空，而「已标注」选项卡中可以看到标注好的图片。</p>
                  <h3 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h3>
                  <p>右侧的标签栏会显示每种分类和对应的图片数量，下方的训练设置可以让我们设置训练时长的上限，高级设置中还有推理时间。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krbt7olrj30jc0nct9q.jpg" alt=""> 这个我们不必理解它的作用，可以按照默认值进行，也可以稍微调整，例如将训练时长的上限改为 0.2。</p>
                  <h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3>
                  <p>设置好后点击「开始训练」按钮就会进入训练状态，耐心等待一段时间（图片越少训练时间越短）。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krc7gotwj30m207sdgg.jpg" alt=""> 训练页左侧会显示训练状态，例如初始化、运行中和运行成功/失败等。训练完成后，右侧会给出运行时长、准确率、评估结果和训练参数等信息。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krcjd7aoj31bs0u0aen.jpg" alt=""></p>
                  <h3 id="服务的自动化部署"><a href="#服务的自动化部署" class="headerlink" title="服务的自动化部署"></a>服务的自动化部署</h3>
                  <p>我们的目的是搭建一个图像分类的 AI 服务，所以在训练结束后点击左侧的「部署」按钮，此时会进入自动化部署的流程。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krd9a59rj31820u078z.jpg" alt=""> 稍微等待些许时间（本次约 10 分钟）后，页面提示部署完成，同时页面将会分为 3 栏。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kre5gknwj31le0u00z8.jpg" alt=""> 左侧 1 区为部署状态和控制。中间 2 区可以在线测试图片分类，右侧 3 区会显示在线测试的结果（包括准确率），右侧 4 区提供了 API 接口，方便我们将其集成到 Web 应用当中。</p>
                  <h3 id="在线预测，训练结果测试"><a href="#在线预测，训练结果测试" class="headerlink" title="在线预测，训练结果测试"></a>在线预测，训练结果测试</h3>
                  <p>我们来测试一下，准备几张没有经过标注的图片，图片中可以包含狗、直升机和坦克。点击中间 2 区的「上传」按钮并选择一张图片，然后点击「预测」按钮。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kredfp9pj314d0u0101.jpg" alt=""> 1 秒中不到，右侧 3 区就会返回本次预测的结果：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"predicted_label"</span>: <span class="string">"狗"</span>,</span><br><span class="line">    <span class="attr">"scores"</span>: [</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"狗"</span>,</span><br><span class="line">            <span class="string">"0.840"</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"直升机"</span>,</span><br><span class="line">            <span class="string">"0.084"</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"坦克"</span>,</span><br><span class="line">            <span class="string">"0.076"</span></span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次我们上传的是包含狗的图片，返回的预测结果中显示本次预测的标签是「狗」，并且列出了可信度较高的几个类别和对应的可信度（1 为 100% 肯定），其中最高的是 「0.840-狗」。 这次上传直升机的图片试试。 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kreijel6j314m0u0qb3.jpg" alt=""> 返回的预测结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"predicted_label"</span>: <span class="string">"直升机"</span>,</span><br><span class="line">    <span class="attr">"scores"</span>: [</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"直升机"</span>,</span><br><span class="line">            <span class="string">"0.810"</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"狗"</span>,</span><br><span class="line">            <span class="string">"0.114"</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"坦克"</span>,</span><br><span class="line">            <span class="string">"0.075"</span></span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>再试试坦克 <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8kreq9m0oj31510u0qbd.jpg" alt=""> 返回的预测结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"predicted_label"</span>: <span class="string">"坦克"</span>,</span><br><span class="line">    <span class="attr">"scores"</span>: [</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"坦克"</span>,</span><br><span class="line">            <span class="string">"0.818"</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"狗"</span>,</span><br><span class="line">            <span class="string">"0.092"</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">"直升机"</span>,</span><br><span class="line">            <span class="string">"0.090"</span></span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>从几次测试的结果可以看出，预测的结果非常准确，而且给出的可信度也比较高。这次准备的图片并不是很多，形态也不是很丰富，但预测效果却非常好，不得不说华为云 ModelArts 开发团队为此做了很多的优化，甚至比我自己（深度学习入门水平）编写代码用卷积神经网络训练和预测的结果要好。 如果想要将其集成到 Web 应用中，只需要根据页面给出的「接口调用指南」的指引进行操作即可。</p>
                  <h3 id="释放资源"><a href="#释放资源" class="headerlink" title="释放资源"></a>释放资源</h3>
                  <p>如果不是真正商用，仅仅作为学习和练习，那么在操作完成后记得点击左侧 1 区的「停止」按钮。然后在华为云导航栏中的搜索框输入「OBS」，点击搜索结果后跳转到 OBS 主页，接着再 OBS 主页点击「管理控制台」，进入到 OBS 控制台中，删除之前创建的桶即可。这样就不会导致资源占用，也不会产生费用了。</p>
                  <h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3>
                  <p>体验了一下 ModelArts，我感觉非常奈斯！ 每处都有提示或教程指引，操作过程流畅，没有出现卡顿、报错等问题。 批量数据标注太好用了！批量导入、批量标注，自动计数，舒服！ 训练速度很快，应该是用了云 GPU，这样就算我的电脑没有显卡也能够快速完成训练。 以前还在考虑，学习 AI 是否需要准备更强的硬件设备，现在好了，在 ModelArts 上操作，就不用考虑这些条件了。 本次我们体验的是自动学习，也就是简洁易用的傻瓜式操作。对于专业的 AI 工程师来说，可以选择全流程开发模式。批量数据标注、本地代码编写、本地调试、云端训练、云端部署等一气呵成。 棒！ 有兴趣的开发者可以前往华为云 <a href="[https://www.huaweicloud.com/product/modelarts.html](https://www.huaweicloud.com/product/modelarts.html">ModelArts</a>) 体验。</p>
                  <hr>
                  <p>备注：文中配图均出自互联网，通过搜索引擎而来。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/韦世东学算法和反爬虫" class="author" itemprop="url" rel="index">韦世东学算法和反爬虫</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-11-05 19:56:04" itemprop="dateCreated datePublished" datetime="2019-11-05T19:56:04+08:00">2019-11-05</time>
                </span>
                <span id="/8012.html" class="post-meta-item leancloud_visitors" data-flag-title="实战！手把手带你搭建图像分类 AI 服务" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.3k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/7844.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/7844.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 9.5-使用代理爬取微信公众号文章</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>前面讲解了代理池的维护和付费代理的相关使用方法，接下来我们进行一下实战演练，利用代理来爬取微信公众号的文章。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们的主要目标是利用代理爬取微信公众号的文章，提取正文、发表日期、公众号等内容，爬取来源是搜狗微信，其链接为 <a href="http://weixin.sogou.com/，然后把爬取结果保存到" target="_blank" rel="noopener">http://weixin.sogou.com/，然后把爬取结果保存到</a> MySQL 数据库。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>首先需要准备并正常运行前文中所介绍的代理池。这里需要用的 Python 库有 aiohttp、requests、redis-py、pyquery、Flask、PyMySQL，如这些库没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="3-爬取分析"><a href="#3-爬取分析" class="headerlink" title="3. 爬取分析"></a>3. 爬取分析</h3>
                  <p>搜狗对微信公众平台的公众号和文章做了整合。我们可以通过上面的链接搜索到相关的公众号和文章，例如搜索 NBA，可以搜索到最新的文章，如图 9-21 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-055950.png" alt=""> 图 9-21 搜索结果 点击搜索后，搜索结果的 URL 中其实有很多无关 GET 请求参数，将无关的参数去掉，只保留 type 和 query 参数，例如 <a href="http://weixin.sogou.com/weixin?type=2&amp;query=NBA，搜索关键词为" target="_blank" rel="noopener">http://weixin.sogou.com/weixin?type=2&amp;query=NBA，搜索关键词为</a> NBA，类型为 2，2 代表搜索微信文章。 下拉网页，点击下一页即可翻页，如图 9-22 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-060001.jpg" alt=""> 图 9-22 翻页列表 注意，如果没有输入账号登录，那只能看到 10 页的内容，登录之后可以看到 100 页内容，如图 9-23 和图 9-24 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-060006.jpg" alt=""> 图 9-23 不登录的结果 <img src="https://cdn.cuiqingcai.com/2019-10-20-060011.jpg" alt=""> 图 9-24 登录后的结果 如果需要爬取更多内容，就需要登录并使用 Cookies 来爬取。 搜狗微信站点的反爬虫能力很强，如连续刷新，站点就会弹出类似如图 9-25 所示的验证。 <img src="https://cdn.cuiqingcai.com/2019-10-20-060018.png" alt=""> 图 9-25 验证码页面 网络请求出现了 302 跳转，返回状态码为 302，跳转的链接开头为 <a href="http://weixin.sogou.com/antispider/，这很明显就是一个反爬虫的验证页面。所以我们得出结论，如果服务器返回状态码为" target="_blank" rel="noopener">http://weixin.sogou.com/antispider/，这很明显就是一个反爬虫的验证页面。所以我们得出结论，如果服务器返回状态码为</a> 302 而非 200，则 IP 访问次数太高，IP 被封禁，此请求就是失败了。 如果遇到这种情况，我们可以选择识别验证码并解封，也可以使用代理直接切换 IP。在这里我们采用第二种方法，使用代理直接跳过这个验证。代理使用上一节所讲的代理池，还需要更改检测的 URL 为搜狗微信的站点。 对于这种反爬能力很强的网站来说，如果我们遇到此种返回状态就需要重试。所以我们采用另一种爬取方式，借助数据库构造一个爬取队列，待爬取的请求都放到队列里，如果请求失败了重新放回队列，就会被重新调度爬取。 在这里我们可以采用 Redis 的队列数据结构，新的请求就加入队列，或者有需要重试的请求也放回队列。调度的时候如果队列不为空，那就把一个个请求取出来执行，得到响应后再进行解析，提取出我们想要的结果。 这次我们采用 MySQL 存储，借助 PyMySQL 库，将爬取结果构造为一个字典，实现动态存储。 综上所述，我们本节实现的功能有如下几点。</p>
                  <ul>
                    <li>修改代理池检测链接为搜狗微信站点</li>
                    <li>构造 Redis 爬取队列，用队列实现请求的存取</li>
                    <li>实现异常处理，失败的请求重新加入队列</li>
                    <li>实现翻页和提取文章列表并把对应请求加入队列</li>
                    <li>实现微信文章的信息的提取</li>
                    <li>将提取到的信息保存到 MySQL</li>
                  </ul>
                  <p>好，那么接下来我们就用代码来实现一下。</p>
                  <h3 id="4-构造-Request"><a href="#4-构造-Request" class="headerlink" title="4. 构造 Request"></a>4. 构造 Request</h3>
                  <p>既然我们要用队列来存储请求，那么肯定要实现一个请求 Request 的数据结构，这个请求需要包含一些必要信息，如请求链接、请求头、请求方式、超时时间。另外对于某个请求，我们需要实现对应的方法来处理它的响应，所以需要再加一个 Callback 回调函数。每次翻页请求需要代理来实现，所以还需要一个参数 NeedProxy。如果一个请求失败次数太多，那就不再重新请求了，所以还需要加失败次数的记录。 这些字段都需要作为 Request 的一部分，组成一个完整的 Request 对象放入队列去调度，这样从队列获取出来的时候直接执行这个 Request 对象就好了。 我们可以采用继承 reqeusts 库中的 Request 对象的方式来实现这个数据结构。requests 库中已经有了 Request 对象，它将请求 Request 作为一个整体对象去执行，得到响应后再返回。其实 requests 库的 get()、post() 等方法都是通过执行 Request 对象实现的。 我们首先看看 Request 对象的源码：</p>
                  <figure class="highlight oxygene">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">class</span> Request(RequestHooksMixin):</span><br><span class="line">    def __init__(<span class="keyword">self</span>,</span><br><span class="line">            <span class="function"><span class="keyword">method</span>=<span class="title">None</span>, <span class="title">url</span>=<span class="title">None</span>, <span class="title">headers</span>=<span class="title">None</span>, <span class="title">files</span>=<span class="title">None</span>, <span class="title">data</span>=<span class="title">None</span>,</span></span><br><span class="line"><span class="function">            <span class="title">params</span>=<span class="title">None</span>, <span class="title">auth</span>=<span class="title">None</span>, <span class="title">cookies</span>=<span class="title">None</span>, <span class="title">hooks</span>=<span class="title">None</span>, <span class="title">json</span>=<span class="title">None</span>):</span></span><br><span class="line"></span><br><span class="line">        # <span class="keyword">Default</span> <span class="keyword">empty</span> dicts <span class="keyword">for</span> dict <span class="keyword">params</span>.</span><br><span class="line">        data = [] <span class="keyword">if</span> data <span class="keyword">is</span> None <span class="keyword">else</span> data</span><br><span class="line">        files = [] <span class="keyword">if</span> files <span class="keyword">is</span> None <span class="keyword">else</span> files</span><br><span class="line">        headers = <span class="comment">&#123;&#125;</span> <span class="keyword">if</span> headers <span class="keyword">is</span> None <span class="keyword">else</span> headers</span><br><span class="line">        <span class="keyword">params</span> = <span class="comment">&#123;&#125;</span> <span class="keyword">if</span> <span class="keyword">params</span> <span class="keyword">is</span> None <span class="keyword">else</span> <span class="keyword">params</span></span><br><span class="line">        hooks = <span class="comment">&#123;&#125;</span> <span class="keyword">if</span> hooks <span class="keyword">is</span> None <span class="keyword">else</span> hooks</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.hooks = default_hooks()</span><br><span class="line">        <span class="keyword">for</span> (k, v) <span class="keyword">in</span> list(hooks.items()):</span><br><span class="line">            <span class="keyword">self</span>.register_hook(<span class="keyword">event</span>=k, hook=v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span><span class="function">.<span class="keyword">method</span> = <span class="title">method</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">url</span> = <span class="title">url</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">headers</span> = <span class="title">headers</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">files</span> = <span class="title">files</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">data</span> = <span class="title">data</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">json</span> = <span class="title">json</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">params</span> = <span class="title">params</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">auth</span> = <span class="title">auth</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">cookies</span> = <span class="title">cookies</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这是 requests 库中 Request 对象的构造方法。这个 Request 已经包含了请求方式、请求链接、请求头这几个属性，但是相比我们需要的还差了几个。我们需要实现一个特定的数据结构，在原先基础上加入上文所提到的额外几个属性。这里我们需要继承 Request 对象重新实现一个请求，将它定义为 WeixinRequest，实现如下：</p>
                  <figure class="highlight oxygene">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">TIMEOUT = <span class="number">10</span></span><br><span class="line"><span class="keyword">from</span> requests import Request</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> WeixinRequest(Request):</span><br><span class="line">    def __init__(<span class="keyword">self</span>, url, callback, <span class="function"><span class="keyword">method</span>='<span class="title">GET</span>', <span class="title">headers</span>=<span class="title">None</span>, <span class="title">need_proxy</span>=<span class="title">False</span>, <span class="title">fail_time</span>=0, <span class="title">timeout</span>=<span class="title">TIMEOUT</span>):</span></span><br><span class="line">        Request.__init__(<span class="keyword">self</span>, <span class="function"><span class="keyword">method</span>, <span class="title">url</span>, <span class="title">headers</span>)</span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">callback</span> = <span class="title">callback</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">need_proxy</span> = <span class="title">need_proxy</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">fail_time</span> = <span class="title">fail_time</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">timeout</span> = <span class="title">timeout</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们实现了 WeixinRequest 数据结构。<strong>init</strong>() 方法先调用了 Request 的<strong>init</strong>() 方法，然后加入额外的几个参数，定义为 callback、need_proxy、fail_time、timeout，分别代表回调函数、是否需要代理爬取、失败次数、超时时间。 我们就可以将 WeixinRequest 作为一个整体来执行，一个个 WeixinRequest 对象都是独立的，每个请求都有自己的属性。例如，我们可以调用它的 callback，就可以知道这个请求的响应应该用什么方法来处理，调用 fail_time 就可以知道这个请求失败了多少次，判断失败次数是不是到了阈值，该不该丢弃这个请求。这里我们采用了面向对象的一些思想。</p>
                  <h3 id="5-实现请求队列"><a href="#5-实现请求队列" class="headerlink" title="5. 实现请求队列"></a>5. 实现请求队列</h3>
                  <p>接下来我们就需要构造请求队列，实现请求的存取。存取无非就是两个操作，一个是放，一个是取，所以这里利用 Redis 的 rpush() 和 lpop() 方法即可。 另外还需要注意，存取不能直接存 Request 对象，Redis 里面存的是字符串。所以在存 Request 对象之前我们先把它序列化，取出来的时候再将其反序列化，这个过程可以利用 pickle 模块实现。</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> pickle <span class="keyword">import</span> dumps, loads</span><br><span class="line"><span class="keyword">from</span> request <span class="keyword">import</span> WeixinRequest</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisQueue</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""初始化 Redis"""</span></span><br><span class="line">        self.db = StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        向队列添加序列化后的 Request</span></span><br><span class="line"><span class="string">        :param request: 请求对象</span></span><br><span class="line"><span class="string">        :param fail_time: 失败次数</span></span><br><span class="line"><span class="string">        :return: 添加结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(request, WeixinRequest):</span><br><span class="line">            <span class="keyword">return</span> self.db.rpush(REDIS_KEY, dumps(request))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        取出下一个 Request 并反序列化</span></span><br><span class="line"><span class="string">        :return: Request or None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.db.llen(REDIS_KEY):</span><br><span class="line">            <span class="keyword">return</span> loads(self.db.lpop(REDIS_KEY))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">empty</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.db.llen(REDIS_KEY) == <span class="number">0</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里实现了一个 RedisQueue，它的 <strong>init</strong>() 构造方法里面初始化了一个 StrictRedis 对象。随后实现了 add() 方法，首先判断 Request 的类型，如果是 WeixinRequest，那么就把程序就会用 pickle 的 dumps() 方法序列化，然后再调用 rpush() 方法加入队列。pop() 方法则相反，调用 lpop() 方法将请求从队列取出，然后再用 pickle 的 loads() 方法将其转为 WeixinRequest 对象。另外，empty() 方法返回队列是否为空，只需要判断队列长度是否为 0 即可。 在调度的时候，我们只需要新建一个 RedisQueue 对象，然后调用 add() 方法，传入 WeixinRequest 对象，即可将 WeixinRequest 加入队列，调用 pop() 方法，即可取出下一个 WeixinRequest 对象，非常简单易用。</p>
                  <h3 id="6-修改代理池"><a href="#6-修改代理池" class="headerlink" title="6. 修改代理池"></a>6. 修改代理池</h3>
                  <p>接下来我们要生成请求并开始爬取。在此之前还需要做一件事，那就是先找一些可用代理。 之前代理池检测的 URL 并不是搜狗微信站点，所以我们需要将代理池检测的 URL 修改成搜狗微信站点，以便于把被搜狗微信站点封禁的代理剔除掉，留下可用代理。 现在将代理池的设置文件中的 TEST_URL 修改一下，如 <a href="http://weixin.sogou.com/weixin?type=2&amp;amp" target="_blank" rel="noopener">http://weixin.sogou.com/weixin?type=2&amp;amp</a>; query=nba，被本站点封的代理就会减分，正常请求的代理就会赋值为 100，最后留下的就是可用代理。 修改之后将获取模块、检测模块、接口模块的开关都设置为 True，让代理池运行一会，如图 9-26 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-060026.jpg" alt=""> 图 9-26 代理池运行结果 这样，数据库中留下的 100 分的代理就是针对搜狗微信的可用代理了，如图 9-27 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-060049.jpg" alt=""> 图 9-27 可用代理列表 同时访问代理接口，接口设置为 5555，访问 <a href="http://127.0.0.1:5555/random，即可获取到随机可用代理，如图" target="_blank" rel="noopener">http://127.0.0.1:5555/random，即可获取到随机可用代理，如图</a> 9-28 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-060055.jpg" alt=""> 图 9-28 代理接口 再定义一个函数来获取随机代理：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">PROXY_POOL_URL = <span class="string">'http://127.0.0.1:5555/random'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    从代理池获取代理</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(PROXY_POOL_URL)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            print(<span class="string">'Get Proxy'</span>, response.text)</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="7-第一个请求"><a href="#7-第一个请求" class="headerlink" title="7. 第一个请求"></a>7. 第一个请求</h3>
                  <p>一切准备工作都做好，下面我们就可以构造第一个请求放到队列里以供调度了。定义一个 Spider 类，实现 start() 方法的代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Session</span><br><span class="line"><span class="keyword">from</span> db <span class="keyword">import</span> RedisQueue</span><br><span class="line"><span class="keyword">from</span> request <span class="keyword">import</span> WeixinRequest</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span><span class="params">()</span>:</span></span><br><span class="line">    base_url = <span class="string">'http://weixin.sogou.com/weixin'</span></span><br><span class="line">    keyword = <span class="string">'NBA'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'</span>,</span><br><span class="line">        <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate'</span>,</span><br><span class="line">        <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,en;q=0.6,ja;q=0.4,zh-TW;q=0.2,mt;q=0.2'</span>,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">'max-age=0'</span>,</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">        <span class="string">'Cookie'</span>: <span class="string">'IPLOC=CN1100; SUID=6FEDCF3C541C940A000000005968CF55; SUV=1500041046435211; ABTEST=0|1500041048|v1; SNUID=CEA85AE02A2F7E6EAFF9C1FE2ABEBE6F; weixinIndexVisited=1; JSESSIONID=aaar_m7LEIW-jg_gikPZv; ld=Wkllllllll2BzGMVlllllVOo8cUlllll5G@HbZllll9lllllRklll5@@@@@@@@@@'</span>,</span><br><span class="line">        <span class="string">'Host'</span>: <span class="string">'weixin.sogou.com'</span>,</span><br><span class="line">        <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    session = Session()</span><br><span class="line">    queue = RedisQueue()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""初始化工作"""</span></span><br><span class="line">        <span class="comment"># 全局更新 Headers</span></span><br><span class="line">        self.session.headers.update(self.headers)</span><br><span class="line">        start_url = self.base_url + <span class="string">'?'</span> + urlencode(&#123;<span class="string">'query'</span>: self.keyword, <span class="string">'type'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">        weixin_request = WeixinRequest(url=start_url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 调度第一个请求</span></span><br><span class="line">        self.queue.add(weixin_request)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了 Spider 类，设置了很多全局变量，比如 keyword 设置为 NBA，headers 就是请求头。在浏览器里登录账号，然后在开发者工具里将请求头复制出来，记得带上 Cookie 字段，这样才能爬取 100 页的内容。然后初始化了 Session 和 RedisQueue 对象，它们分别用来执行请求和存储请求。 首先，start() 方法全局更新了 headers，使得所有请求都能应用 Cookies。然后构造了一个起始 URL：<a href="http://weixin.sogou.com/weixin?type=2&amp;query=NBA，随后用改" target="_blank" rel="noopener">http://weixin.sogou.com/weixin?type=2&amp;query=NBA，随后用改</a> URL 构造了一个 WeixinRequest 对象。回调函数是 Spider 类的 parse_index() 方法，也就是当这个请求成功之后就用 parse_index() 来处理和解析。need_proxy 参数设置为 True，代表执行这个请求需要用到代理。随后我们调用了 RedisQueue 的 add() 方法，将这个请求加入队列，等待调度。</p>
                  <h3 id="8-调度请求"><a href="#8-调度请求" class="headerlink" title="8. 调度请求"></a>8. 调度请求</h3>
                  <p>加入第一个请求之后，调度开始了。我们首先从队列中取出这个请求，将它的结果解析出来，生成新的请求加入队列，然后拿出新的请求，将结果解析，再生成新的请求加入队列，这样循环往复执行，直到队列中没有请求，则代表爬取结束。我们用代码实现如下：</p>
                  <figure class="highlight php">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">VALID_STATUSES = [<span class="number">200</span>]</span><br><span class="line"></span><br><span class="line">def schedule(<span class="keyword">self</span>):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    调度请求</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="keyword">while</span> not <span class="keyword">self</span>.queue.<span class="keyword">empty</span>():</span><br><span class="line">        weixin_request = <span class="keyword">self</span>.queue.pop()</span><br><span class="line">        callback = weixin_request.callback</span><br><span class="line">        <span class="keyword">print</span>(<span class="string">'Schedule'</span>, weixin_request.url)</span><br><span class="line">        response = <span class="keyword">self</span>.request(weixin_request)</span><br><span class="line">        <span class="keyword">if</span> response <span class="keyword">and</span> response.status_code in VALID_STATUSES:</span><br><span class="line">            results = <span class="keyword">list</span>(callback(response))</span><br><span class="line">            <span class="keyword">if</span> results:</span><br><span class="line">                <span class="keyword">for</span> result in results:</span><br><span class="line">                    <span class="keyword">print</span>(<span class="string">'New Result'</span>, result)</span><br><span class="line">                    <span class="keyword">if</span> isinstance(result, WeixinRequest):</span><br><span class="line">                        <span class="keyword">self</span>.queue.add(result)</span><br><span class="line">                    <span class="keyword">if</span> isinstance(result, dict):</span><br><span class="line">                        <span class="keyword">self</span>.mysql.insert(<span class="string">'articles'</span>, result)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">self</span>.error(weixin_request)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">self</span>.error(weixin_request)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里实现了一个 schedule() 方法，其内部是一个循环，循环的判断是队列不为空。 当队列不为空时，调用 pop() 方法取出下一个请求，调用 request() 方法执行这个请求，request() 方法的实现如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> requests import ReadTimeout, ConnectionError</span><br><span class="line"></span><br><span class="line">def request(self, weixin_request):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    执行请求</span></span><br><span class="line"><span class="string">    :param weixin_request: 请求</span></span><br><span class="line"><span class="string">    :return: 响应</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    try:</span><br><span class="line">        <span class="keyword">if</span> weixin_request.need_proxy:</span><br><span class="line">           <span class="built_in"> proxy </span>= get_proxy()</span><br><span class="line">            <span class="keyword">if</span> proxy:</span><br><span class="line">                proxies = &#123;</span><br><span class="line">                    <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">                    <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">                &#125;</span><br><span class="line">                return self.session.send(weixin_request.prepare(),</span><br><span class="line">                                         <span class="attribute">timeout</span>=weixin_request.timeout, <span class="attribute">allow_redirects</span>=<span class="literal">False</span>, <span class="attribute">proxies</span>=proxies)</span><br><span class="line">        return self.session.send(weixin_request.prepare(), <span class="attribute">timeout</span>=weixin_request.timeout, <span class="attribute">allow_redirects</span>=<span class="literal">False</span>)</span><br><span class="line">    except (ConnectionError, ReadTimeout) as e:</span><br><span class="line">        <span class="builtin-name">print</span>(e.args)</span><br><span class="line">        return <span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先判断这个请求是否需要代理，如果需要代理，则调用 get_proxy() 方法获取代理，然后调用 Session 的 send() 方法执行这个请求。这里的请求调用了 prepare() 方法转化为 Prepared Request，具体的用法可以参考 <a href="http://docs.python-requests.org/en/master/user/advanced/#prepared-requests，同时设置" target="_blank" rel="noopener">http://docs.python-requests.org/en/master/user/advanced/#prepared-requests，同时设置</a> allow_redirects 为 False，timeout 是该请求的超时时间，最后响应返回。 执行 request() 方法之后会得到两种结果：一种是 False，即请求失败，连接错误；另一种是 Response 对象，还需要判断状态码，如果状态码合法，那么就进行解析，否则重新将请求加回队列。 如果状态码合法，解析的时候就会调用 WeixinRequest 的回调函数进行解析。比如这里的回调函数是 parse_index()，其实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    解析索引页</span></span><br><span class="line"><span class="string">    :param response: 响应</span></span><br><span class="line"><span class="string">    :return: 新的响应</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    doc = pq(response.text)</span><br><span class="line">    items = doc(<span class="string">'.news-box .news-list li .txt-box h3 a'</span>).items()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        url = item.attr(<span class="string">'href'</span>)</span><br><span class="line">        weixin_request = WeixinRequest(url=url, callback=self.parse_detail)</span><br><span class="line">        <span class="keyword">yield</span> weixin_request</span><br><span class="line">    next = doc(<span class="string">'#sogou_next'</span>).attr(<span class="string">'href'</span>)</span><br><span class="line">    <span class="keyword">if</span> next:</span><br><span class="line">        url = self.base_url + str(next)</span><br><span class="line">        weixin_request = WeixinRequest(url=url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">yield</span> weixin_request</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此方法做了两件事：一件事就是获取本页的所有微信文章链接，另一件事就是获取下一页的链接，再构造成 WeixinRequest 之后 yield 返回。 然后，schedule() 方法将返回的结果进行遍历，利用 isinstance() 方法判断返回结果，如果返回结果是 WeixinRequest，就将其重新加入队列。 至此，第一次循环结束。 这时 while 循环会继续执行。队列已经包含第一页内容的文章详情页请求和下一页的请求，所以第二次循环得到的下一个请求就是文章详情页的请求，程序重新调用 request() 方法获取其响应，然后调用其对应的回调函数解析。这时详情页请求的回调方法就不同了，这次是 parse_detail() 方法，此方法实现如下：</p>
                  <figure class="highlight ceylon">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse<span class="number">_</span>detail(self, response):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    解析详情页</span></span><br><span class="line"><span class="string">    :param response: 响应</span></span><br><span class="line"><span class="string">    :return: 微信公众号文章</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="meta">doc</span> = pq(response.text)</span><br><span class="line">    data = &#123;<span class="string">'title'</span>: <span class="meta">doc</span>(<span class="string">'.rich_media_title'</span>).text(),</span><br><span class="line">        <span class="string">'content'</span>: <span class="meta">doc</span>(<span class="string">'.rich_media_content'</span>).text(),</span><br><span class="line">        <span class="string">'date'</span>: <span class="meta">doc</span>(<span class="string">'#post-date'</span>).text(),</span><br><span class="line">        <span class="string">'nickname'</span>: <span class="meta">doc</span>(<span class="string">'#js_profile_qrcode&gt; div &gt; strong'</span>).text(),</span><br><span class="line">        <span class="string">'wechat'</span>: <span class="meta">doc</span>(<span class="string">'#js_profile_qrcode&gt; div &gt; p:nth-child(3) &gt; span'</span>).text()&#125;</span><br><span class="line">    yield data</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个方法解析了微信文章详情页的内容，提取出它的标题、正文文本、发布日期、发布人昵称、微信公众号名称，将这些信息组合成一个字典返回。 结果返回之后还需要判断类型，如是字典类型，程序就调用 mysql 对象的 insert() 方法将数据存入数据库。 这样，第二次循环执行完毕。 第三次循环、第四次循环，循环往复，每个请求都有各自的回调函数，索引页解析完毕之后会继续生成后续请求，详情页解析完毕之后会返回结果以便存储，直到爬取完毕。 现在，整个调度就完成了。 我们完善一下整个 Spider 代码，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Session</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> db <span class="keyword">import</span> RedisQueue</span><br><span class="line"><span class="keyword">from</span> mysql <span class="keyword">import</span> MySQL</span><br><span class="line"><span class="keyword">from</span> request <span class="keyword">import</span> WeixinRequest</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> ReadTimeout, ConnectionError</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span><span class="params">()</span>:</span></span><br><span class="line">    base_url = <span class="string">'http://weixin.sogou.com/weixin'</span></span><br><span class="line">    keyword = <span class="string">'NBA'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'</span>,</span><br><span class="line">        <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate'</span>,</span><br><span class="line">        <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,en;q=0.6,ja;q=0.4,zh-TW;q=0.2,mt;q=0.2'</span>,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">'max-age=0'</span>,</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">        <span class="string">'Cookie'</span>: <span class="string">'IPLOC=CN1100; SUID=6FEDCF3C541C940A000000005968CF55; SUV=1500041046435211; ABTEST=0|1500041048|v1; SNUID=CEA85AE02A2F7E6EAFF9C1FE2ABEBE6F; weixinIndexVisited=1; JSESSIONID=aaar_m7LEIW-jg_gikPZv; ld=Wkllllllll2BzGMVlllllVOo8cUlllll5G@HbZllll9lllllRklll5@@@@@@@@@@'</span>,</span><br><span class="line">        <span class="string">'Host'</span>: <span class="string">'weixin.sogou.com'</span>,</span><br><span class="line">        <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    session = Session()</span><br><span class="line">    queue = RedisQueue()</span><br><span class="line">    mysql = MySQL()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        从代理池获取代理</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = requests.get(PROXY_POOL_URL)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                print(<span class="string">'Get Proxy'</span>, response.text)</span><br><span class="line">                <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""初始化工作"""</span></span><br><span class="line">        <span class="comment"># 全局更新 Headers</span></span><br><span class="line">        self.session.headers.update(self.headers)</span><br><span class="line">        start_url = self.base_url + <span class="string">'?'</span> + urlencode(&#123;<span class="string">'query'</span>: self.keyword, <span class="string">'type'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">        weixin_request = WeixinRequest(url=start_url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 调度第一个请求</span></span><br><span class="line">        self.queue.add(weixin_request)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        解析索引页</span></span><br><span class="line"><span class="string">        :param response: 响应</span></span><br><span class="line"><span class="string">        :return: 新的响应</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = pq(response.text)</span><br><span class="line">        items = doc(<span class="string">'.news-box .news-list li .txt-box h3 a'</span>).items()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            url = item.attr(<span class="string">'href'</span>)</span><br><span class="line">            weixin_request = WeixinRequest(url=url, callback=self.parse_detail)</span><br><span class="line">            <span class="keyword">yield</span> weixin_request</span><br><span class="line">        next = doc(<span class="string">'#sogou_next'</span>).attr(<span class="string">'href'</span>)</span><br><span class="line">        <span class="keyword">if</span> next:</span><br><span class="line">            url = self.base_url + str(next)</span><br><span class="line">            weixin_request = WeixinRequest(url=url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">yield</span> weixin_request</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        解析详情页</span></span><br><span class="line"><span class="string">        :param response: 响应</span></span><br><span class="line"><span class="string">        :return: 微信公众号文章</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = pq(response.text)</span><br><span class="line">        data = &#123;<span class="string">'title'</span>: doc(<span class="string">'.rich_media_title'</span>).text(),</span><br><span class="line">            <span class="string">'content'</span>: doc(<span class="string">'.rich_media_content'</span>).text(),</span><br><span class="line">            <span class="string">'date'</span>: doc(<span class="string">'#post-date'</span>).text(),</span><br><span class="line">            <span class="string">'nickname'</span>: doc(<span class="string">'#js_profile_qrcode&gt; div &gt; strong'</span>).text(),</span><br><span class="line">            <span class="string">'wechat'</span>: doc(<span class="string">'#js_profile_qrcode&gt; div &gt; p:nth-child(3) &gt; span'</span>).text()&#125;</span><br><span class="line">        <span class="keyword">yield</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">(self, weixin_request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行请求</span></span><br><span class="line"><span class="string">        :param weixin_request: 请求</span></span><br><span class="line"><span class="string">        :return: 响应</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> weixin_request.need_proxy:</span><br><span class="line">                proxy = self.get_proxy()</span><br><span class="line">                <span class="keyword">if</span> proxy:</span><br><span class="line">                    proxies = &#123;</span><br><span class="line">                        <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">                        <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> self.session.send(weixin_request.prepare(),</span><br><span class="line">                                             timeout=weixin_request.timeout, allow_redirects=<span class="literal">False</span>, proxies=proxies)</span><br><span class="line">            <span class="keyword">return</span> self.session.send(weixin_request.prepare(), timeout=weixin_request.timeout, allow_redirects=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">except</span> (ConnectionError, ReadTimeout) <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(self, weixin_request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        错误处理</span></span><br><span class="line"><span class="string">        :param weixin_request: 请求</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        weixin_request.fail_time = weixin_request.fail_time + <span class="number">1</span></span><br><span class="line">        print(<span class="string">'Request Failed'</span>, weixin_request.fail_time, <span class="string">'Times'</span>, weixin_request.url)</span><br><span class="line">        <span class="keyword">if</span> weixin_request.fail_time &lt; MAX_FAILED_TIME:</span><br><span class="line">            self.queue.add(weixin_request)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">schedule</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        调度请求</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> self.queue.empty():</span><br><span class="line">            weixin_request = self.queue.pop()</span><br><span class="line">            callback = weixin_request.callback</span><br><span class="line">            print(<span class="string">'Schedule'</span>, weixin_request.url)</span><br><span class="line">            response = self.request(weixin_request)</span><br><span class="line">            <span class="keyword">if</span> response <span class="keyword">and</span> response.status_code <span class="keyword">in</span> VALID_STATUSES:</span><br><span class="line">                results = list(callback(response))</span><br><span class="line">                <span class="keyword">if</span> results:</span><br><span class="line">                    <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">                        print(<span class="string">'New Result'</span>, result)</span><br><span class="line">                        <span class="keyword">if</span> isinstance(result, WeixinRequest):</span><br><span class="line">                            self.queue.add(result)</span><br><span class="line">                        <span class="keyword">if</span> isinstance(result, dict):</span><br><span class="line">                            self.mysql.insert(<span class="string">'articles'</span>, result)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.error(weixin_request)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.error(weixin_request)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        入口</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.start()</span><br><span class="line">        self.schedule()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = Spider()</span><br><span class="line">    spider.run()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>最后，我们加了一个 run() 方法作为入口，启动的时候只需要执行 Spider 的 run() 方法即可。</p>
                  <h3 id="9-MySQL-存储"><a href="#9-MySQL-存储" class="headerlink" title="9. MySQL 存储"></a>9. MySQL 存储</h3>
                  <p>整个调度模块完成了，上面还没提及到的就是存储模块，在这里还需要定义一个 MySQL 类供存储数据，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">REDIS_HOST = <span class="string">'localhost'</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br><span class="line">REDIS_PASSWORD = <span class="string">'foobared'</span></span><br><span class="line">REDIS_KEY = <span class="string">'weixin'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=MYSQL_HOST, username=MYSQL_USER, password=MYSQL_PASSWORD, port=MYSQL_PORT,</span></span></span><br><span class="line"><span class="function"><span class="params">                 database=MYSQL_DATABASE)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        MySQL 初始化</span></span><br><span class="line"><span class="string">        :param host:</span></span><br><span class="line"><span class="string">        :param username:</span></span><br><span class="line"><span class="string">        :param password:</span></span><br><span class="line"><span class="string">        :param port:</span></span><br><span class="line"><span class="string">        :param database:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.db = pymysql.connect(host, username, password, database, charset=<span class="string">'utf8'</span>, port=port)</span><br><span class="line">            self.cursor = self.db.cursor()</span><br><span class="line">        <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, table, data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        插入数据</span></span><br><span class="line"><span class="string">        :param table:</span></span><br><span class="line"><span class="string">        :param data:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">        values = <span class="string">', '</span>.join([<span class="string">'% s'</span>] * len(data))</span><br><span class="line">        sql_query = <span class="string">'insert into % s (% s) values (% s)'</span> % (table, keys, values)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.cursor.execute(sql_query, tuple(data.values()))</span><br><span class="line">            self.db.commit()</span><br><span class="line">        <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">            self.db.rollback()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p><strong>init</strong>() 方法初始化了 MySQL 连接，需要 MySQL 的用户、密码、端口、数据库名等信息。数据库名为 weixin，需要自己创建。 insert() 方法传入表名和字典即可动态构造 SQL，在 5.2 节中也有讲到，SQL 构造之后执行即可插入数据。 我们还需要提前建立一个数据表，表名为 articles，建表的 SQL 语句如下：</p>
                  <figure class="highlight sql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`articles`</span> (<span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`title`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`content`</span> <span class="built_in">text</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`date`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`wechat`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`nickname`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span></span><br><span class="line">) <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="string">`articles`</span> <span class="keyword">ADD</span> PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>);</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>现在，我们的整个爬虫就算完成了。</p>
                  <h3 id="10-运行"><a href="#10-运行" class="headerlink" title="10. 运行"></a>10. 运行</h3>
                  <p>示例运行结果如图 9-29 所示： <img src="https://cdn.cuiqingcai.com/2019-10-20-060111.jpg" alt=""> 图 9-29 运行结果 程序首先调度了第一页结果对应的请求，获取了代理执行此请求，随后得到了 11 个新请求，请求都是 WeixinRequest 类型，将其再加入队列。随后继续调度新加入的请求，也就是文章详情页对应的请求，再执行，得到的就是文章详情对应的提取结果，提取结果是字典类型。 程序循环往复，不断爬取，直至所有结果爬取完毕，程序终止，爬取完成。 爬取结果如图 9-30 所示。 <img src="https://cdn.cuiqingcai.com/2019-10-20-060114.jpg" alt=""> 图 9-30 爬取结果 我们可以看到，相关微信文章都已被存储到数据库里了。</p>
                  <h3 id="11-本节代码"><a href="#11-本节代码" class="headerlink" title="11. 本节代码"></a>11. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/Weixin" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weixin</a>，运行之前请先配置好代理池。</p>
                  <h3 id="12-结语"><a href="#12-结语" class="headerlink" title="12. 结语"></a>12. 结语</h3>
                  <p>以上内容便是使用代理爬取微信公众号文章的方法，涉及的新知识点不少，希望大家可以好好消化。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-10-20 14:15:26" itemprop="dateCreated datePublished" datetime="2019-10-20T14:15:26+08:00">2019-10-20</time>
                </span>
                <span id="/7844.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 9.5-使用代理爬取微信公众号文章" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>16k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>14 分钟</span>
                </span>
              </div>
            </article>
            <script>
              document.querySelectorAll('.random').forEach(item => item.src = "https://picsum.photos/id/" + Math.floor(Math.random() * Math.floor(300)) + "/200/133")

            </script>
            <nav class="pagination">
              <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
            </nav>
          </div>
          <script>
            window.addEventListener('tabs:register', () =>
            {
              let
              {
                activeClass
              } = CONFIG.comments;
              if (CONFIG.comments.storage)
              {
                activeClass = localStorage.getItem('comments_active') || activeClass;
              }
              if (activeClass)
              {
                let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
                if (activeTab)
                {
                  activeTab.click();
                }
              }
            });
            if (CONFIG.comments.storage)
            {
              window.addEventListener('tabs:click', event =>
              {
                if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
                let commentClass = event.target.classList[1];
                localStorage.setItem('comments_active', commentClass);
              });
            }

          </script>
        </div>
        <div class="toggle sidebar-toggle">
          <span class="toggle-line toggle-line-first"></span>
          <span class="toggle-line toggle-line-middle"></span>
          <span class="toggle-line toggle-line-last"></span>
        </div>
        <aside class="sidebar">
          <div class="sidebar-inner">
            <ul class="sidebar-nav motion-element">
              <li class="sidebar-nav-toc"> 文章目录 </li>
              <li class="sidebar-nav-overview"> 站点概览 </li>
            </ul>
            <!--noindex-->
            <div class="post-toc-wrap sidebar-panel">
            </div>
            <!--/noindex-->
            <div class="site-overview-wrap sidebar-panel">
              <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
                <img class="site-author-image" itemprop="image" alt="崔庆才" src="/images/avatar.png">
                <p class="site-author-name" itemprop="name">崔庆才</p>
                <div class="site-description" itemprop="description">静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。</div>
              </div>
              <div class="site-state-wrap motion-element">
                <nav class="site-state">
                  <div class="site-state-item site-state-posts">
                    <a href="/archives/">
                      <span class="site-state-item-count">710</span>
                      <span class="site-state-item-name">日志</span>
                    </a>
                  </div>
                  <div class="site-state-item site-state-categories">
                    <a href="/categories/">
                      <span class="site-state-item-count">43</span>
                      <span class="site-state-item-name">分类</span></a>
                  </div>
                  <div class="site-state-item site-state-tags">
                    <a href="/tags/">
                      <span class="site-state-item-count">260</span>
                      <span class="site-state-item-name">标签</span></a>
                  </div>
                </nav>
              </div>
              <div class="links-of-author motion-element">
                <span class="links-of-author-item">
                  <a href="https://github.com/Germey" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Germey" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
                </span>
                <span class="links-of-author-item">
                  <a href="mailto:cqc@cuiqingcai.com.com" title="邮件 → mailto:cqc@cuiqingcai.com.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮件</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://weibo.com/cuiqingcai" title="微博 → https:&#x2F;&#x2F;weibo.com&#x2F;cuiqingcai" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>微博</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/Germey" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;Germey" rel="noopener" target="_blank"><i class="fa fa-magic fa-fw"></i>知乎</a>
                </span>
              </div>
            </div>
            <div style=" width: 100%;" class="sidebar-panel sidebar-panel-image sidebar-panel-active">
              <a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">
                <img src="https://cdn.cuiqingcai.com/ei5og.jpg" style=" width: 100%;">
              </a>
            </div>
            <div class="sidebar-panel sidebar-panel-categories sidebar-panel-active">
              <h4 class="name"> 分类 </h4>
              <div class="content">
                <ul class="category-list">
                  <li class="category-list-item"><a class="category-list-link" href="/categories/API/">API</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/C-C/">C/C++</a><span class="category-list-count">23</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Claude/">Claude</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Gemini/">Gemini</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Google-SERP/">Google SERP</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/HTML/">HTML</a><span class="category-list-count">14</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/JavaScript/">JavaScript</a><span class="category-list-count">26</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">14</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Luma/">Luma</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Markdown/">Markdown</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Midjourney/">Midjourney</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Nano-Banana/">Nano Banana</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Net/">Net</a><span class="category-list-count">4</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Nexior/">Nexior</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Other/">Other</a><span class="category-list-count">40</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/PHP/">PHP</a><span class="category-list-count">27</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Producer/">Producer</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">303</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/SeeDance/">SeeDance</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/SeeDream/">SeeDream</a><span class="category-list-count">3</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Sora/">Sora</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/TypeScript/">TypeScript</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Veo/">Veo</a><span class="category-list-count">3</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/nano-banana/">nano-banana</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E5%B1%95%E7%A4%BA/">个人展示</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E6%97%A5%E8%AE%B0/">个人日记</a><span class="category-list-count">9</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E8%AE%B0%E5%BD%95/">个人记录</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/">个人随笔</a><span class="category-list-count">21</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">安装配置</a><span class="category-list-count">59</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/">技术杂谈</a><span class="category-list-count">96</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">4</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB%E7%AC%94%E8%AE%B0/">生活笔记</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A6%8F%E5%88%A9%E4%B8%93%E5%8C%BA/">福利专区</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E8%81%8C%E4%BD%8D%E6%8E%A8%E8%8D%90/">职位推荐</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E8%89%BA%E6%9C%AF%E4%BA%8C%E7%BB%B4%E7%A0%81/">艺术二维码</a><span class="category-list-count">1</span></li>
                </ul>
              </div>
            </div>
            <div class="sidebar-panel sidebar-panel-friends sidebar-panel-active">
              <h4 class="name"> 友情链接 </h4>
              <ul class="friends">
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/j2dub.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.findhao.net/" target="_blank" rel="noopener">FindHao</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/6apxu.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.51dev.com/" target="_blank" rel="noopener">IT技术社区</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/bqlbs.png">
                  </span>
                  <span class="link">
                    <a href="http://www.urselect.com/" target="_blank" rel="noopener">优社电商</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/8s88c.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yuanrenxue.com/" target="_blank" rel="noopener">猿人学</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/2wgg5.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yunlifang.cn/" target="_blank" rel="noopener">云立方</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="http://qianxunclub.com/favicon.png">
                  </span>
                  <span class="link">
                    <a href="http://qianxunclub.com/" target="_blank" rel="noopener">千寻啊千寻</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/0044u.jpg">
                  </span>
                  <span class="link">
                    <a href="http://kodcloud.com/" target="_blank" rel="noopener">可道云</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/ygnpn.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.kunkundashen.cn/" target="_blank" rel="noopener">坤坤大神</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/x714o.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.hubwiz.com/" target="_blank" rel="noopener">汇智网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/44hxf.png">
                  </span>
                  <span class="link">
                    <a href="http://redstonewill.com/" target="_blank" rel="noopener">红色石头</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/wkaus.jpg">
                  </span>
                  <span class="link">
                    <a href="https://zhaoshuai.me/" target="_blank" rel="noopener">碎念</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/pgo0r.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.chenwenguan.com/" target="_blank" rel="noopener">陈文管的博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/kk82a.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.lxlinux.net/" target="_blank" rel="noopener">良许Linux教程网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/lj0t2.jpg">
                  </span>
                  <span class="link">
                    <a href="https://tanqingbo.cn/" target="_blank" rel="noopener">IT码农</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/i8cdr.png">
                  </span>
                  <span class="link">
                    <a href="https://junyiseo.com/" target="_blank" rel="noopener">均益个人博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/chwv2.png">
                  </span>
                  <span class="link">
                    <a href="https://brucedone.com/" target="_blank" rel="noopener">大鱼的鱼塘</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://www.91vps.com/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="http://www.91vps.com/" target="_blank" rel="noopener">91VPS</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://webpage.qidian.qq.com/qidian/chatv3-gray/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="https://www.qg.net/" target="_blank" rel="noopener">青果网络</a>
                  </span>
                </li>
              </ul>
            </div>
            <div class="sidebar-panel sidebar-panel-tags sidebar-panel-active">
              <h4 class="name"> 标签云 </h4>
              <div class="content">
                <a href="/tags/2022/" style="font-size: 20px;">2022</a> <a href="/tags/2048/" style="font-size: 10px;">2048</a> <a href="/tags/ACE-Data/" style="font-size: 13px;">ACE Data</a> <a href="/tags/ADSL/" style="font-size: 10px;">ADSL</a> <a href="/tags/AI%E7%BC%96%E7%A8%8B/" style="font-size: 10px;">AI编程</a> <a href="/tags/API/" style="font-size: 19px;">API</a> <a href="/tags/Ajax/" style="font-size: 12px;">Ajax</a> <a href="/tags/Audios/" style="font-size: 11px;">Audios</a> <a href="/tags/Bootstrap/" style="font-size: 11px;">Bootstrap</a> <a href="/tags/Bug/" style="font-size: 10px;">Bug</a> <a href="/tags/CDN/" style="font-size: 10px;">CDN</a> <a href="/tags/CQC/" style="font-size: 10px;">CQC</a> <a href="/tags/CSS/" style="font-size: 10px;">CSS</a> <a href="/tags/CSS-%E5%8F%8D%E7%88%AC%E8%99%AB/" style="font-size: 10px;">CSS 反爬虫</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/ChatGPT/" style="font-size: 10px;">ChatGPT</a> <a href="/tags/Cookie/" style="font-size: 10px;">Cookie</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Eclipse/" style="font-size: 11px;">Eclipse</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/FTP/" style="font-size: 10px;">FTP</a> <a href="/tags/Flux/" style="font-size: 10px;">Flux</a> <a href="/tags/Gemini/" style="font-size: 10px;">Gemini</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/GitHub/" style="font-size: 13px;">GitHub</a> <a href="/tags/Google-SERP/" style="font-size: 11px;">Google SERP</a> <a href="/tags/HTML5/" style="font-size: 10px;">HTML5</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/Hailuo/" style="font-size: 10px;">Hailuo</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Hook/" style="font-size: 10px;">Hook</a> <a href="/tags/IP/" style="font-size: 10px;">IP</a> <a href="/tags/IT/" style="font-size: 10px;">IT</a> <a href="/tags/Images/" style="font-size: 11px;">Images</a> <a href="/tags/JSON/" style="font-size: 10px;">JSON</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/JavaScript/" style="font-size: 14px;">JavaScript</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/LOGO/" style="font-size: 10px;">LOGO</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Luma/" style="font-size: 10px;">Luma</a> <a href="/tags/MIUI/" style="font-size: 10px;">MIUI</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/Midjourney/" style="font-size: 12px;">Midjourney</a> <a href="/tags/MongoDB/" style="font-size: 11px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Mysql/" style="font-size: 10px;">Mysql</a> <a href="/tags/NBA/" style="font-size: 10px;">NBA</a> <a href="/tags/Nano-Banana/" style="font-size: 11px;">Nano Banana</a> <a href="/tags/Nexior/" style="font-size: 10px;">Nexior</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/OpenCV/" style="font-size: 10px;">OpenCV</a> <a href="/tags/PHP/" style="font-size: 11px;">PHP</a> <a href="/tags/PPT/" style="font-size: 10px;">PPT</a> <a href="/tags/PS/" style="font-size: 10px;">PS</a> <a href="/tags/Pathlib/" style="font-size: 10px;">Pathlib</a> <a href="/tags/PhantomJS/" style="font-size: 10px;">PhantomJS</a> <a href="/tags/Playwright/" style="font-size: 10px;">Playwright</a> <a href="/tags/Producer/" style="font-size: 11px;">Producer</a> <a href="/tags/Python/" style="font-size: 16px;">Python</a> <a href="/tags/Python-%E7%88%AC%E8%99%AB/" style="font-size: 17px;">Python 爬虫</a> <a href="/tags/Python3/" style="font-size: 11px;">Python3</a> <a href="/tags/Python3%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B/" style="font-size: 12px;">Python3爬虫教程</a> <a href="/tags/Pythonic/" style="font-size: 10px;">Pythonic</a> <a href="/tags/Python%E7%88%AC%E8%99%AB/" style="font-size: 18px;">Python爬虫</a> <a href="/tags/Python%E7%88%AC%E8%99%AB%E4%B9%A6/" style="font-size: 12px;">Python爬虫书</a> <a href="/tags/Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B/" style="font-size: 15px;">Python爬虫教程</a> <a href="/tags/QQ/" style="font-size: 10px;">QQ</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/ReCAPTCHA/" style="font-size: 10px;">ReCAPTCHA</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Riffusion/" style="font-size: 11px;">Riffusion</a> <a href="/tags/SAE/" style="font-size: 10px;">SAE</a> <a href="/tags/SSH/" style="font-size: 10px;">SSH</a> <a href="/tags/SVG/" style="font-size: 10px;">SVG</a> <a href="/tags/Scrapy-redis/" style="font-size: 10px;">Scrapy-redis</a> <a href="/tags/Scrapy%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 10px;">Scrapy分布式</a> <a href="/tags/SeeDance/" style="font-size: 14px;">SeeDance</a> <a href="/tags/SeeDream/" style="font-size: 12px;">SeeDream</a> <a href="/tags/Selenium/" style="font-size: 11px;">Selenium</a> <a href="/tags/Session/" style="font-size: 10px;">Session</a> <a href="/tags/Shell/" style="font-size: 10px;">Shell</a> <a href="/tags/Sora/" style="font-size: 10px;">Sora</a> <a href="/tags/Sora2/" style="font-size: 11px;">Sora2</a> <a href="/tags/Suno/" style="font-size: 11px;">Suno</a> <a href="/tags/TKE/" style="font-size: 10px;">TKE</a> <a href="/tags/TXT/" style="font-size: 10px;">TXT</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/Ubuntu/" style="font-size: 11px;">Ubuntu</a> <a href="/tags/VS-Code/" style="font-size: 10px;">VS Code</a> <a href="/tags/Veo/" style="font-size: 13px;">Veo</a> <a href="/tags/Vercel/" style="font-size: 10px;">Vercel</a> <a href="/tags/Videos/" style="font-size: 12px;">Videos</a> <a href="/tags/Vs-Code/" style="font-size: 10px;">Vs Code</a> <a href="/tags/Vue/" style="font-size: 11px;">Vue</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a> <a href="/tags/Webpack/" style="font-size: 10px;">Webpack</a> <a href="/tags/Web%E7%BD%91%E9%A1%B5/" style="font-size: 10px;">Web网页</a> <a href="/tags/Windows/" style="font-size: 10px;">Windows</a> <a href="/tags/Winpcap/" style="font-size: 10px;">Winpcap</a>
              </div>
              <script>
                const tagsColors = ['#00a67c', '#5cb85c', '#d9534f', '#567e95', '#b37333', '#f4843d', '#15a287']
                const tagsElements = document.querySelectorAll('.sidebar-panel-tags .content a')
                tagsElements.forEach((item) =>
                {
                  item.style.backgroundColor = tagsColors[Math.floor(Math.random() * tagsColors.length)]
                })

              </script>
            </div>
          </div>
        </aside>
        <div id="sidebar-dimmer"></div>
      </div>
    </main>
    <footer class="footer">
      <div class="footer-inner">
        <div class="copyright">
          <span class="author" itemprop="copyrightHolder">崔庆才丨静觅</span> &copy; <span itemprop="copyrightYear">2026</span>
          <span class="with-love">
            <i class="fa fa-heart"></i>
          </span>
          <a href="https://cuiqingcai.com/sitemap.xml" style="display:none" title="爬虫教程" target="_blank"><strong>爬虫教程</strong></a>
          <a href="https://cuiqingcai.com/sitemap.html" style="display:none" title="爬虫教程" target="_blank"><strong>爬虫教程</strong></a>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-chart-area"></i>
          </span>
          <span title="站点总字数">3.5m</span>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-coffee"></i>
          </span>
          <span title="站点阅读时长">52:30</span>
        </div>
        <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动 </div>
        <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备18015597号-1 </a>
        </div>
        <script>
          (function ()
          {
            function leancloudSelector(url)
            {
              url = encodeURI(url);
              return document.getElementById(url).querySelector('.leancloud-visitors-count');
            }

            function addCount(Counter)
            {
              var visitors = document.querySelector('.leancloud_visitors');
              var url = decodeURI(visitors.id);
              var title = visitors.dataset.flagTitle;
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                if (results.length > 0)
                {
                  var counter = results[0];
                  leancloudSelector(url).innerText = counter.time + 1;
                  Counter('put', '/classes/Counter/' + counter.objectId,
                  {
                    time:
                    {
                      '__op': 'Increment',
                      'amount': 1
                    }
                  }).catch(error =>
                  {
                    console.error('Failed to save visitor count', error);
                  });
                }
                else
                {
                  Counter('post', '/classes/Counter',
                  {
                    title,
                    url,
                    time: 1
                  }).then(response => response.json()).then(() =>
                  {
                    leancloudSelector(url).innerText = 1;
                  }).catch(error =>
                  {
                    console.error('Failed to create', error);
                  });
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }

            function showTime(Counter)
            {
              var visitors = document.querySelectorAll('.leancloud_visitors');
              var entries = [...visitors].map(element =>
              {
                return decodeURI(element.id);
              });
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url:
                {
                  '$in': entries
                }
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                for (let url of entries)
                {
                  let target = results.find(item => item.url === url);
                  leancloudSelector(url).innerText = target ? target.time : 0;
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }
            let
            {
              app_id,
              app_key,
              server_url
            } = {
              "enable": true,
              "app_id": "6X5dRQ0pnPWJgYy8SXOg0uID-gzGzoHsz",
              "app_key": "ziLDVEy73ne5HtFTiGstzHMS",
              "server_url": "https://6x5drq0p.lc-cn-n1-shared.com",
              "security": false
            };

            function fetchData(api_server)
            {
              var Counter = (method, url, data) =>
              {
                return fetch(`${api_server}/1.1${url}`,
                {
                  method,
                  headers:
                  {
                    'X-LC-Id': app_id,
                    'X-LC-Key': app_key,
                    'Content-Type': 'application/json',
                  },
                  body: JSON.stringify(data)
                });
              };
              if (CONFIG.page.isPost)
              {
                if (CONFIG.hostname !== location.hostname) return;
                addCount(Counter);
              }
              else if (document.querySelectorAll('.post-title-link').length >= 1)
              {
                showTime(Counter);
              }
            }
            let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;
            if (api_server)
            {
              fetchData(api_server);
            }
            else
            {
              fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id).then(response => response.json()).then((
              {
                api_server
              }) =>
              {
                fetchData('https://' + api_server);
              });
            }
          })();

        </script>
      </div>
      <div class="footer-stat">
        <span id="cnzz_stat_icon_1279355174"></span>
        <script type="text/javascript">
          document.write(unescape("%3Cspan id='cnzz_stat_icon_1279355174'%3E%3C/span%3E%3Cscript src='https://v1.cnzz.com/z_stat.php%3Fid%3D1279355174%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));

        </script>
      </div>
    </footer>
  </div>
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/js/utils.js"></script>
  <script src="/.js"></script>
  <script src="/js/schemes/pisces.js"></script>
  <script src="/.js"></script>
  <script src="/js/next-boot.js"></script>
  <script src="/.js"></script>
  <script>
    (function ()
    {
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x = document.getElementsByTagName("link");
      //Find the last canonical URL
      if (x.length > 0)
      {
        for (i = 0; i < x.length; i++)
        {
          if (x[i].rel.toLowerCase() == 'canonical' && x[i].href)
          {
            canonicalURL = x[i].href;
          }
        }
      }
      //Get protocol
      if (!canonicalURL)
      {
        curProtocol = window.location.protocol.split(':')[0];
      }
      else
      {
        curProtocol = canonicalURL.split(':')[0];
      }
      //Get current URL if the canonical URL does not exist
      if (!canonicalURL) canonicalURL = window.location.href;
      //Assign script content. Replace current URL with the canonical URL
      ! function ()
      {
        var e = /([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,
          r = canonicalURL,
          t = document.referrer;
        if (!e.test(r))
        {
          var n = (String(curProtocol).toLowerCase() === 'https') ? "https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif" : "//api.share.baidu.com/s.gif";
          t ? (n += "?r=" + encodeURIComponent(document.referrer), r && (n += "&l=" + r)) : r && (n += "?l=" + r);
          var i = new Image;
          i.src = n
        }
      }(window);
    })();

  </script>
  <script src="/js/local-search.js"></script>
  <script src="/.js"></script>
</body>

</html>
