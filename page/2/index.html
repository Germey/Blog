<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
  <meta name="theme-color" content="#222">
  <meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>
  <script id="hexo-configurations">
    var NexT = window.NexT ||
    {};
    var CONFIG = {
      "hostname": "cuiqingcai.com",
      "root": "/",
      "scheme": "Pisces",
      "version": "7.8.0",
      "exturl": false,
      "sidebar":
      {
        "position": "right",
        "width": 360,
        "display": "post",
        "padding": 18,
        "offset": 12,
        "onmobile": false,
        "widgets": [
          {
            "type": "image",
            "name": "阿布云",
            "enable": false,
            "url": "https://www.abuyun.com/http-proxy/introduce.html",
            "src": "https://cdn.cuiqingcai.com/88au8.jpg",
            "width": "100%"
      },
          {
            "type": "image",
            "name": "爬虫书",
            "url": "https://item.jd.com/13527222.html",
            "src": "https://cdn.cuiqingcai.com/ei5og.jpg",
            "width": "100%",
            "enable": true
      },
          {
            "type": "categories",
            "name": "分类",
            "enable": true
      },
          {
            "type": "image",
            "name": "IPIDEA",
            "url": "http://www.ipidea.net/?utm-source=cqc&utm-keyword=?cqc",
            "src": "https://cdn.cuiqingcai.com/0ywun.png",
            "width": "100%",
            "enable": false
      },
          {
            "type": "image",
            "name": "Storm Proxies",
            "src": "https://cdn.cuiqingcai.com/a2zad8.png",
            "url": "https://www.stormproxies.cn/?keyword=jingmi",
            "width": "100%",
            "enable": false
      },
          {
            "type": "friends",
            "name": "友情链接",
            "enable": true
      },
          {
            "type": "hot",
            "name": "猜你喜欢",
            "enable": true
      },
          {
            "type": "tags",
            "name": "标签云",
            "enable": true
      }]
      },
      "copycode":
      {
        "enable": true,
        "show_result": true,
        "style": "mac"
      },
      "back2top":
      {
        "enable": true,
        "sidebar": false,
        "scrollpercent": true
      },
      "bookmark":
      {
        "enable": false,
        "color": "#222",
        "save": "auto"
      },
      "fancybox": false,
      "mediumzoom": false,
      "lazyload": false,
      "pangu": true,
      "comments":
      {
        "style": "tabs",
        "active": "gitalk",
        "storage": true,
        "lazyload": false,
        "nav": null,
        "activeClass": "gitalk"
      },
      "algolia":
      {
        "hits":
        {
          "per_page": 10
        },
        "labels":
        {
          "input_placeholder": "Search for Posts",
          "hits_empty": "We didn't find any results for the search: ${query}",
          "hits_stats": "${hits} results found in ${time} ms"
        }
      },
      "localsearch":
      {
        "enable": true,
        "trigger": "auto",
        "top_n_per_article": 10,
        "unescape": false,
        "preload": false
      },
      "motion":
      {
        "enable": false,
        "async": false,
        "transition":
        {
          "post_block": "bounceDownIn",
          "post_header": "slideDownIn",
          "post_body": "slideDownIn",
          "coll_header": "slideLeftIn",
          "sidebar": "slideUpIn"
        }
      },
      "path": "search.xml"
    };

  </script>
  <meta name="keywords" content="爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书,静觅,崔庆才">
  <meta name="robots" content="index,follow">
  <meta name="GOOGLEBOT" content="index,follow">
  <meta name="author" content="静觅丨崔庆才的个人站点">
  <meta name="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
  <meta property="og:type" content="website">
  <meta property="og:title" content="静觅">
  <meta property="og:url" content="https://cuiqingcai.com/page/2/index.html">
  <meta property="og:site_name" content="静觅">
  <meta property="og:description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
  <meta property="og:locale" content="zh_CN">
  <meta property="article:author" content="崔庆才">
  <meta property="article:tag" content="爬虫教程">
  <meta property="article:tag" content="爬虫">
  <meta property="article:tag" content="Python">
  <meta property="article:tag" content="Python爬虫">
  <meta property="article:tag" content="Python爬虫教程">
  <meta property="article:tag" content="爬虫书">
  <meta property="article:tag" content="静觅">
  <meta property="article:tag" content="崔庆才">
  <meta name="twitter:card" content="summary">
  <link rel="canonical" href="https://cuiqingcai.com/page/2/">
  <script id="page-configurations">
    // https://hexo.io/docs/variables.html
    CONFIG.page = {
      sidebar: "",
      isHome: true,
      isPost: false,
      lang: 'zh-CN'
    };

  </script>
  <title>静觅丨崔庆才的个人站点 - Python爬虫教程</title>
  <meta name="google-site-verification" content="p_bIcnvirkFzG2dYKuNDivKD8-STet5W7D-01woA2fc" />
  <meta name="sogou_site_verification" content="kBOV53NQqT" />
  <noscript>
    <style>
      .use-motion .brand,
      .use-motion .menu-item,
      .sidebar-inner,
      .use-motion .post-block,
      .use-motion .pagination,
      .use-motion .comments,
      .use-motion .post-header,
      .use-motion .post-body,
      .use-motion .collection-header
      {
        opacity: initial;
      }

      .use-motion .site-title,
      .use-motion .site-subtitle
      {
        opacity: initial;
        top: initial;
      }

      .use-motion .logo-line-before i
      {
        left: initial;
      }

      .use-motion .logo-line-after i
      {
        right: initial;
      }

    </style>
  </noscript>
  <link rel="alternate" href="/atom.xml" title="静觅" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner">
        <div class="site-brand-container">
          <div class="site-nav-toggle">
            <div class="toggle" aria-label="切换导航栏">
              <span class="toggle-line toggle-line-first"></span>
              <span class="toggle-line toggle-line-middle"></span>
              <span class="toggle-line toggle-line-last"></span>
            </div>
          </div>
          <div class="site-meta">
            <a href="/" class="brand" rel="start">
              <span class="logo-line-before"><i></i></span>
              <h1 class="site-title">静觅 <span class="site-subtitle"> 崔庆才的个人站点 - Python爬虫教程 </span>
              </h1>
              <span class="logo-line-after"><i></i></span>
            </a>
          </div>
          <div class="site-nav-right">
            <div class="toggle popup-trigger">
              <i class="fa fa-search fa-fw fa-lg"></i>
            </div>
          </div>
        </div>
        <nav class="site-nav">
          <ul id="menu" class="main-menu menu">
            <li class="menu-item menu-item-home">
              <a href="/" rel="section">首页</a>
            </li>
            <li class="menu-item menu-item-archives">
              <a href="/archives/" rel="section">文章列表</a>
            </li>
            <li class="menu-item menu-item-tags">
              <a href="/tags/" rel="section">文章标签</a>
            </li>
            <li class="menu-item menu-item-categories">
              <a href="/categories/" rel="section">文章分类</a>
            </li>
            <li class="menu-item menu-item-about">
              <a href="/about/" rel="section">关于博主</a>
            </li>
            <li class="menu-item menu-item-message">
              <a href="/message/" rel="section">给我留言</a>
            </li>
            <li class="menu-item menu-item-search">
              <a role="button" class="popup-trigger">搜索 </a>
            </li>
          </ul>
        </nav>
        <div class="search-pop-overlay">
          <div class="popup search-popup">
            <div class="search-header">
              <span class="search-icon">
                <i class="fa fa-search"></i>
              </span>
              <div class="search-input-container">
                <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
              </div>
              <span class="popup-btn-close">
                <i class="fa fa-times-circle"></i>
              </span>
            </div>
            <div id="search-result">
              <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>
    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
      <span>0%</span>
    </div>
    <div class="reading-progress-bar"></div>
    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div class="content index posts-expand">
            <div class="carousel">
              <div id="wowslider-container">
                <div class="ws_images">
                  <ul>
                    <li><a target="_blank" href="https://item.jd.com/13527222.html"><img title="Python3网络爬虫开发实战（第二版）上市了！" src="https://cdn.cuiqingcai.com/prwgs.png" /></a></li>
                    <li><a target="_blank" href="https://t.lagou.com/fRCBRsRCSN6FA"><img title="52讲轻松搞定网络爬虫" src="https://cdn.cuiqingcai.com/fqq5e.png" /></a></li>
                    <li><a target="_blank" href="https://cuiqingcai.com/4320.html"><img title="Python3网络爬虫开发视频教程" src="https://cdn.cuiqingcai.com/bjrny.jpg" /></a></li>
                    <li><a target="_blank" href="https://cuiqingcai.com/5094.html"><img title="爬虫代理哪家强？十大付费代理详细对比评测出炉！" src="https://cdn.cuiqingcai.com/nifs6.jpg" /></a></li>
                  </ul>
                </div>
                <div class="ws_thumbs">
                  <div>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/prwgs.png" /></a>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/fqq5e.png" /></a>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/bjrny.jpg" /></a>
                    <a target="_blank" href="#"><img src="https://cdn.cuiqingcai.com/nifs6.jpg" /></a>
                  </div>
                </div>
                <div class="ws_shadow"></div>
              </div>
            </div>
            <link rel="stylesheet" href="/lib/wowslide/slide.css">
            <script src="/lib/wowslide/jquery.min.js"></script>
            <script src="/lib/wowslide/slider.js"></script>
            <script>
              jQuery("#wowslider-container").wowSlider(
              {
                effect: "cube",
                prev: "",
                next: "",
                duration: 20 * 100,
                delay: 100 * 100,
                width: 716,
                height: 297,
                autoPlay: true,
                playPause: true,
                stopOnHover: false,
                loop: false,
                bullets: 0,
                caption: true,
                captionEffect: "slide",
                controls: true,
                onBeforeStep: 0,
                images: 0
              });

            </script>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202292.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202292.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - OpenCV 图像匹配识别滑动验证码缺口</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>上一节我们学习了利用 OCR 技术对图形验证码进行识别的方法，但随着互联网技术的发展，各种新型验证码层出不穷，最具有代表性的便是滑动验证码了。</p>
                  <p>本节我们首先介绍下滑动验证码的验证流程，然后介绍一个简易的利用图像处理技术来识别滑动验证码缺口的方法。</p>
                  <h2 id="1-滑动验证码"><a href="#1-滑动验证码" class="headerlink" title="1. 滑动验证码"></a>1. 滑动验证码</h2>
                  <p>说起滑动验证码，比较有代表性的服务商有极验、网易易盾等，验证码效果如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/imm7t.png" alt="极验"></p>
                  <p><img src="https://cdn.cuiqingcai.com/6s9pe.png" alt="网易易盾"></p>
                  <p>验证码下方通常会有一个滑轨，同时带有文字提示「拖动滑块完成拼图」，我们需要按住滑轨上的滑块向右拖拽，这时候验证码最左侧的滑块便会跟随滑轨上的滑块向右移动，在验证码右侧会有一个滑块缺口，我们需要恰好将滑块拖动到目标缺口处，这时候就算验证成功了，验证成功的效果如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/6c095.png" alt="image-20210418114633889"></p>
                  <p>所以，如果我们想要用爬虫来自动化完成这一流程的话，关键步骤有如下两个：</p>
                  <ul>
                    <li>识别出目标缺口的位置</li>
                    <li>将缺口滑动到对应位置</li>
                  </ul>
                  <p>其中第二步的实现有多种方式，比如我们可以用 Selenium 等自动化工具模拟完成这个流程，验证并登录成功之后获取对应的 Cookies 或 Token 等信息再进行后续的操作，但这种方法运行效率会比较低。另一种方法便是直接逆向验证码背后的 JavaScript 逻辑，将缺口信息直接传给 JavaScript 代码执行获取一些类似“密钥”的信息，再利用这些“密钥”进行下一步的操作。</p>
                  <blockquote>
                    <p>注意：由于某些出于安全考虑的原因，本书不会再介绍第二步的具体操作，而是只针对于第一步的技术问题进行讲解。</p>
                  </blockquote>
                  <p>因此，本节只会针对于第一步即如何识别出目标缺口的位置进行介绍，即给定一张验证码图片，如何用图像识别的方法识别出缺口的位置。</p>
                  <h2 id="2-基本原理"><a href="#2-基本原理" class="headerlink" title="2.基本原理"></a>2.基本原理</h2>
                  <p>本节我们会介绍利用 OpenCV 进行缺口识别的方法，输入一张带有缺口的验证码图片，输出缺口的位置（一般为缺口左侧横坐标）。</p>
                  <p>比如输入的验证码图片如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/eu5ko.png" alt="captcha"></p>
                  <p>最后输出的识别结果如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/hpetq.png" alt="image_label"></p>
                  <p>本节介绍的方法是利用 OpenCV 进行基本的图像处理来实现的，主要步骤包括：</p>
                  <ul>
                    <li>对验证码图片进行高斯模糊滤波处理，消除部分噪声干扰</li>
                    <li>对验证码图片应用边缘检测算法，通过调整相应阈值识别出滑块边缘</li>
                    <li>对上一步得到的各个边缘轮廓信息，通过对比面积、位置、周长等特征筛选出最可能的轮廓位置，得到缺口位置。</li>
                  </ul>
                  <h2 id="3-准备工作"><a href="#3-准备工作" class="headerlink" title="3.准备工作"></a>3.准备工作</h2>
                  <p>在本节开始之前请确保已经安装好了 python-opencv 库，安装方式如下：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> python-opencv</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果安装出现问题，可以参考详细的安装步骤：<a href="https://setup.scrape.center/python-opencv。" target="_blank" rel="noopener">https://setup.scrape.center/python-opencv。</a></p>
                  <p>另外建议提前准备一张滑动验证码图片，样例图片下载地址：<a href="https://github.com/Python3WebSpider/CrackSlideCaptcha/blob/cv/captcha.png，当然也可以从" target="_blank" rel="noopener">https://github.com/Python3WebSpider/CrackSlideCaptcha/blob/cv/captcha.png，当然也可以从</a> <a href="https://captcha1.scrape.center/" target="_blank" rel="noopener">https://captcha1.scrape.center/</a> 自行截取，最终的图片如上文所示。</p>
                  <h2 id="4-基础知识"><a href="#4-基础知识" class="headerlink" title="4.基础知识"></a>4.基础知识</h2>
                  <p>在真正开始介绍之前，我们先需要了解一些 OpenCV 的基础 API，以帮助我们更好地理解整个原理。</p>
                  <h3 id="高斯滤波"><a href="#高斯滤波" class="headerlink" title="高斯滤波"></a>高斯滤波</h3>
                  <p>高斯滤波是用来去除图像中的一些噪声的，基本效果其实就是把一张图像变得模糊化，减少一些图像噪声干扰，从而为下一步的边缘检测做好铺垫。</p>
                  <p>OpenCV 提供了一个用于实现高斯模糊的方法，叫做 GaussianBlur，方法声明如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GaussianBlur</span><span class="params">(src, ksize, sigmaX, dst=None, sigmaY=None, borderType=None)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>比较重要的参数介绍如下：</p>
                  <ul>
                    <li>src：即需要被处理的图像。</li>
                    <li>ksize：进行高斯滤波处理所用的高斯内核大小，它需要是一个元组，包含 x 和 y 两个维度。</li>
                    <li>sigmaX：表示高斯核函数在 X 方向的的标准偏差。</li>
                    <li>sigmaY：表示高斯核函数在 Y 方向的的标准偏差，若 sigmaY 为 0，就将它设为 sigmaX，如果 sigmaX 和 sigmaY 都是 0，那么 sigmaX 和 sigmaY 就通过 ksize 计算得出。</li>
                  </ul>
                  <p>这里 ksize 和 sigmaX 是必传参数，对本节样例图片，ksize 我们可以取 <code>(5, 5)</code>，sigmaX 可以取 0。</p>
                  <p>经过高斯滤波处理后，图像会变得模糊，效果如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/egiex.png" alt="image_gaussian_blur"></p>
                  <h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3>
                  <p>由于验证码目标缺口通常具有比较明显的边缘，所以借助于一些边缘检测算法并通过调整阈值是可以找出它的位置的。目前应用比较广泛的边缘检测算法是 Canny，它是 John F. Canny 于 1986 年开发出来的一个多级边缘检测算法，效果还是不错的，OpenCV 也对此算法进行了实现，方法名称就叫做 Canny，声明如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Canny</span><span class="params">(image, threshold1, threshold2, edges=None, apertureSize=None, L2gradient=None)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>比较重要的参数介绍如下：</p>
                  <ul>
                    <li>image：即需要被处理的图像。</li>
                    <li>threshold1、threshold2：两个阈值，分别为最小和最大判定临界点。</li>
                    <li>apertureSize：用于查找图像渐变的 Sobel 内核的大小。</li>
                    <li>L2gradient：指定用于查找梯度幅度的等式。</li>
                  </ul>
                  <p>通常来说，我们只需要设定 threshold1 和 threshold2 即可，其数值大小需要视不同图像而定，比如本节样例图片可以分别取 200 和 450。</p>
                  <p>经过边缘检测算法处理后，一些比较明显的边缘信息会被保留下来，效果如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/r9g5i.png" alt="image-20210418142819176"></p>
                  <h3 id="轮廓提取"><a href="#轮廓提取" class="headerlink" title="轮廓提取"></a>轮廓提取</h3>
                  <p>进行边缘检测处理后，我们可以看到图像中会保留有比较明显的边缘信息，下一步我们可以用 OpenCV 将边缘轮廓提取出来，这里需要用到 findContours 方法，方法声明如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findContours</span><span class="params">(image, mode, method, contours=None, hierarchy=None, offset=None)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>比较重要的参数介绍如下：</p>
                  <ul>
                    <li>image：即需要被处理的图像。</li>
                    <li>mode：定义轮廓的检索模式，详情见 OpenCV 的 RetrievalModes 的介绍。</li>
                    <li>method：定义轮廓的近似方法，详情见 OpenCV 的 ContourApproximationModes 的介绍。</li>
                  </ul>
                  <p>在这里，我们选取 mode 为 RETR_CCOMP，method 为 CHAIN_APPROX_SIMPLE，具体的选型标准可以参考 OpenCV 的文档介绍，这里不再展开讲解。</p>
                  <h3 id="外接矩形"><a href="#外接矩形" class="headerlink" title="外接矩形"></a>外接矩形</h3>
                  <p>提取到轮廓之后，为了方便进行判定，我们可以将轮廓的外界矩形计算出来，这样方便我们根据面积、位置、周长等参数进行判定，以得出该轮廓是不是目标滑块的轮廓。</p>
                  <p>计算外接矩形使用的方法是 boundingRect，方法声明如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">boundingRect</span><span class="params">(array)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>只有一个参数：</p>
                  <ul>
                    <li>array：可以是一个灰度图或者 2D 点集，这里可以传入轮廓信息。</li>
                  </ul>
                  <p>经过轮廓信息和外接矩形判定之后，我们可以得到类似如下结果：</p>
                  <p><img src="https://cdn.cuiqingcai.com/6s7ys.png" alt="image-20210418142752172"></p>
                  <p>可以看到这样就能成功获取各个轮廓的外接矩形，接下来我们根据外接矩形的面积、和位置就能筛选出缺口对应的位置了。</p>
                  <h3 id="轮廓面积"><a href="#轮廓面积" class="headerlink" title="轮廓面积"></a>轮廓面积</h3>
                  <p>现在已经得到了各个外接矩形，但是很明显有些矩形不是我们想要的，我们可以根据面积、周长等来进行筛选，这里就需要用到计算面积的方法，叫做 contourArea，方法定义如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">contourArea</span><span class="params">(contour, oriented=None)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数介绍如下：</p>
                  <ul>
                    <li>contour：轮廓信息。</li>
                    <li>oriented：面向区域标识符。有默认值 False。若为 True，该函数返回一个带符号的面积值，正负取决于轮廓的方向（顺时针还是逆时针）。若为 False，表示以绝对值返回。</li>
                  </ul>
                  <p>返回结果就是轮廓的面积。</p>
                  <h3 id="轮廓周长"><a href="#轮廓周长" class="headerlink" title="轮廓周长"></a>轮廓周长</h3>
                  <p>同样，周长的计算也有对应的方法，叫做 arcLength，方法定义如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">arcLength</span><span class="params">(curve, closed)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>参数介绍如下：</p>
                  <ul>
                    <li>curve：轮廓信息。</li>
                    <li>closed：表示轮廓是否封闭。</li>
                  </ul>
                  <p>返回结果就是轮廓的周长。</p>
                  <p>以上内容介绍了一些 OpenCV 内置方法，了解了这些方法的用法，我们可以对下文的具体实现有更透彻的理解。</p>
                  <h2 id="5-缺口识别"><a href="#5-缺口识别" class="headerlink" title="5.缺口识别"></a>5.缺口识别</h2>
                  <p>接下来我们就开始真正实现一下缺口识别算法了。</p>
                  <p>首先我们定义高斯滤波、边缘检测、轮廓提取的三个方法，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">GAUSSIAN_BLUR_KERNEL_SIZE = (<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">GAUSSIAN_BLUR_SIGMA_X = <span class="number">0</span></span><br><span class="line">CANNY_THRESHOLD1 = <span class="number">200</span></span><br><span class="line">CANNY_THRESHOLD2 = <span class="number">450</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gaussian_blur_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cv2.GaussianBlur(image, GAUSSIAN_BLUR_KERNEL_SIZE, GAUSSIAN_BLUR_SIGMA_X)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_canny_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cv2.Canny(image, CANNY_THRESHOLD1, CANNY_THRESHOLD2)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_contours</span><span class="params">(image)</span>:</span></span><br><span class="line">    contours, _ = cv2.findContours(image, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">    <span class="keyword">return</span> contours</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>三个方法介绍如下：</p>
                  <ul>
                    <li>get_gaussian_blur_image：传入待处理图像信息，返回高斯滤波处理后的图像，ksize 定义为 <code>(5, 5)</code>，sigmaX 定义为 0。</li>
                    <li>get_canny_image：传入待处理图像信息，返回边缘检测处理后的图像，threshold1 和 threshold2 分别定义为 200 和 450。</li>
                    <li>get_contours：传入待处理图像信息，返回检测到的轮廓信息，这里 mode 设定为 RETR_CCOMP，method 设定为 CHAIN_APPROX_SIMPLE。</li>
                  </ul>
                  <p>原始待识别验证码命名为 captcha.png，接下来我们分别调用以上方法对验证码进行处理：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">image_raw = cv2.imread(<span class="string">'captcha.png'</span>)</span><br><span class="line">image_height, image_width, _ = image_raw.shape</span><br><span class="line">image_gaussian_blur = get_gaussian_blur_image(image_raw)</span><br><span class="line">image_canny = get_canny_image(image_gaussian_blur)</span><br><span class="line">contours = get_contours(image_canny)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>原始图片我们命名为 image_raw 变量，读取图片之后获取其宽高像素信息，接着调用了 get_gaussian_blur_image 方法进行高斯滤波处理，返回结果命名为 image_gaussian_blur，接着将 image_gaussian_blur 传给 get_canny_image 方法进行边缘检测处理，返回结果命名为 image_canny，接着调用 get_contours 方法得到各个边缘的轮廓信息，赋值为 contours 变量。</p>
                  <p>好，得到各个轮廓信息之后，我们便需要根据各个轮廓的外接矩形的面积、周长、位置来筛选我们想要结果了。</p>
                  <p>所以，我们需要先确定怎么来筛选，比如面积我们可以设定一个范围，周长设定一个范围，缺口位置设定一个范围，通过实际测量，我们可以得出目标缺口的外接矩形的高度大约是验证码高度的 0.25 倍，宽度大约是验证码宽度的 0.15 倍。在允许误差 20% 的情况下，根据验证码的宽高信息我们大约可以计算出面积、周长的范围，同时缺口位置（缺口左侧）也有一个最小偏移值，比如最小偏移是验证码宽度的 0.2 倍，最大偏移是验证码宽度的 0.85 倍。综合这些内容，我们可以定义三个阈值方法：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_contour_area_threshold</span><span class="params">(image_width, image_height)</span>:</span></span><br><span class="line">    contour_area_min = (image_width * <span class="number">0.15</span>) * (image_height * <span class="number">0.25</span>) * <span class="number">0.8</span></span><br><span class="line">    contour_area_max = (image_width * <span class="number">0.15</span>) * (image_height * <span class="number">0.25</span>) * <span class="number">1.2</span></span><br><span class="line">    <span class="keyword">return</span> contour_area_min, contour_area_max</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_arc_length_threshold</span><span class="params">(image_width, image_height)</span>:</span></span><br><span class="line">    arc_length_min = ((image_width * <span class="number">0.15</span>) + (image_height * <span class="number">0.25</span>)) * <span class="number">2</span> * <span class="number">0.8</span></span><br><span class="line">    arc_length_max = ((image_width * <span class="number">0.15</span>) + (image_height * <span class="number">0.25</span>)) * <span class="number">2</span> * <span class="number">1.2</span></span><br><span class="line">    <span class="keyword">return</span> arc_length_min, arc_length_max</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_offset_threshold</span><span class="params">(image_width)</span>:</span></span><br><span class="line">    offset_min = <span class="number">0.2</span> * image_width</span><br><span class="line">    offset_max = <span class="number">0.85</span> * image_width</span><br><span class="line">    <span class="keyword">return</span> offset_min, offset_max</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>三个方法介绍如下：</p>
                  <ul>
                    <li>get_contour_area_threshold：定义目标轮廓的下限和上限面积，分别为 contour_area_min 和 contour_area_max。</li>
                    <li>get_arc_length_threshold：定义目标轮廓的下限和上限周长，分别为 arc_length_min 和 arc_length_max。</li>
                    <li>get_offset_threshold：定义目标轮廓左侧的下限和上限偏移量，分别为 offset_min 和 offset_max。</li>
                  </ul>
                  <p>最后我们只需要遍历各个轮廓信息，根据上述限定条件进行筛选，最后得出目标轮廓信息即可，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">contour_area_min, contour_area_max = get_contour_area_threshold(image_width, image_height)</span><br><span class="line">arc_length_min, arc_length_max = get_arc_length_threshold(image_width, image_height)</span><br><span class="line">offset_min, offset_max = get_offset_threshold(image_width)</span><br><span class="line">offset = <span class="literal">None</span></span><br><span class="line"><span class="keyword">for</span> contour <span class="keyword">in</span> contours:</span><br><span class="line">    x, y, w, h = cv2.boundingRect(contour)</span><br><span class="line">    <span class="keyword">if</span> contour_area_min &lt; cv2.contourArea(contour) &lt; contour_area_max <span class="keyword">and</span> \</span><br><span class="line">            arc_length_min &lt; cv2.arcLength(contour, <span class="literal">True</span>) &lt; arc_length_max <span class="keyword">and</span> \</span><br><span class="line">            offset_min &lt; x &lt; offset_max:</span><br><span class="line">        cv2.rectangle(image_raw, (x, y), (x + w, y + h), (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">        offset = x</span><br><span class="line">cv2.imwrite(<span class="string">'image_label.png'</span>, image_raw)</span><br><span class="line">print(<span class="string">'offset'</span>, offset)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们首先调用了 get_contour_area_threshold、get_arc_length_threshold、get_offset_threshold 方法获取了轮廓的判定阈值，然后遍历了 contours 根据这些阈值进行了筛选，最终得到的外接矩形的 x 值就是目标缺口的偏移量。</p>
                  <p>同时目标缺口的外接矩形我们也调用了 rectangle 方法进行了标注，最终将其保存为 image_label.png 图像。</p>
                  <p>最终运行结果如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">offset <span class="number">163</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>同时得到输出的 image_label.png 文件如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/e361d.png" alt="image_label"></p>
                  <p>这样我们就成功提取出来了目标滑块的位置了，本节的问题得以解决。</p>
                  <blockquote>
                    <p>注意：出于安全考虑，本书只针对于第一步 - 识别验证码缺口位置的的技术问题进行讲解，关于怎样去模拟滑动或者绕过验证码，本书不再进行介绍，可以自行搜索相关资料探索。</p>
                  </blockquote>
                  <h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2>
                  <p>本节我们介绍了利用 OpenCV 来识别滑动验证码缺口的方法，其中涉及到了一些关键的图像处理和识别技术，如高斯模糊、边缘检测、轮廓提取等算法。了解了基本的图像识别技术后，我们可以举一反三，将其应用到其他类型的工作上，也会很有帮助。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/CrackSlideCaptcha/tree/cv，注意这里是" target="_blank" rel="noopener">https://github.com/Python3WebSpider/CrackSlideCaptcha/tree/cv，注意这里是</a> cv 分支。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-03-01 08:55:45" itemprop="dateCreated datePublished" datetime="2022-03-01T08:55:45+08:00">2022-03-01</time>
                </span>
                <span id="/202292.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - OpenCV 图像匹配识别滑动验证码缺口" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>6.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>6 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202291.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202291.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - OCR 识别图形验证码</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>各类网站采用了各种各样的措施来反爬虫，其中一个措施便是使用验证码。随着技术的发展，验证码的花样越来越多。验证码最初是几个数字组合的简单的图形，后来加入了英文字母和混淆曲线。还有一些网站使用了中文字符验证码，这使得识别愈发困难。</p>
                  <p>12306 验证码的出现使得行为验证码开始发展起来，用过 12306 的用户肯定多少为它的验证码头疼过，我们需要识别文字，点击与文字描述相符的图片，验证码完全正确，验证才能通过。随着技术的发展，现在这种交互式验证码越来越多，如滑动验证码需要将对应的滑块拖动到指定位置才能完成验证，点选验证码则需要点击正确的图形或文字才能通过验证。</p>
                  <p>验证码变得越来越复杂，爬虫的工作也变得越发艰难，有时候我们必须通过验证码的验证才可以访问页面。</p>
                  <p>本章就针对验证码的识别进行统一讲解，涉及的验证码有普通图形验证码、滑动验证码、点选验证码、手机验证码等，这些验证码识别的方式和思路各有不同，有直接使用图像处理库完成的，有的则是借助于深度学习技术完成的，有的则是借助于一些工具和平台完成的。虽然说技术各有不同，但了解这些验证码的识别方式之后，我们可以举一反三，用类似的方法识别其他类型验证码。</p>
                  <p>我们首先来看最简单的一种验证码，即图形验证码，这种验证码最早出现，现在依然也很常见，一般由 4 位左右字母或者数字组成。</p>
                  <p>例如这个案例网站 <a href="https://captcha7.scrape.center/" target="_blank" rel="noopener">https://captcha7.scrape.center/</a> 就可以看到类似的验证码，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/iiutw.png" alt=""></p>
                  <p>这类验证码整体上比较规整，没有过多干扰线和干扰点，且文字没有大幅度的变形和旋转。</p>
                  <p>对于这一类的验证码我们就可以使用 OCR 技术来进行识别。</p>
                  <h2 id="1-OCR-技术"><a href="#1-OCR-技术" class="headerlink" title="1. OCR 技术"></a>1. OCR 技术</h2>
                  <p>OCR，即 Optical Character Recognition，中文翻译叫做光学字符识别。它是指电子设备（例如扫描仪或数码相机）检查纸上打印的字符，通过检测暗、亮的模式确定其形状，然后用字符识别方法将形状翻译成计算机文字的过程。OCR 现在已经广泛应用于生产生活中，如文档识别、证件识别、字幕识别、文档检索等等。当然对于本节所述的图形验证码的识别也没有问题。</p>
                  <p>本节我们会以当前示例网站的验证码为例来讲解利用 OCR 来识别图形验证码的流程，输入上是一上图验证码的图片，输出就是验证码识别结果。</p>
                  <h2 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h2>
                  <p>识别图形验证码需要 Tesserocr 库，本库的安装相对没有那么简单，可以参考 <a href="https://setup.scrape.center/tesserocr" target="_blank" rel="noopener">https://setup.scrape.center/tesserocr</a></p>
                  <p>另外在本节学习过程中还需要安装 Selenium、Pillow、Numpy，Retrying 库用作模拟登录、图像处理和操作重试，我们可以使用 pip3 来进行安装：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">pip3</span> install <span class="keyword">selenium </span>pillow numpy retrying</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果某个库安装有问题，可以参考如下链接：</p>
                  <ul>
                    <li>Selenium：<a href="https://setup.scrape.center/selenium" target="_blank" rel="noopener">https://setup.scrape.center/selenium</a></li>
                    <li>Pillow：<a href="https://setup.scrape.center/pillow" target="_blank" rel="noopener">https://setup.scrape.center/pillow</a></li>
                    <li>Numpy：<a href="https://setup.scrape.center/numpy" target="_blank" rel="noopener">https://setup.scrape.center/numpy</a></li>
                    <li>retrying：<a href="https://setup.scrape.center/retrying" target="_blank" rel="noopener">https://setup.scrape.center/retrying</a></li>
                  </ul>
                  <p>安装好了如上库之后，我们就可以开始本节的学习了。</p>
                  <h2 id="3-获取验证码"><a href="#3-获取验证码" class="headerlink" title="3. 获取验证码"></a>3. 获取验证码</h2>
                  <p>为了便于实验，我们先将验证码的图片保存到本地。</p>
                  <p>我们可以在浏览器中打开上述示例网站，然后右键点击这张验证码图片，将其保存到本地，命名为 captcha.png，示例如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/3tfw7.png" alt=""></p>
                  <p>这样我们就可以得到一张验证码图片，以供测试识别使用。</p>
                  <h2 id="4-识别测试"><a href="#4-识别测试" class="headerlink" title="4. 识别测试"></a>4. 识别测试</h2>
                  <p>接下来新建一个项目，将验证码图片放到项目根目录下，用 tesserocr 库识别该验证码，代码如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> tesserocr</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">image = Image.open(<span class="string">'captcha.png'</span>)</span><br><span class="line">result = tesserocr.image_to_text(image)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们新建了一个 Image 对象，调用了 tesserocr 的 <code>image_to_text</code>方法。传入该 Image 对象即可完成识别，实现过程非常简单，结果如下所示：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">d241</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外，tesserocr 还有一个更加简单的方法，这个方法可直接将图片文件转为字符串，代码如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> tesserocr</span><br><span class="line">print(tesserocr.file_to_text(<span class="string">'captcha.png'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以得到同样的输出结果。</p>
                  <p>这时候我们可以看到，通过 OCR 技术我们便可以成功识别出验证码的内容了。</p>
                  <h2 id="5-验证码处理"><a href="#5-验证码处理" class="headerlink" title="5. 验证码处理"></a>5. 验证码处理</h2>
                  <p>接下来我们换一个验证码，将其命名为 captcha2.png，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/dgfbq.png" alt=""></p>
                  <p>重新用下面的代码来测试：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> tesserocr</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">image = Image.open(<span class="string">'captcha2.png'</span>)</span><br><span class="line">result = tesserocr.image_to_text(image)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到如下输出结果：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">-b32d</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次识别和实际结果有偏差，多了一些干扰结果，这是因为验证码内的多余的点干扰了图像的识别，导致出现了一些多余的内容。</p>
                  <p>对于这种情况，我们可以需要做一下额外的处理，把一些干扰信息去掉。</p>
                  <p>这里观察到图片里面其实有一些杂乱的点，而这些点的颜色大都比文本更浅一点，因此我们可以做一些预处理，将干扰的点通过颜色来排除掉。</p>
                  <p>我们可以首先将原来的图像转化为数组看下维度：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> tesserocr</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">image = Image.open(<span class="string">'captcha2.png'</span>)</span><br><span class="line">print(np.array(image).shape)</span><br><span class="line">print(image.mode)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">(<span class="number">38</span>, <span class="number">112</span>, <span class="number">4</span>)</span><br><span class="line">RGBA</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现这个图片其实是一个三维数组，前两维 38 和 112 代表其高和宽，最后一维 4 则是每个像素点的表示向量。为什么是 4 呢，因为最后一维是一个长度为 4 的数组，分别代表 R（红色）、G（绿色）、B（蓝色）、A（透明度），即一个像素点有四个数字表示。那为什么是 RGBA 四个数字而不是 RGB 或其他呢？这是因为 image 的模式 mode 是 RGBA，即有透明通道的真彩色，我们看到第二行输出也印证了这一点。</p>
                  <p>模式 mode 定义了图像的类型和像素的位宽，一共有 9 种类型：</p>
                  <ul>
                    <li>1：像素用 1 位表示，Python 中表示为 True 或 False，即二值化。</li>
                    <li>L：像素用 8 位表示，取值 0-255，表示灰度图像，数字越小，颜色越黑。</li>
                    <li>P：像素用 8 位表示，即调色板数据。</li>
                    <li>RGB：像素用 3x8 位表示，即真彩色。</li>
                    <li>RGBA：像素用 4x8 位表示，即有透明通道的真彩色。</li>
                    <li>CMYK：像素用 4x8 位表示，即印刷四色模式。</li>
                    <li>YCbCr：像素用 3x8 位表示，即彩色视频格式。</li>
                    <li>I：像素用 32 位整型表示。</li>
                    <li>F：像素用 32 位浮点型表示。</li>
                  </ul>
                  <p>为了方便处理，我们可以将 RGBA 模式转为更简单的 L 模式，即灰度图像。</p>
                  <p>我们可以利用 Image 对象的 convert 方法参数传入 L，即可将图片转化为灰度图像，代码如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">image = image.convert(<span class="string">'L'</span>)</span><br><span class="line">image.show()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>或者传入 1 即可将图片进行二值化处理，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">image = image.convert(<span class="string">'1'</span>)</span><br><span class="line">image.show()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们就转为灰度图像，然后根据阈值筛选掉图片中的干扰点，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">image = Image.open(<span class="string">'captcha2.png'</span>)</span><br><span class="line">image = image.convert(<span class="string">'L'</span>)</span><br><span class="line">threshold = <span class="number">50</span></span><br><span class="line">array = np.array(image)</span><br><span class="line">array = np.where(array &gt; threshold, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">image = Image.fromarray(array.astype(<span class="string">'uint8'</span>))</span><br><span class="line">image.show()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里，变量 threshold 代表灰度的阈值，这里设置为 50。接着我们将图片 image 转化为了 Numpy 数组，接着利用 Numpy 的 where 方法对数组进行筛选和处理，这里指定了大于阈值的就设置为 255，即白色，否则就是 0，即黑色。</p>
                  <p>最后看下图片处理完之后是什么结果：</p>
                  <p><img src="https://cdn.cuiqingcai.com/nx1tz.png" alt=""></p>
                  <p>我们发现原来验证码中的很多点已经被去掉了，整个验证码变得黑白分明。这时重新识别验证码，代码如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> tesserocr</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">image = Image.open(<span class="string">'captcha2.png'</span>)</span><br><span class="line">image = image.convert(<span class="string">'L'</span>)</span><br><span class="line">threshold = <span class="number">50</span></span><br><span class="line">array = np.array(image)</span><br><span class="line">array = np.where(array &gt; threshold, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">image = Image.fromarray(array.astype(<span class="string">'uint8'</span>))</span><br><span class="line">print(tesserocr.image_to_text(image))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>即可发现运行结果变成如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">b32d</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>所以，针对一些有干扰的图片，我们可以做一些去噪处理，这会提高图片识别的正确率。</p>
                  <h2 id="6-识别实战"><a href="#6-识别实战" class="headerlink" title="6. 识别实战"></a>6. 识别实战</h2>
                  <p>最后，我们可以来尝试下用自动化的方式来对案例进行验证码识别处理，这里我们使用 Selenium 来完成这个操作，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> tesserocr</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(image)</span>:</span></span><br><span class="line">    image = image.convert(<span class="string">'L'</span>)</span><br><span class="line">    array = np.array(image)</span><br><span class="line">    array = np.where(array &gt; <span class="number">50</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line">    image = Image.fromarray(array.astype(<span class="string">'uint8'</span>))</span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@retry(stop_max_attempt_number=10, retry_on_result=lambda x: x is False)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">'https://captcha7.scrape.center/'</span>)</span><br><span class="line">    browser.find_element_by_css_selector(<span class="string">'.username input[type="text"]'</span>).send_keys(<span class="string">'admin'</span>)</span><br><span class="line">    browser.find_element_by_css_selector(<span class="string">'.password input[type="password"]'</span>).send_keys(<span class="string">'admin'</span>)</span><br><span class="line">    captcha = browser.find_element_by_css_selector(<span class="string">'#captcha'</span>)</span><br><span class="line">    image = Image.open(BytesIO(captcha.screenshot_as_png))</span><br><span class="line">    image = preprocess(image)</span><br><span class="line">    captcha = tesserocr.image_to_text(image)</span><br><span class="line">    captcha = re.sub(<span class="string">'[^A-Za-z0-9]'</span>, <span class="string">''</span>, captcha)</span><br><span class="line">    browser.find_element_by_css_selector(<span class="string">'.captcha input[type="text"]'</span>).send_keys(captcha)</span><br><span class="line">    browser.find_element_by_css_selector(<span class="string">'.login'</span>).click()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        WebDriverWait(browser, <span class="number">10</span>).until(EC.presence_of_element_located((By.XPATH, <span class="string">'//h2[contains(., "登录成功")]'</span>)))</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">        browser.close()</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">except</span> TimeoutException:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    browser = webdriver.Chrome()</span><br><span class="line">    login()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们首先定义了一个 preprocess 方法，用于验证码的噪声处理，逻辑就和前面说的是一样的。</p>
                  <p>接着我们定义了一个 login 方法，其逻辑执行步骤是：</p>
                  <ul>
                    <li>打开样例网站</li>
                    <li>找到用户名输入框，输入用户名</li>
                    <li>找到密码输入框，输入密码</li>
                    <li>找到验证码图片并截取，转化为 Image 对象</li>
                    <li>预处理验证码，去除噪声</li>
                    <li>对验证码进行识别，得到识别结果</li>
                    <li>识别结果去除一些非字母和数字字符</li>
                    <li>找到验证码输入框，输入验证码结果</li>
                    <li>点击登录按钮</li>
                    <li>等待「登录成功」字样的出现，如果出现则证明登录成功，否则重复以上步骤重试。</li>
                  </ul>
                  <p>在这里我们还用到了 retrying 来指定了重试条件和重试次数，以保证在识别出错的情况下反复重试，增加总的成功概率。</p>
                  <p>运行代码我们可以观察到浏览器弹出并执行以上流程，可能重试几次后得到登录成功的页面，运行过程如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/ixnm2.png" alt=""></p>
                  <p>登录成功后的结果如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/bvczz.png" alt=""></p>
                  <p>到这里，我们就能成功通过 OCR 技术识别成功验证码，并将其应用到模拟登录的过程中了。</p>
                  <h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2>
                  <p>本节我们了解了利用 Tesserocr 识别验证码的过程并将其应用于实战案例中实现了模拟登录。为了提高 Tesserocr 的识别准确率，我们可以对验证码图像进行预处理去除一些干扰，识别准确率会大大提高。但总归来说 Tesserocr 识别验证码的准确率并不是很高，下一节我们来介绍其他识别验证码的方案。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/CrackImageCaptcha" target="_blank" rel="noopener">https://github.com/Python3WebSpider/CrackImageCaptcha</a></p>
                  <p>本文参考资料：</p>
                  <ul>
                    <li>文档 - OCR - 百度百科：<a href="https://baike.baidu.com/item/OCR" target="_blank" rel="noopener">https://baike.baidu.com/item/OCR</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-28 07:31:21" itemprop="dateCreated datePublished" datetime="2022-02-28T07:31:21+08:00">2022-02-28</time>
                </span>
                <span id="/202291.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - OCR 识别图形验证码" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>6.2k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>6 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36055.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 个人随笔 <i class="label-arrow"></i>
                  </a>
                  <a href="/36055.html" class="post-title-link" itemprop="url">你当像鸟，飞往你的山</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>这篇文章其实是我对一本书《你当像鸟飞往你的山》的读后感。</p>
                  <p>你可能在逛书店的时候看到过这本书，因为这本书一直占据畅销书的前几名，也曾作为比尔盖茨年度荐书 第一名和比尔盖茨年度荐书第一名畅销世界。</p>
                  <p><img src="https://cdn.cuiqingcai.com/87wx9.jpg" alt=""></p>
                  <p>说起来这本书，真的我从开始读到完全读完花了大半年的时间，我其实对阅读这样的“长篇”记述性的书读起来并不怎么在行，一直断断续续在读，也一直断断续续在领悟这本书传达给我的深意，于是最后，这篇文章就诞生了。</p>
                  <p>本书的作者叫塔拉·韦斯特弗，在 1986 年生于美国爱达荷州，在她 17 岁之前从来没有上过学，一直在大山里和父母、哥哥姐姐们生活在肥料厂，但她通过自身的努力考上了大学，进而取得了剑桥大学的博士学位。一开始我看这本书的宣传和介绍以为就是一本讲差生克服种种困难逆袭变身学霸的故事，但是读了之后才发现，整本书的重点并不在描写自己多么刻苦学习，描写的是自己的整个成长和转变历程，是一个有创伤、成长和最终蜕变的故事，讲述的是作者如何冲破原生家庭的重重阻碍、如何和自己心理作斗争和抉择、如何寻找到真正自我的故事。</p>
                  <p>这本书的中文名叫《你当像鸟飞往你的山》，但英文就叫《Educated》，看起来毫不相关，一开始我非常诧异这俩名字到底有啥联系，然而读了之后，我才发现二者联系还是很密切的。Educated 意思就是教育，这是本书的核心关键词，作者通过教育救赎了自己，通过不断地教育，完成了自我的成长和蜕变。而《你当像鸟飞往你的山》其实就是在教育之上的两层含义，包括逃离和追寻真正的自我。</p>
                  <p>作者塔拉出生在一个非常让我难以想象的家庭之中，在一座大山里，父亲是摩门教的忠实信徒，同时性格比较抑郁狂躁，他不相信政府、学校、医院等任何组织，同时也在塔拉小的时候向她灌输类似的理念。而且父亲觉得世界末日终有一天会到来的，所以他还在自家的地窖中存储各种食物、罐头、汽油等等物资，母亲则是基本依附于父亲的，整体的家庭就是“男尊女卑”。塔拉一共有五个哥哥和一个姐姐，父亲会让自己的各个儿子女儿去废料厂搬运和整理各种废弃物、钢铁赚钱。没错，十几年来，塔拉就是这么过来的。其实我们就想象成，在一个偏远的大山里面，塔拉整个家庭生活条件困苦，从小没有上过学，和几个哥哥姐姐、父亲去拣拾废料为生，同时期间也受父母灌输的思想教育而成长。我们想想，假如真的有这么一个人，可能她的一生就在这样的节奏下慢慢过去了，从出生到死亡，伴随着自己生活的就是一堆废铜烂铁，生活一眼望得到头。</p>
                  <p>在这样阴暗的生活条件下，会有一束光吗？有的，她的哥哥泰勒就是那一束光，是他引领塔拉走向了教育的大门。</p>
                  <p>在本书的扉页印着四个字，“献给泰勒”，所以在阅读之初我就比较诧异，这个泰勒是何许人也？所以在阅读的时候我就去留意泰勒这个人物。真的，可以说，没有泰勒，塔拉的生活可能就如同前面所说的那样，在大山里面终其她的一生了。</p>
                  <p>泰勒是个比较内向的孩子，还容易紧张，还天生口吃，他唯一的朋友可能就是唱片和书籍。在塔拉年少的时候，泰勒带塔拉了解了唱片、书籍等东西，同时泰勒还通过自学考取了杨百翰大学。泰勒曾经对塔拉说过：“你可以选择像现在生活，也可以选择像我一样，考进杨百翰大学。”塔拉后来选择了后者，在和父亲一起打工的日子里，塔拉找书自学，终于她成功考取了杨百翰大学。后来，她凭借自己的努力和天赋，后来又获得了剑桥大学的博士学位，完成了自己的蜕变。</p>
                  <p>但这个过程是非常艰难的，尤其是她从小接受了原生家庭这样的启蒙，迈出这一步对她来说何其艰难。塔拉的蜕变和成长历经了各种反复挣扎和思想斗争，也承受了难以想象的艰辛。</p>
                  <p>在我理解，难点可能有这么两点：</p>
                  <ul>
                    <li>塔拉从小就没有接受过什么教育，家里也很难给到什么支持，她的学习条件很差，考取大学之前都得挤时间来学习。考上大学之后基础也肯定不好，跟上同龄人甚至超越同龄人需要付出常人难以想象的努力。</li>
                    <li>从小塔拉就在大山里面成长，她从小的思想就被父母灌输，原生家庭的影响是巨大的。很多很多人可能在这样的环境下就妥协了，放弃挣扎了，逃离这样的生活需要面临巨大的阻力，不仅来自于家庭的阻拦，更多的是冲破自身的思想禁锢，能思考到自己究竟想要什么。</li>
                  </ul>
                  <p>是的，塔拉最终做到了。她很努力，当然也很聪慧，同时也有不少贵人相助。比如她的老师给她思想上的引导，帮她申请助学金，推荐上剑桥大学等等。这几点我觉得真的都是缺一不可。我们可以说她运气不错，但是少了她自身的努力和拼搏，再多的聪慧和贵人相助都是白搭。</p>
                  <p>塔拉在蜕变和成长的过程中学了很多哲学、历史等书籍的熏陶，在学习过程中，她了解到了一些思想上的差异和碰撞，比如即使是史学家也可能由于认知局限而产生错误的观点。所以，她也慢慢思考到，父亲从小对自己灌输的观点也未必是正确的。在不断学习和教育的过程中，塔拉的认知被提高，不断更新自己的挂念，不断重塑自己的思想，最终蜕变并成长成了更好的自我。</p>
                  <p>但不得不提的是，塔拉最终逃离大山，最终也付出了和家庭分离甚至说决裂的代价。后来她和她的父母、在大山的哥哥们几乎没有了联系。多年之后，塔拉试图回到大山和家庭和解，但是最终也没有看到团圆的结局，毕竟真的没法回去了。但塔拉为什么选择去尝试和解呢？或许还是出于爱吧。其实塔拉的父母还是爱塔拉的，有一个画面我印象非常深刻，在父亲得知她要去大洋彼岸的剑桥大学读书的时候，父亲对塔拉说：“无论你在哪个角落，我们都可以去找你。我在地下埋了一千加仑汽油，世界末日来临时我可以去接你，带你回家，让你平平安安的，但要是你去了大洋彼岸…”。是的，父亲是爱她的，但爱并不能让她放弃自己的人生。</p>
                  <p>塔拉说：“你可以爱一个人，但仍然选择和他说再见；你可以每天都想念一个人，但仍然庆幸他已不在你的生命中…”。</p>
                  <p>嗯，写到这里，我又理解到了什么。</p>
                  <p>是的，或许总有一些人即使互相深爱着彼此，但如果二者无法达成观念上的一致，无法真正理解对方的话，最好的结局或许就是分开吧。在这里我说的是塔拉的家庭的理解，但也可以扩展到其他的地方。</p>
                  <p>嗯，最近我也在看阿德勒心理学，像《被讨厌的勇气》，阿德勒有这样的一句话：“幸福的人用童年治愈一生，不幸的人用一生治愈童年。”原生家庭的影响对一个人真的是巨大的，这个影响可能需要用一生来弥补和改变。</p>
                  <p>但是，这本书告诉我们，生活在不幸的家庭，将来就一定会不幸吗？未必的。塔拉面临这样的家庭，面临这样的逆境，她最终成功了，一般情况下，我们面临的困难可能比塔拉小多了，塔拉可以，我们其实也可以。但这个蜕变的过程中，什么才是最重要的呢？是自己强大的内心，只有内心的强大的力量才能促成这种改变。</p>
                  <p>我想进一步展开升华下主旨。</p>
                  <p>反过来映射一下，对于我们的家庭来说。可能从小父母就说过：“我这么做是为了你好”，年少的时候，我们很多事都是听父母的，小时候的很多的选择一直到长大，读中学、上大学、选专业、就业、结婚、生子仿佛很多事情都很多受到父母的引导、操控，甚至我们自己就主动变得事事都去听父母的，甚至习以为常，甚至都觉得不应该去反抗，以为这些都是理所当然。但想想，真的是对的吗？</p>
                  <p>另外试想，如果说这一生，我们就是在这样就业、赚钱、结婚、生子、抚养孩子，终老一生，这是我们想要的吗？你心甘情愿自己的一生就这么过去吗？不想着去经历些什么吗？你小时候的梦想还在吗？多问问自己，真的是这样的吗？我们一直追逐的金钱、地位，到头来真的是最重要的吗？我们从小到老，承担着的这些角色，这些生活，真的是自己想要的吗？如果你的确想清楚了，这就是你想要的，或者和父母的设想完全一致，那可以，勇敢去做。如果答案是否，那或许要想想，是否要做出一些改变？</p>
                  <p>嗯，我还想说的是，父母不应该以爱之名去操控孩子的成长，可以给予帮助，但不能决定孩子的未来。反过来，孩子也是一样，不能以自己以为的正确去改变父母。</p>
                  <p>每个人，注定地只能去自我探寻自我、自我选择、自我教育、自我塑造。</p>
                  <p>走大家都觉得“正确”的事情很难，改变也可能很难，想清楚，每一种方式都会有牺牲，每个改变都可能带来不一样的生活。</p>
                  <p>每个人的生命其实都是一种自我救赎，有时虽然孤独，但是充满力量，遵从自己的内心，想想自己真正想要什么，想想自己想变成怎样的人。如果现在没有答案，那多去看看，多去思考思考。</p>
                  <p>希望你和我，都能有一个无悔的人生。</p>
                  <p>你当像鸟，飞往你的山。</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-28 04:00:42" itemprop="dateCreated datePublished" datetime="2022-02-28T04:00:42+08:00">2022-02-28</time>
                </span>
                <span id="/36055.html" class="post-meta-item leancloud_visitors" data-flag-title="你当像鸟，飞往你的山" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.4k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36052.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 爬虫 <i class="label-arrow"></i>
                  </a>
                  <a href="/36052.html" class="post-title-link" itemprop="url">一个神器，大幅提升爬取效率</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>在做爬虫的时候，我们往往可能这些情况：</p>
                  <ul>
                    <li>网站比较复杂，会碰到很多重复请求。</li>
                    <li>有时候爬虫意外中断了，但我们没有保存爬取状态，再次运行就需要重新爬取。</li>
                  </ul>
                  <p>还有诸如此类的问题。</p>
                  <p>那怎么解决这些重复爬取的问题呢？大家很可能都想到了“缓存”，也就是说，爬取过一遍就直接跳过爬取。</p>
                  <p>那一般怎么做呢？</p>
                  <p>比如我写一个逻辑，把已经爬取过的 URL 保存到文件或者数据库里面，每次爬取之前检查一下是不是在列表或数据库里面就好了。</p>
                  <p>是的，这个思路没问题，但有没有想过这些问题：</p>
                  <ul>
                    <li>写入到文件或者数据库可能是永久性的，如果我想控制缓存的有效时间，那就还得有个过期时间控制。</li>
                    <li>这个缓存根据什么来判断？如果仅仅是 URL 本身够吗？还有 Request Method、Request Headers 呢，如果它们不一样了，那还要不要用缓存？</li>
                    <li>如果我们有好多项目，难道都没有一个通用的解决方案吗？</li>
                  </ul>
                  <p>的确是些问题，实现起来确实要考虑很多问题。</p>
                  <p>不过不用担心，今天给大家介绍一个神器，可以帮助我们通通解决如上的问题。</p>
                  <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2>
                  <p>它就是 requests-cache，是 requests 库的一个扩展包，利用它我们可以非常方便地实现请求的缓存，直接得到对应的爬取结果。</p>
                  <ul>
                    <li>GitHub：<a href="https://github.com/reclosedev/requests-cache" target="_blank" rel="noopener">https://github.com/reclosedev/requests-cache</a></li>
                    <li>PyPi：<a href="https://pypi.org/project/requests-cache/" target="_blank" rel="noopener">https://pypi.org/project/requests-cache/</a></li>
                    <li>官方文档：<a href="https://requests-cache.readthedocs.io/en/stable/index.html" target="_blank" rel="noopener">https://requests-cache.readthedocs.io/en/stable/index.html</a></li>
                  </ul>
                  <p>下面我们来介绍下它的使用。</p>
                  <h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2>
                  <p>安装非常简单，使用 pip3 即可：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">pip3</span> <span class="selector-tag">install</span> <span class="selector-tag">requests-cache</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>安装完毕之后我们来了解下它的基本用法。</p>
                  <h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2>
                  <p>下面我们首先来看一个基础实例：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">time</span></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">session = requests.Session()</span><br><span class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(10):</span><br><span class="line">    session.get('http://httpbin.org/delay/1')</span><br><span class="line">    print(f'Finished &#123;i + 1&#125; requests')</span><br><span class="line">end = time.time()</span><br><span class="line">print('Cost time', end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们请求了一个网站，是 <a href="http://httpbin.org/delay/1" target="_blank" rel="noopener">http://httpbin.org/delay/1</a>，这个网站模拟了一秒延迟，也就是请求之后它会在 1 秒之后才会返回响应。</p>
                  <p>这里请求了 10 次，那就至少得需要 10 秒才能完全运行完毕。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">Finished</span> 1 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 2 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 3 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 4 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 5 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 6 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 7 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 8 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 9 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 10 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Cost</span> <span class="selector-tag">time</span> 13<span class="selector-class">.17966604232788</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里一共用了 13 秒。</p>
                  <p>那如果我们用上 requests-cache 呢？结果会怎样？</p>
                  <p>代码改写如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests_cache</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">time</span></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">session = requests_cache.CachedSession('demo_cache')</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(10):</span><br><span class="line">    session.get('http://httpbin.org/delay/1')</span><br><span class="line">    print(f'Finished &#123;i + 1&#125; requests')</span><br><span class="line">end = time.time()</span><br><span class="line">print('Cost time', end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们声明了一个 CachedSession，将原本的 Session 对象进行了替换，还是请求了 10 次。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">Finished</span> 1 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 2 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 3 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 4 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 5 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 6 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 7 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 8 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 9 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 10 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Cost</span> <span class="selector-tag">time</span> 1<span class="selector-class">.6248838901519775</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，一秒多就爬取完毕了！</p>
                  <p>发生了什么？</p>
                  <p>这时候我们可以发现，在本地生成了一个 demo_cache.sqlite 的数据库。</p>
                  <p>我们打开之后可以发现里面有个 responses 表，里面多了一个 key-value 记录，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/7uim6.png" alt=""></p>
                  <p>我们可以可以看到，这个 key-value 记录中的 key 是一个 hash 值，value 是一个 Blob 对象，里面的内容就是 Response 的结果。</p>
                  <p>可以猜到，每次请求都会有一个对应的 key 生成，然后 requests-cache 把对应的结果存储到了 SQLite 数据库中了，后续的请求和第一次请求的 URL 是一样的，经过一些计算它们的 key 也都是一样的，所以后续 2-10 请求就立马返回了。</p>
                  <p>是的，利用这个机制，我们就可以跳过很多重复请求了，大大节省爬取时间。</p>
                  <h2 id="Patch-写法"><a href="#Patch-写法" class="headerlink" title="Patch 写法"></a>Patch 写法</h2>
                  <p>但是，刚才我们在写的时候把 requests 的 session 对象直接替换了。有没有别的写法呢？比如我不影响当前代码，只在代码前面加几行初始化代码就完成 requests-cache 的配置呢？</p>
                  <p>当然是可以的，代码如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">time</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests_cache</span></span><br><span class="line"></span><br><span class="line">requests_cache.install_cache('demo_cache')</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">session = requests.Session()</span><br><span class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(10):</span><br><span class="line">    session.get('http://httpbin.org/delay/1')</span><br><span class="line">    print(f'Finished &#123;i + 1&#125; requests')</span><br><span class="line">end = time.time()</span><br><span class="line">print('Cost time', end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次我们直接调用了 requests-cache 库的 install_cache 方法就好了，其他的 requests 的 Session 照常使用即可。</p>
                  <p>我们再运行一遍：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">Finished</span> 1 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 2 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 3 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 4 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 5 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 6 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 7 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 8 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 9 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 10 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Cost</span> <span class="selector-tag">time</span> 0<span class="selector-class">.018644094467163086</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次比上次更快了，为什么呢？因为这次所有的请求都命中了 Cache，所以很快返回了结果。</p>
                  <h2 id="后端配置"><a href="#后端配置" class="headerlink" title="后端配置"></a>后端配置</h2>
                  <p>刚才我们知道了，requests-cache 默认使用了 SQLite 作为缓存对象，那这个能不能换啊？比如用文件，或者其他的数据库呢？</p>
                  <p>自然是可以的。</p>
                  <p>比如我们可以把后端换成本地文件，那可以这么做：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">time</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests_cache</span></span><br><span class="line"></span><br><span class="line">requests_cache.install_cache('demo_cache', backend='filesystem')</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">session = requests.Session()</span><br><span class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(10):</span><br><span class="line">    session.get('http://httpbin.org/delay/1')</span><br><span class="line">    print(f'Finished &#123;i + 1&#125; requests')</span><br><span class="line">end = time.time()</span><br><span class="line">print('Cost time', end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们添加了一个 backend 参数，然后指定为 filesystem，这样运行之后本地就会生成一个 demo_cache 的文件夹用作缓存，如果不想用缓存的话把这个文件夹删了就好了。</p>
                  <p>当然我们还可以更改缓存文件夹的位置，比如：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">requests_cache.install_cache('demo_cache', backend='filesystem', use_temp=True)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里添加一个 <code>use_temp</code> 参数，缓存文件夹便会使用系统的临时目录，而不会在代码区创建缓存文件夹。</p>
                  <p>当然也可以这样：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">requests_cache.install_cache('demo_cache', backend='filesystem', use_cache_dir=True)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里添加一个 <code>use_cache_dir</code> 参数，缓存文件夹便会使用系统的专用缓存文件夹，而不会在代码区创建缓存文件夹。</p>
                  <p>另外除了文件系统，requests-cache 也支持其他的后端，比如 Redis、MongoDB、GridFS 甚至内存，但也需要对应的依赖库支持，具体可以参见下表：</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th>Backend</th>
                          <th>Class</th>
                          <th>Alias</th>
                          <th>Dependencies</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td><a href="https://www.sqlite.org/" target="_blank" rel="noopener">SQLite</a></td>
                          <td><a href="https://requests-cache.readthedocs.io/en/stable/modules/requests_cache.backends.sqlite.html#requests_cache.backends.sqlite.SQLiteCache" target="_blank" rel="noopener"><code>SQLiteCache</code></a></td>
                          <td><code>&#39;sqlite&#39;</code></td>
                          <td></td>
                        </tr>
                        <tr>
                          <td><a href="https://redis.io/" target="_blank" rel="noopener">Redis</a></td>
                          <td><a href="https://requests-cache.readthedocs.io/en/stable/modules/requests_cache.backends.redis.html#requests_cache.backends.redis.RedisCache" target="_blank" rel="noopener"><code>RedisCache</code></a></td>
                          <td><code>&#39;redis&#39;</code></td>
                          <td><a href="https://github.com/andymccurdy/redis-py" target="_blank" rel="noopener">redis-py</a></td>
                        </tr>
                        <tr>
                          <td><a href="https://www.mongodb.com/" target="_blank" rel="noopener">MongoDB</a></td>
                          <td><a href="https://requests-cache.readthedocs.io/en/stable/modules/requests_cache.backends.mongodb.html#requests_cache.backends.mongodb.MongoCache" target="_blank" rel="noopener"><code>MongoCache</code></a></td>
                          <td><code>&#39;mongodb&#39;</code></td>
                          <td><a href="https://github.com/mongodb/mongo-python-driver" target="_blank" rel="noopener">pymongo</a></td>
                        </tr>
                        <tr>
                          <td><a href="https://docs.mongodb.com/manual/core/gridfs/" target="_blank" rel="noopener">GridFS</a></td>
                          <td><a href="https://requests-cache.readthedocs.io/en/stable/modules/requests_cache.backends.gridfs.html#requests_cache.backends.gridfs.GridFSCache" target="_blank" rel="noopener"><code>GridFSCache</code></a></td>
                          <td><code>&#39;gridfs&#39;</code></td>
                          <td><a href="https://github.com/mongodb/mongo-python-driver" target="_blank" rel="noopener">pymongo</a></td>
                        </tr>
                        <tr>
                          <td><a href="https://aws.amazon.com/dynamodb" target="_blank" rel="noopener">DynamoDB</a></td>
                          <td><a href="https://requests-cache.readthedocs.io/en/stable/modules/requests_cache.backends.dynamodb.html#requests_cache.backends.dynamodb.DynamoDbCache" target="_blank" rel="noopener"><code>DynamoDbCache</code></a></td>
                          <td><code>&#39;dynamodb&#39;</code></td>
                          <td><a href="https://github.com/boto/boto3" target="_blank" rel="noopener">boto3</a></td>
                        </tr>
                        <tr>
                          <td>Filesystem</td>
                          <td><a href="https://requests-cache.readthedocs.io/en/stable/modules/requests_cache.backends.filesystem.html#requests_cache.backends.filesystem.FileCache" target="_blank" rel="noopener"><code>FileCache</code></a></td>
                          <td><code>&#39;filesystem&#39;</code></td>
                          <td></td>
                        </tr>
                        <tr>
                          <td>Memory</td>
                          <td><a href="https://requests-cache.readthedocs.io/en/stable/modules/requests_cache.backends.base.html#requests_cache.backends.base.BaseCache" target="_blank" rel="noopener"><code>BaseCache</code></a></td>
                          <td><code>&#39;memory&#39;</code></td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>比如使用 Redis 就可以改写如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">backend = requests_cache.RedisCache(host='localhost', port=6379)</span><br><span class="line">requests_cache.install_cache('demo_cache', backend=backend)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更多详细配置可以参考官方文档：<a href="https://requests-cache.readthedocs.io/en/stable/user_guide/backends.html#backends" target="_blank" rel="noopener">https://requests-cache.readthedocs.io/en/stable/user_guide/backends.html#backends</a>。</p>
                  <h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2>
                  <p>当然，我们有时候也想指定有些请求不缓存，比如只缓存 POST 请求，不缓存 GET 请求，那可以这样来配置：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">time</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests_cache</span></span><br><span class="line"></span><br><span class="line">requests_cache.install_cache('demo_cache2', allowable_methods=['POST'])</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">session = requests.Session()</span><br><span class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(10):</span><br><span class="line">    session.get('http://httpbin.org/delay/1')</span><br><span class="line">    print(f'Finished &#123;i + 1&#125; requests')</span><br><span class="line">end = time.time()</span><br><span class="line">print('Cost time for get', end - start)</span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(10):</span><br><span class="line">    session.post('http://httpbin.org/delay/1')</span><br><span class="line">    print(f'Finished &#123;i + 1&#125; requests')</span><br><span class="line">end = time.time()</span><br><span class="line">print('Cost time for post', end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们添加了一个 <code>allowable_methods</code> 指定了一个过滤器，只有 POST 请求会被缓存，GET 请求就不会。</p>
                  <p>看下运行结果：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">Finished</span> 1 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 2 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 3 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 4 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 5 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 6 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 7 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 8 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 9 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 10 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Cost</span> <span class="selector-tag">time</span> <span class="selector-tag">for</span> <span class="selector-tag">get</span> 12<span class="selector-class">.916549682617188</span></span><br><span class="line"><span class="selector-tag">Finished</span> 1 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 2 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 3 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 4 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 5 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 6 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 7 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 8 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 9 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Finished</span> 10 <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">Cost</span> <span class="selector-tag">time</span> <span class="selector-tag">for</span> <span class="selector-tag">post</span> 1<span class="selector-class">.2473630905151367</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候就看到 GET 请求由于没有缓存，就花了 12 多秒才结束，而 POST 由于使用了缓存，一秒多就结束了。</p>
                  <p>另外我们还可以针对 Response Status Code 进行过滤，比如只有 200 会缓存，则可以这样写：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">time</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests_cache</span></span><br><span class="line"></span><br><span class="line">requests_cache.install_cache('demo_cache2', allowable_codes=(200,))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然我们还可以匹配 URL，比如针对哪种 Pattern 的 URL 缓存多久，则可以这样写：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">urls_expire_after = &#123;'*.site_1.com': 30, 'site_2.com/static': -1&#125;</span><br><span class="line"><span class="selector-tag">requests_cache</span><span class="selector-class">.install_cache</span>(</span><br><span class="line">    'demo_cache2', urls_expire_after=urls_expire_after)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的话，site_1.com 的内容就会缓存 30 秒，site_2.com/static 的内容就永远不会过期。</p>
                  <p>当然，我们也可以自定义 Filter，具体可以参见：<a href="https://requests-cache.readthedocs.io/en/stable/user_guide/filtering.html#custom-cache-filtering" target="_blank" rel="noopener">https://requests-cache.readthedocs.io/en/stable/user_guide/filtering.html#custom-cache-filtering</a>。</p>
                  <h2 id="Cache-Headers"><a href="#Cache-Headers" class="headerlink" title="Cache Headers"></a>Cache Headers</h2>
                  <p>除了我们自定义缓存，requests-cache 还支持解析 HTTP Request / Response Headers 并根据 Headers 的内容来缓存。</p>
                  <p>比如说，我们知道 HTTP 里面有个 <code>Cache-Control</code> 的 Request / Response Header，它可以指定浏览器要不要对本次请求进行缓存，那 requests-cache 怎么来支持呢？</p>
                  <p>实例如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">time</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests</span></span><br><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">requests_cache</span></span><br><span class="line"></span><br><span class="line">requests_cache.install_cache('demo_cache3')</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">session = requests.Session()</span><br><span class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(10):</span><br><span class="line">    session.get('http://httpbin.org/delay/1',</span><br><span class="line">                headers=&#123;</span><br><span class="line">                    'Cache-Control': 'no-store'</span><br><span class="line">                &#125;)</span><br><span class="line">    print(f'Finished &#123;i + 1&#125; requests')</span><br><span class="line">end = time.time()</span><br><span class="line">print('Cost time for get', end - start)</span><br><span class="line">start = time.time()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们在 Request Headers 里面加上了 <code>Cache-Control</code> 为 <code>no-store</code>，这样的话，即使我们声明了缓存那也不会生效。</p>
                  <p>当然 Response Headers 的解析也是支持的，我们可以这样开启：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">requests_cache.install_cache('demo_cache3', cache_control=True)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果我们配置了这个参数，那么 <code>expire_after</code> 的配置就会被覆盖而不会生效。</p>
                  <p>更多的用法可以参见：<a href="https://requests-cache.readthedocs.io/en/stable/user_guide/headers.html#cache-headers" target="_blank" rel="noopener">https://requests-cache.readthedocs.io/en/stable/user_guide/headers.html#cache-headers</a>。</p>
                  <h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>
                  <p>好了，到现在为止，一些基本配置、过期时间配置、后端配置、过滤器配置等基本常见的用法就介绍到这里啦，更多详细的用法大家可以参考官方文档：<a href="https://requests-cache.readthedocs.io/en/stable/user_guide.html" target="_blank" rel="noopener">https://requests-cache.readthedocs.io/en/stable/user_guide.html</a>。</p>
                  <p>希望对大家有帮助。</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-27 18:50:22" itemprop="dateCreated datePublished" datetime="2022-02-27T18:50:22+08:00">2022-02-27</time>
                </span>
                <span id="/36052.html" class="post-meta-item leancloud_visitors" data-flag-title="一个神器，大幅提升爬取效率" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>6 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36053.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 个人随笔 <i class="label-arrow"></i>
                  </a>
                  <a href="/36053.html" class="post-title-link" itemprop="url">一些学习经验分享</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h2 id="利用好搜索引擎"><a href="#利用好搜索引擎" class="headerlink" title="利用好搜索引擎"></a>利用好搜索引擎</h2>
                  <p>互联网时代，我们面临的是知识爆炸而不是知识匮乏。网上有很多很多好的学习资源，比如一些学习文档、疑难问题解决方案，很多都可以在网上搜到。</p>
                  <p>虽然网上有这些内容，但不同的搜索方法和用不同的搜索引擎搜到的结果就大不一样。</p>
                  <p>比如说，我们平时遇到了一些编程相关的问题，在谷歌中用英文搜索的结果在绝大多数情况下都会比在百度用中文搜索的结果好。比如说前者的结果通常就会是一些官方文档说明，而后者大多都是一些中文版 CSDN 博客，谁更前沿、更权威高下立判了。</p>
                  <p>我是做技术相关的，所以对于一些技术内容，我个人是非常建议首选谷歌英文搜索的，多数情况下能够更快更好地解决问题。</p>
                  <h2 id="多看一手资料"><a href="#多看一手资料" class="headerlink" title="多看一手资料"></a>多看一手资料</h2>
                  <p>我们知道，现在网上很多框架、工具，其都会配一个官方文档，比如 Python 的某个工具库、Vue 的的某个脚手架等等，同时很多源码也会在 GitHub 上公开。</p>
                  <p>我们如果要进行学习这些内容的话，我个人推荐尽量多去查询一些一手的资料，比如一些入门使用方法，可以尽量去看官方文档的一些 Get Started 部分；比如一些疑难 Issues，可以去 GitHub Issues 区搜索下关键词。</p>
                  <p>当然有的同学会说，官方文档都是些英文的，我看不懂啊，所以通常都会去搜索一些博客文章来看，比如一些中文博客的教程。可以是可以，也的确有一些优秀的博主能写出一些不错的文章，但毕竟还是少，而且这永远都不是一手资料，多数情况下，博客的文章也会有一些实效性的问题或者难免会出现一些错误。</p>
                  <p>所以，我个人还是推荐尽量去看一手资料。但一手的资料通常英文居多，但还是建议大家能够尽量地适应去读英文文档，如果能够做到的话，我们获取知识的能力会继续上一个台阶。</p>
                  <h2 id="时间管理和分类"><a href="#时间管理和分类" class="headerlink" title="时间管理和分类"></a>时间管理和分类</h2>
                  <p>每个人的精力都是有限的，一些工作和其他的琐事可以说是无穷无尽的。</p>
                  <p>所以，在做一些工作和学习的事情的时候，我们需要去区分优先级和重要程度，也就是能够合理地管理好自己要做的事情。</p>
                  <p>我个人会用一个清单软件（我用的滴答清单）来记录我所有需要做的事情，然后会给每个事情进行分类，比如说我会划分工作、学习、私人、购物、电影等等各种分类，然后每个任务都会指定好优先级和过期事件。指定好了之后，清单软件有一些功能可以给我筛选出来哪些是紧急重要，哪些是不紧急重要，哪些是紧急不重要等等的事情，然后我会有选择地去做对应的事情。比如说我会把大量的时间花在重要的事情上，不紧急不重要的可以看看能否尽量规避或者找人代做，总之不同的类型需要有不同的应对方案。</p>
                  <p>另外还有一些习惯养成类的事情，比如说定期的学习计划、定期的健身、定期的冥想、定期的跑步，也可以列入到个人的事件规划中。我通常以打卡的形式记录在清单软件中，每天都会有定期提醒，这样做完之后我就会打一次卡，看着越来越满的打卡记录，会感觉比较有成就感，大家也可以来试试。</p>
                  <h2 id="要有一个短期目标"><a href="#要有一个短期目标" class="headerlink" title="要有一个短期目标"></a>要有一个短期目标</h2>
                  <p>我们有时候做事的时候，脑子里知道很多长远的目标是什么。比如说，我长远计划里面有一个事情是要做一个网站系统，这是一个大目标，同时也有一个长远计划是要学习精通一门编程语言，这也是一个大目标。很多大目标都在我们的潜意识里面存留着。</p>
                  <p>现在问一个问题，虽然这些大的目标都在我们脑海里，但有没有一个时间，自己突然闲下来或者临时没有事的时候，却不知道这个空闲的时间去做什么？</p>
                  <p>如果有，那很可能就是因为没有短期目标。因为这个目标在我们的脑海中太大了，根本无法落实到执行的地步，所以我们需要做的事就是把一些目标进行拆解，拆解到什么地步呢？拆解到能够想到就能立马开始做的地步，这就是一些短期目标。</p>
                  <p>比如说，我们要学习一门课，我们可以给自己列个计划，比如哪天可以看哪个视频，或者一篇文章，这是知道了就能立马去做的事情。</p>
                  <p>所以，有了这个短期目标，我们能够更好地落实到执行上，这也是能够有效延缓拖延的方法。</p>
                  <h2 id="不要完美主义"><a href="#不要完美主义" class="headerlink" title="不要完美主义"></a>不要完美主义</h2>
                  <p>在做一些事情的时候，我们不要过分地追求完美主义。不是说不好，是因为这样很容易消磨我们的精力和耐心。</p>
                  <p>比如说，我们学习背单词吧，比如每天的计划是 20 个单词，好第一天背了 20 个，然后接着第二天的时候发现前 20 个单词没有背过，然后就接着背前 20 个单词，然后第三天的时候发现第一天和第二天的 40 个还是记得不牢固，然后就觉得好难，最后就放弃了，这就是因为过分追求完美主义导致的问题。</p>
                  <p>学习并不是非 0 即 1 的，我们如果能够学会 20%、60%、80% 也是一个不错的进步。</p>
                  <p>所以，我们不要执着于完美主义，非要做到 100% 不可，这样会把自己的精力和耐心慢慢消磨，直到放弃。</p>
                  <h2 id="不是所有教材都适合每个人"><a href="#不是所有教材都适合每个人" class="headerlink" title="不是所有教材都适合每个人"></a>不是所有教材都适合每个人</h2>
                  <p>并不是所有权威教材都是适合每个人的，要去寻找适合自己的学习方式。</p>
                  <p>市面上其实有很多所谓的权威教材或者网红教材，但这些教材并不是万能的，众口是很难调的，因为每个人的基础、水平都是不同的。</p>
                  <p>比如说一本书里面在前面的章节写了一些基础的环境配置和基础知识，有些人就会觉得非常友好，会觉得非常实用，但有些人就会觉得非常啰嗦，没有重点。比如说有人在学习一个框架和库的时候就喜欢看视频学习，因为这样能够看到具体的操作流程，但有些人就会觉得看视频学习非常浪费时间，而且知识点不好找，还是看官方文档或看书更方便。</p>
                  <p>这些学习方法和偏好没有绝对的对与不对，我们也不用非要跟风去购买和学习某个特定的教材和学习形式，适合自己的才是最好的。</p>
                  <h2 id="多进行总结和记录"><a href="#多进行总结和记录" class="headerlink" title="多进行总结和记录"></a>多进行总结和记录</h2>
                  <p>这个是非常非常非常重要的，在学习的过程中把学习笔记记录下来是一个非常好而且有效的学习习惯。</p>
                  <p>好处有这么以下几点：</p>
                  <ul>
                    <li>自己的学习笔记是对自己学习过程的梳理和总结，梳理和总结的过程就是一个学习复盘的过程，能够加深自己对知识点的印象。</li>
                    <li>方便复习会看，好记性不如烂笔头。写下来之后，如果我们想要对某个知识点进行复习，是非常容易的，因为文章的整体思路本身就是自己的，要捡起来也非常容易。</li>
                    <li>如果我们能够把学习内容整理发表出去，大家也可以对文章进行阅读和评论，在讨论的过程中可以有更多思维火花的碰撞，说不定能有更深入的了解。</li>
                    <li>能够帮助更多的人，因为我们遇到的问题通常也是别人遇到的，如果能够帮助更多的人，心里肯定也是很有成就感的。</li>
                  </ul>
                  <h2 id="学习要有深有浅"><a href="#学习要有深有浅" class="headerlink" title="学习要有深有浅"></a>学习要有深有浅</h2>
                  <p>学习一个知识点，我们也是需要有深浅的控制的，也是需要评估一些学习时间和成本的。</p>
                  <p>比如一个知识点，我们可以给它划分成三个层级，第一层级是会用即可，第二层级是熟练运用，第三层是深刻理解并改写。</p>
                  <p>在我们日常的工作中，由于不同技术栈和项目的需要，对一些知识的需求也会不一样，比如一些核心的技术，我们就需要深入理解并改写。比如说假如我是做 Scrapy 爬虫的，那对于 Scrapy 框架我就需要做到第三个层次，即深入理解并能改写；对于一些较高频的工具，比如 argparse，那我们就需要做到熟练运用；但对于一些低频且比较边角的知识点，我们只要花最少的时间知道它最基本的用法就好了，因为可能我们就是用到了它的最基本的用法解决了一个边角问题，所以没必要花太多时间在上面。</p>
                  <p>所以，对于一些学习内容，我们要能够分清楚这个知识点应该学到什么地步，然后采取对应的学习方案。 </p>
                  <h2 id="路径依赖"><a href="#路径依赖" class="headerlink" title="路径依赖"></a>路径依赖</h2>
                  <p>我们在学习的时候要尽量避免一些路径依赖的问题。</p>
                  <p>比如说，一位同学要学习 Python 机器学习相关内容，Python 机器学习的基础是一些 Python 和数学相关的内容，那他就非要把 Python 和数学的知识先全部研究透，比如说把所有的 Python 基础全学一遍、把所有的高等数学、统计学的知识全都学一遍，然后再回过头来学习 Python 机器学习，结果学习的时候发现很多 Python 基础和数学基础都用不到，然后久而久之，用不到的 Python 基础和数学基础就慢慢忘记了，而且 Python 机器学习的学习周期也被大大拉长。</p>
                  <p>这个例子中出现的就是路径依赖问题，我们其实没必要非要把所有的依赖项都完美一个个地彻底解决了再来学习对应的知识，知识点都是有关联的，我们在学习的时候可以以最终的结果为导向。</p>
                  <p>比如说，我今天要学 Python 机器学习，比如一个分类算法的实现，那我就把 Python 的模型定义、类定义、方法定义学会，同时研究好数学中的分类算法的思路，那就可以去学习 Python 机器学习了，这样整体效果也会更快更好，同时学习到的知识也能够用得上，且紧密关联。</p>
                  <h2 id="学习优秀的源码"><a href="#学习优秀的源码" class="headerlink" title="学习优秀的源码"></a>学习优秀的源码</h2>
                  <p>很多很多优秀的编程思路和方法其实都隐藏在一些优秀的源代码库里面。</p>
                  <p>比如说，学习爬虫，Scrapy 框架为什么能够做到这么好的扩展性？比如说，学习网站开发，Vue 为什么能够吸引这么多开发者学习？这其中都是有一定原因的，这些优秀的框架也是有它们的过人之处的，另外一些优秀的源码里面通常质量也会很高。</p>
                  <p>所以，我们如果能够多去阅读一些优秀的框架或库的源码，能够学到很多有用的编程思路和技巧的，如果能够把这些思路和知识运用到自己的工作和项目中，那一定会大有帮助。</p>
                  <h2 id="实践很重要"><a href="#实践很重要" class="headerlink" title="实践很重要"></a>实践很重要</h2>
                  <p>这个就不用多说了，光说不练，等于白搭。</p>
                  <p>对于我们做技术的来说，如果我们只是干巴巴地阅读一些官方文档和教程，而不去实际编写一些代码运行的话，收获是很少的。</p>
                  <p>一般来说，如果我们学习一些框架和库的时候，如果能够跟着把一些样例敲下来，真的能够理解深入很多。通常，阅读的时候我们不会发现问题，但一但一点点跟着敲下来，把代码运行起来，我们会发现很多潜在的问题，而且会对问题的认识更加深刻。</p>
                  <p>还有就是，遇到问题的时候，我们也需要多去实践和探索，如果不是十分紧急，我们可以尽量去尝试去搜索问题的解决方案，去 debug，去找 root cause，这样我们就能对某个问题有更加深刻的认识，同时自己解决问题的能力也会大大提高。</p>
                  <h2 id="贵有恒"><a href="#贵有恒" class="headerlink" title="贵有恒"></a>贵有恒</h2>
                  <p>是的，做一件事或者学习一个知识，一个非常非常重要的要素就是有恒心，即坚持。</p>
                  <p>贵有恒，何必三更起五更睡。</p>
                  <p>是的，做成一件事一个很大的拦路虎就是半途而废、三天打鱼两天晒网，这样很容易做着做着就没有下文了，然后就再也没有然后了，很多很多的事情就是因为这个而失败了。</p>
                  <p>贵有恒，坚持下来，做好计划，一件事，如果我们能够坚持做下来，一天天慢慢积累，其威力是无穷的。</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-27 18:50:22" itemprop="dateCreated datePublished" datetime="2022-02-27T18:50:22+08:00">2022-02-27</time>
                </span>
                <span id="/36053.html" class="post-meta-item leancloud_visitors" data-flag-title="一些学习经验分享" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202282.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202282.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - Session + Cookie 模拟登录爬取实战</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在上一节我们了解了网站登录验证和模拟登录的基本原理。网站登录验证主要有两种实现方式，一种是基于 Session + Cookies 的登录验证，另一种是基于 JWT 的登录验证。接下来两节，我们就通过两个实例来分别讲解这两种登录验证的分析和模拟登录流程。</p>
                  <p>本节主要介绍 Session + Cookie 模拟登录的流程。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>在本节开始之前，我们需要先做好如下准备工作。</p>
                  <ul>
                    <li>安装好了 requests 请求库并学会了其基本用法。</li>
                    <li>安装好了 Selenium 库并学会了其基本用法。</li>
                  </ul>
                  <p>下面我们就用两个案例来分别讲解模拟登录的实现。</p>
                  <h2 id="2-案例介绍"><a href="#2-案例介绍" class="headerlink" title="2. 案例介绍"></a>2. 案例介绍</h2>
                  <p>本节有一个适用于 Session + Cookie 模拟登录的案例网站，网址为：<a href="https://login2.scrape.center/，访问之后，我们会看到一个登录页面，如图所示：" target="_blank" rel="noopener">https://login2.scrape.center/，访问之后，我们会看到一个登录页面，如图所示：</a></p>
                  <p><img src="https://cdn.cuiqingcai.com/mo9zq.png" alt="image-20210711021407260"></p>
                  <p>我们输入用户名和密码（用户名和密码都是 admin），然后点击登录。登录成功后，我们便可以看到一个和之前案例类似的电影网站，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/zh7fp.png" alt="image-20210711021454920"></p>
                  <p>这个网站是基于传统的 MVC 模式开发的，因此也比较适合 Session + Cookie 的模拟登录。</p>
                  <h2 id="3-模拟登录"><a href="#3-模拟登录" class="headerlink" title="3. 模拟登录"></a>3. 模拟登录</h2>
                  <p>对于这个网站，我们如果要模拟登录，就需要先分析登录过程究竟发生了什么。我们打开开发者工具，重新执行登录操作，查看其登录过程中发生的请求，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/kdz9q.png" alt="image-20210711021940703"></p>
                  <p>图 10-5 登录过程中发生的请求</p>
                  <p>从图 10-5 中我们可以看到，在登录的瞬间，浏览器发起了一个 POST 请求，目标 URL 为 <a href="https://login2.scrape.center/login，并通过表单提交的方式像服务器提交了登录数据，其中包括" target="_blank" rel="noopener">https://login2.scrape.center/login，并通过表单提交的方式像服务器提交了登录数据，其中包括</a> username 和 password 两个字段，返回的状态码是 302，Response Headers 的 <code>location</code> 字段为根页面，同时 Response Headers 还包含了 <code>set-cookie</code> 信息，设置了 Session ID。</p>
                  <p>由此我们可以发现，要实现模拟登录，我们只需要模拟这个请求就好了。登录完成后获取 Response 设置的 Cookie，将它保存好，后续发出请求的时候带上 Cookies 就可以正常访问了。</p>
                  <p>好，那么我们就来用代码实现一下吧！</p>
                  <p>在默认情况下，每次 requests 请求都是独立且互不干扰的，比如我们第一次调用了 <code>post</code> 方法模拟登录了一下，紧接着再调用 <code>get</code> 方法请求主页面。其实这是两个完全独立的请求，第一次请求获取的 Cookie 并不能传给第二次请求，因此常规的顺序调用是不能起到模拟登录效果的。</p>
                  <p>我们来看一段无效的代码：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">'https://login2.scrape.center/'</span></span><br><span class="line">LOGIN_URL = urljoin(BASE_URL, <span class="string">'/login'</span>)</span><br><span class="line">INDEX_URL = urljoin(BASE_URL, <span class="string">'/page/1'</span>)</span><br><span class="line">USERNAME = <span class="string">'admin'</span></span><br><span class="line">PASSWORD = <span class="string">'admin'</span></span><br><span class="line"></span><br><span class="line">response_login = requests.post(LOGIN_URL, data=&#123;</span><br><span class="line">    <span class="string">'username'</span>: USERNAME,</span><br><span class="line">    <span class="string">'password'</span>: PASSWORD</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">response_index = requests.get(INDEX_URL)</span><br><span class="line">print(<span class="string">'Response Status'</span>, response_index.status_code)</span><br><span class="line">print(<span class="string">'Response URL'</span>, response_index.url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们先定义了几个基本的 URL 、用户名和密码，然后我们分别用 requests 请求了登录的 URL 进行模拟登录，紧接着请求了首页来获取页面内容，能正常获取数据吗？由于 requests 可以自动处理重定向，我们可以在最后把 Response 的 URL 打印出来，如果它的结果是 <code>INDEX_URL</code>，那么证明模拟登录成功并成功爬取到了首页的内容。如果它跳回到了登录页面，那就说明模拟登录失败。</p>
                  <p>我们通过结果来验证一下，运行结果如下：</p>
                  <figure class="highlight crystal">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Response Status <span class="number">200</span></span><br><span class="line">Response URL <span class="symbol">https:</span>/<span class="regexp">/login2.scrape.center/login</span>?<span class="keyword">next</span>=<span class="regexp">/page/</span><span class="number">1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里可以看到，其最终的页面 URL 是登录页面的 URL。另外这里也可以通过 Response 的 <code>text</code> 属性来验证下页面源码，其源码内容就是登录页面的源码内容，由于内容较多，这里就不再输出比对了。</p>
                  <p>总之，这个现象说明我们并没有成功完成模拟登录，这是因为 requests 直接调用 <code>post</code>、<code>get</code> 等方法，每次请求都是一个独立的请求，都相当于是新开了一个浏览器打开这些链接，所以这两次请求对应的 Session 并不是同一个，这里我们模拟了第一个 Session 登录，并不能影响第二个 Session 的状态，因此模拟登录也就无效了。</p>
                  <p>那么怎样才能实现正确的模拟登录呢？</p>
                  <p>我们知道 Cookie 里面是保存了 Session ID 信息的，刚才也观察到了登录成功后 Response Headers 里面有 <code>set-cookie</code> 字段，实际上这就是让浏览器生成了 Cookie。因为 Cookies 里面包含了 Session ID 的信息，所以只要后续的请求带着这些 Cookie，服务器便能通过 Cookie 里的 Session ID 信息找到对应的 Session 了，因此，服务端对于这两次请求就会使用同一个 Session 了。因为第一次我们已经成功完成了模拟登录，所以 Session 里面就记录了用户的登录信息，在第二次访问的时候，由于是同一个 Session，服务器就能知道用户当前是登录状态，那就能够返回正确的结果而不再是跳转到登录页面了。</p>
                  <p>所以，这里的关键在于两次请求的 Cookie 的传递。这里我们可以把第一次模拟登录后的 Cookie 保存下来，在第二次请求的时候加上这个 Cookie，代码可以改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">'https://login2.scrape.center/'</span></span><br><span class="line">LOGIN_URL = urljoin(BASE_URL, <span class="string">'/login'</span>)</span><br><span class="line">INDEX_URL = urljoin(BASE_URL, <span class="string">'/page/1'</span>)</span><br><span class="line">USERNAME = <span class="string">'admin'</span></span><br><span class="line">PASSWORD = <span class="string">'admin'</span></span><br><span class="line"></span><br><span class="line">response_login = requests.post(LOGIN_URL, data=&#123;</span><br><span class="line">    <span class="string">'username'</span>: USERNAME,</span><br><span class="line">    <span class="string">'password'</span>: PASSWORD</span><br><span class="line">&#125;, allow_redirects=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">cookies = response_login.cookies</span><br><span class="line">print(<span class="string">'Cookies'</span>, cookies)</span><br><span class="line"></span><br><span class="line">response_index = requests.get(INDEX_URL, cookies=cookies)</span><br><span class="line">print(<span class="string">'Response Status'</span>, response_index.status_code)</span><br><span class="line">print(<span class="string">'Response URL'</span>, response_index.url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于 requests 可以自动处理重定向，所以我们模拟登录的过程要加上 <code>allow_redirects</code> 参数并将其设置为 <code>False</code>，使其不自动处理重定向。我们将登录之后返回的 Response 赋值为 <code>response_login</code>，这样调用 <code>response_login</code> 的 <code>cookies</code> 就是获取了网站的 Cookie 信息了。这里 requests 自动帮我们解析了 Response Headers 的 <code>set-cookie</code> 字段并设置了 Cookie，所以我们不用再去手动解析 Response Headers 的内容了，直接使用 <code>response_login</code> 对象的 <code>cookies</code> 方法即可获取 Cookie。</p>
                  <p>好，接下来我们再次用 requests 的 <code>get</code> 方法来请求网站的 <code>INDEX_URL</code>。不过这里和之前不同，<code>get</code> 方法增加了一个参数 <code>cookies</code>，这就是第一次模拟登录完之后获取的 Cookie，这样第二次请求就能携带第一次模拟登录获取的 Cookie 信息了，此时网站会根据 Cookie 里面的 Session ID 信息查找到同一个 Session，校验其已经是登录状态，然后返回正确的结果。</p>
                  <p>这里我们还是输出最终的 URL，如果它是 <code>INDEX_URL</code>，就代表模拟登录成功并获取了有效数据，否则就代表模拟登录失败。</p>
                  <p>我们看下运行结果：</p>
                  <figure class="highlight fsharp">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Cookies &lt;RequestsCookieJar<span class="meta">[&lt;Cookie sessionid=psnu8ij69f0ltecd5wasccyzc6ud41tc for login2.scrape.center/&gt;]</span>&gt;</span><br><span class="line">Response Status <span class="number">200</span></span><br><span class="line">Response URL https:<span class="comment">//login2.scrape.center/page/1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这下没有问题了，我们发现其 URL 就是 INDEX_URL，模拟登录成功了！同时还可以进一步输出 <code>response_index</code> 的 <code>text</code> 属性看下是否获取成功。</p>
                  <p>后续用同样的方式爬取即可。但其实我们发现，这种实现方式比较烦琐，每次还需要处理 Cookie 并一次传递，有没有更简便的方法呢？</p>
                  <p>有的，我们可以直接借助于 requests 内置的 Session 对象来帮我们自动处理 Cookie，使用了 Session 对象之后，requests 会自动保存每次请求后需要设置的 Cookie ，并在下次请求时自动携带它，就相当于帮我们维持了一个 Session 对象，这样就更方便了。</p>
                  <p>所以，刚才的代码可以简化如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">'https://login2.scrape.center/'</span></span><br><span class="line">LOGIN_URL = urljoin(BASE_URL, <span class="string">'/login'</span>)</span><br><span class="line">INDEX_URL = urljoin(BASE_URL, <span class="string">'/page/1'</span>)</span><br><span class="line">USERNAME = <span class="string">'admin'</span></span><br><span class="line">PASSWORD = <span class="string">'admin'</span></span><br><span class="line"></span><br><span class="line">session = requests.Session()</span><br><span class="line"></span><br><span class="line">response_login = session.post(LOGIN_URL, data=&#123;</span><br><span class="line">    <span class="string">'username'</span>: USERNAME,</span><br><span class="line">    <span class="string">'password'</span>: PASSWORD</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">cookies = session.cookies</span><br><span class="line">print(<span class="string">'Cookies'</span>, cookies)</span><br><span class="line"></span><br><span class="line">response_index = session.get(INDEX_URL)</span><br><span class="line">print(<span class="string">'Response Status'</span>, response_index.status_code)</span><br><span class="line">print(<span class="string">'Response URL'</span>, response_index.url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里我们无须再关心 Cookie 的处理和传递问题，我们声明了一个 Session 对象，然后每次调用请求的时候都直接使用 Session 对象的 <code>post</code> 或 <code>get</code> 方法就好了。</p>
                  <p>运行效果是完全一样的，结果如下：</p>
                  <figure class="highlight fsharp">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Cookies &lt;RequestsCookieJar<span class="meta">[&lt;Cookie sessionid=ssngkl4i7en9vm73bb36hxif05k10k13 for login2.scrape.center/&gt;]</span>&gt;</span><br><span class="line">Response Status <span class="number">200</span></span><br><span class="line">Response URL https:<span class="comment">//login2.scrape.center/page/1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>因此，为了简化写法，这里建议直接使用 Session 对象进行请求，这样我们无须关心 Cookie 的操作了，实现起来会更加方便。</p>
                  <p>这个案例整体来说比较简单，但是如果碰上复杂一点的网站，如带有验证码，带有加密参数等，直接用 requests 并不好处理模拟登录，如果登录不了，那整个页面不就都没法爬取了吗？有没有其他的方式来解决这个问题呢？当然是有的，比如说我们可以使用 Selenium 来模拟浏览器，进而实现模拟登录，然后获取模拟登录成功后的 Cookie，再把获取的 Cookie 交由 requests 等来爬取就好了。</p>
                  <p>这里我们还是以刚才的页面为例，把模拟登录这块交由 Selenium 来实现，后续的爬取交由 requests 来实现，相关的代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">'https://login2.scrape.center/'</span></span><br><span class="line">LOGIN_URL = urljoin(BASE_URL, <span class="string">'/login'</span>)</span><br><span class="line">INDEX_URL = urljoin(BASE_URL, <span class="string">'/page/1'</span>)</span><br><span class="line">USERNAME = <span class="string">'admin'</span></span><br><span class="line">PASSWORD = <span class="string">'admin'</span></span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(BASE_URL)</span><br><span class="line">browser.find_element_by_css_selector(<span class="string">'input[name="username"]'</span>).send_keys(USERNAME)</span><br><span class="line">browser.find_element_by_css_selector(<span class="string">'input[name="password"]'</span>).send_keys(PASSWORD)</span><br><span class="line">browser.find_element_by_css_selector(<span class="string">'input[type="submit"]'</span>).click()</span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get cookies from selenium</span></span><br><span class="line">cookies = browser.get_cookies()</span><br><span class="line">print(<span class="string">'Cookies'</span>, cookies)</span><br><span class="line">browser.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># set cookies to requests</span></span><br><span class="line">session = requests.Session()</span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookies:</span><br><span class="line">    session.cookies.set(cookie[<span class="string">'name'</span>], cookie[<span class="string">'value'</span>])</span><br><span class="line"></span><br><span class="line">response_index = session.get(INDEX_URL)</span><br><span class="line">print(<span class="string">'Response Status'</span>, response_index.status_code)</span><br><span class="line">print(<span class="string">'Response URL'</span>, response_index.url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用 Selenium 先打开了 Chrome，然后跳转到了登录页面，随后模拟输入了用户名和密码，接着点击了登录按钮，我们可以发现浏览器提示登录成功，然后跳转到了主页面。</p>
                  <p>这时候，我们通过调用 <code>get_cookies</code> 方法便能获取当前浏览器所有的 Cookie，这就是模拟登录成功之后的 Cookie，用这些 Cookie 我们就能访问其他数据了。</p>
                  <p>接下来，我们声明了 requests 的 Session 对象，然后遍历了刚才的 Cookie 并将其设置到 Session 对象的 cookies 属性上，接着再拿着这个 Session 对象去请求 INDEX_URL，就也能够获取对应的信息而不会跳转到登录页面了。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight clean">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Cookies [&#123;<span class="string">'domain'</span>: <span class="string">'login2.scrape.center'</span>, <span class="string">'expiry'</span>: <span class="number">1589043753.553155</span>, <span class="string">'httpOnly'</span>: <span class="literal">True</span>, <span class="string">'name'</span>: <span class="string">'sessionid'</span>, <span class="string">'path'</span>: <span class="string">'/'</span>, <span class="string">'sameSite'</span>: <span class="string">'Lax'</span>, <span class="string">'secure'</span>: <span class="literal">False</span>, <span class="string">'value'</span>: <span class="string">'rdag7ttjqhvazavpxjz31y0tmze81zur'</span>&#125;]</span><br><span class="line">Response Status <span class="number">200</span></span><br><span class="line">Response URL https:<span class="comment">//login2.scrape.center/page/1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里的模拟登录和后续的爬取也成功了。所以说，如果碰到难以模拟登录的过程，我们也可以使用 Selenium 等模拟浏览器的操作方式来实现，其目的就是获取登录后的 Cookie，有了 Cookie 之后，我们再用这些 Cookie 爬取其他页面就好了。</p>
                  <p>所以这里我们也可以发现，对于基于 Session + Cookie 验证的网站，模拟登录的核心要点就是获取 Cookie。这个 Cookie 可以被保存下来或传递给其他的程序继续使用，甚至可以将 Cookie 持久化存储或传输给其他终端来使用。</p>
                  <p>另外，为了提高 Cookie 利用率或降低封号概率，可以搭建一个账号池实现 Cookie 的随机取用。</p>
                  <h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2>
                  <p>以上我们通过一个示例来演示了模拟登录爬取的过程，以后遇到这种情形的时候就可以用类似的思路解决了。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/ScrapeLogin2。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapeLogin2。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-25 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-25T12:42:44+08:00">2022-02-25</time>
                </span>
                <span id="/202282.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - Session + Cookie 模拟登录爬取实战" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>7.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>6 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202272.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202272.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - aiohttp 的基本使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在上一节中，我们介绍了异步爬虫的基本原理和 asyncio 的基本用法，并且在最后简单提及了使用 aiohttp 来实现网页爬取的过程。在本节中，我们来介绍一下 aiohttp 的常见用法。</p>
                  <h2 id="1-基本介绍"><a href="#1-基本介绍" class="headerlink" title="1. 基本介绍"></a>1. 基本介绍</h2>
                  <p>前面介绍的 asyncio 模块内部实现了对 TCP、UDP、SSL 协议的异步操作，但是对于 HTTP 请求来说，我们就需要用到 aiohttp 来实现了。</p>
                  <p>aiohttp 是一个基于 asyncio 的异步 HTTP 网络模块，它既提供了服务端，又提供了客户端。其中我们用服务端可以搭建一个支持异步处理的服务器，就是用来处理请求并返回响应的，类似于 Django、Flask、Tornado 等一些 Web 服务器。而客户端可以用来发起请求，类似于使用 requests 发起一个 HTTP 请求然后获得响应，但 requests 发起的是同步的网络请求，aiohttp 则是异步的。</p>
                  <p>本节中，我们主要了解一下 aiohttp 客户端部分的用法。</p>
                  <h2 id="2-基本实例"><a href="#2-基本实例" class="headerlink" title="2. 基本实例"></a>2. 基本实例</h2>
                  <p>首先，我们来看一个基本的 aiohttp 请求案例，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">fetch</span><span class="params">(session, url)</span>:</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">await</span> response.text(), response.status</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        html, status = <span class="keyword">await</span> fetch(session, <span class="string">'https://cuiqingcai.com'</span>)</span><br><span class="line">        print(<span class="string">f'html: <span class="subst">&#123;html[:<span class="number">100</span>]&#125;</span>...'</span>)</span><br><span class="line">        print(<span class="string">f'status: <span class="subst">&#123;status&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(main())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用 aiohttp 来爬取我的个人博客，获得了源码和响应状态码并输出出来，运行结果如下：</p>
                  <figure class="highlight xml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">html: <span class="meta">&lt;!DOCTYPE <span class="meta-keyword">HTML</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"baidu-tc-verification"</span> <span class="attr">content</span>=<span class="string">...</span></span></span><br><span class="line"><span class="tag"><span class="attr">status:</span> <span class="attr">200</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里网页源码过长，只截取输出了一部分。可以看到，这里我们成功获取了网页的源代码及响应状态码 200，也就完成了一次基本的 HTTP 请求，即我们成功使用 aiohttp 通过异步的方式来进行了网页爬取。当然，这个操作用之前讲的 requests 也可以做到。</p>
                  <p>可以看到，其请求方法的定义和之前有了明显的区别，主要有如下几点：</p>
                  <ul>
                    <li>首先在导入库的时候，我们除了必须要引入 aiohttp 这个库之外，还必须要引入 asyncio 这个库。因为要实现异步爬取，需要启动协程，而协程则需要借助于 asyncio 里面的事件循环来执行。除了事件循环，asyncio 里面也提供了很多基础的异步操作。</li>
                    <li>异步爬取方法的定义和之前有所不同，在每个异步方法前面统一要加 <code>async</code> 来修饰。</li>
                    <li><code>with as</code> 语句前面同样需要加 <code>async</code> 来修饰。在 Python 中，<code>with as</code> 语句用于声明一个上下文管理器，能够帮我们自动分配和释放资源。而在异步方法中，<code>with as</code> 前面加上 <code>async</code> 代表声明一个支持异步的上下文管理器。</li>
                    <li>对于一些返回 <code>coroutine</code> 的操作，前面需要加 <code>await</code> 来修饰。比如 <code>response</code> 调用 <code>text</code> 方法，查询 API 可以发现，其返回的是 <code>coroutine</code> 对象，那么前面就要加 <code>await</code>；而对于状态码来说，其返回值就是一个数值类型，那么前面就不需要加 <code>await</code>。所以，这里可以按照实际情况处理，参考官方文档说明，看看其对应的返回值是怎样的类型，然后决定加不加 <code>await</code> 就可以了。</li>
                    <li>最后，定义完爬取方法之后，实际上是 <code>main</code> 方法调用了 <code>fetch</code> 方法。要运行的话，必须要启用事件循环，而事件循环就需要使用 asyncio 库，然后使用 <code>run_until_complete</code> 方法来运行。</li>
                  </ul>
                  <blockquote>
                    <p>注意：在 Python 3.7 及以后的版本中，我们可以使用 <code>asyncio.run(main())</code> 来代替最后的启动操作，不需要显示声明事件循环，<code>run</code> 方法内部会自动启动一个事件循环。但这里为了兼容更多的 Python 版本，依然还是显式声明了事件循环。</p>
                  </blockquote>
                  <h2 id="3-URL-参数设置"><a href="#3-URL-参数设置" class="headerlink" title="3. URL 参数设置"></a>3. URL 参数设置</h2>
                  <p>对于 URL 参数的设置，我们可以借助于 <code>params</code> 参数，传入一个字典即可，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    params = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="number">25</span>&#125;</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.get(<span class="string">'https://httpbin.org/get'</span>, params=params) <span class="keyword">as</span> response:</span><br><span class="line">            print(<span class="keyword">await</span> response.text())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    asyncio.get_event_loop().run_until_complete(main())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"args"</span>: &#123;</span><br><span class="line">    <span class="attr">"age"</span>: <span class="string">"25"</span>,</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"headers"</span>: &#123;</span><br><span class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>,</span><br><span class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"Python/3.7 aiohttp/3.6.2"</span>,</span><br><span class="line">    <span class="attr">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5e85eed2-d240ac90f4dddf40b4723ef0"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"origin"</span>: <span class="string">"17.20.255.122"</span>,</span><br><span class="line">  <span class="attr">"url"</span>: <span class="string">"https://httpbin.org/get?name=germey&amp;age=25"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里可以看到，其实际请求的 URL 为 <a href="https://httpbin.org/get?name=germey&amp;age=25，其" target="_blank" rel="noopener">https://httpbin.org/get?name=germey&amp;age=25，其</a> URL 请求参数就对应了 <code>params</code> 的内容。</p>
                  <h2 id="4-其他请求类型"><a href="#4-其他请求类型" class="headerlink" title="4. 其他请求类型"></a>4. 其他请求类型</h2>
                  <p>另外，aiohttp 还支持其他请求类型，如 POST、PUT、DELETE 等，这和 requests 的使用方式有点类似，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">session.post(<span class="string">'http://httpbin.org/post'</span>, data=<span class="string">b'data'</span>)</span><br><span class="line">session.put(<span class="string">'http://httpbin.org/put'</span>, data=<span class="string">b'data'</span>)</span><br><span class="line">session.delete(<span class="string">'http://httpbin.org/delete'</span>)</span><br><span class="line">session.head(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">session.options(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">session.patch(<span class="string">'http://httpbin.org/patch'</span>, data=<span class="string">b'data'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>要使用这些方法，只需要把对应的方法和参数替换一下即可。</p>
                  <h2 id="5-POST-请求"><a href="#5-POST-请求" class="headerlink" title="5. POST 请求"></a>5. POST 请求</h2>
                  <p>对于 POST 表单提交，其对应的请求头的 <code>Content-Type</code> 为 <code>application/x-www-form-urlencoded</code>，我们可以用如下方式来实现，代码示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="number">25</span>&#125;</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.post(<span class="string">'https://httpbin.org/post'</span>, data=data) <span class="keyword">as</span> response:</span><br><span class="line">            print(<span class="keyword">await</span> response.text())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    asyncio.get_event_loop().run_until_complete(main())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">"data"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="attr">"files"</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">"form"</span>: &#123;</span><br><span class="line">    <span class="attr">"age"</span>: <span class="string">"25"</span>,</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"headers"</span>: &#123;</span><br><span class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>,</span><br><span class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="attr">"Content-Length"</span>: <span class="string">"18"</span>,</span><br><span class="line">    <span class="attr">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"Python/3.7 aiohttp/3.6.2"</span>,</span><br><span class="line">    <span class="attr">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5e85f0b2-9017ea603a68dc285e0552d0"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"json"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="attr">"origin"</span>: <span class="string">"17.20.255.58"</span>,</span><br><span class="line">  <span class="attr">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>对于 POST JSON 数据提交，其对应的请求头的 <code>Content-Type</code> 为 <code>application/json</code>，我们只需要将 <code>post</code> 方法的 <code>data</code> 参数改成 <code>json</code> 即可，代码示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="number">25</span>&#125;</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.post(<span class="string">'https://httpbin.org/post'</span>, json=data) <span class="keyword">as</span> response:</span><br><span class="line">            print(<span class="keyword">await</span> response.text())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">"data"</span>: <span class="string">"&#123;\"name\": \"germey\", \"age\": 25&#125;"</span>,</span><br><span class="line">  <span class="attr">"files"</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">"form"</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">"headers"</span>: &#123;</span><br><span class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>,</span><br><span class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="attr">"Content-Length"</span>: <span class="string">"29"</span>,</span><br><span class="line">    <span class="attr">"Content-Type"</span>: <span class="string">"application/json"</span>,</span><br><span class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"Python/3.7 aiohttp/3.6.2"</span>,</span><br><span class="line">    <span class="attr">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5e85f03e-c91c9a20c79b9780dbed7540"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"json"</span>: &#123;</span><br><span class="line">    <span class="attr">"age"</span>: <span class="number">25</span>,</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"origin"</span>: <span class="string">"17.20.255.58"</span>,</span><br><span class="line">  <span class="attr">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，其实现也和 requests 非常像，不同的参数支持不同类型的请求内容。</p>
                  <h2 id="6-响应"><a href="#6-响应" class="headerlink" title="6. 响应"></a>6. 响应</h2>
                  <p>对于响应来说，我们可以用如下方法分别获取响应的状态码、响应头、响应体、响应体二进制内容、响应体 JSON 结果，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="number">25</span>&#125;</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.post(<span class="string">'https://httpbin.org/post'</span>, data=data) <span class="keyword">as</span> response:</span><br><span class="line">            print(<span class="string">'status:'</span>, response.status)</span><br><span class="line">            print(<span class="string">'headers:'</span>, response.headers)</span><br><span class="line">            print(<span class="string">'body:'</span>, <span class="keyword">await</span> response.text())</span><br><span class="line">            print(<span class="string">'bytes:'</span>, <span class="keyword">await</span> response.read())</span><br><span class="line">            print(<span class="string">'json:'</span>, <span class="keyword">await</span> response.json())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    asyncio.get_event_loop().run_until_complete(main())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">status: 200</span><br><span class="line">headers: &lt;CIMultiDictProxy('Date': 'Thu, 02 Apr 2020 14:13:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '503', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true')&gt;</span><br><span class="line">body: &#123;</span><br><span class="line">  "args": &#123;&#125;,</span><br><span class="line">  "data": "",</span><br><span class="line">  "files": &#123;&#125;,</span><br><span class="line">  "form": &#123;</span><br><span class="line">    "age": "25",</span><br><span class="line">    "name": "germey"</span><br><span class="line">  &#125;,</span><br><span class="line">  "headers": &#123;</span><br><span class="line">    "Accept": "*/*",</span><br><span class="line">    "Accept-Encoding": "gzip, deflate",</span><br><span class="line">    "Content-Length": "18",</span><br><span class="line">    "Content-Type": "application/x-www-form-urlencoded",</span><br><span class="line">    "Host": "httpbin.org",</span><br><span class="line">    "User-Agent": "Python/3.7 aiohttp/3.6.2",</span><br><span class="line">    "X-Amzn-Trace-Id": "Root=1-5e85f2f1-f55326ff5800b15886c8e029"</span><br><span class="line">  &#125;,</span><br><span class="line">  "json": null,</span><br><span class="line">  "origin": "17.20.255.58",</span><br><span class="line">  "url": "https://httpbin.org/post"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bytes: b'&#123;\n  "args": &#123;&#125;, \n  "data": "", \n  "files": &#123;&#125;, \n  "form": &#123;\n    "age": "25", \n    "name": "germey"\n  &#125;, \n  "headers": &#123;\n    "Accept": "*/*", \n    "Accept-Encoding": "gzip, deflate", \n    "Content-Length": "18", \n    "Content-Type": "application/x-www-form-urlencoded", \n    "Host": "httpbin.org", \n    "User-Agent": "Python/3.7 aiohttp/3.6.2", \n    "X-Amzn-Trace-Id": "Root=1-5e85f2f1-f55326ff5800b15886c8e029"\n  &#125;, \n  "json": null, \n  "origin": "17.20.255.58", \n  "url": "https://httpbin.org/post"\n&#125;\n'</span><br><span class="line">json: &#123;'args': &#123;&#125;, 'data': '', 'files': &#123;&#125;, 'form': &#123;'age': '25', 'name': 'germey'&#125;, 'headers': &#123;'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '18', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'Python/3.7 aiohttp/3.6.2', 'X-Amzn-Trace-Id': 'Root=1-5e85f2f1-f55326ff5800b15886c8e029'&#125;, 'json': None, 'origin': '17.20.255.58', 'url': 'https://httpbin.org/post'&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们可以看到有些字段前面需要加 <code>await</code>，有的则不需要。其原则是，如果它返回的是一个 <code>coroutine</code> 对象（如 <code>async</code> 修饰的方法），那么前面就要加 <code>await</code>，具体可以看 aiohttp 的 API，其链接为：<a href="https://docs.aiohttp.org/en/stable/client_reference.html。" target="_blank" rel="noopener">https://docs.aiohttp.org/en/stable/client_reference.html。</a></p>
                  <h2 id="7-超时设置"><a href="#7-超时设置" class="headerlink" title="7. 超时设置"></a>7. 超时设置</h2>
                  <p>对于超时设置，我们可以借助 <code>ClientTimeout</code> 对象，比如这里要设置 1 秒的超时，可以这么实现：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    timeout = aiohttp.ClientTimeout(total=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession(timeout=timeout) <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.get(<span class="string">'https://httpbin.org/get'</span>) <span class="keyword">as</span> response:</span><br><span class="line">            print(<span class="string">'status:'</span>, response.status)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    asyncio.get_event_loop().run_until_complete(main())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果在 1 秒之内成功获取响应的话，运行结果如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">200</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果超时的话，会抛出 <code>TimeoutError</code> 异常，其类型为 <code>asyncio.TimeoutError</code>，我们再进行异常捕获即可。</p>
                  <p>另外，声明 <code>ClientTimeout</code> 对象时还有其他参数，如 <code>connect</code>、<code>socket_connect</code> 等，详细可以参考官方文档：<a href="https://docs.aiohttp.org/en/stable/client_quickstart.html#timeouts。" target="_blank" rel="noopener">https://docs.aiohttp.org/en/stable/client_quickstart.html#timeouts。</a></p>
                  <h2 id="8-并发限制"><a href="#8-并发限制" class="headerlink" title="8. 并发限制"></a>8. 并发限制</h2>
                  <p>由于 aiohttp 可以支持非常大的并发，比如上万、十万、百万都是能做到的，但对于这么大的并发量，目标网站很可能在短时间内无法响应，而且很可能瞬时间将目标网站爬挂掉，所以我们需要控制一下爬取的并发量。</p>
                  <p>一般情况下，我们可以借助于 asyncio 的 <code>Semaphore</code> 来控制并发量，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"></span><br><span class="line">CONCURRENCY = <span class="number">5</span></span><br><span class="line">URL = <span class="string">'https://www.baidu.com'</span></span><br><span class="line"></span><br><span class="line">semaphore = asyncio.Semaphore(CONCURRENCY)</span><br><span class="line">session = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">scrape_api</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> semaphore:</span><br><span class="line">        print(<span class="string">'scraping'</span>, URL)</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.get(URL) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">await</span> response.text()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> session</span><br><span class="line">    session = aiohttp.ClientSession()</span><br><span class="line">    scrape_index_tasks = [asyncio.ensure_future(scrape_api()) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10000</span>)]</span><br><span class="line">    <span class="keyword">await</span> asyncio.gather(*scrape_index_tasks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    asyncio.get_event_loop().run_until_complete(main())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们声明了 <code>CONCURRENCY</code>（代表爬取的最大并发量）为 5，同时声明爬取的目标 URL 为百度。接着，我们借助于 <code>Semaphore</code> 创建了一个信号量对象，将其赋值为 <code>semaphore</code>，这样我们就可以用它来控制最大并发量了。怎么使用呢？这里我们把它直接放置在对应的爬取方法里面，使用 <code>async with</code> 语句将 <code>semaphore</code> 作为上下文对象即可。这样的话，信号量可以控制进入爬取的最大协程数量，即我们声明的 <code>CONCURRENCY</code> 的值。</p>
                  <p>在 <code>main</code> 方法里面，我们声明了 10000 个 <code>task</code>，将其传递给 <code>gather</code> 方法运行。倘若不加以限制，这 10000 个 <code>task</code> 会被同时执行，并发数量太大。但有了信号量的控制之后，同时运行的 <code>task</code> 的数量最大会被控制在 5 个，这样就能给 aiohttp 限制速度了。</p>
                  <h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a>9. 总结</h2>
                  <p>本节我们了解了 aiohttp 的基本使用方法，更详细的内容还是推荐大家到官方文档查阅，详见 <a href="https://docs.aiohttp.org/。" target="_blank" rel="noopener">https://docs.aiohttp.org/。</a></p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/AsyncTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/AsyncTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-22 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-22T12:42:44+08:00">2022-02-22</time>
                </span>
                <span id="/202272.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - aiohttp 的基本使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>9.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>8 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202281.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202281.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 模拟登录的基本原理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>很多情况下，一些网站的页面或资源我们通常需要登录才能看到。比如说访问 GitHub 的个人设置页面，如果不登录是无法查看的；比如说 12306 买票提交订单的页面，如果不登录是无法提交订单的；比如说要发一条微博，如果不登录是无法发送的。</p>
                  <p>我们之前学习的案例都是爬取的无需登录即可访问的站点，但是诸如上面例子的情况非常非常多，那假如我们想要用爬虫来访问这些页面，比如用爬虫修改 GitHub 的个人设置，用爬虫提交购票订单，用爬虫发微博，能做到吗？</p>
                  <p>答案是可以，这里就需要用到一些模拟登录相关的技术了。</p>
                  <p>那么本节我们就先来了解一下模拟登录的一些基本原理和实现吧。</p>
                  <h2 id="1-网站登录验证的实现"><a href="#1-网站登录验证的实现" class="headerlink" title="1. 网站登录验证的实现"></a>1. 网站登录验证的实现</h2>
                  <p>我们要实现模拟登录，那就得首先了解网站登录验证的实现。</p>
                  <p>登录一般是需要两个内容，用户名和密码，有的网站可能是手机号和验证码，有的是微信扫码，有的是 OAuth 验证等等，但根本上来说，都是把一些可供认证的信息提交给了服务器。</p>
                  <p>比如这里我们就拿用户名和密码来说吧。用户在一个网页表单里面输入了这些内容，然后点击登录按钮的一瞬间，浏览器客户端就会向服务器发送一个登录请求，这个请求里面肯定就包含了用户名和密码信息，这时候，服务器需要处理一下这些信息，然后返回给客户端一个类似「凭证」的东西，有了这个「凭证」以后呢，客户端拿着这个「凭证」再去访问某些需要登录才能查看的页面，服务器自然就能”放行“了，返回对应的内容或执行对应的操作就好了。</p>
                  <p>形象点说呢，我们拿登录发微博和买票坐火车这两件事来类比。发微博就好像要坐火车，没票是没法坐火车的吧，要坐火车怎么办呢？当然是先买票了，我们拿钱去火车站买个票，有了票之后，进站口查验一下，没问题就自然能去坐火车了，这个票就是坐火车的「凭证」。那发微博也一样，我们有用户名和密码，请求下服务器，获得一个「凭证」，这就相当于买到了火车票，然后在发微博的时候拿着这个「凭证」去请求服务器，服务器校验没问题，自然就把微博发出去了。</p>
                  <p>那么问题来了，这个「凭证」到底是怎么生成和验证的呢？目前比较流行的实现方式有两种，一种是基于 Session + Cookie 的验证，一种是基于 JWT（JSON Web Token）的验证，下面我们来介绍下。</p>
                  <h2 id="2-Session-和-Cookie"><a href="#2-Session-和-Cookie" class="headerlink" title="2. Session 和 Cookie"></a>2. Session 和 Cookie</h2>
                  <p>我们在第一章了解了 Session 和 Cookie 的基本概念。简而言之呢，Session 就是存在服务端的，里面保存了用户此次访问的会话信息，Cookie 则是保存在用户本地浏览器的，它会在每次用户访问网站的时候发送给服务器，Cookie 会作为 Request Headers 的一部分发送给服务器，服务器根据 Cookie 里面包含的信息判断找出其 Session 对象并做一些校验，不同的 Session 对象里面维持了不同访问用户的状态，服务器可以根据这些信息决定返回 Response 的内容。</p>
                  <p>我们以用户登录的情形来说吧，其实不同的网站对于用户的登录状态的实现是可能不同的，但是 Session 和 Cookie 一定是相互配合工作的。</p>
                  <p>下面梳理如下：</p>
                  <ul>
                    <li>比如说，Cookie 里面可能只存了 Session ID 相关信息，服务器能根据 Cookie 找到对应的 Session，用户登录之后，服务器会把对应的 Session 里面标记一个字段，代表已登录状态或者其他信息（如角色、登录时间）等等，这样用户每次访问网站的时候都带着 Cookie 来访问，服务器就能找到对应的 Session，然后看一下 Session 里面的状态是登录状态，那就可以返回对应的结果或执行某些操作。</li>
                    <li>当然 Cookie 里面也可能直接存了某些凭证信息。比如说用户在发起登录请求之后，服务器校验通过，返回给客户端的 Response Headers 里面可能带有 <code>Set-Cookie</code> 字段，里面可能就包含了类似凭证的信息，这样客户端会执行设置 Cookie 的操作，将这些信息保存到 Cookie 里面，以后再访问网页时携带这些 Cookie 信息，服务器拿着这里面的信息校验，自然也能实现登录状态检测了。</li>
                  </ul>
                  <p>以上两种情况几乎能涵盖大部分的 Session 和 Cookie 登录验证的实现，具体的实现逻辑因服务器而异，但 Session 和 Cookie 一定是需要相互配合才能实现的。</p>
                  <h2 id="3-JWT"><a href="#3-JWT" class="headerlink" title="3. JWT"></a>3. JWT</h2>
                  <p>Web 开发技术是一直在发展的，近几年前后端分离的趋势越来越火，很多 Web 网站都采取了前后端分离的技术来实现。而且传统的基于 Session 和 Cookie 的校验也存在一定问题，比如服务器需要维护登录用户的 Session 信息，而且分布式部署不方便，也不太适合前后端分离的项目。</p>
                  <p>所以，JWT 技术应运而生。</p>
                  <p>JWT，英文全称为 JSON Web Token，是为了在网络应用环境间传递声明而执行的一种基于 JSON 的开放标准。实际上就是在每次登录的时候通过一个 Token 字符串来校验登录状态。JWT 的声明一般被用来在身份提供者和服务提供者之间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的业务逻辑所必须的声明信息，所以这个 Token 也可直接被用于认证，也可传递一些额外信息。</p>
                  <p>有了 JWT，一些认证就不需要借助于 Session 和 Cookie 了，服务器也无须维护 Session 信息，减少了服务器的开销。服务器只需要有一个校验 JWT 的功能就好了，同时也可以做到分布式部署和跨语言的支持。</p>
                  <p>JWT 通常就是一个加密的字符串，它也有自己的标准，类似下面的这种格式：</p>
                  <figure class="highlight gcode">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">eyJ<span class="number">0</span>eXAxIjoiMTIz<span class="symbol">NCIsImFsZzIiOiJhZG1</span>pbiIsI<span class="symbol">nR5</span>cCI<span class="number">6</span>IkpX<span class="attr">VCIsImFsZyI6</span>IkhTMjU<span class="number">2</span>I<span class="symbol">n0</span>.eyJ<span class="attr">Vc2</span>VySWQiOjEyMywiVX<span class="symbol">Nlck5</span>hbWUiOiJhZ<span class="name">G1</span>pbiIsImV<span class="number">4</span>cCI<span class="number">6</span>MTU<span class="number">1</span>MjI<span class="number">4</span><span class="symbol">Njc0</span><span class="symbol">Ni44</span><span class="symbol">Nzc0</span>MDE<span class="number">4</span>fQ.pEgdmFAy<span class="number">73</span>walFo<span class="symbol">nEm2</span>zbx<span class="name">g46</span>Oth<span class="number">3</span>dlT<span class="number">02</span>HR<span class="number">9</span>iVzXa<span class="number">8</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们可以发现中间有两个用来分割的 <code>.</code> ，因此可以把它看成是一个三段式的加密字符串。</p>
                  <p>它由三部分构成，分别是 Header、Payload、Signature。</p>
                  <ul>
                    <li>Header，声明了 JWT 的签名算法，如 RSA、SHA256 等，也可能包含 JWT 编号或类型等数据，然后对整个信息进行 Base64 编码即可。</li>
                    <li>Payload，通常用来存放一些业务需要但不敏感的信息，如 UserID 等，另外它也有很多默认是字段，如 JWT 签发者、JWT 接受者、JWT 过期时间等，Base64 编码即可。</li>
                    <li>Signature，就是一个签名，是把 Header、Payload 的信息用秘钥 secret 加密后形成的，这个 secret 是保存在服务器端的，不能被轻易泄露。如此一来，即使一些 Payload 的信息被篡改，服务器也能通过 Signature 判断出非法请求，拒绝服务。</li>
                  </ul>
                  <p>这三部分通过 <code>.</code> 组合起来就形成了 JWT 的字符串，就是用户的访问凭证。</p>
                  <p>所以这个登录认证流程也很简单了，用户拿着用户名密码登录，然后服务器生成 JWT 字符串返回给客户端。客户端每次请求都带着这个 JWT 就行了，服务器会自动判断其有效情况，如果有效，自然就返回对应的数据。JWT 的传输就多种多样了，可以将其放在 Request Headers 中，也可以放在 URL 里，甚至也有的网站把它放在 Cookie 里面，但总而言之，能传给服务器进行校验就好了。</p>
                  <p>好，到此为止呢，我们就已经了解了网站登录验证的实现了。</p>
                  <h2 id="4-模拟登录"><a href="#4-模拟登录" class="headerlink" title="4. 模拟登录"></a>4. 模拟登录</h2>
                  <p>好，那了解了网站登录验证的实现后，模拟登录自然就有思路了。</p>
                  <p>下面我们同样分两种认证方式来说明。</p>
                  <h3 id="Session-和-Cookie"><a href="#Session-和-Cookie" class="headerlink" title="Session 和 Cookie"></a>Session 和 Cookie</h3>
                  <p>基于 Session 和 Cookie 的模拟登录，如果我们要用爬虫实现的话，其实最主要的就是把 Cookie 的信息维护好就行了，因为爬虫就相当于客户端浏览器，我们模拟好浏览器做的事情就好了。</p>
                  <p>一般怎么实现模拟登录呢？接下来我们结合之前所讲的技术总结一下。</p>
                  <ul>
                    <li>第一，如果我们已经在浏览器中登录了自己的账号，要想用爬虫模拟，那么可以直接把 Cookie 复制过来交给爬虫。这是最省时省力的方式，相当于我们用浏览器手动操作登录了。我们把 Cookie 放到代码里，爬虫每次请求的时候再将其放到 Request Headers 中，完全模拟了浏览器的操作。之后服务器会通过 Cookie 校验登录状态，如果没问题，自然就可以执行某些操作或返回某些内容了。</li>
                    <li>第二，如果我们不想有任何手工操作，那么可以直接使用爬虫模拟登录过程。其实登录的过程多数也是一个 POST 请求。我们用爬虫提交了用户名、密码等信息给服务器，服务器返回的 Response Headers 里面可能会带有 <code>Set-Cookie</code> 的字段，我们只需要把这些 Cookie 保存下来就行了。所以，最主要的就是把这个过程中的 Cookie 维持好。当然这里可能会遇到一些困难，比如登录过程中伴随着各种校验参数，不好直接模拟请求；网站设置 Cookie 的过程是通过 JavaScript 实现的，所以可能还得仔细分析下其中的逻辑，尤其是我们用 requests 这样的请求库进行模拟登录的时候，遇到的问题经常比较多。</li>
                    <li>第三，我们也可以用一些简单的方式来实现模拟登录，即实现登录过程的自动化。比如我们用 Selenium、Pyppeteer 或 Playwright 来驱动浏览器模拟执行一些操作，如填写用户名和密码、提交表单等。登录成功后，通过 Selenium 或 Pyppeteer 获取当前浏览器的 Cookie 并保存即可。这样后续就可以拿着 Cookie 的内容发起请求，同样也能实现模拟登录。</li>
                  </ul>
                  <p>以上介绍的就是一些常用的爬虫模拟登录的方案，其目的是维护好客户端的 Cookie 信息。总之，每次请求都携带好 Cookie 信息就能实现模拟登录了。</p>
                  <h3 id="JWT"><a href="#JWT" class="headerlink" title="JWT"></a>JWT</h3>
                  <p>基于 JWT 的模拟登录思路也比较清晰了，由于 JWT 的字符串就是用户访问的凭证，所以模拟登录只需要做到下面几步。</p>
                  <ul>
                    <li>第一步，模拟网站登录操作的请求。比如拿着用户名和密码信息请求登录接口，获取服务器返回的结果，这个结果中通常包含 JWT 字符串的信息，将它保存即可。</li>
                    <li>第二步，后续的请求携带 JWT 进行访问。在 JWT 不过期的情况下，通常能正常访问和执行对应的操作。携带方式多种多样，因网站而异。</li>
                    <li>第三步，如果 JWT 过期了，可能需要再次进行第一步，重新获取 JWT。</li>
                  </ul>
                  <p>当然，模拟登录的过程肯定会带有一些其他的加密参数，需要根据实际情况具体分析。</p>
                  <h2 id="4-账号池"><a href="#4-账号池" class="headerlink" title="4. 账号池"></a>4. 账号池</h2>
                  <p>如果爬虫要求爬取的数据量比较大或爬取速度比较快，而网站又有单账号并发限制或者访问状态检测等反爬虫手段，那么我们的账号可能就会无法访问或者面临封号的风险了。</p>
                  <p>这时候一般怎么办呢？</p>
                  <p>我们可以使用分流的方案来实现。假设某个网站设置一分钟之内检测到同一个账号访问 3 次或 3 次以上则封号，我们就可以建立一个账号池，用多个账号来随机访问或爬取数据，这样就能大幅提高爬虫的并发量，降低被封号的风险了。比如我们可以准备 100 个账号，然后 100 个账号都模拟登录，把对应的 Cookie 或 JWT 存下来，每次访问的时候随机取一个来，由于账号多，所以每个账号被取用的概率也就降下来了，这样就能避免单账号并发过大的问题，也降低封号风险。</p>
                  <h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2>
                  <p>本节我们首先了解了 Session + Cookie 和 JWT 模拟登录的原理，接着初步了解了两种模拟登录方式的实现思路，最后初步介绍了一下账号池的作用。</p>
                  <p>后文我们会通过几个实战案例来实现上述两种方案的模拟登录，为了更好地理解后文的实战内容，建议好好理解本节所介绍的内容。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-22 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-22T12:42:44+08:00">2022-02-22</time>
                </span>
                <span id="/202281.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 模拟登录的基本原理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.8k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202261.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202261.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 经典动态渲染工具 Selenium 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>前面我们讲解了 Ajax 的分析方法，利用 Ajax 接口我们可以非常方便地完成数据爬取。只要我们能找到 Ajax 接口的规律，就可以通过某些参数构造出对应的请求，数据自然就能轻松爬取到。</p>
                  <p>但是在很多情况下，一些 Ajax 请求的接口通常会包含加密参数，如<code>token</code>、<code>sign</code> 等，如：<a href="https://spa2.scrape.center/，它的" target="_blank" rel="noopener">https://spa2.scrape.center/，它的</a> Ajax 接口是包含一个 <code>token</code> 参数的，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/y6w4q.png" alt="包含 `token` 参数的 Ajax 接口"></p>
                  <p>由于请求接口时必须加上 <code>token</code> 参数，所以我们如果不深入分析找到 <code>token</code> 的构造逻辑，是难以直接模拟这些 Ajax 请求的。</p>
                  <p>此时解决方法通常有两种：一种就是深挖其中的逻辑，把其中 <code>token</code> 的构造逻辑完全找出来，再用 Python 复现，构造 Ajax 请求；另外一种方法就是直接通过模拟浏览器的方式来绕过这个过程，因为在浏览器里我们可以看到这个数据，如果能把看到的数据直接爬取下来，当然也就能获取对应的信息了。</p>
                  <p>由于第一种方法难度较高，这里我们就先介绍第二种方法：模拟浏览器爬取。</p>
                  <p>这里使用的工具为 Selenium，这里就来先了解一下 Selenium 的基本使用方法。</p>
                  <p>Selenium 是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等操作，同时还可以获取浏览器当前呈现的页面的源代码，做到可见即可爬。对于一些 JavaScript 动态渲染的页面来说，此种抓取方式非常有效。本节中，就让我们来感受一下它的强大之处吧。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>本节以 Chrome 为例来讲解 Selenium 的用法。在开始之前，请确保已经正确安装好了 Chrome 浏览器并配置好了 ChromeDriver。另外，还需要正确安装好 Python 的 Selenium 库。</p>
                  <p>安装方法可以参考：<a href="https://setup.scrape.center/selenium，全部配置完成之后，我们便可以开始本节的学习了。" target="_blank" rel="noopener">https://setup.scrape.center/selenium，全部配置完成之后，我们便可以开始本节的学习了。</a></p>
                  <h2 id="2-基本用法"><a href="#2-基本用法" class="headerlink" title="2. 基本用法"></a>2. 基本用法</h2>
                  <p>准备工作做好之后，首先来大体看一下 Selenium 的功能。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    input = browser.find_element_by_id(<span class="string">'kw'</span>)</span><br><span class="line">    input.send_keys(<span class="string">'Python'</span>)</span><br><span class="line">    input.send_keys(Keys.ENTER)</span><br><span class="line">    wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">    wait.until(EC.presence_of_element_located((By.ID, <span class="string">'content_left'</span>)))</span><br><span class="line">    print(browser.current_url)</span><br><span class="line">    print(browser.get_cookies())</span><br><span class="line">    print(browser.page_source)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行代码后发现，会自动弹出一个 Chrome 浏览器。浏览器首先会跳转到百度，然后在搜索框中输入 Python，接着跳转到搜索结果页，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/fu94j.png" alt=""></p>
                  <p>此时在控制台的输出结果如下：</p>
                  <figure class="highlight sas">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https://www.baidu.com/s?ie=utf-8<span class="variable">&amp;f</span>=8<span class="variable">&amp;rsv_bp</span>=0<span class="variable">&amp;rsv_idx</span>=1<span class="variable">&amp;tn</span>=baidu<span class="variable">&amp;wd</span>=Python<span class="variable">&amp;rsv_pq</span>=c94d0df9000a72d0<span class="variable">&amp;rsv_t</span>=07099xvun1ZmC0bf6eQvygJ43IUTTUOl5FCJVPgwG2YREs70GplJjH2F%2BCQ<span class="variable">&amp;rqlang</span>=cn<span class="variable">&amp;rsv_enter</span>=1<span class="variable">&amp;rsv_sug3</span>=6<span class="variable">&amp;rsv_sug2</span>=0<span class="variable">&amp;inputT</span>=87<span class="variable">&amp;rsv_sug4</span>=87</span><br><span class="line">[&#123;<span class="string">'secure'</span>: False, <span class="string">'value'</span>: <span class="string">'B490B5EBF6F3CD402E515D22BCDA1598'</span>, <span class="string">'domain'</span>: <span class="string">'.baidu.com'</span>, <span class="string">'path'</span>: <span class="string">'/'</span>, <span class="string">'httpOnly'</span>: False, <span class="string">'name'</span>: <span class="string">'BDORZ'</span>, <span class="string">'expiry'</span>: 1491688071.707553&#125;, &#123;<span class="string">'secure'</span>: False, <span class="string">'value'</span>: <span class="string">'22473_1441_21084_17001'</span>, <span class="string">'domain'</span>: <span class="string">'.baidu.com'</span>, <span class="string">'path'</span>: <span class="string">'/'</span>, <span class="string">'httpOnly'</span>: False, <span class="string">'name'</span>: <span class="string">'H_PS_PSSID'</span>&#125;, &#123;<span class="string">'secure'</span>: False, <span class="string">'value'</span>: <span class="string">'12883875381399993259_00_0_I_R_2_0303_C02F_N_I_I_0'</span>, <span class="string">'domain'</span>: <span class="string">'.www.baidu.com'</span>, <span class="string">'path'</span>: <span class="string">'/'</span>, <span class="string">'httpOnly'</span>: False, <span class="string">'name'</span>: <span class="string">'__bsi'</span>, <span class="string">'expiry'</span>: 1491601676.69722&#125;]</span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;...&lt;/html&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>源代码过长，在此省略。可以看到，我们得到的当前 URL、Cookies 和源代码都是浏览器中的真实内容。</p>
                  <p>所以说，如果用 Selenium 来驱动浏览器加载网页的话，就可以直接拿到 JavaScript 渲染的结果了，不用担心使用的是什么加密系统。</p>
                  <p>下面来详细了解一下 Selenium 的用法。</p>
                  <h2 id="3-声明浏览器对象"><a href="#3-声明浏览器对象" class="headerlink" title="3. 声明浏览器对象"></a>3. 声明浏览器对象</h2>
                  <p>Selenium 支持非常多的浏览器，如 Chrome、Firefox、Edge 等，还有 Android、BlackBerry 等手机端的浏览器。我们可以用如下方式初始化：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser = webdriver.Firefox()</span><br><span class="line">browser = webdriver.Edge()</span><br><span class="line">browser = webdriver.Safari()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就完成了浏览器对象的初始化并将其赋值为 <code>browser</code> 对象。接下来，我们要做的就是调用 <code>browser</code> 对象，让其执行各个动作以模拟浏览器操作。</p>
                  <h2 id="4-访问页面"><a href="#4-访问页面" class="headerlink" title="4. 访问页面"></a>4. 访问页面</h2>
                  <p>我们可以用 <code>get</code> 方法来请求网页，其参数传入链接 URL 即可。比如，这里用 <code>get</code> 方法访问淘宝，然后打印出源代码，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">print(browser.page_source)</span><br><span class="line">browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行后发现，此时弹出了 Chrome 浏览器并且自动访问了淘宝，然后控制台输出了淘宝页面的源代码，随后浏览器关闭。</p>
                  <p>通过这几行简单的代码，我们可以实现浏览器的驱动并获取网页源码，非常便捷。</p>
                  <h2 id="5-查找节点"><a href="#5-查找节点" class="headerlink" title="5. 查找节点"></a>5. 查找节点</h2>
                  <p>Selenium 可以驱动浏览器完成各种操作，比如填充表单、模拟点击等。比如，我们想要完成向某个输入框输入文字的操作，总需要知道这个输入框在哪里吧？而 Selenium 提供了一系列查找节点的方法，我们可以用这些方法来获取想要的节点，以便下一步执行一些动作或者提取信息。</p>
                  <h3 id="单个节点"><a href="#单个节点" class="headerlink" title="单个节点"></a>单个节点</h3>
                  <p>比如，想要从淘宝页面中提取搜索框这个节点，首先要观察它的源代码，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/lwmpo.png" alt="源代码"></p>
                  <p>可以发现，它的 <code>id</code> 是 <code>q</code>，<code>name</code> 也是 <code>q</code>。此外，还有许多其他属性，此时我们就可以用多种方式获取它了。比如，<code>find_element_by_name</code> 是根据 <code>name</code> 值获取，<code>find_element_by_id</code> 是根据 <code>id</code> 获取。另外，还有根据 XPath、CSS 选择器等获取的方式。</p>
                  <p>下面我们用代码实现一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">input_first = browser.find_element_by_id(<span class="string">'q'</span>)</span><br><span class="line">input_second = browser.find_element_by_css_selector(<span class="string">'#q'</span>)</span><br><span class="line">input_third = browser.find_element_by_xpath(<span class="string">'//*[@id="q"]'</span>)</span><br><span class="line">print(input_first, input_second, input_third)</span><br><span class="line">browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用 3 种方式获取输入框，分别是根据 ID、CSS 选择器和 XPath 获取，它们返回的结果完全一致。运行结果如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;selenium.webdriver.remote.webelement.WebElement (<span class="attribute">session</span>=<span class="string">"5e53d9e1c8646e44c14c1c2880d424af"</span>, <span class="attribute">element</span>=<span class="string">"0.5649563096161541-1"</span>)&gt;</span><br><span class="line">&lt;selenium.webdriver.remote.webelement.WebElement (<span class="attribute">session</span>=<span class="string">"5e53d9e1c8646e44c14c1c2880d424af"</span>, <span class="attribute">element</span>=<span class="string">"0.5649563096161541-1"</span>)&gt;</span><br><span class="line">&lt;selenium.webdriver.remote.webelement.WebElement (<span class="attribute">session</span>=<span class="string">"5e53d9e1c8646e44c14c1c2880d424af"</span>, <span class="attribute">element</span>=<span class="string">"0.5649563096161541-1"</span>)&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这 3 个节点都是 <code>WebElement</code> 类型，是完全一致的。</p>
                  <p>下面列出所有获取单个节点的方法：</p>
                  <figure class="highlight ceylon">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_</span>id</span><br><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_n</span>ame</span><br><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_</span>xpath</span><br><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_</span>link<span class="number">_</span>text</span><br><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_p</span>artial<span class="number">_</span>link<span class="number">_</span>text</span><br><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_</span>tag<span class="number">_n</span>ame</span><br><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_</span><span class="keyword">class</span><span class="number">_n</span>ame</span><br><span class="line">find<span class="number">_</span>element<span class="number">_</span><span class="meta">by</span><span class="number">_</span>css<span class="number">_</span>selector</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外，Selenium 还提供了通用方法 <code>find_element</code>，它需要传入两个参数：查找方式 <code>By</code> 和值。实际上，它就是 <code>find_element_by_id</code> 这种方法的通用函数版本，比如 <code>find_element_by_id(id)</code> 就等价于 <code>find_element(By.ID, id)</code>，二者得到的结果完全一致。我们用代码实现一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">input_first = browser.find_element(By.ID, <span class="string">'q'</span>)</span><br><span class="line">print(input_first)</span><br><span class="line">browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>实际上，这种查找方式的功能和上面列举的查找函数完全一致，不过参数更加灵活。</p>
                  <h3 id="多个节点"><a href="#多个节点" class="headerlink" title="多个节点"></a>多个节点</h3>
                  <p>如果查找的目标在网页中只有一个，那么完全可以用 <code>find_element</code> 方法。但如果有多个节点，再用 <code>find_element</code> 方法查找，就只能得到第一个节点了。如果要查找所有满足条件的节点，需要用 <code>find_elements</code> 这样的方法。注意，在这个方法的名称中，element 多了一个 s，注意区分。</p>
                  <p>比如，要查找淘宝左侧导航条的所有条目，就可以这样来实现：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">lis = browser.find_elements_by_css_selector(<span class="string">'.service-bd li'</span>)</span><br><span class="line">print(lis)</span><br><span class="line">browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&lt;selenium<span class="selector-class">.webdriver</span><span class="selector-class">.remote</span><span class="selector-class">.webelement</span><span class="selector-class">.WebElement</span> (session=<span class="string">"c26290835d4457ebf7d96bfab3740d19"</span>, element=<span class="string">"0.09221044033125603-1"</span>)&gt;, &lt;selenium<span class="selector-class">.webdriver</span><span class="selector-class">.remote</span><span class="selector-class">.webelement</span><span class="selector-class">.WebElement</span> (session=<span class="string">"c26290835d4457ebf7d96bfab3740d19"</span>, element=<span class="string">"0.09221044033125603-2"</span>)&gt;, &lt;selenium<span class="selector-class">.webdriver</span><span class="selector-class">.remote</span><span class="selector-class">.webelement</span><span class="selector-class">.WebElement</span> (session=<span class="string">"c26290835d4457ebf7d96bfab3740d19"</span>, element=<span class="string">"0.09221044033125603-3"</span>)&gt;...&lt;selenium<span class="selector-class">.webdriver</span><span class="selector-class">.remote</span><span class="selector-class">.webelement</span><span class="selector-class">.WebElement</span> (session=<span class="string">"c26290835d4457ebf7d96bfab3740d19"</span>, element=<span class="string">"0.09221044033125603-16"</span>)&gt;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里简化了输出结果，中间部分省略。</p>
                  <p>可以看到，得到的内容变成了列表类型，列表中的每个节点都是 <code>WebElement</code> 类型。</p>
                  <p>也就是说，如果我们用 <code>find_element</code> 方法，只能获取匹配的第一个节点，结果是 <code>WebElement</code> 类型。如果用 <code>find_elements</code> 方法，则结果是列表类型，列表中的每个节点都是 <code>WebElement</code> 类型。</p>
                  <p>这里列出所有获取多个节点的方法：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">find_elements_by_id</span><br><span class="line">find_elements_by_name</span><br><span class="line">find_elements_by_xpath</span><br><span class="line">find_elements_by_link_text</span><br><span class="line">find_elements_by_partial_link_text</span><br><span class="line">find_elements_by_tag_name</span><br><span class="line">find_elements_by_class_name</span><br><span class="line">find_elements_by_css_selector</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，我们也可以直接用 <code>find_elements</code> 方法来选择，这时可以这样写：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">lis = browser.find_elements(By.CSS_SELECTOR, <span class="string">'.service-bd li'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>结果是完全一致的。</p>
                  <h2 id="6-节点交互"><a href="#6-节点交互" class="headerlink" title="6. 节点交互"></a>6. 节点交互</h2>
                  <p>Selenium 可以驱动浏览器来执行一些操作，也就是说可以让浏览器模拟执行一些动作。比较常见的用法有：输入文字时用 <code>send_keys</code> 方法，清空文字时用 <code>clear</code> 方法，点击按钮时用 <code>click</code> 方法。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">input = browser.find_element_by_id(<span class="string">'q'</span>)</span><br><span class="line">input.send_keys(<span class="string">'iPhone'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line">input.clear()</span><br><span class="line">input.send_keys(<span class="string">'iPad'</span>)</span><br><span class="line">button = browser.find_element_by_class_name(<span class="string">'btn-search'</span>)</span><br><span class="line">button.click()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先驱动浏览器打开淘宝，然后用 <code>find_element_by_id</code> 方法获取输入框，然后用 <code>send_keys</code> 方法输入 iPhone 文字，等待一秒后用 <code>clear</code> 方法清空输入框，再次调用 <code>send_keys</code> 方法输入 iPad 文字，之后再用 <code>find_element_by_class_name</code> 方法获取搜索按钮，最后调用 <code>click</code> 方法完成搜索动作。</p>
                  <p>通过上面的方法，我们完成了一些常见节点的操作，更多的操作可以参见官方文档的交互动作介绍 ：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement。</a></p>
                  <h2 id="7-动作链"><a href="#7-动作链" class="headerlink" title="7. 动作链"></a>7. 动作链</h2>
                  <p>在上面的实例中，一些交互动作都是针对某个节点执行的。比如，对于输入框，我们就调用它的输入文字和清空文字方法；对于按钮，就调用它的点击方法。其实，还有另外一些操作，它们没有特定的执行对象，比如鼠标拖曳、键盘按键等，这些动作用另一种方式来执行，那就是动作链。</p>
                  <p>比如，现在实现一个节点的拖曳操作，将某个节点从一处拖曳到另外一处，可以这样实现：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ActionChains</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">browser.switch_to.frame(<span class="string">'iframeResult'</span>)</span><br><span class="line">source = browser.find_element_by_css_selector(<span class="string">'#draggable'</span>)</span><br><span class="line">target = browser.find_element_by_css_selector(<span class="string">'#droppable'</span>)</span><br><span class="line">actions = ActionChains(browser)</span><br><span class="line">actions.drag_and_drop(source, target)</span><br><span class="line">actions.perform()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先，打开网页中的一个拖曳实例，然后依次选中要拖曳的节点和拖曳到的目标节点，接着声明 <code>ActionChains</code> 对象并将其赋值为 <code>actions</code> 变量，然后通过调用 <code>actions</code> 变量的 <code>drag_and_drop</code> 方法，再调用 <code>perform</code> 方法执行动作，此时就完成了拖曳操作，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/7650u.jpg" alt="拖曳前页面"></p>
                  <p><img src="https://cdn.cuiqingcai.com/z8xjy.jpg" alt="拖曳后页面"></p>
                  <p>更多的动作链操作可以参考官方文档的动作链介绍：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains。</a></p>
                  <h2 id="8-执行-JavaScript"><a href="#8-执行-JavaScript" class="headerlink" title="8. 执行 JavaScript"></a>8. 执行 JavaScript</h2>
                  <p>对于某些操作，Selenium API 并没有提供。比如，下拉进度条，它可以直接模拟运行 JavaScript，此时使用 <code>execute_script</code> 方法即可实现，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'alert("To Bottom")'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里就利用 <code>execute_script</code> 方法将进度条下拉到最底部，然后弹出 alert 提示框。</p>
                  <p>所以说有了这个方法，基本上 API 没有提供的所有功能都可以用执行 JavaScript 的方式来实现了。</p>
                  <h2 id="9-获取节点信息"><a href="#9-获取节点信息" class="headerlink" title="9. 获取节点信息"></a>9. 获取节点信息</h2>
                  <p>前面说过，通过 <code>page_source</code> 属性可以获取网页的源代码，接着就可以使用解析库（如正则表达式、Beautiful Soup、pyquery 等）来提取信息了。</p>
                  <p>不过，既然 Selenium 已经提供了选择节点的方法，返回的是 <code>WebElement</code> 类型，那么它也有相关的方法和属性来直接提取节点信息，如属性、文本等。这样的话，我们就可以不用通过解析源代码来提取信息了，非常方便。</p>
                  <p>接下来，我们就来看看怎样获取节点信息吧。</p>
                  <h3 id="获取属性"><a href="#获取属性" class="headerlink" title="获取属性"></a>获取属性</h3>
                  <p>我们可以使用 <code>get_attribute</code> 方法来获取节点的属性，但是其前提是先选中这个节点，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'https://spa2.scrape.center/'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">logo = browser.find_element_by_class_name(<span class="string">'logo-image'</span>)</span><br><span class="line">print(logo)</span><br><span class="line">print(logo.get_attribute(<span class="string">'src'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后，程序便会驱动浏览器打开该页面，然后获取 <code>class</code> 为 <code>logo-image</code> 的节点，最后打印出它的 <code>src</code>。</p>
                  <p>控制台的输出结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;selenium.webdriver.remote.webelement.WebElement (session=<span class="string">"7f4745d35a104759239b53f68a6f27d0"</span>, element=<span class="string">"cd7c72b4-4920-47ed-91c5-ea06601dc509"</span>)&gt;</span><br><span class="line">https://spa2.scrape.center/img/logo.a508a8f0.png</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过 <code>get_attribute</code> 方法，然后传入想要获取的属性名，就可以得到它的值了。</p>
                  <h3 id="获取文本值"><a href="#获取文本值" class="headerlink" title="获取文本值"></a>获取文本值</h3>
                  <p>每个 <code>WebElement</code> 节点都有 <code>text</code> 属性，直接调用这个属性就可以得到节点内部的文本信息，这相当于 pyquery 的 <code>text</code> 方法，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'https://spa2.scrape.center/'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'logo-title'</span>)</span><br><span class="line">print(input.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里依然先打开页面，然后获取 <code>class</code> 为 <code>logo-title</code> 这个节点，再将其文本值打印出来。</p>
                  <p>控制台的输出结果如下：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">Scrape</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="获取-ID、位置、标签名和大小"><a href="#获取-ID、位置、标签名和大小" class="headerlink" title="获取 ID、位置、标签名和大小"></a>获取 ID、位置、标签名和大小</h3>
                  <p>另外，<code>WebElement</code> 节点还有一些其他属性，比如 <code>id</code> 属性可以获取节点 ID，<code>location</code> 属性可以获取该节点在页面中的相对位置，<code>tag_name</code> 属性可以获取标签名称，<code>size</code> 属性可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'https://spa2.scrape.center/'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'logo-title'</span>)</span><br><span class="line">print(input.id)</span><br><span class="line">print(input.location)</span><br><span class="line">print(input.tag_name)</span><br><span class="line">print(input.size)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先获得 <code>class</code> 为 <code>logo-title</code> 这个节点，然后调用其 <code>id</code>、<code>location</code>、<code>tag_name</code>、<code>size</code> 属性来获取对应的属性值。</p>
                  <h2 id="10-切换-Frame"><a href="#10-切换-Frame" class="headerlink" title="10. 切换 Frame"></a>10. 切换 Frame</h2>
                  <p>我们知道网页中有一种节点叫作 iframe，也就是子 Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。Selenium 打开页面后，它默认是在父级 Frame 里面操作，而此时如果页面中还有子 Frame，它是不能获取到子 Frame 里面的节点的。这时就需要使用 <code>switch_to.frame</code> 方法来切换 Frame。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> NoSuchElementException</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">browser.switch_to.frame(<span class="string">'iframeResult'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    logo = browser.find_element_by_class_name(<span class="string">'logo'</span>)</span><br><span class="line"><span class="keyword">except</span> NoSuchElementException:</span><br><span class="line">    print(<span class="string">'NO LOGO'</span>)</span><br><span class="line">browser.switch_to.parent_frame()</span><br><span class="line">logo = browser.find_element_by_class_name(<span class="string">'logo'</span>)</span><br><span class="line">print(logo)</span><br><span class="line">print(logo.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>控制台输出结果如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">NO</span> LOGO</span><br><span class="line">&lt;selenium.webdriver.remote.webelement.WebElement (<span class="attribute">session</span>=<span class="string">"4bb8ac03ced4ecbdefef03ffdc0e4ccd"</span>, <span class="attribute">element</span>=<span class="string">"0.13792611320464965-2"</span>)&gt;</span><br><span class="line">RUNOOB.COM</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里还是以前面演示动作链操作的网页为实例，首先通过 <code>switch_to.frame</code> 方法切换到子 Frame 里面，然后尝试获取子 Frame 里的 logo 节点（这是找不到的），如果找不到的话，就会抛出 <code>NoSuchElementException</code> 异常，异常被捕捉之后，就会输出 <code>NO LOGO</code>。接下来，重新切换回父级 Frame，然后再次重新获取节点，发现此时可以成功获取了。</p>
                  <p>所以，当页面中包含子 Frame 时，如果想获取子 Frame 中的节点，需要先调用 <code>switch_to.frame</code> 方法切换到对应的 Frame，然后再进行操作。</p>
                  <h2 id="11-延时等待"><a href="#11-延时等待" class="headerlink" title="11. 延时等待"></a>11. 延时等待</h2>
                  <p>在 Selenium 中，<code>get</code> 方法会在网页框架加载结束后结束执行，此时如果获取 <code>page_source</code>，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的 Ajax 请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来。</p>
                  <p>这里等待方式有两种：一种是隐式等待，一种是显式等待。</p>
                  <h3 id="隐式等待"><a href="#隐式等待" class="headerlink" title="隐式等待"></a>隐式等待</h3>
                  <p>当使用隐式等待执行测试的时候，如果 Selenium 没有在 DOM 中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。换句话说，当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是 0。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.implicitly_wait(<span class="number">10</span>)</span><br><span class="line">browser.get(<span class="string">'https://spa2.scrape.center/'</span>)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'logo-image'</span>)</span><br><span class="line">print(input)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们用 <code>implicitly_wait</code> 方法实现了隐式等待。</p>
                  <h3 id="显式等待"><a href="#显式等待" class="headerlink" title="显式等待"></a>显式等待</h3>
                  <p>隐式等待的效果其实并没有那么好，因为我们只规定了一个固定时间，而页面的加载时间会受到网络条件的影响。</p>
                  <p>这里还有一种更合适的显式等待方法，它指定要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com/'</span>)</span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">input = wait.until(EC.presence_of_element_located((By.ID, <span class="string">'q'</span>)))</span><br><span class="line">button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, <span class="string">'.btn-search'</span>)))</span><br><span class="line">print(input, button)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先引入 <code>WebDriverWait</code> 这个对象，指定最长等待时间，然后调用它的 <code>until</code> 方法，传入等待条件 <code>expected_conditions</code>。比如，这里传入了 <code>presence_of_element_located</code> 这个条件，代表节点出现的意思，其参数是节点的定位元组，也就是 ID 为 <code>q</code> 的节点搜索框。</p>
                  <p>这样可以做到的效果就是，在 10 秒内如果 ID 为 <code>q</code> 的节点（即搜索框）成功加载出来，就返回该节点；如果超过 10 秒还没有加载出来，就抛出异常。</p>
                  <p>对于按钮，可以更改一下等待条件，比如改为 <code>element_to_be_clickable</code>，也就是可点击，所以查找按钮时查找 CSS 选择器为 <code>.btn-search</code> 的按钮，如果 10 秒内它是可点击的，也就是成功加载出来了，就返回这个按钮节点；如果超过 10 秒还不可点击，也就是没有加载出来，就抛出异常。</p>
                  <p>运行代码，在网速较佳的情况下是可以成功加载出来的。</p>
                  <p>控制台的输出如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;selenium.webdriver.remote.webelement.WebElement (session=<span class="string">"07dd2fbc2d5b1ce40e82b9754aba8fa8"</span>, element=<span class="string">"0.5642646294074107-1"</span>)&gt;</span><br><span class="line">&lt;selenium.webdriver.remote.webelement.WebElement (session=<span class="string">"07dd2fbc2d5b1ce40e82b9754aba8fa8"</span>, element=<span class="string">"0.5642646294074107-2"</span>)&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，控制台成功输出了两个节点，它们都是 <code>WebElement</code> 类型。</p>
                  <p>如果网络有问题，10 秒内没有成功加载，那就抛出 <code>TimeoutException</code> 异常，此时控制台的输出如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">TimeoutException Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-4</span>-f3d73973b223&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">      <span class="number">7</span> browser.get(<span class="string">'https://www.taobao.com/'</span>)</span><br><span class="line">      <span class="number">8</span> wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">----&gt; 9 input = wait.until(EC.presence_of_element_located((By.ID, 'q')))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>关于等待条件，其实还有很多，比如判断标题内容，判断某个节点内是否出现了某文字等。下表列出了所有的等待条件。</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th>等待条件</th>
                          <th>含义</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td><code>title_is</code></td>
                          <td>标题是某内容</td>
                        </tr>
                        <tr>
                          <td><code>title_contains</code></td>
                          <td>标题包含某内容</td>
                        </tr>
                        <tr>
                          <td><code>presence_of_element_located</code></td>
                          <td>节点加载出来，传入定位元组，如 <code>(By.ID, &#39;p&#39;)</code></td>
                        </tr>
                        <tr>
                          <td><code>visibility_of_element_located</code></td>
                          <td>节点可见，传入定位元组</td>
                        </tr>
                        <tr>
                          <td><code>visibility_of</code></td>
                          <td>可见，传入节点对象</td>
                        </tr>
                        <tr>
                          <td><code>presence_of_all_elements_located</code></td>
                          <td>所有节点加载出来</td>
                        </tr>
                        <tr>
                          <td><code>text_to_be_present_in_element</code></td>
                          <td>某个节点文本包含某文字</td>
                        </tr>
                        <tr>
                          <td><code>text_to_be_present_in_element_value</code></td>
                          <td>某个节点值包含某文字</td>
                        </tr>
                        <tr>
                          <td><code>frame_to_be_available_and_switch_to_it frame</code></td>
                          <td>加载并切换</td>
                        </tr>
                        <tr>
                          <td><code>invisibility_of_element_located</code></td>
                          <td>节点不可见</td>
                        </tr>
                        <tr>
                          <td><code>element_to_be_clickable</code></td>
                          <td>节点可点击</td>
                        </tr>
                        <tr>
                          <td><code>staleness_of</code></td>
                          <td>判断一个节点是否仍在 DOM，可判断页面是否已经刷新</td>
                        </tr>
                        <tr>
                          <td><code>element_to_be_selected</code></td>
                          <td>节点可选择，传入节点对象</td>
                        </tr>
                        <tr>
                          <td><code>element_located_to_be_selected</code></td>
                          <td>节点可选择，传入定位元组</td>
                        </tr>
                        <tr>
                          <td><code>element_selection_state_to_be</code></td>
                          <td>传入节点对象以及状态，相等返回 <code>True</code>，否则返回 <code>False</code></td>
                        </tr>
                        <tr>
                          <td><code>element_located_selection_state_to_be</code></td>
                          <td>传入定位元组以及状态，相等返回 <code>True</code>，否则返回 <code>False</code></td>
                        </tr>
                        <tr>
                          <td><code>alert_is_present</code></td>
                          <td>是否出现 Alert</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>更多等待条件的参数及用法介绍可以参考官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions。</a></p>
                  <h2 id="12-前进后退"><a href="#12-前进后退" class="headerlink" title="12. 前进后退"></a>12. 前进后退</h2>
                  <p>平常使用浏览器时，都有前进和后退功能，Selenium 也可以完成这个操作，它使用 <code>back</code> 方法后退，使用 <code>forward</code> 方法前进。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com/'</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com/'</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.python.org/'</span>)</span><br><span class="line">browser.back()</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line">browser.forward()</span><br><span class="line">browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们连续访问 3 个页面，然后调用 <code>back</code> 方法回到第二个页面，接下来再调用 <code>forward</code> 方法又可以前进到第三个页面。</p>
                  <h2 id="13-Cookies"><a href="#13-Cookies" class="headerlink" title="13. Cookies"></a>13. Cookies</h2>
                  <p>使用 Selenium，还可以方便地对 Cookies 进行操作，例如获取、添加、删除 Cookies 等。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">print(browser.get_cookies())</span><br><span class="line">browser.add_cookie(&#123;<span class="string">'name'</span>: <span class="string">'name'</span>, <span class="string">'domain'</span>: <span class="string">'www.zhihu.com'</span>, <span class="string">'value'</span>: <span class="string">'germey'</span>&#125;)</span><br><span class="line">print(browser.get_cookies())</span><br><span class="line">browser.delete_all_cookies()</span><br><span class="line">print(browser.get_cookies())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先，我们访问了知乎。加载完成后，浏览器实际上已经生成 Cookies 了。接着，调用 <code>get_cookies</code> 方法获取所有的 Cookies。然后，我们添加一个 Cookie，这里传入一个字典，有 <code>name</code>、<code>domain</code> 和 <code>value</code> 等内容。接下来，再次获取所有的 Cookies。可以发现，结果就多了这一项新加的 Cookie。最后，调用 <code>delete_all_cookies</code> 方法删除所有的 Cookies。再重新获取，发现结果就为空了。</p>
                  <p>控制台的输出如下：</p>
                  <figure class="highlight scheme">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&#123;<span class="symbol">'secure</span><span class="symbol">':</span> False, <span class="symbol">'value</span><span class="symbol">':</span> '<span class="string">"NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0"</span>', <span class="symbol">'domain</span><span class="symbol">':</span> <span class="symbol">'.zhihu.com</span>', <span class="symbol">'path</span><span class="symbol">':</span> <span class="symbol">'/</span>', <span class="symbol">'httpOnly</span><span class="symbol">':</span> False, <span class="symbol">'name</span><span class="symbol">':</span> <span class="symbol">'l_cap_id</span>', <span class="symbol">'expiry</span><span class="symbol">':</span> <span class="number">1494196091.403418</span>&#125;, ...]</span><br><span class="line">[&#123;<span class="symbol">'secure</span><span class="symbol">':</span> False, <span class="symbol">'value</span><span class="symbol">':</span> <span class="symbol">'germey</span>', <span class="symbol">'domain</span><span class="symbol">':</span> <span class="symbol">'.www.zhihu.com</span>', <span class="symbol">'path</span><span class="symbol">':</span> <span class="symbol">'/</span>', <span class="symbol">'httpOnly</span><span class="symbol">':</span> False, <span class="symbol">'name</span><span class="symbol">':</span> <span class="symbol">'name</span>'&#125;, &#123;<span class="symbol">'secure</span><span class="symbol">':</span> False, <span class="symbol">'value</span><span class="symbol">':</span> '<span class="string">"NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0"</span>', <span class="symbol">'domain</span><span class="symbol">':</span> <span class="symbol">'.zhihu.com</span>', <span class="symbol">'path</span><span class="symbol">':</span> <span class="symbol">'/</span>', <span class="symbol">'httpOnly</span><span class="symbol">':</span> False, <span class="symbol">'name</span><span class="symbol">':</span> <span class="symbol">'l_cap_id</span>', <span class="symbol">'expiry</span><span class="symbol">':</span> <span class="number">1494196091.403418</span>&#125;, ...]</span><br><span class="line">[]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过以上方法来操作 Cookies 还是非常方便的。</p>
                  <h2 id="14-选项卡管理"><a href="#14-选项卡管理" class="headerlink" title="14. 选项卡管理"></a>14. 选项卡管理</h2>
                  <p>在访问网页的时候，会开启一个个选项卡。在 Selenium 中，我们也可以对选项卡进行操作。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'window.open()'</span>)</span><br><span class="line">print(browser.window_handles)</span><br><span class="line">browser.switch_to.window(browser.window_handles[<span class="number">1</span>])</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line">browser.switch_to.window(browser.window_handles[<span class="number">0</span>])</span><br><span class="line">browser.get(<span class="string">'https://python.org'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>控制台的输出如下：</p>
                  <figure class="highlight lsl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">['CDwindow<span class="number">-4</span>f58e3a7<span class="number">-7167</span><span class="number">-4587</span>-bedf<span class="number">-9</span>cd8c867f435', 'CDwindow<span class="number">-6e05</span>f076<span class="number">-6</span>d77<span class="number">-453</span>a-a36c<span class="number">-32</span>baacc447df']</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先访问了百度，然后调用了 <code>execute_script</code> 方法，这里传入 <code>window.open</code> 这个 JavaScript 语句新开启一个选项卡。接下来，我们想切换到该选项卡。这里调用 <code>window_handles</code> 属性获取当前开启的所有选项卡，返回的是选项卡的代号列表。要想切换选项卡，只需要调用 <code>switch_to.window</code> 方法即可，其中参数是选项卡的代号。这里我们将第二个选项卡代号传入，即跳转到第二个选项卡，接下来在第二个选项卡下打开一个新页面，然后切换回第一个选项卡重新调用 <code>switch_to.window</code> 方法，再执行其他操作即可。</p>
                  <h2 id="15-异常处理"><a href="#15-异常处理" class="headerlink" title="15. 异常处理"></a>15. 异常处理</h2>
                  <p>在使用 Selenium 的过程中，难免会遇到一些异常，例如超时、节点未找到等错误，一旦出现此类错误，程序便不会继续运行了。这里我们可以使用 <code>try except</code> 语句来捕获各种异常。</p>
                  <p>首先，演示一下节点未找到的异常，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">browser.find_element_by_id(<span class="string">'hello'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先打开百度页面，然后尝试选择一个并不存在的节点，此时就会遇到异常。</p>
                  <p>运行之后控制台的输出如下：</p>
                  <figure class="highlight vim">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">NoSuchElementException Traceback (most recent <span class="keyword">call</span> <span class="keyword">last</span>)</span><br><span class="line"><span class="symbol">&lt;ipython-input-23-978945848a1b&gt;</span> in <span class="symbol">&lt;module&gt;</span>()</span><br><span class="line">      <span class="number">3</span> browser = webdriver.Chrome()</span><br><span class="line">      <span class="number">4</span> browser.<span class="built_in">get</span>(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">----&gt; <span class="number">5</span> browser.find_element_by_id(<span class="string">'hello'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里抛出了 <code>NoSuchElementException</code> 异常，这通常是节点未找到的异常。为了防止程序遇到异常而中断，我们需要捕获这些异常，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException, NoSuchElementException</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">except</span> TimeoutException:</span><br><span class="line">    print(<span class="string">'Time Out'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.find_element_by_id(<span class="string">'hello'</span>)</span><br><span class="line"><span class="keyword">except</span> NoSuchElementException:</span><br><span class="line">    print(<span class="string">'No Element'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用 <code>try except</code> 来捕获各类异常。比如，我们对 <code>find_element_by_id</code> 查找节点的方法捕获 <code>NoSuchElementException</code> 异常，这样一旦出现这样的错误，就进行异常处理，程序也不会中断了。</p>
                  <p>控制台的输出如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">No</span> <span class="string">Element</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>关于更多的异常类，可以参考官方文档：：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions。</a></p>
                  <h2 id="16-反屏蔽"><a href="#16-反屏蔽" class="headerlink" title="16. 反屏蔽"></a>16. 反屏蔽</h2>
                  <p>现在很多网站都加上了对 Selenium 的检测，来防止一些爬虫的恶意爬取。即如果检测到有人在使用 Selenium 打开浏览器，那就直接屏蔽。</p>
                  <p>在大多数情况下，检测的基本原理是检测当前浏览器窗口下的 <code>window.navigator</code> 对象是否包含 <code>webdriver</code> 这个属性。因为在正常使用浏览器的情况下，这个属性是 <code>undefined</code>，然而一旦我们使用了 Selenium，Selenium 会给 <code>window.navigator</code> 设置 <code>webdriver</code> 属性。很多网站就通过 JavaScript 判断如果 <code>webdriver</code> 属性存在，那就直接屏蔽。</p>
                  <p>这边有一个典型的案例网站：<a href="https://antispider1.scrape.center/，这个网站就使用了上述原理实现了" target="_blank" rel="noopener">https://antispider1.scrape.center/，这个网站就使用了上述原理实现了</a> WebDriver 的检测，如果使用 Selenium 直接爬取的话，那就会返回如图所示的页面。</p>
                  <p><img src="https://cdn.cuiqingcai.com/l13mw.png" alt="image-20210705014022028"></p>
                  <p>这时候我们可能想到直接使用 JavaScript 语句把这个 <code>webdriver</code> 属性置空，比如通过调用 <code>execute_script</code> 方法来执行如下代码：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="built_in">Object</span>.defineProperty(navigator, <span class="string">"webdriver"</span>, &#123; <span class="attr">get</span>: <span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">undefined</span> &#125;);</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这行 JavaScript 语句的确可以把 <code>webdriver</code> 属性置空，但是 <code>execute_script</code> 调用这行 JavaScript 语句实际上是在页面加载完毕之后才执行的，执行太晚了，网站早在最初页面渲染之前就已经对 <code>webdriver</code> 属性进行了检测，所以用上述方法并不能达到效果。</p>
                  <p>在 Selenium 中，我们可以使用 CDP（即 Chrome Devtools-Protocol，Chrome 开发工具协议）来解决这个问题，通过它我们可以实现在每个页面刚加载的时候执行 JavaScript 代码，执行的 CDP 方法叫作 <code>Page.addScriptToEvaluateOnNewDocument</code>，然后传入上文的 JavaScript 代码即可，这样我们就可以在每次页面加载之前将 <code>webdriver</code> 属性置空了。另外，我们还可以加入几个选项来隐藏 WebDriver 提示条和自动化扩展信息，代码实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ChromeOptions</span><br><span class="line"></span><br><span class="line">option = ChromeOptions()</span><br><span class="line">option.add_experimental_option(<span class="string">'excludeSwitches'</span>, [<span class="string">'enable-automation'</span>])</span><br><span class="line">option.add_experimental_option(<span class="string">'useAutomationExtension'</span>, <span class="literal">False</span>)</span><br><span class="line">browser = webdriver.Chrome(options=option)</span><br><span class="line">browser.execute_cdp_cmd(<span class="string">'Page.addScriptToEvaluateOnNewDocument'</span>, &#123;</span><br><span class="line">    <span class="string">'source'</span>: <span class="string">'Object.defineProperty(navigator, "webdriver", &#123;get: () =&gt; undefined&#125;)'</span></span><br><span class="line">&#125;)</span><br><span class="line">browser.get(<span class="string">'https://antispider1.scrape.center/'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样整个页面就能被加载出来了，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/tywxa.png" alt=""></p>
                  <p>对于大多数情况，以上方法均可以实现 Selenium 反屏蔽。但对于一些特殊网站，如果它有更多的 WebDriver 特征检测，可能需要具体排查。</p>
                  <h2 id="17-无头模式"><a href="#17-无头模式" class="headerlink" title="17. 无头模式"></a>17. 无头模式</h2>
                  <p>我们可以观察到，上面的案例在运行的时候，总会弹出一个浏览器窗口，虽然有助于观察页面爬取状况，但在有时候窗口弹来弹去也会形成一些干扰。</p>
                  <p>Chrome 浏览器从 60 版本已经支持了无头模式，即 Headless。无头模式在运行的时候不会再弹出浏览器窗口，减少了干扰，而且它减少了一些资源的加载，如图片等，所以也在一定程度上节省了资源加载时间和网络带宽。</p>
                  <p>我们可以借助于 ChromeOptions 来开启 Chrome Headless 模式，代码实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ChromeOptions</span><br><span class="line"></span><br><span class="line">option = ChromeOptions()</span><br><span class="line">option.add_argument(<span class="string">'--headless'</span>)</span><br><span class="line">browser = webdriver.Chrome(options=option)</span><br><span class="line">browser.set_window_size(<span class="number">1366</span>, <span class="number">768</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">browser.get_screenshot_as_file(<span class="string">'preview.png'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们通过 ChromeOptions 的 <code>add_argument</code> 方法添加了一个参数 <code>--headless</code>，开启了无头模式。在无头模式下，我们最好设置一下窗口的大小，接着打开页面，最后我们调用 <code>get_screenshot_as_file</code> 方法输出了页面的截图。</p>
                  <p>运行代码之后，我们发现 Chrome 窗口就不会再弹出来了，代码依然正常运行，最后输出的页面如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/8h0oa.png" alt="输出的页面"></p>
                  <p>这样我们就在无头模式下完成了页面的抓取和截图操作。</p>
                  <h2 id="18-总结"><a href="#18-总结" class="headerlink" title="18. 总结"></a>18. 总结</h2>
                  <p>现在，我们基本上对 Selenium 的常规用法有了大体的了解。使用 Selenium，处理 JavaScript 渲染的页面不再是难事，后面我们会用一个实例来演示 Selenium 爬取网站的流程。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/SeleniumTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/SeleniumTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-20 12:33:32" itemprop="dateCreated datePublished" datetime="2022-02-20T12:33:32+08:00">2022-02-20</time>
                </span>
                <span id="/202261.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 经典动态渲染工具 Selenium 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>20k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>18 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202246.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202246.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 便于高效检索的 Elasticsearch 存储</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>想查数据，就免不了搜索，而搜索离不开搜索引擎。百度、谷歌都是非常庞大、复杂的搜索引擎，它们几乎索引了互联网上开放的所有网页和数据。然而对于我们自己的业务数据来说，没必要用这么复杂的技术。如果我们想实现自己的搜索引擎，为了便于存储和检索，Elasticsearch 就是不二选择。它是一个全文搜索引擎，可以快速存储、搜索和分析海量数据。</p>
                  <p>所以，如果我们我们将爬取到的数据存储到 Elasticsearch 里面，那将会非常方便检索。</p>
                  <h2 id="1-Elasticsearch-介绍"><a href="#1-Elasticsearch-介绍" class="headerlink" title="1. Elasticsearch 介绍"></a>1. Elasticsearch 介绍</h2>
                  <p>Elasticsearch 是一个开源的搜索引擎，建立在一个全文搜索引擎库 Apache Lucene™ 基础之上。</p>
                  <p>那 Lucene 又是什么呢？Lucene 可能是目前存在的（不论开源还是私有的）拥有最先进、高性能和全功能搜索引擎功能的库，但也仅仅只是一个库。要想用 Lucene，我们需要编写 Java 并引用 Lucene 包才可以，而且我们需要对信息检索有一定程度的理解。</p>
                  <p>为了解决这个问题，Elasticsearch 就诞生了。Elasticsearch 也是使用 Java 编写的，它的内部使用 Lucene 做索引与搜索，但是它的目标是使全文检索变得简单，相当于 Lucene 的一层封装，它提供了一套简单一致的 RESTful API 来帮助我们实现存储和检索。</p>
                  <p>所以 Elasticsearch 仅仅就是一个简易版的 Lucene 封装吗？那就大错特错了，Elasticsearch 不仅仅是 Lucene，并且也不仅仅只是一个全文搜索引擎。它可以这样准确形容：</p>
                  <ul>
                    <li>一个分布式的实时文档存储，每个字段可以被索引与搜索；</li>
                    <li>一个分布式实时分析搜索引擎；</li>
                    <li>能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据。</li>
                  </ul>
                  <p>总之，它是一个非常强大的搜索引擎，维基百科、Stack Overflow、GitHub 都纷纷采用它来做搜索，不仅仅提供强大的检索能力，也提供强大的存储能力。</p>
                  <h2 id="2-Elasticsearch-相关概念"><a href="#2-Elasticsearch-相关概念" class="headerlink" title="2. Elasticsearch 相关概念"></a>2. Elasticsearch 相关概念</h2>
                  <p>在 Elasticsearch 中有几个基本概念，如节点、索引、文档等，下面分别说明一下。理解了这些概念，对熟悉 Elasticsearch 是非常有帮助的。</p>
                  <h3 id="节点和集群"><a href="#节点和集群" class="headerlink" title="节点和集群"></a>节点和集群</h3>
                  <p>Elasticsearch 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elasticsearch 实例。</p>
                  <p>单个 Elasticsearch 实例称为一个节点（Node），一组节点构成一个集群（Cluster）。</p>
                  <h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3>
                  <p>索引，即 Index，Elasticsearch 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。</p>
                  <p>所以，Elasticsearch 数据管理的顶层单位就叫作索引，其实就相当于 MySQL、MongoDB 等中数据库的概念。另外，值得注意的是，每个索引 （即数据库）的名字必须小写。</p>
                  <h3 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h3>
                  <p>文档，即 Document。索引里面单条记录称为文档，许多条文档构成了一个索引。</p>
                  <p>同一个索引里面的文档，不要求有相同的结构（Schema），但是最好保持一致，因为这样有利于提高搜索效率。</p>
                  <h3 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h3>
                  <p>文档可以分组，比如 weather 这个索引里面，既可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫作类型（Type），它是虚拟的逻辑分组，用来过滤文档，类似 MySQL 中的数据表、MongoDB 中的 Collection。</p>
                  <p>不同的类型应该有相似的结构。举例来说，<code>id</code> 字段不能在这个组中是字符串，在另一个组中是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如 <code>products</code> 和 <code>logs</code>）应该存成两个索引，而不是一个索引里面的两个类型（虽然可以做到）。</p>
                  <p>根据规划，Elastic 6.x 版只允许每个索引包含一个类型，Elastic 7.x 开始将会将其彻底移除。</p>
                  <h3 id="字段"><a href="#字段" class="headerlink" title="字段"></a>字段</h3>
                  <p>每个文档都类似一个 JSON 结构，它包含了许多字段，每个字段都有其对应的值，多个字段组成了一个文档，其实就可以类比 MySQL 数据表中的字段。</p>
                  <p>在 Elasticsearch 中，文档归属于一种类型（Type），而这些类型存在于索引中，我们可以画一些简单的对比图来类比传统关系型数据库：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; Columns</span><br><span class="line">Elasticsearch -&gt; Indices   -&gt; Types  -&gt; Documents -&gt; Fields</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上就是 Elasticsearch 里面的一些基本概念，通过和关系型数据库的对比更加有助于理解。</p>
                  <h2 id="3-准备工作"><a href="#3-准备工作" class="headerlink" title="3. 准备工作"></a>3. 准备工作</h2>
                  <p>在开始本节实际操作之前，请确保已经正确安装好了 Elasticsearch，安装方式可以参考：<a href="https://setup.scrape.center/elasticsearch，安装完成之后确保其在本地" target="_blank" rel="noopener">https://setup.scrape.center/elasticsearch，安装完成之后确保其在本地</a> 9200 端口上正常运行即可。</p>
                  <p>Elasticsearch 实际上提供了一系列 Restful API 来进行存取和查询操作，我们可以使用 <code>curl</code> 等命令或者直接调用 API 来进行数据存储和修改操作，但总归来说并不是很方便。所以这里我们就直接介绍一个专门用来对接 Elasticsearch 操作的 Python 库，名称也叫做 Elasticsearch，使用 pip3 安装即可：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 install elasticsearch</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更详细的安装方式可以参考：<a href="https://setup.scrape.center/elasticsearch-py。" target="_blank" rel="noopener">https://setup.scrape.center/elasticsearch-py。</a></p>
                  <p>安装好了之后我们就可以开始本节的学习了。</p>
                  <h2 id="4-创建索引"><a href="#4-创建索引" class="headerlink" title="4. 创建索引"></a>4. 创建索引</h2>
                  <p>我们先来看下怎样创建一个索引，这里我们创建一个名为 <code>news</code> 的索引：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">result = es.indices.create(index=<span class="string">'news'</span>, ignore=<span class="number">400</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们首先创建了一个 Elasticsearch 对象，并且没有设置任何参数，默认情况下它会连接本地 9200 端口运行的 Elasticsearch 服务，我们也可以设置特定的连接信息，如：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">es = Elasticsearch(</span><br><span class="line">    [<span class="string">'https://[username:password@]hostname:port'</span>],</span><br><span class="line">    verify_certs=<span class="literal">True</span>, <span class="comment"># 是否验证 SSL 证书</span></span><br><span class="line">)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一个参数我们可以构造特定格式的链接字符串并传入，hostname 和 port 即 Elasticsearch 运行的地址和端口，username 和 password 是可选的，代表连接 Elasticsearch 需要的用户名和密码，另外而且还有其他的参数设置，比如 verify_certs 代表是否验证证书有效性。更多参数的设置可以参考：<a href="https://elasticsearch-py.readthedocs.io/en/latest/api.html#elasticsearch。" target="_blank" rel="noopener">https://elasticsearch-py.readthedocs.io/en/latest/api.html#elasticsearch。</a></p>
                  <p>声明 Elasticsearch 对象之后，我们调用了 es 的 indices 对象的 create 方法传入了 index 的名称，如果创建成功，会返回如下结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'acknowledged'</span>: <span class="literal">True</span>, <span class="string">'shards_acknowledged'</span>: <span class="literal">True</span>, <span class="string">'index'</span>: <span class="string">'news'</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，其返回结果是 JSON 格式，其中的 <code>acknowledged</code> 字段表示创建操作执行成功。</p>
                  <p>但这时如果我们再把代码执行一次的话，就会返回如下结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'error'</span>: &#123;<span class="string">'root_cause'</span>: [&#123;<span class="string">'type'</span>: <span class="string">'resource_already_exists_exception'</span>, <span class="string">'reason'</span>: <span class="string">'index [news/hHEYozoqTzK_qRvV4j4a3w] already exists'</span>, <span class="string">'index_uuid'</span>: <span class="string">'hHEYozoqTzK_qRvV4j4a3w'</span>, <span class="string">'index'</span>: <span class="string">'news'</span>&#125;], <span class="string">'type'</span>: <span class="string">'resource_already_exists_exception'</span>, <span class="string">'reason'</span>: <span class="string">'index [news/hHEYozoqTzK_qRvV4j4a3w] already exists'</span>, <span class="string">'index_uuid'</span>: <span class="string">'hHEYozoqTzK_qRvV4j4a3w'</span>, <span class="string">'index'</span>: <span class="string">'news'</span>&#125;, <span class="string">'status'</span>: <span class="number">400</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>它提示创建失败，<code>status</code> 状态码是 400，错误原因是索引已经存在了。</p>
                  <p>注意在这里的代码中，我们使用的 <code>ignore</code> 参数为 400，这说明如果返回结果是 400 的话，就忽略这个错误，不会报错，程序不会抛出异常。</p>
                  <p>假如我们不加 <code>ignore</code> 这个参数的话：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">es = Elasticsearch()</span><br><span class="line">result = es.indices.create(index=<span class="string">'news'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>再次执行就会报错了：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">raise</span> HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)</span><br><span class="line">elasticsearch.exceptions.RequestError: TransportError(<span class="number">400</span>, <span class="string">'resource_already_exists_exception'</span>, <span class="string">'index [news/QM6yz2W8QE-bflKhc5oThw] already exists'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样程序的执行就会出现问题。因此，我们需要善用 <code>ignore</code> 参数，把一些意外情况排除，这样可以保证程序正常执行而不会中断。</p>
                  <p>创建完之后，我们还可以设置下索引的字段映射定义，可以参考：<a href="https://elasticsearch-py.readthedocs.io/en/latest/api.html?#elasticsearch.client.IndicesClient.put_mapping。" target="_blank" rel="noopener">https://elasticsearch-py.readthedocs.io/en/latest/api.html?#elasticsearch.client.IndicesClient.put_mapping。</a></p>
                  <h2 id="5-删除索引"><a href="#5-删除索引" class="headerlink" title="5. 删除索引"></a>5. 删除索引</h2>
                  <p>删除索引也是类似的，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">result = es.indices.delete(index=<span class="string">'news'</span>, ignore=[<span class="number">400</span>, <span class="number">404</span>])</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里也使用了 <code>ignore</code> 参数来忽略索引不存在而删除失败导致程序中断的问题。</p>
                  <p>如果删除成功，会输出如下结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'acknowledged'</span>: <span class="literal">True</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果索引已经被删除，再执行删除，则会输出如下结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'error'</span>: &#123;<span class="string">'root_cause'</span>: [&#123;<span class="string">'type'</span>: <span class="string">'index_not_found_exception'</span>, <span class="string">'reason'</span>: <span class="string">'no such index [news]'</span>, <span class="string">'resource.type'</span>: <span class="string">'index_or_alias'</span>, <span class="string">'resource.id'</span>: <span class="string">'news'</span>, <span class="string">'index_uuid'</span>: <span class="string">'_na_'</span>, <span class="string">'index'</span>: <span class="string">'news'</span>&#125;], <span class="string">'type'</span>: <span class="string">'index_not_found_exception'</span>, <span class="string">'reason'</span>: <span class="string">'no such index [news]'</span>, <span class="string">'resource.type'</span>: <span class="string">'index_or_alias'</span>, <span class="string">'resource.id'</span>: <span class="string">'news'</span>, <span class="string">'index_uuid'</span>: <span class="string">'_na_'</span>, <span class="string">'index'</span>: <span class="string">'news'</span>&#125;, <span class="string">'status'</span>: <span class="number">404</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个结果表明当前索引不存在，删除失败。返回的结果同样是 JSON，状态码是 404，但是由于我们添加了 <code>ignore</code> 参数，忽略了 404 状态码，因此程序正常执行，输出 JSON 结果，而不是抛出异常。</p>
                  <h2 id="6-插入数据"><a href="#6-插入数据" class="headerlink" title="6. 插入数据"></a>6. 插入数据</h2>
                  <p>Elasticsearch 就像 MongoDB 一样，在插入数据的时候可以直接插入结构化字典数据，插入数据可以调用 <code>create</code> 方法。例如，这里我们插入一条新闻数据：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">es.indices.create(index=<span class="string">'news'</span>, ignore=<span class="number">400</span>)</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">  <span class="string">'title'</span>: <span class="string">'乘风破浪不负韶华，奋斗青春圆梦高考'</span>,</span><br><span class="line">  <span class="string">'url'</span>: <span class="string">'http://view.inews.qq.com/a/EDU2021041600732200'</span></span><br><span class="line">&#125;</span><br><span class="line">result = es.create(index=<span class="string">'news'</span>, id=<span class="number">1</span>, body=data)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们首先声明了一条新闻数据，包括标题和链接，然后通过调用 <code>create</code> 方法插入了这条数据。在调用 <code>create</code> 方法时，我们传入了 4 个参数，<code>index</code> 参数代表了索引名称，<code>id</code> 则是数据的唯一标识 ID，<code>body</code> 则代表了文档的具体内容。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'1'</span>, <span class="string">'_version'</span>: <span class="number">1</span>, <span class="string">'result'</span>: <span class="string">'created'</span>, <span class="string">'_shards'</span>: &#123;<span class="string">'total'</span>: <span class="number">2</span>, <span class="string">'successful'</span>: <span class="number">1</span>, <span class="string">'failed'</span>: <span class="number">0</span>&#125;, <span class="string">'_seq_no'</span>: <span class="number">0</span>, <span class="string">'_primary_term'</span>: <span class="number">1</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>结果中 <code>result</code> 字段为 <code>created</code>，代表该数据插入成功。</p>
                  <p>另外，其实我们也可以使用 <code>index</code> 方法来插入数据。但与 <code>create</code> 不同的是，<code>create</code> 方法需要我们指定 <code>id</code> 字段来唯一标识该条数据，而 <code>index</code> 方法则不需要，如果不指定 <code>id</code>，会自动生成一个 <code>id</code>。调用 <code>index</code> 方法的写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">es.index(index=<span class="string">'news'</span>, body=data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p><code>create</code> 方法内部其实也是调用了 <code>index</code> 方法，是对 <code>index</code> 方法的封装。</p>
                  <h2 id="7-更新数据"><a href="#7-更新数据" class="headerlink" title="7. 更新数据"></a>7. 更新数据</h2>
                  <p>更新数据也非常简单，我们同样需要指定数据的 <code>id</code> 和内容，调用 <code>update</code> 方法即可，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'title'</span>: <span class="string">'乘风破浪不负韶华，奋斗青春圆梦高考'</span>,</span><br><span class="line">    <span class="string">'url'</span>: <span class="string">'http://view.inews.qq.com/a/EDU2021041600732200'</span>,</span><br><span class="line">    <span class="string">'date'</span>: <span class="string">'2021-07-05'</span></span><br><span class="line">&#125;</span><br><span class="line">result = es.update(index=<span class="string">'news'</span>, body=data, id=<span class="number">1</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们为数据增加了一个日期字段，然后调用了 <code>update</code> 方法，结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'1'</span>, <span class="string">'_version'</span>: <span class="number">2</span>, <span class="string">'result'</span>: <span class="string">'updated'</span>, <span class="string">'_shards'</span>: &#123;<span class="string">'total'</span>: <span class="number">2</span>, <span class="string">'successful'</span>: <span class="number">1</span>, <span class="string">'failed'</span>: <span class="number">0</span>&#125;, <span class="string">'_seq_no'</span>: <span class="number">1</span>, <span class="string">'_primary_term'</span>: <span class="number">1</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，返回结果中 <code>result</code> 字段为 <code>updated</code>，即表示更新成功。另外，我们还注意到一个字段 <code>_version</code>，这代表更新后的版本号数，2 代表这是第二个版本。因为之前已经插入过一次数据，所以第一次插入的数据是版本 1，可以参见上例的运行结果，这次更新之后版本号就变成了 2，以后每更新一次，版本号都会加 1。</p>
                  <p>另外，更新操作利用 <code>index</code> 方法同样可以做到，其写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">es.index(index=<span class="string">'news'</span>, body=data, id=<span class="number">1</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，<code>index</code> 方法可以代替我们完成插入和更新数据这两个操作。如果数据不存在，就执行插入操作，如果已经存在，就执行更新操作，非常方便。</p>
                  <h2 id="8-删除数据"><a href="#8-删除数据" class="headerlink" title="8. 删除数据"></a>8. 删除数据</h2>
                  <p>如果想删除一条数据，可以调用 <code>delete</code> 方法并指定需要删除的数据 <code>id</code> 即可。其写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">result = es.delete(index=<span class="string">'news'</span>, id=<span class="number">1</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'1'</span>, <span class="string">'_version'</span>: <span class="number">2</span>, <span class="string">'result'</span>: <span class="string">'deleted'</span>, <span class="string">'_shards'</span>: &#123;<span class="string">'total'</span>: <span class="number">2</span>, <span class="string">'successful'</span>: <span class="number">1</span>, <span class="string">'failed'</span>: <span class="number">0</span>&#125;, <span class="string">'_seq_no'</span>: <span class="number">3</span>, <span class="string">'_primary_term'</span>: <span class="number">1</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，运行结果中 <code>result</code> 字段为 <code>deleted</code>，代表删除成功；<code>_version</code> 变成了 3，又增加了 1。</p>
                  <h2 id="9-查询数据"><a href="#9-查询数据" class="headerlink" title="9. 查询数据"></a>9. 查询数据</h2>
                  <p>上面的几个操作都是非常简单的操作，普通的数据库如 MongoDB 都可以完成，看起来并没有什么了不起的，Elasticsearch 更特殊的地方在于其异常强大的检索功能。</p>
                  <p>对于中文来说，我们需要安装一个分词插件，这里使用的是 elasticsearch-analysis-ik，其 GitHub 链接为<a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik</a>。这里我们使用 Elasticsearch 的另一个命令行工具 elasticsearch-plugin 来安装，这里安装的版本是 7.13.2，请确保和 Elasticsearch 的版本对应起来，命令如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">elasticsearch-plugin install https:&#x2F;&#x2F;github.com&#x2F;medcl&#x2F;elasticsearch-analysis-ik&#x2F;releases&#x2F;download&#x2F;v7.13.2&#x2F;elasticsearch-analysis-ik-7.13.2.zip</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的版本号请替换成你的 Elasticsearch 版本号。</p>
                  <p>安装之后，我们需要重新启动 Elasticsearch，启动之后它会自动加载安装好的插件。</p>
                  <p>首先，我们重新新建一个索引并指定需要分词的字段，相应代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">mapping = &#123;</span><br><span class="line">    <span class="string">'properties'</span>: &#123;</span><br><span class="line">        <span class="string">'title'</span>: &#123;</span><br><span class="line">            <span class="string">'type'</span>: <span class="string">'text'</span>,</span><br><span class="line">            <span class="string">'analyzer'</span>: <span class="string">'ik_max_word'</span>,</span><br><span class="line">            <span class="string">'search_analyzer'</span>: <span class="string">'ik_max_word'</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">es.indices.delete(index=<span class="string">'news'</span>, ignore=[<span class="number">400</span>, <span class="number">404</span>])</span><br><span class="line">es.indices.create(index=<span class="string">'news'</span>, ignore=<span class="number">400</span>)</span><br><span class="line">result = es.indices.put_mapping(index=<span class="string">'news'</span>, body=mapping)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们先将之前的索引删除了，然后新建了一个索引，接着更新了它的 <code>mapping</code> 信息。<code>mapping</code> 信息中指定了分词的字段，指定了字段的类型 <code>type</code> 为 <code>text</code>，分词器 <code>analyzer</code> 和搜索分词器 <code>search_analyzer</code> 为 <code>ik_max_word</code>，即使用我们刚才安装的中文分词插件。如果不指定的话，则使用默认的英文分词器。</p>
                  <p>接下来，我们插入几条新数据：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line"></span><br><span class="line">datas = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'title'</span>: <span class="string">'高考结局大不同'</span>,</span><br><span class="line">        <span class="string">'url'</span>: <span class="string">'https://k.sina.com.cn/article_7571064628_1c3454734001011lz9.html'</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'title'</span>: <span class="string">'进入职业大洗牌时代，“吃香”职业还吃香吗？'</span>,</span><br><span class="line">        <span class="string">'url'</span>: <span class="string">'https://new.qq.com/omn/20210828/20210828A025LK00.html'</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'title'</span>: <span class="string">'乘风破浪不负韶华，奋斗青春圆梦高考'</span>,</span><br><span class="line">        <span class="string">'url'</span>: <span class="string">'http://view.inews.qq.com/a/EDU2021041600732200'</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'title'</span>: <span class="string">'他，活出了我们理想的样子'</span>,</span><br><span class="line">        <span class="string">'url'</span>: <span class="string">'https://new.qq.com/omn/20210821/20210821A020ID00.html'</span>,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> datas:</span><br><span class="line">    es.index(index=<span class="string">'news'</span>, body=data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们指定了 4 条数据，它们都带有 <code>title</code> 和<code>url</code> 字段，然后通过 <code>index</code> 方法将其插入 Elasticsearch 中，索引名称为 <code>news</code>。</p>
                  <p>接下来，我们根据关键词查询一下相关内容：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = es.search(index=<span class="string">'news'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里查询出了插入的 4 条数据：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'took'</span>: <span class="number">11</span>, <span class="string">'timed_out'</span>: <span class="literal">False</span>, <span class="string">'_shards'</span>: &#123;<span class="string">'total'</span>: <span class="number">1</span>, <span class="string">'successful'</span>: <span class="number">1</span>, <span class="string">'skipped'</span>: <span class="number">0</span>, <span class="string">'failed'</span>: <span class="number">0</span>&#125;, <span class="string">'hits'</span>: &#123;<span class="string">'total'</span>: &#123;<span class="string">'value'</span>: <span class="number">4</span>, <span class="string">'relation'</span>: <span class="string">'eq'</span>&#125;, <span class="string">'max_score'</span>: <span class="number">1.0</span>, <span class="string">'hits'</span>: [&#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'jebpkHsBm-BAny-7hOYp'</span>, <span class="string">'_score'</span>: <span class="number">1.0</span>, <span class="string">'_source'</span>: &#123;<span class="string">'title'</span>: <span class="string">'高考结局大不同'</span>, <span class="string">'url'</span>: <span class="string">'https://k.sina.com.cn/article_7571064628_1c3454734001011lz9.html'</span>&#125;&#125;, &#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'jubpkHsBm-BAny-7hObz'</span>, <span class="string">'_score'</span>: <span class="number">1.0</span>, <span class="string">'_source'</span>: &#123;<span class="string">'title'</span>: <span class="string">'进入职业大洗牌时代，“吃香”职业还吃香吗？'</span>, <span class="string">'url'</span>: <span class="string">'https://new.qq.com/omn/20210828/20210828A025LK00.html'</span>&#125;&#125;, &#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'j-bpkHsBm-BAny-7heZN'</span>, <span class="string">'_score'</span>: <span class="number">1.0</span>, <span class="string">'_source'</span>: &#123;<span class="string">'title'</span>: <span class="string">'乘风破浪不负韶华，奋斗青春圆梦高考'</span>, <span class="string">'url'</span>: <span class="string">'http://view.inews.qq.com/a/EDU2021041600732200'</span>&#125;&#125;, &#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'kObpkHsBm-BAny-7hean'</span>, <span class="string">'_score'</span>: <span class="number">1.0</span>, <span class="string">'_source'</span>: &#123;<span class="string">'title'</span>: <span class="string">'他，活出了我们理想的样子'</span>, <span class="string">'url'</span>: <span class="string">'https://new.qq.com/omn/20210821/20210821A020ID00.html'</span>&#125;&#125;]&#125;&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，返回结果会出现在 <code>hits</code> 字段里面，其中 <code>total</code> 字段标明了查询的结果条目数，<code>max_score</code> 代表了最大匹配分数。</p>
                  <p>另外，我们还可以进行全文检索，这才是体现 Elasticsearch 搜索引擎特性的地方：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">dsl = &#123;</span><br><span class="line">    <span class="string">'query'</span>: &#123;</span><br><span class="line">        <span class="string">'match'</span>: &#123;</span><br><span class="line">            <span class="string">'title'</span>: <span class="string">'高考 圆梦'</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">result = es.search(index=<span class="string">'news'</span>, body=dsl)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用 Elasticsearch 支持的 DSL 语句来进行查询，使用 <code>match</code> 指定全文检索，检索的字段是 <code>title</code>，内容是“中国 领事馆”，搜索结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'took'</span>: <span class="number">6</span>, <span class="string">'timed_out'</span>: <span class="literal">False</span>, <span class="string">'_shards'</span>: &#123;<span class="string">'total'</span>: <span class="number">1</span>, <span class="string">'successful'</span>: <span class="number">1</span>, <span class="string">'skipped'</span>: <span class="number">0</span>, <span class="string">'failed'</span>: <span class="number">0</span>&#125;, <span class="string">'hits'</span>: &#123;<span class="string">'total'</span>: &#123;<span class="string">'value'</span>: <span class="number">2</span>, <span class="string">'relation'</span>: <span class="string">'eq'</span>&#125;, <span class="string">'max_score'</span>: <span class="number">1.7796917</span>, <span class="string">'hits'</span>: [&#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'j-bpkHsBm-BAny-7heZN'</span>, <span class="string">'_score'</span>: <span class="number">1.7796917</span>, <span class="string">'_source'</span>: &#123;<span class="string">'title'</span>: <span class="string">'乘风破浪不负韶华，奋斗青春圆梦高考'</span>, <span class="string">'url'</span>: <span class="string">'http://view.inews.qq.com/a/EDU2021041600732200'</span>&#125;&#125;, &#123;<span class="string">'_index'</span>: <span class="string">'news'</span>, <span class="string">'_type'</span>: <span class="string">'_doc'</span>, <span class="string">'_id'</span>: <span class="string">'jebpkHsBm-BAny-7hOYp'</span>, <span class="string">'_score'</span>: <span class="number">0.81085134</span>, <span class="string">'_source'</span>: &#123;<span class="string">'title'</span>: <span class="string">'高考结局大不同'</span>, <span class="string">'url'</span>: <span class="string">'https://k.sina.com.cn/article_7571064628_1c3454734001011lz9.html'</span>&#125;&#125;]&#125;&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们看到匹配的结果有两条，第一条的分数为 1.7796917，第二条的分数为 0.81085134，这是因为第一条匹配的数据中含有“高考”和“圆梦”两个词，第二条匹配的数据中不包含“圆梦”，但是包含了“高考”这个词，所以也被检索出来了，但是分数比较低。</p>
                  <p>因此，可以看出，检索时会对对应的字段进行全文检索，结果还会按照检索关键词的相关性进行排序，这就是一个基本的搜索引擎雏形。</p>
                  <p>另外，Elasticsearch 还支持非常多的查询方式，这里就不再一一展开描述了，总之其功能非常强大，详情可以参考官方文档：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl.html。" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl.html。</a></p>
                  <h2 id="10-总结"><a href="#10-总结" class="headerlink" title="10. 总结"></a>10. 总结</h2>
                  <p>以上便是对 Elasticsearch 的基本介绍以及使用 Python 操作 Elasticsearch 的基本用法，但这仅仅是 Elasticsearch 的基本功能，它还有更多强大的功能等待着我们去探索。</p>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/ElasticSearchTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ElasticSearchTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-18 10:12:57" itemprop="dateCreated datePublished" datetime="2022-02-18T10:12:57+08:00">2022-02-18</time>
                </span>
                <span id="/202246.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 便于高效检索的 Elasticsearch 存储" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>11k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>10 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202271.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202271.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 协程的基本原理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>我们知道爬虫是 IO 密集型任务，比如如果我们使用 requests 库来爬取某个站点的话，发出一个请求之后，程序必须要等待网站返回响应之后才能接着运行，而在等待响应的过程中，整个爬虫程序是一直在等待的，实际上没有做任何事情。对于这种情况，我们有没有优化方案呢？</p>
                  <p>当然有，下面我们就来了解一下异步爬虫的基本概念和实现。</p>
                  <p>要实现异步机制的爬虫，那自然和协程脱不了关系。</p>
                  <h2 id="1-案例引入"><a href="#1-案例引入" class="headerlink" title="1. 案例引入"></a>1. 案例引入</h2>
                  <p>在介绍协程之前，我们先来看一个案例网站，链接地址为：<a href="https://httpbin.org/delay/5" target="_blank" rel="noopener">https://httpbin.org/delay/5</a>，如果我们访问这个链接，需要等待五秒之后才能得到结果，这是因为服务器强制等待了 5 秒的时间才返回响应。</p>
                  <p>平时我们浏览网页的时候，绝大部分网页响应速度还是很快的，如果我们写爬虫来爬取的话，发出 Request 到收到 Response 的时间不会很长，因此我们需要等待的时间并不多。</p>
                  <p>然而像上面这个网站，一次 Request 就需要 5 秒才能得到 Response，如果我们用 requests 写爬虫来爬取的话，那每次 requests 都要等待 5 秒才能拿到结果了。</p>
                  <p>我们来测试下，下面我们来用 requests 写一个遍历程序，直接遍历 100 次试试看，实现代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO,</span><br><span class="line">                    format=<span class="string">'%(asctime)s - %(levelname)s: %(message)s'</span>)</span><br><span class="line"></span><br><span class="line">TOTAL_NUMBER = <span class="number">100</span></span><br><span class="line">URL = <span class="string">'https://httpbin.org/delay/5'</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>, TOTAL_NUMBER + <span class="number">1</span>):</span><br><span class="line">    logging.info(<span class="string">'scraping %s'</span>, URL)</span><br><span class="line">    response = requests.get(URL)</span><br><span class="line">end_time = time.time()</span><br><span class="line">logging.info(<span class="string">'total time %s seconds'</span>, end_time - start_time)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们直接用循环的方式构造了 100 个 Request，使用的是 requests 单线程，在爬取之前和爬取之后记录了时间，最后输出爬取了 100 个页面消耗的时间。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">01</span>:<span class="number">36</span>,<span class="number">781</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">01</span>:<span class="number">43</span>,<span class="number">410</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">01</span>:<span class="number">50</span>,<span class="number">029</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">01</span>:<span class="number">56</span>,<span class="number">702</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">02</span>:<span class="number">03</span>,<span class="number">345</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">02</span>:<span class="number">09</span>,<span class="number">958</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">02</span>:<span class="number">16</span>,<span class="number">500</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">02</span>:<span class="number">23</span>,<span class="number">143</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line">...</span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">12</span>:<span class="number">19</span>,<span class="number">867</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">12</span>:<span class="number">26</span>,<span class="number">479</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">12</span>:<span class="number">33</span>,<span class="number">083</span> - INFO: scraping https:<span class="comment">//httpbin.org/delay/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-08</span><span class="number">-03</span> <span class="number">01</span>:<span class="number">12</span>:<span class="number">39</span>,<span class="number">758</span> - INFO: total time <span class="number">662.9764430522919</span> seconds</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于每个页面至少要等待 5 秒才能加载出来，因此 100 个页面至少要花费 500 秒的时间，加上网站本身负载的问题，总的爬取时间最终为 663 秒，大约 11 分钟。</p>
                  <p>这在实际情况下是很常见的，有些网站本身加载速度就比较慢，稍慢的可能 1~3 秒，更慢的说不定 10 秒以上。如果我们用 requests 单线程这么爬取的话，总的耗时是非常多的。此时如果我们开了多线程或多进程来爬取的话，其爬取速度确实会成倍提升，那是否有更好的解决方案呢？</p>
                  <p>本节就来了解一下使用协程来加速的方法，此种方法对于 IO 密集型任务非常有效。如将其应用到网络爬虫中，爬取效率甚至可以成百倍地提升。</p>
                  <h2 id="2-基础知识"><a href="#2-基础知识" class="headerlink" title="2. 基础知识"></a>2. 基础知识</h2>
                  <p>在了解协程之前，我们首先了解一些基础概念，如阻塞和非阻塞、同步和异步、多进程和协程。</p>
                  <h3 id="阻塞"><a href="#阻塞" class="headerlink" title="阻塞"></a>阻塞</h3>
                  <p>阻塞状态指程序未得到所需计算资源时被挂起的状态。程序在等待某个操作完成期间，自身无法继续干别的事情，则称该程序在该操作上是阻塞的。</p>
                  <p>常见的阻塞形式有：网络 I/O 阻塞、磁盘 I/O 阻塞、用户输入阻塞等。阻塞是无处不在的，包括 CPU 切换上下文时，所有的进程都无法真正干事情，它们也会被阻塞。如果是多核 CPU，则正在执行上下文切换操作的核不可被利用。</p>
                  <h3 id="非阻塞"><a href="#非阻塞" class="headerlink" title="非阻塞"></a>非阻塞</h3>
                  <p>程序在等待某操作的过程中，自身不被阻塞，可以继续运行干别的事情，则称该程序在该操作上是非阻塞的。</p>
                  <p>非阻塞并不是在任何程序级别、任何情况下都存在的。仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态。</p>
                  <p>非阻塞的存在是因为阻塞存在，正因为某个操作阻塞导致的耗时与效率低下，我们才要把它变成非阻塞的。</p>
                  <h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3>
                  <p>不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，此时这些程序单元是同步执行的。</p>
                  <p>例如在购物系统中更新商品库存时，需要用“行锁”作为通信信号，让不同的更新请求强制排队顺序执行，那更新库存的操作是同步的。</p>
                  <p>简言之，同步意味着有序。</p>
                  <h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3>
                  <p>为了完成某个任务，有时不同程序单元之间无须通信协调也能完成任务，此时不相关的程序单元之间可以是异步的。</p>
                  <p>例如，爬取下载网页。调度程序调用下载程序后，即可调度其他任务，而无须与该下载任务保持通信以协调行为。不同网页的下载、保存等操作都是无关的，也无须相互通知协调。这些异步操作的完成时刻并不确定。</p>
                  <p>简言之，异步意味着无序。</p>
                  <h3 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h3>
                  <p>多进程就是利用 CPU 的多核优势，在同一时间并行执行多个任务，可以大大提高执行效率。</p>
                  <h3 id="协程"><a href="#协程" class="headerlink" title="协程"></a>协程</h3>
                  <p>协程，英文叫作 coroutine，又称微线程、纤程，它是一种用户态的轻量级线程。</p>
                  <p>协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此，协程能保留上一次调用时的状态，即所有局部状态的一个特定组合，每次过程重入时，就相当于进入上一次调用的状态。</p>
                  <p>协程本质上是个单进程，它相对于多进程来说，无须线程上下文切换的开销，无须原子操作锁定及同步的开销，编程模型也非常简单。</p>
                  <p>我们可以使用协程来实现异步操作，比如在网络爬虫场景下，我们发出一个请求之后，需要等待一定时间才能得到响应，但其实在这个等待过程中，程序可以干许多其他事情，等到响应得到之后才切换回来继续处理，这样可以充分利用 CPU 和其他资源，这就是协程的优势。</p>
                  <h2 id="3-协程的用法"><a href="#3-协程的用法" class="headerlink" title="3. 协程的用法"></a>3. 协程的用法</h2>
                  <p>接下来，让我们来了解一下协程的实现。从 Python 3.4 开始，Python 中加入了协程的概念，但这个版本的协程还是以生成器对象为基础，Python 3.5 则增加了 <code>async</code>/<code>await</code>，使得协程的实现更加方便。</p>
                  <p>Python 中使用协程最常用的库莫过于 asyncio，所以本节会以 asyncio 为基础来介绍协程的用法。</p>
                  <p>首先，我们需要了解下面几个概念：</p>
                  <ul>
                    <li><code>event_loop</code>：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足条件发生的时候，就会调用对应的处理方法。</li>
                    <li><code>coroutine</code>：中文翻译叫协程，在 Python 中常指代协程对象类型，我们可以将协程对象注册到时间循环中，它会被事件循环调用。我们可以使用 <code>async</code> 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象。</li>
                    <li><code>task</code>：任务，它是对协程对象的进一步封装，包含了任务的各个状态。</li>
                    <li><code>future</code>：代表将来执行或没有执行的任务的结果，实际上和 <code>task</code> 没有本质区别。</li>
                  </ul>
                  <p>另外，我们还需要了解 <code>async</code>/<code>await</code> 关键字，它是从 Python 3.5 才出现的，专门用于定义协程。其中，<code>async</code> 定义一个协程，<code>await</code> 用来挂起阻塞方法的执行。</p>
                  <h2 id="4-准备工作"><a href="#4-准备工作" class="headerlink" title="4. 准备工作"></a>4. 准备工作</h2>
                  <p>在本节开始之前，请确保安装的 Python 版本为 3.5 及以上，如果版本是 3.4 及以下，则下方的案例是不能运行的。</p>
                  <p>具体的安装方法可以参考：<a href="https://setup.scrape.center/python。" target="_blank" rel="noopener">https://setup.scrape.center/python。</a></p>
                  <p>安装好合适的 Python 版本之后我们就可以开始本节的学习了。</p>
                  <h2 id="5-定义协程"><a href="#5-定义协程" class="headerlink" title="5. 定义协程"></a>5. 定义协程</h2>
                  <p>首先，我们来定义一个协程，体验一下它和普通进程在实现上的不同之处，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(<span class="string">'Number:'</span>, x)</span><br><span class="line"></span><br><span class="line">coroutine = execute(<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'Coroutine:'</span>, coroutine)</span><br><span class="line">print(<span class="string">'After calling execute'</span>)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(coroutine)</span><br><span class="line">print(<span class="string">'After calling loop'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Coroutine: &lt;coroutine object execute at <span class="number">0x1034cf830</span>&gt;</span><br><span class="line">After calling execute</span><br><span class="line">Number: <span class="number">1</span></span><br><span class="line">After calling loop</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先，我们引入了 asyncio 这个包，这样我们才可以使用 <code>async</code> 和 <code>await</code>，然后使用 <code>async</code> 定义了一个 <code>execute</code> 方法，该方法接收一个数字参数，执行之后会打印这个数字。</p>
                  <p>随后我们直接调用了这个方法，然而这个方法并没有执行，而是返回了一个 <code>coroutine</code> 协程对象。随后我们使用 <code>get_event_loop</code> 方法创建了一个事件循环 <code>loop</code>，并调用了 <code>loop</code> 对象的 <code>run_until_complete</code> 方法将协程注册到事件循环 <code>loop</code> 中，然后启动。最后，我们才看到 <code>execute</code> 方法打印了输出结果。</p>
                  <p>可见，<code>async</code> 定义的方法就会变成一个无法直接执行的 <code>coroutine</code> 对象，必须将其注册到事件循环中才可以执行。</p>
                  <p>前面我们还提到了 <code>task</code>，它是对 <code>coroutine</code> 对象的进一步封装，比 <code>coroutine</code> 对象多了运行状态，比如 <code>running</code>、<code>finished</code> 等，我们可以用这些状态来获取协程对象的执行情况。</p>
                  <p>在上面的例子中，当我们将 <code>coroutine</code> 对象传递给 <code>run_until_complete</code> 方法的时候，实际上它进行了一个操作，就是将 <code>coroutine</code> 封装成了 <code>task</code> 对象。我们也可以显式地进行声明，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(<span class="string">'Number:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">coroutine = execute(<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'Coroutine:'</span>, coroutine)</span><br><span class="line">print(<span class="string">'After calling execute'</span>)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">task = loop.create_task(coroutine)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br><span class="line">print(<span class="string">'After calling loop'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Coroutine: &lt;coroutine object execute at <span class="number">0x10e0f7830</span>&gt;</span><br><span class="line">After calling execute</span><br><span class="line">Task: &lt;Task pending coro=&lt;execute() running at demo.py:<span class="number">4</span>&gt;&gt;</span><br><span class="line">Number: <span class="number">1</span></span><br><span class="line">Task: &lt;Task finished coro=&lt;execute() done, defined at demo.py:<span class="number">4</span>&gt; result=<span class="number">1</span>&gt;</span><br><span class="line">After calling loop</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了 <code>loop</code> 对象之后，接着调用了它的 <code>create_task</code> 方法将 <code>coroutine</code> 对象转化为 <code>task</code> 对象，随后我们打印输出一下，发现它是 <code>pending</code> 状态。接着，我们将 <code>task</code> 对象添加到事件循环中执行，随后打印输出 <code>task</code> 对象，发现它的状态变成了 <code>finished</code>，同时还可以看到其 <code>result</code> 变成了 1，也就是我们定义的 <code>execute</code> 方法的返回结果。</p>
                  <p>另外，定义 <code>task</code> 对象还有一种方式，就是直接通过 asyncio 的 <code>ensure_future</code> 方法，返回结果也是 <code>task</code> 对象，这样的话我们就可以不借助 <code>loop</code> 来定义。即使我们还没有声明 <code>loop</code>，也可以提前定义好 <code>task</code> 对象，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(<span class="string">'Number:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">coroutine = execute(<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'Coroutine:'</span>, coroutine)</span><br><span class="line">print(<span class="string">'After calling execute'</span>)</span><br><span class="line"></span><br><span class="line">task = asyncio.ensure_future(coroutine)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br><span class="line">print(<span class="string">'After calling loop'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Coroutine: &lt;coroutine object execute at <span class="number">0x10aa33830</span>&gt;</span><br><span class="line">After calling execute</span><br><span class="line">Task: &lt;Task pending coro=&lt;execute() running at demo.py:<span class="number">4</span>&gt;&gt;</span><br><span class="line">Number: <span class="number">1</span></span><br><span class="line">Task: &lt;Task finished coro=&lt;execute() done, defined at demo.py:<span class="number">4</span>&gt; result=<span class="number">1</span>&gt;</span><br><span class="line">After calling loop</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，其运行效果都是一样的。</p>
                  <h2 id="6-绑定回调"><a href="#6-绑定回调" class="headerlink" title="6. 绑定回调"></a>6. 绑定回调</h2>
                  <p>另外，我们也可以为某个 <code>task</code> 绑定一个回调方法。比如，我们来看下面的例子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://www.baidu.com'</span></span><br><span class="line">    status = requests.get(url)</span><br><span class="line">    <span class="keyword">return</span> status</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span><span class="params">(task)</span>:</span></span><br><span class="line">    print(<span class="string">'Status:'</span>, task.result())</span><br><span class="line"></span><br><span class="line">coroutine = request()</span><br><span class="line">task = asyncio.ensure_future(coroutine)</span><br><span class="line">task.add_done_callback(callback)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 <code>request</code> 方法，请求了百度，获取其状态码，但是这个方法里面我们没有任何 <code>print</code> 语句。随后我们定义了一个 <code>callback</code> 方法，这个方法接收一个参数，是 <code>task</code> 对象，然后调用 <code>print</code> 方法打印了 <code>task</code> 对象的结果。这样我们就定义好了一个 <code>coroutine</code> 对象和一个回调方法。我们现在希望的效果是，当 <code>coroutine</code> 对象执行完毕之后，就去执行声明的 <code>callback</code> 方法。</p>
                  <p>那么它们两者怎样关联起来呢？很简单，只需要调用 <code>add_done_callback</code> 方法即可。我们将 <code>callback</code> 方法传递给封装好的 <code>task</code> 对象，这样当 <code>task</code> 执行完毕之后，就可以调用 <code>callback</code> 方法了。同时 <code>task</code> 对象还会作为参数传递给 <code>callback</code> 方法，调用 <code>task</code> 对象的 <code>result</code> 方法就可以获取返回结果了。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Task: &lt;Task pending coro=&lt;request() running at demo.py:<span class="number">5</span>&gt; cb=[callback() at demo.py:<span class="number">11</span>]&gt;</span><br><span class="line">Status: &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Task: &lt;Task finished coro=&lt;request() done, defined at demo.py:<span class="number">5</span>&gt; result=&lt;Response [<span class="number">200</span>]&gt;&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>实际上不用回调方法，直接在 <code>task</code> 运行完毕之后，也可以直接调用 <code>result</code> 方法获取结果，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://www.baidu.com'</span></span><br><span class="line">    status = requests.get(url)</span><br><span class="line">    <span class="keyword">return</span> status</span><br><span class="line"></span><br><span class="line">coroutine = request()</span><br><span class="line">task = asyncio.ensure_future(coroutine)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line">print(<span class="string">'Task:'</span>, task)</span><br><span class="line">print(<span class="string">'Task Result:'</span>, task.result())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果是一样的：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Task: &lt;Task pending coro=&lt;request() running at demo.py:<span class="number">4</span>&gt;&gt;</span><br><span class="line">Task: &lt;Task finished coro=&lt;request() done, defined at demo.py:<span class="number">4</span>&gt; result=&lt;Response [<span class="number">200</span>]&gt;&gt;</span><br><span class="line">Task Result: &lt;Response [<span class="number">200</span>]&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h2 id="7-多任务协程"><a href="#7-多任务协程" class="headerlink" title="7. 多任务协程"></a>7. 多任务协程</h2>
                  <p>上面的例子我们只执行了一次请求，如果想执行多次请求，应该怎么办呢？我们可以定义一个 <code>task</code> 列表，然后使用 asyncio 的 <code>wait</code> 方法即可执行。看下面的例子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://www.baidu.com'</span></span><br><span class="line">    status = requests.get(url)</span><br><span class="line">    <span class="keyword">return</span> status</span><br><span class="line"></span><br><span class="line">tasks = [asyncio.ensure_future(request()) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line">print(<span class="string">'Tasks:'</span>, tasks)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> tasks:</span><br><span class="line">    print(<span class="string">'Task Result:'</span>, task.result())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用一个 <code>for</code> 循环创建了 5 个 <code>task</code>，组成了一个列表，然后把这个列表首先传递给了 asyncio 的 <code>wait</code> 方法，再将其注册到时间循环中，就可以发起 5 个任务了。最后，我们再将任务的运行结果输出出来，具体如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Tasks: [&lt;Task pending coro=&lt;request() running at demo.py:<span class="number">5</span>&gt;&gt;, &lt;Task pending coro=&lt;request() running at demo.py:<span class="number">5</span>&gt;&gt;, &lt;Task pending coro=&lt;request() running at demo.py:<span class="number">5</span>&gt;&gt;, &lt;Task pending coro=&lt;request() running at demo.py:<span class="number">5</span>&gt;&gt;, &lt;Task pending coro=&lt;request() running at demo.py:<span class="number">5</span>&gt;&gt;]</span><br><span class="line">Task Result: &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Task Result: &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Task Result: &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Task Result: &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Task Result: &lt;Response [<span class="number">200</span>]&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，5 个任务被顺次执行了，并得到了运行结果。</p>
                  <h2 id="8-协程实现"><a href="#8-协程实现" class="headerlink" title="8. 协程实现"></a>8. 协程实现</h2>
                  <p>前面说了这么一通，又是 <code>async</code>，又是 <code>coroutine</code>，又是 <code>task</code>，又是 <code>callback</code>，但似乎并没有看出协程的优势？反而写法上更加奇怪和麻烦了。别急，上面的案例只是为后面的使用作铺垫。接下来，我们正式来看下协程在解决 IO 密集型任务上有怎样的优势。</p>
                  <p>在上面的代码中，我们用一个网络请求作为示例，这就是一个耗时等待操作，因为我们请求网页之后需要等待页面响应并返回结果。耗时等待操作一般都是 IO 操作，比如文件读取、网络请求等。协程对于处理这种操作是有很大优势的，当遇到需要等待的情况时，程序可以暂时挂起，转而去执行其他操作，从而避免一直等待一个程序而耗费过多的时间，充分利用资源。</p>
                  <p>为了表现出协程的优势，我们还是以本节开头介绍的网站 <a href="https://httpbin.org/delay/5" target="_blank" rel="noopener">https://httpbin.org/delay/5</a> 为例，因为该网站响应比较慢，所以我们可以通过爬取时间来直观感受到爬取速度的提升。</p>
                  <p>为了让大家更好地理解协程的正确使用方法，这里我们先来看看大家使用协程时常犯的错误，后面再给出正确的例子来对比一下。</p>
                  <p>首先，我们还是拿之前的 requests 库来进行网页请求，接下来再重新使用上面的方法请求一遍：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://httpbin.org/delay/5'</span></span><br><span class="line">    print(<span class="string">'Waiting for'</span>, url)</span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    print(<span class="string">'Get response from'</span>, url, <span class="string">'response'</span>, response)</span><br><span class="line"></span><br><span class="line">tasks = [asyncio.ensure_future(request()) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line">print(<span class="string">'Cost time:'</span>, end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们还是创建了 10 个 <code>task</code>，然后将 <code>task</code> 列表传给 <code>wait</code> 方法并注册到时间循环中执行。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">...</span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Cost time: <span class="number">66.64284420013428</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，这和正常的请求并没有什么区别，依然还是顺次执行的，耗时 66 秒，平均一个请求耗时 6.6 秒，说好的异步处理呢？</p>
                  <p>其实，要实现异步处理，我们得先要有挂起的操作，当一个任务需要等待 IO 结果的时候，可以挂起当前任务，转而去执行其他任务，这样我们才能充分利用好资源。上面的方法都是一本正经地串行走下来，连个挂起都没有，怎么可能实现异步？想太多了。</p>
                  <p>要实现异步，接下来我们再了解一下 <code>await</code> 的用法，它可以将耗时等待的操作挂起，让出控制权。当协程执行的时候遇到 <code>await</code>，时间循环就会将本协程挂起，转而去执行别的协程，直到其他协程挂起或执行完毕。</p>
                  <p>所以，我们可能会将代码中的 <code>request</code> 方法改成如下的样子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://httpbin.org/delay/5'</span></span><br><span class="line">    print(<span class="string">'Waiting for'</span>, url)</span><br><span class="line">    response = <span class="keyword">await</span> requests.get(url)</span><br><span class="line">    print(<span class="string">'Get response from'</span>, url, <span class="string">'response'</span>, response)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>仅仅是在 requests 前面加了一个关键字 <code>await</code>，然而此时执行代码，会得到如下报错：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">...</span><br><span class="line">Task exception was never retrieved</span><br><span class="line">future: &lt;Task finished coro=&lt;request() done, defined at demo.py:<span class="number">8</span>&gt; exception=TypeError(<span class="string">"object Response can't be used in 'await' expression"</span>)&gt;</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"demo.py"</span>, line <span class="number">11</span>, <span class="keyword">in</span> request</span><br><span class="line">    response = <span class="keyword">await</span> requests.get(url)</span><br><span class="line">TypeError: object Response can<span class="string">'t be used in '</span><span class="keyword">await</span><span class="string">' expression</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次它遇到 <code>await</code> 方法确实挂起了，也等待了，但是最后却报了这个错误。这个错误的意思是 requests 返回的 <code>Response</code> 对象不能和 <code>await</code> 一起使用，为什么呢？因为根据官方文档说明，<code>await</code> 后面的对象必须是如下格式之一（具体可以参见 <a href="https://www.python.org/dev/peps/pep-0492/#await-expression" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0492/#await-expression</a>）：</p>
                  <ul>
                    <li>一个原生 coroutine 对象；</li>
                    <li>一个由 <code>types.coroutine</code> 修饰的生成器，这个生成器可以返回 <code>coroutine</code> 对象；</li>
                    <li>一个包含 <code>__await__</code> 方法的对象返回的一个迭代器。</li>
                  </ul>
                  <p>reqeusts 返回的 <code>Response</code> 对象不符合上面任一条件，因此就会报上面的错误了。</p>
                  <p>有的读者可能已经发现了，既然 <code>await</code> 后面可以跟一个 <code>coroutine</code> 对象，那么我用 <code>async</code> 把请求的方法改成 <code>coroutine</code> 对象不就可以了吗？所以就改写成如下的样子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> requests.get(url)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://httpbin.org/delay/5'</span></span><br><span class="line">    print(<span class="string">'Waiting for'</span>, url)</span><br><span class="line">    response = <span class="keyword">await</span> get(url)</span><br><span class="line">    print(<span class="string">'Get response from'</span>, url, <span class="string">'response'</span>, response)</span><br><span class="line"></span><br><span class="line">tasks = [asyncio.ensure_future(request()) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line">print(<span class="string">'Cost time:'</span>, end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们将请求页面的方法独立出来，并用 <code>async</code> 修饰，这样就得到了一个 <code>coroutine</code> 对象。运行一下看看：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Get response fromhttps://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">...</span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;Response [<span class="number">200</span>]&gt;</span><br><span class="line">Cost time: <span class="number">65.394437756259273</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>还是不行，它还不是异步执行的，也就是说我们仅仅将涉及 IO 操作的代码封装到 <code>async</code> 修饰的方法里面是不可行的。我们必须要使用支持异步操作的请求方式才可以实现真正的异步，所以这里就需要 aiohttp 派上用场了。</p>
                  <h2 id="9-使用-aiohttp"><a href="#9-使用-aiohttp" class="headerlink" title="9. 使用 aiohttp"></a>9. 使用 aiohttp</h2>
                  <p>aiohttp 是一个支持异步请求的库，配合使用它和 asyncio，我们可以非常方便地实现异步请求操作。我们使用 pip3 安装即可：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> aiohttp</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>具体的安装方法可以参考：<a href="https://setup.scrape.center/aiohttp。" target="_blank" rel="noopener">https://setup.scrape.center/aiohttp。</a></p>
                  <p>aiohttp 的官方文档链接为 <a href="https://aiohttp.readthedocs.io/" target="_blank" rel="noopener">https://aiohttp.readthedocs.io/</a>，它分为两部分，一部分是 Client，一部分是 Server，详细的内容可以参考官方文档。</p>
                  <p>下面我们将 aiohttp 用上来，将代码改成如下样子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(url)</span>:</span></span><br><span class="line">    session = aiohttp.ClientSession()</span><br><span class="line">    response = <span class="keyword">await</span> session.get(url)</span><br><span class="line">    <span class="keyword">await</span> response.text()</span><br><span class="line">    <span class="keyword">await</span> session.close()</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://httpbin.org/delay/5'</span></span><br><span class="line">    print(<span class="string">'Waiting for'</span>, url)</span><br><span class="line">    response = <span class="keyword">await</span> get(url)</span><br><span class="line">    print(<span class="string">'Get response from'</span>, url, <span class="string">'response'</span>, response)</span><br><span class="line"></span><br><span class="line">tasks = [asyncio.ensure_future(request()) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line">print(<span class="string">'Cost time:'</span>, end - start)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们将请求库由 requests 改成了 aiohttp，通过 aiohttp 的 <code>ClientSession</code> 类的 <code>get</code> 方法进行请求，结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">Waiting <span class="keyword">for</span> https://httpbin.org/delay/<span class="number">5</span></span><br><span class="line">...</span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;ClientResponse(https://httpbin.org/delay/<span class="number">5</span>) [<span class="number">200</span> OK]&gt;</span><br><span class="line">&lt;CIMultiDictProxy(<span class="string">'Date'</span>: <span class="string">'Sun, 09 Aug 2020 14:30:22 GMT'</span>, <span class="string">'Content-Type'</span>: <span class="string">'application/json'</span>, <span class="string">'Content-Length'</span>: <span class="string">'360'</span>, <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>, <span class="string">'Server'</span>: <span class="string">'gunicorn/19.9.0'</span>, <span class="string">'Access-Control-Allow-Origin'</span>: <span class="string">'*'</span>, <span class="string">'Access-Control-Allow-Credentials'</span>: <span class="string">'true'</span>)&gt;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">Get response <span class="keyword">from</span> https://httpbin.org/delay/<span class="number">5</span> response &lt;ClientResponse(https://httpbin.org/delay/<span class="number">5</span>) [<span class="number">200</span> OK]&gt;</span><br><span class="line">&lt;CIMultiDictProxy(<span class="string">'Date'</span>: <span class="string">'Sun, 09 Aug 2020 14:30:22 GMT'</span>, <span class="string">'Content-Type'</span>: <span class="string">'application/json'</span>, <span class="string">'Content-Length'</span>: <span class="string">'360'</span>, <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>, <span class="string">'Server'</span>: <span class="string">'gunicorn/19.9.0'</span>, <span class="string">'Access-Control-Allow-Origin'</span>: <span class="string">'*'</span>, <span class="string">'Access-Control-Allow-Credentials'</span>: <span class="string">'true'</span>)&gt;</span><br><span class="line">Cost time: <span class="number">6.033240079879761</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>成功了！我们发现这次请求的耗时由 51 秒直接变成了 6 秒，耗费时间减少了非常多。</p>
                  <p>在代码里面，我们使用了 <code>await</code>，后面跟了 <code>get</code> 方法。在执行这 10 个协程的时候，如果遇到了 <code>await</code>，就会将当前协程挂起，转而去执行其他协程，直到其他协程也挂起或执行完毕，再执行下一个协程。</p>
                  <p>开始运行时，时间循环会运行第一个 <code>task</code>。针对第一个 <code>task</code> 来说，当执行到第一个 <code>await</code> 跟着的 <code>get</code> 方法时，它被挂起，但这个 <code>get</code> 方法第一步的执行是非阻塞的，挂起之后立马被唤醒，所以立即又进入执行，创建了 <code>ClientSession</code> 对象，接着遇到了第二个 <code>await</code>，调用了 <code>session.get</code> 请求方法，然后就被挂起了。由于请求需要耗时很久，所以一直没有被唤醒，好在第一个 <code>task</code> 被挂起了，那么接下来该怎么办呢？事件循环会寻找当前未被挂起的协程继续执行，于是就转而执行第二个 <code>task</code> 了，也是一样的流程操作，直到执行了第十个 <code>task</code> 的 <code>session.get</code> 方法之后，全部的 <code>task</code> 都被挂起了。所有 <code>task</code> 都已经处于挂起状态，那咋办？只好等待了。5 秒之后，几个请求几乎同时都有了响应，然后几个 <code>task</code> 也被唤醒接着执行，输出请求结果，最后总耗时 6 秒！</p>
                  <p>怎么样？这就是异步操作的便捷之处，当遇到阻塞式操作时，任务被挂起，程序接着去执行其他任务，而不是傻傻地等着，这样可以充分利用 CPU 时间，而不必把时间浪费在等待 IO 上。</p>
                  <p>有人会说，既然这样的话，在上面的例子中，在发出网络请求后，既然接下来的 5 秒都是在等待的，在 5 秒之内，CPU 可以处理的 <code>task</code> 数量远不止这些，那么岂不是我们放 10 个、20 个、50 个、100 个、1000 个 <code>task</code> 一起执行，最后得到所有结果的耗时不都是差不多的吗？因为这几个任务被挂起后都是一起等待的。</p>
                  <p>理论来说，确实是这样的，不过有个前提，那就是服务器在同一时刻接受无限次请求都能保证正常返回结果，也就是服务器无限抗压。另外，还要忽略 IO 传输时延，确实可以做到无限 <code>task</code> 一起执行且在预想时间内得到结果。但由于不同服务器处理的实现机制不同，可能某些服务器并不能承受这么高的并发，因此响应速度也会减慢。</p>
                  <p>这里我们以百度为例，测试一下并发数量为 1、3、5、10…500 的情况下的耗时情况，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(number)</span>:</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(url)</span>:</span></span><br><span class="line">        session = aiohttp.ClientSession()</span><br><span class="line">        response = <span class="keyword">await</span> session.get(url)</span><br><span class="line">        <span class="keyword">await</span> response.text()</span><br><span class="line">        <span class="keyword">await</span> session.close()</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">()</span>:</span></span><br><span class="line">        url = <span class="string">'https://www.baidu.com/'</span></span><br><span class="line">        <span class="keyword">await</span> get(url)</span><br><span class="line"></span><br><span class="line">    tasks = [asyncio.ensure_future(request()) <span class="keyword">for</span> _ <span class="keyword">in</span> range(number)]</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'Number:'</span>, number, <span class="string">'Cost time:'</span>, end - start)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> number <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">75</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">500</span>]:</span><br><span class="line">    test(number)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Number: <span class="number">1</span> Cost time: <span class="number">0.05885505676269531</span></span><br><span class="line">Number: <span class="number">3</span> Cost time: <span class="number">0.05773782730102539</span></span><br><span class="line">Number: <span class="number">5</span> Cost time: <span class="number">0.05768704414367676</span></span><br><span class="line">Number: <span class="number">10</span> Cost time: <span class="number">0.15174412727355957</span></span><br><span class="line">Number: <span class="number">15</span> Cost time: <span class="number">0.09603095054626465</span></span><br><span class="line">Number: <span class="number">30</span> Cost time: <span class="number">0.17843103408813477</span></span><br><span class="line">Number: <span class="number">50</span> Cost time: <span class="number">0.3741800785064697</span></span><br><span class="line">Number: <span class="number">75</span> Cost time: <span class="number">0.2894289493560791</span></span><br><span class="line">Number: <span class="number">100</span> Cost time: <span class="number">0.6185381412506104</span></span><br><span class="line">Number: <span class="number">200</span> Cost time: <span class="number">1.0894129276275635</span></span><br><span class="line">Number: <span class="number">500</span> Cost time: <span class="number">1.8213098049163818</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，即使我们增加了并发数量，但在服务器能承受高并发的前提下，其爬取速度几乎不太受影响。</p>
                  <p>综上所述，使用了异步请求之后，我们几乎可以在相同的时间内实现成百上千倍次的网络请求，把这个运用在爬虫中，速度提升可谓是非常可观了。</p>
                  <h2 id="10-总结"><a href="#10-总结" class="headerlink" title="10. 总结"></a>10. 总结</h2>
                  <p>以上便是 Python 中协程的基本原理和用法，在后面一节中我们会详细介绍 aiohttp 的用法和爬取实战，实现快速高并发的爬取。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/AsyncTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/AsyncTest</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-18 08:11:31" itemprop="dateCreated datePublished" datetime="2022-02-18T08:11:31+08:00">2022-02-18</time>
                </span>
                <span id="/202271.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 协程的基本原理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>17k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>15 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202262.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202262.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 新兴动态渲染工具 Playwright 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>Playwright 是微软在 2020 年初开源的新一代自动化测试工具，它的功能类似于 Selenium、Pyppeteer 等，都可以驱动浏览器进行各种自动化操作。它的功能也非常强大，对市面上的主流浏览器都提供了支持，API 功能简洁又强大。虽然诞生比较晚，但是现在发展得非常火热。</p>
                  <h2 id="1-Playwright-的特点"><a href="#1-Playwright-的特点" class="headerlink" title="1. Playwright 的特点"></a>1. Playwright 的特点</h2>
                  <ul>
                    <li>Playwright 支持当前所有主流浏览器，包括 Chrome 和 Edge（基于 Chromium）、Firefox、Safari（基于 WebKit） ，提供完善的自动化控制的 API。</li>
                    <li>Playwright 支持移动端页面测试，使用设备模拟技术可以使我们在移动 Web 浏览器中测试响应式 Web 应用程序。</li>
                    <li>Playwright 支持所有浏览器的 Headless 模式和非 Headless 模式的测试。</li>
                    <li>Playwright 的安装和配置非常简单，安装过程中会自动安装对应的浏览器和驱动，不需要额外配置 WebDriver 等。</li>
                    <li>Playwright 提供了自动等待相关的 API，当页面加载的时候会自动等待对应的节点加载，大大简化了 API 编写复杂度。</li>
                  </ul>
                  <p>本节我们就来了解下 Playwright 的使用方法。</p>
                  <h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h2>
                  <p>要使用 Playwright，需要 Python 3.7 版本及以上，请确保 Python 的版本符合要求。</p>
                  <p>要安装 Playwright，可以直接使用 pip3，命令如下：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> playwright</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>安装完成之后需要进行一些初始化操作：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">playwright <span class="keyword">install</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候 Playwrigth 会安装 Chromium, Firefox and WebKit 浏览器并配置一些驱动，我们不必关心中间配置的过程，Playwright 会为我们配置好。</p>
                  <p>具体的安装说明可以参考：<a href="https://setup.scrape.center/playwright。" target="_blank" rel="noopener">https://setup.scrape.center/playwright。</a></p>
                  <p>安装完成之后，我们便可以使用 Playwright 启动 Chromium 或 Firefox 或 WebKit 浏览器来进行自动化操作了。</p>
                  <h2 id="3-基本使用"><a href="#3-基本使用" class="headerlink" title="3. 基本使用"></a>3. 基本使用</h2>
                  <p>Playwright 支持两种编写模式，一种是类似 Pyppetter 一样的异步模式，另一种是像 Selenium 一样的同步模式，我们可以根据实际需要选择使用不同的模式。</p>
                  <p>我们先来看一个基本同步模式的例子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    <span class="keyword">for</span> browser_type <span class="keyword">in</span> [p.chromium, p.firefox, p.webkit]:</span><br><span class="line">        browser = browser_type.launch(headless=<span class="literal">False</span>)</span><br><span class="line">        page = browser.new_page()</span><br><span class="line">        page.goto(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">        page.screenshot(path=<span class="string">f'screenshot-<span class="subst">&#123;browser_type.name&#125;</span>.png'</span>)</span><br><span class="line">        print(page.title())</span><br><span class="line">        browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先我们导入了 sync_playwright 方法，然后直接调用了这个方法，该方法返回的是一个 PlaywrightContextManager 对象，可以理解是一个浏览器上下文管理器，我们将其赋值为变量 p。</p>
                  <p>接着我们调用了 PlaywrightContextManager 对象的 chromium、firefox、webkit 属性依次创建了一个 Chromium、Firefox 以及 Webkit 浏览器实例，接着用一个 for 循环依次执行了它们的 launch 方法，同时设置了 headless 参数为 False。</p>
                  <blockquote>
                    <p>注意：如果不设置为 False，默认是无头模式启动浏览器，我们看不到任何窗口。</p>
                  </blockquote>
                  <p>launch 方法返回的是一个 Browser 对象，我们将其赋值为 browser 变量。然后调用 browser 的 new_page 方法，相当于新建了一个选项卡，返回的是一个 Page 对象，将其赋值为 page，这整个过程其实和 Pyppeteer 非常类似。接着我们就可以调用 page 的一系列 API 来进行各种自动化操作了，比如调用 goto，就是加载某个页面，这里我们访问的是百度的首页。接着我们调用了 page 的 screenshot 方法，参数传一个文件名称，这样截图就会自动保存为该图片名称，这里名称中我们加入了 browser_type 的 name 属性，代表浏览器的类型，结果分别就是 chromium, firefox, webkit。另外我们还调用了 title 方法，该方法会返回页面的标题，即 HTML 中 title 节点中的文字，也就是选项卡上的文字，我们将该结果打印输出到控制台。最后操作完毕，调用 browser 的 close 方法关闭整个浏览器，运行结束。</p>
                  <p>运行一下，这时候我们可以看到有三个浏览器依次启动并加载了百度这个页面，分别是 Chromium、Firefox 和 Webkit 三个浏览器，页面加载完成之后，生成截图、控制台打印结果就退出了。</p>
                  <p>这时候当前目录便会生成三个截图文件，都是百度的首页，文件名中都带有了浏览器的名称，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/m6auk.png" alt=""></p>
                  <p>控制台运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">百度一下，你就知道</span><br><span class="line">百度一下，你就知道</span><br><span class="line">百度一下，你就知道</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过运行结果我们可以发现，我们非常方便地启动了三种浏览器并完成了自动化操作，并通过几个 API 就完成了截图和数据的获取，整个运行速度是非常快的，者就是 Playwright 最最基本的用法。</p>
                  <p>当然除了同步模式，Playwright 还提供异步的 API，如果我们项目里面使用了 asyncio，那就应该使用异步模式，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> playwright.async_api <span class="keyword">import</span> async_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> async_playwright() <span class="keyword">as</span> p:</span><br><span class="line">        <span class="keyword">for</span> browser_type <span class="keyword">in</span> [p.chromium, p.firefox, p.webkit]:</span><br><span class="line">            browser = <span class="keyword">await</span> browser_type.launch()</span><br><span class="line">            page = <span class="keyword">await</span> browser.new_page()</span><br><span class="line">            <span class="keyword">await</span> page.goto(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">            <span class="keyword">await</span> page.screenshot(path=<span class="string">f'screenshot-<span class="subst">&#123;browser_type.name&#125;</span>.png'</span>)</span><br><span class="line">            print(<span class="keyword">await</span> page.title())</span><br><span class="line">            <span class="keyword">await</span> browser.close()</span><br><span class="line"></span><br><span class="line">asyncio.run(main())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到整个写法和同步模式基本类似，导入的时候使用的是 async_playwright 方法，而不再是 sync_playwright 方法。写法上添加了 async/await 关键字的使用，最后的运行效果是一样的。</p>
                  <p>另外我们注意到，这例子中使用了 with as 语句，with 用于上下文对象的管理，它可以返回一个上下文管理器，也就对应一个 PlaywrightContextManager 对象，无论运行期间是否抛出异常，它能够帮助我们自动分配并且释放 Playwright 的资源。</p>
                  <h2 id="4-代码生成"><a href="#4-代码生成" class="headerlink" title="4. 代码生成"></a>4. 代码生成</h2>
                  <p>Playwright 还有一个强大的功能，那就是可以录制我们在浏览器中的操作并将代码自动生成出来，有了这个功能，我们甚至都不用写任何一行代码，这个功能可以通过 playwright 命令行调用 codegen 来实现，我们先来看看 codegen 命令都有什么参数，输入如下命令：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">playwright codegen <span class="comment">--help</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>结果类似如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">Usage</span>: npx playwright codegen [options] [url]</span><br><span class="line"></span><br><span class="line">open page and generate code for user actions</span><br><span class="line"></span><br><span class="line"><span class="attribute">Options:</span></span><br><span class="line">  -o, --output &lt;file name&gt;     saves the generated script to a file</span><br><span class="line">  --target &lt;language&gt;          language to use, one of javascript, python, python-async, csharp (default: "python")</span><br><span class="line">  -b, --browser &lt;browserType&gt;  browser to use, one of cr, chromium, ff, firefox, wk, webkit (default: "chromium")</span><br><span class="line">  --channel &lt;channel&gt;          Chromium distribution channel, "chrome", "chrome-beta", "msedge-dev", etc</span><br><span class="line">  --color-scheme &lt;scheme&gt;      emulate preferred color scheme, "light" or "dark"</span><br><span class="line">  --device &lt;deviceName&gt;        emulate device, for example  "iPhone 11"</span><br><span class="line">  --geolocation &lt;coordinates&gt;  specify geolocation coordinates, for example "37.819722,-122.478611"</span><br><span class="line">  --load-storage &lt;filename&gt;    load context storage state from the file, previously saved with --save-storage</span><br><span class="line">  --lang &lt;language&gt;            specify language / locale, for example "en-GB"</span><br><span class="line">  --proxy-server &lt;proxy&gt;       specify proxy server, for example "http://myproxy:3128" or "socks5://myproxy:8080"</span><br><span class="line">  --save-storage &lt;filename&gt;    save context storage state at the end, for later use with --load-storage</span><br><span class="line">  --timezone &lt;time zone&gt;       time zone to emulate, for example "Europe/Rome"</span><br><span class="line">  --timeout &lt;timeout&gt;          timeout for Playwright actions in milliseconds (default: "10000")</span><br><span class="line">  --user-agent &lt;ua string&gt;     specify user agent string</span><br><span class="line">  --viewport-size &lt;size&gt;       specify browser viewport size in pixels, for example "1280, 720"</span><br><span class="line">  -h, --help                   display help for command</span><br><span class="line"></span><br><span class="line"><span class="attribute">Examples:</span></span><br><span class="line"><span class="attribute"></span></span><br><span class="line">  $ codegen</span><br><span class="line">  $ codegen --target=python</span><br><span class="line">  $ codegen -b webkit https://example.com</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到这里有几个选项，比如 -o 代表输出的代码文件的名称；—target 代表使用的语言，默认是 python，即会生成同步模式的操作代码，如果传入 python-async 就会生成异步模式的代码；-b 代表的是使用的浏览器，默认是 Chromium，其他还有很多设置，比如 —device 可以模拟使用手机浏览器，比如 iPhone 11，—lang 代表设置浏览器的语言，—timeout 可以设置页面加载超时时间。</p>
                  <p>好，了解了这些用法，那我们就来尝试启动一个 Firefox 浏览器，然后将操作结果输出到 script.py 文件，命令如下：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">playwright codegen -o <span class="keyword">script.py </span>-<span class="keyword">b </span>firefox</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候就弹出了一个 Firefox 浏览器，同时右侧会输出一个脚本窗口，实时显示当前操作对应的代码。</p>
                  <p>我们可以在浏览器中做任何操作，比如打开百度，然后点击输入框并输入 nba，然后再点击搜索按钮，浏览器窗口如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/shz6p.png" alt=""></p>
                  <p>可以看见浏览器中还会高亮显示我们正在操作的页面节点，同时还显示了对应的选择器字符串 <code>input[name=&quot;wd&quot;]</code>，右侧的窗口如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/4z2ew.png" alt=""></p>
                  <p>在操作过程中，该窗口中的代码就实时变化，可以看到这里生成了我们一系列操作的对应代码，比如在搜索框中输入 nba，就对应如下代码：</p>
                  <figure class="highlight less">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">page</span><span class="selector-class">.fill</span>(<span class="string">"input[name=\"</span>wd\<span class="string">"]"</span>, <span class="string">"nba"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>操作完毕之后，关闭浏览器，Playwright 会生成一个 script.py 文件，内容如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(playwright)</span>:</span></span><br><span class="line">    browser = playwright.firefox.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    context = browser.new_context()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Open new page</span></span><br><span class="line">    page = context.new_page()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Go to https://www.baidu.com/</span></span><br><span class="line">    page.goto(<span class="string">"https://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Click input[name="wd"]</span></span><br><span class="line">    page.click(<span class="string">"input[name=\"wd\"]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fill input[name="wd"]</span></span><br><span class="line">    page.fill(<span class="string">"input[name=\"wd\"]"</span>, <span class="string">"nba"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Click text=百度一下</span></span><br><span class="line">    <span class="keyword">with</span> page.expect_navigation():</span><br><span class="line">        page.click(<span class="string">"text=百度一下"</span>)</span><br><span class="line"></span><br><span class="line">    context.close()</span><br><span class="line">    browser.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> playwright:</span><br><span class="line">    run(playwright)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到这里生成的代码和我们之前写的示例代码几乎差不多，而且也是完全可以运行的，运行之后就可以看到它又可以复现我们刚才所做的操作了。</p>
                  <p>所以，有了这个功能，我们甚至都不用编写任何代码，只通过简单的可视化点击就能把代码生成出来，可谓是非常方便了！</p>
                  <p>另外这里有一个值得注意的点，仔细观察下生成的代码，和前面的例子不同的是，这里 new_page 方法并不是直接通过 browser 调用的，而是通过 context 变量调用的，这个 context 又是由 browser 通过调用 new_context 方法生成的。有读者可能就会问了，这个 context 究竟是做什么的呢？</p>
                  <p>其实这个 context 变量对应的是一个 BrowserContext 对象，BrowserContext 是一个类似隐身模式的独立上下文环境，其运行资源是单独隔离的，在做一些自动化测试过程中，每个测试用例我们都可以单独创建一个 BrowserContext 对象，这样可以保证每个测试用例之间互不干扰，具体的 API 可以参考 <a href="https://playwright.dev/python/docs/api/class-browsercontext" target="_blank" rel="noopener">https://playwright.dev/python/docs/api/class-browsercontext</a>。</p>
                  <h2 id="5-移动端浏览器支持"><a href="#5-移动端浏览器支持" class="headerlink" title="5. 移动端浏览器支持"></a>5. 移动端浏览器支持</h2>
                  <p>Playwright 另外一个特色功能就是可以支持移动端浏览器的模拟，比如模拟打开 iPhone 12 Pro Max 上的 Safari 浏览器，然后手动设置定位，并打开百度地图并截图。首先我们可以选定一个经纬度，比如故宫的经纬度是 39.913904, 116.39014，我们可以通过 geolocation 参数传递给 Webkit 浏览器并初始化。</p>
                  <p>示例代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    iphone_12_pro_max = p.devices[<span class="string">'iPhone 12 Pro Max'</span>]</span><br><span class="line">    browser = p.webkit.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    context = browser.new_context(</span><br><span class="line">        **iphone_12_pro_max,</span><br><span class="line">        locale=<span class="string">'zh-CN'</span>,</span><br><span class="line">        geolocation=&#123;<span class="string">'longitude'</span>: <span class="number">116.39014</span>, <span class="string">'latitude'</span>: <span class="number">39.913904</span>&#125;,</span><br><span class="line">        permissions=[<span class="string">'geolocation'</span>]</span><br><span class="line">    )</span><br><span class="line">    page = context.new_page()</span><br><span class="line">    page.goto(<span class="string">'https://amap.com'</span>)</span><br><span class="line">    page.wait_for_load_state(state=<span class="string">'networkidle'</span>)</span><br><span class="line">    page.screenshot(path=<span class="string">'location-iphone.png'</span>)</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们先用 PlaywrightContextManager 对象的 devices 属性指定了一台移动设备，这里传入的是手机的型号，比如 iPhone 12 Pro Max，当然也可以传其他名称，比如 iPhone 8，Pixel 2 等。</p>
                  <p>前面我们已经了解了 BrowserContext 对象，BrowserContext 对象也可以用来模拟移动端浏览器，初始化一些移动设备信息、语言、权限、位置等信息，这里我们就用它来创建了一个移动端 BrowserContext 对象，通过 geolocation 参数传入了经纬度信息，通过 permissions 参数传入了赋予的权限信息，最后将得到的 BrowserContext 对象赋值为 context 变量。</p>
                  <p>接着我们就可以用 BrowserContext 对象来新建一个页面，还是调用 new_page 方法创建一个新的选项卡，然后跳转到高德地图，并调用了 wait_for_load_state 方法等待页面某个状态完成，这里我们传入的 state 是 networkidle，也就是网络空闲状态。因为在页面初始化和加载过程中，肯定是伴随有网络请求的，所以加载过程中肯定不算 networkidle 状态，所以这里我们传入 networkidle 就可以标识当前页面和数据加载完成的状态。加载完成之后，我们再调用 screenshot 方法获取当前页面截图，最后关闭浏览器。</p>
                  <p>运行下代码，可以发现这里就弹出了一个移动版浏览器，然后加载了高德地图，并定位到了故宫的位置，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/jkxf2.png" alt=""></p>
                  <p>输出的截图也是浏览器中显示的结果。</p>
                  <p>所以这样我们就成功实现了移动端浏览器的模拟和一些设置，其操作 API 和 PC 版浏览器是完全一样的。</p>
                  <h2 id="6-选择器"><a href="#6-选择器" class="headerlink" title="6. 选择器"></a>6. 选择器</h2>
                  <p>前面我们注意到 click 和 fill 等方法都传入了一个字符串，这些字符串有的符合 CSS 选择器的语法，有的又是 text= 开头的，感觉似乎没太有规律的样子，它到底支持怎样的匹配规则呢？下面我们来了解下。</p>
                  <p>传入的这个字符串，我们可以称之为 Element Selector，它不仅仅支持 CSS 选择器、XPath，Playwright 还扩展了一些方便好用的规则，比如直接根据文本内容筛选，根据节点层级结构筛选等等。</p>
                  <h3 id="文本选择"><a href="#文本选择" class="headerlink" title="文本选择"></a>文本选择</h3>
                  <p>文本选择支持直接使用 <code>text=</code> 这样的语法进行筛选，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(<span class="string">"text=Log in"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就代表选择文本是 Log in 的节点，并点击。</p>
                  <h3 id="CSS-选择器"><a href="#CSS-选择器" class="headerlink" title="CSS 选择器"></a>CSS 选择器</h3>
                  <p>CSS 选择器之前也介绍过了，比如根据 id 或者 class 筛选：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(<span class="string">"button"</span>)</span><br><span class="line">page.click(<span class="string">"#nav-bar .contact-us-item"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>根据特定的节点属性筛选：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(<span class="string">"[data-test=login-button]"</span>)</span><br><span class="line">page.click(<span class="string">"[aria-label='Sign in']"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="CSS-选择器-文本"><a href="#CSS-选择器-文本" class="headerlink" title="CSS 选择器 + 文本"></a>CSS 选择器 + 文本</h3>
                  <p>我们还可以使用 CSS 选择器结合文本值进行海选，比较常用的就是 has-text 和 text，前者代表包含指定的字符串，后者代表字符串完全匹配，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(<span class="string">"article:has-text('Playwright')"</span>)</span><br><span class="line">page.click(<span class="string">"#nav-bar :text('Contact us')"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一个就是选择文本中包含 Playwright 的 article 节点，第二个就是选择 id 为 nav-bar 节点中文本值等于 Contact us 的节点。</p>
                  <h3 id="CSS-选择器-节点关系"><a href="#CSS-选择器-节点关系" class="headerlink" title="CSS 选择器 + 节点关系"></a>CSS 选择器 + 节点关系</h3>
                  <p>还可以结合节点关系来筛选节点，比如使用 has 来指定另外一个选择器，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(<span class="string">".item-description:has(.item-promo-banner)"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>比如这里选择的就是选择 class 为 item-description 的节点，且该节点还要包含 class 为 item-promo-banner 的子节点。</p>
                  <p>另外还有一些相对位置关系，比如 right-of 可以指定位于某个节点右侧的节点，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(<span class="string">"input:right-of(:text('Username'))"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里选择的就是一个 input 节点，并且该 input 节点要位于文本值为 Username 的节点的右侧。</p>
                  <h3 id="XPath"><a href="#XPath" class="headerlink" title="XPath"></a>XPath</h3>
                  <p>当然 XPath 也是支持的，不过 xpath 这个关键字需要我们自行制定，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(<span class="string">"xpath=//button"</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里需要在开头指定 <code>xpath=</code> 字符串，代表后面是一个 XPath 表达式。</p>
                  <p>关于更多选择器的用法和最佳实践，可以参考官方文档：<a href="https://playwright.dev/python/docs/selectors。" target="_blank" rel="noopener">https://playwright.dev/python/docs/selectors。</a></p>
                  <h2 id="7-常用操作方法"><a href="#7-常用操作方法" class="headerlink" title="7. 常用操作方法"></a>7. 常用操作方法</h2>
                  <p>上面我们了解了浏览器的一些初始化设置和基本的操作实例，下面我们再对一些常用的操作 API 进行说明。</p>
                  <p>常见的一些 API 如点击 click，输入 fill 等操作，这些方法都是属于 Page 对象的，所以所有的方法都从 Page 对象的 API 文档查找就好了，文档地址：<a href="https://playwright.dev/python/docs/api/class-page。" target="_blank" rel="noopener">https://playwright.dev/python/docs/api/class-page。</a></p>
                  <p>下面介绍几个常见的 API 用法。</p>
                  <h3 id="事件监听"><a href="#事件监听" class="headerlink" title="事件监听"></a>事件监听</h3>
                  <p>Page 对象提供了一个 on 方法，它可以用来监听页面中发生的各个事件，比如 close、console、load、request、response 等等。</p>
                  <p>比如这里我们可以监听 response 事件，response 事件可以在每次网络请求得到响应的时候触发，我们可以设置对应的回调方法获取到对应 Response 的全部信息，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_response</span><span class="params">(response)</span>:</span></span><br><span class="line">    print(<span class="string">f'Statue <span class="subst">&#123;response.status&#125;</span>: <span class="subst">&#123;response.url&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line">    page.on(<span class="string">'response'</span>, on_response)</span><br><span class="line">    page.goto(<span class="string">'https://spa6.scrape.center/'</span>)</span><br><span class="line">    page.wait_for_load_state(<span class="string">'networkidle'</span>)</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们在创建 Page 对象之后，就开始监听 response 事件，同时将回调方法设置为 on_response，on_response 对象接收一个参数，然后把 Response 的状态码和链接都输出出来了。</p>
                  <p>运行之后，可以看到控制台输出结果如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/css/app.ea9d802a.css</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/js/app.5ef0d454.js</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/js/chunk-vendors.77daf991.js</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/css/chunk-19c920f8.2a6496e0.css</span></span><br><span class="line">...</span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/css/chunk-19c920f8.2a6496e0.css</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/js/chunk-19c920f8.c3a1129d.js</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/img/logo.a508a8f0.png</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/fonts/element-icons.535877f5.woff</span></span><br><span class="line">Statue <span class="number">301</span>: https:<span class="comment">//spa6.scrape.center/api/movie?limit=10&amp;offset=0&amp;token=NGMwMzFhNGEzMTFiMzJkOGE0ZTQ1YjUzMTc2OWNiYTI1Yzk0ZDM3MSwxNjIyOTE4NTE5</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//spa6.scrape.center/api/movie/?limit=10&amp;offset=0&amp;token=NGMwMzFhNGEzMTFiMzJkOGE0ZTQ1YjUzMTc2OWNiYTI1Yzk0ZDM3MSwxNjIyOTE4NTE5</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//p0.meituan.net/movie/da64660f82b98cdc1b8a3804e69609e041108.jpg@464w_644h_1e_1c</span></span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//p0.meituan.net/movie/283292171619cdfd5b240c8fd093f1eb255670.jpg@464w_644h_1e_1c</span></span><br><span class="line">....</span><br><span class="line">Statue <span class="number">200</span>: https:<span class="comment">//p1.meituan.net/movie/b607fba7513e7f15eab170aac1e1400d878112.jpg@464w_644h_1e_1c</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <blockquote>
                    <p>注意：这里省略了部分重复的内容。</p>
                  </blockquote>
                  <p>可以看到，这里的输出结果其实正好对应浏览器 Network 面板中所有的请求和响应内容，和下图是一一对应的：</p>
                  <p><img src="https://cdn.cuiqingcai.com/0dj23.png" alt=""></p>
                  <p>这个网站我们之前分析过，其真实的数据都是 Ajax 加载的，同时 Ajax 请求中还带有加密参数，不好轻易获取。</p>
                  <p>但有了这个方法，这里如果我们想要截获 Ajax 请求，岂不是就非常容易了？</p>
                  <p>改写一下判定条件，输出对应的 JSON 结果，改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_response</span><span class="params">(response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'/api/movie/'</span> <span class="keyword">in</span> response.url <span class="keyword">and</span> response.status == <span class="number">200</span>:</span><br><span class="line">        print(response.json())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line">    page.on(<span class="string">'response'</span>, on_response)</span><br><span class="line">    page.goto(<span class="string">'https://spa6.scrape.center/'</span>)</span><br><span class="line">    page.wait_for_load_state(<span class="string">'networkidle'</span>)</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>控制台输入如下：</p>
                  <figure class="highlight 1c">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;'count': <span class="number">100</span>, 'results': [&#123;'id': <span class="number">1</span>, 'name': '霸王别姬', 'alias': 'Farewell My Concubine', 'cover': 'https://p0.meituan.net/movie/ce4da3e03e655b5b88ed31b5cd<span class="number">7896</span>cf<span class="number">6247</span>2.jpg@464w_644h_1e_1c', 'categories': ['剧情', '爱情'], 'published_at': '<span class="number">1993-07-26</span>', 'minute': <span class="number">171</span>, 'score': <span class="number">9.5</span>, 'regions': ['中国大陆', '中国香港']&#125;,</span><br><span class="line">...</span><br><span class="line">'published_at': None, 'minute': <span class="number">103</span>, 'score': <span class="number">9.0</span>, 'regions': ['美国']&#125;, &#123;'id': <span class="number">10</span>, 'name': '狮子王', 'alias': 'The Lion King', 'cover': 'https://p0.meituan.net/movie/27b76fe6cf<span class="number">3903</span>f3d<span class="number">7496</span>3f<span class="number">70786001</span>e<span class="number">143840</span>6.jpg@464w_644h_1e_1c', 'categories': ['动画', '歌舞', '冒险'], 'published_at': '<span class="number">1995-07-15</span>', 'minute': <span class="number">89</span>, 'score': <span class="number">9.0</span>, 'regions': ['美国']&#125;]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>简直是得来全不费工夫，我们直接通过这个方法拦截了 Ajax 请求，直接把响应结果拿到了，即使这个 Ajax 请求有加密参数，我们也不用关心，因为我们直接截获了 Ajax 最后响应的结果，这对数据爬取来说实在是太方便了。</p>
                  <p>另外还有很多其他的事件监听，这里不再一一介绍了，可以查阅官方文档，参考类似的写法实现。</p>
                  <h3 id="获取页面源码"><a href="#获取页面源码" class="headerlink" title="获取页面源码"></a>获取页面源码</h3>
                  <p>要获取页面的 HTML 代码其实很简单，我们直接通过 content 方法获取即可，用法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line">    page.goto(<span class="string">'https://spa6.scrape.center/'</span>)</span><br><span class="line">    page.wait_for_load_state(<span class="string">'networkidle'</span>)</span><br><span class="line">    html = page.content()</span><br><span class="line">    print(html)</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果就是页面的 HTML 代码。获取了 HTML 代码之后，我们通过一些解析工具就可以提取想要的信息了。</p>
                  <h3 id="页面点击"><a href="#页面点击" class="headerlink" title="页面点击"></a>页面点击</h3>
                  <p>刚才我们通过示例也了解了页面点击的方法，那就是 click，这里详细说一下其使用方法。</p>
                  <p>页面点击的 API 定义如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.click(selector, **kwargs)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里可以看到必传的参数是 selector，其他的参数都是可选的。第一个 selector 就代表选择器，可以用来匹配想要点击的节点，如果传入的选择器匹配了多个节点，那么只会用第一个节点。</p>
                  <p>这个方法的内部执行逻辑如下：</p>
                  <ul>
                    <li>根据 selector 找到匹配的节点，如果没有找到，那就一直等待直到超时，超时时间可以由额外的 timeout 参数设置，默认是 30 秒。</li>
                    <li>等待对该节点的可操作性检查的结果，比如说如果某个按钮设置了不可点击，那它会等待该按钮变成了可点击的时候才去点击，除非通过 force 参数设置跳过可操作性检查步骤强制点击。</li>
                    <li>如果需要的话，就滚动下页面，将需要被点击的节点呈现出来。</li>
                    <li>调用 page 对象的 mouse 方法，点击节点中心的位置，如果指定了 position 参数，那就点击指定的位置。</li>
                  </ul>
                  <p>click 方法的一些比较重要的参数如下：</p>
                  <ul>
                    <li>click_count：点击次数，默认为 1。</li>
                    <li>timeout：等待要点击的节点的超时时间，默认是 30 秒。</li>
                    <li>position：需要传入一个字典，带有 x 和 y 属性，代表点击位置相对节点左上角的偏移位置。</li>
                    <li>force：即使不可点击，那也强制点击。默认是 False。</li>
                  </ul>
                  <p>具体的 API 设置参数可以参考官方文档：<a href="https://playwright.dev/python/docs/api/class-page/#pageclickselector-kwargs。" target="_blank" rel="noopener">https://playwright.dev/python/docs/api/class-page/#pageclickselector-kwargs。</a></p>
                  <h3 id="文本输入"><a href="#文本输入" class="headerlink" title="文本输入"></a>文本输入</h3>
                  <p>文本输入对应的方法是 fill，API 定义如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.fill(selector, value, **kwargs)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个方法有两个必传参数，第一个参数也是 selector，第二个参数是 value，代表输入的内容，另外还可以通过 timeout 参数指定对应节点的最长等待时间。</p>
                  <h3 id="获取节点属性"><a href="#获取节点属性" class="headerlink" title="获取节点属性"></a>获取节点属性</h3>
                  <p>除了对节点进行操作，我们还可以获取节点的属性，方法就是 get_attribute，API 定义如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">page.get_attribute(selector, name, **kwargs)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个方法有两个必传参数，第一个参数也是 selector，第二个参数是 name，代表要获取的属性名称，另外还可以通过 timeout 参数指定对应节点的最长等待时间。</p>
                  <p>示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line">    page.goto(<span class="string">'https://spa6.scrape.center/'</span>)</span><br><span class="line">    page.wait_for_load_state(<span class="string">'networkidle'</span>)</span><br><span class="line">    href = page.get_attribute(<span class="string">'a.name'</span>, <span class="string">'href'</span>)</span><br><span class="line">    print(href)</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们调用了 get_attribute 方法，传入的 selector 是 <code>a.name</code>，选定了 class 为 name 的 a 节点，然后第二个参数传入了 href，获取超链接的内容，输出结果如下：</p>
                  <figure class="highlight gcode">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">/detail/ZWYz<span class="symbol">NCN0</span>ZXVxMGJ<span class="number">0</span>dWEjKC<span class="number">01</span><span class="symbol">N3</span>cxcTVv<span class="symbol">NS0</span>takA<span class="number">5</span>OHh<span class="number">5</span>Z<span class="number">2</span>ltbHlmeHMqLSFpLTAtbWIx</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到对应 href 属性就获取出来了，但这里只有一条结果，因为这里有个条件，那就是如果传入的选择器匹配了多个节点，那么只会用第一个节点。</p>
                  <p>那怎么获取所有的节点呢？</p>
                  <h3 id="获取多个节点"><a href="#获取多个节点" class="headerlink" title="获取多个节点"></a>获取多个节点</h3>
                  <p>获取所有节点可以使用 query_selector_all 方法，它可以返回节点列表，通过遍历获取到单个节点之后，我们可以接着调用单个节点的方法来进行一些操作和属性获取，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line">    page.goto(<span class="string">'https://spa6.scrape.center/'</span>)</span><br><span class="line">    page.wait_for_load_state(<span class="string">'networkidle'</span>)</span><br><span class="line">    elements = page.query_selector_all(<span class="string">'a.name'</span>)</span><br><span class="line">    <span class="keyword">for</span> element <span class="keyword">in</span> elements:</span><br><span class="line">        print(element.get_attribute(<span class="string">'href'</span>))</span><br><span class="line">        print(element.text_content())</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们通过 query_selector_all 方法获取了所有匹配到的节点，每个节点对应的是一个 ElementHandle 对象，然后 ElementHandle 对象也有 get_attribute 方法来获取节点属性，另外还可以通过 text_content 方法获取节点文本。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight awk">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIx</span><br><span class="line">霸王别姬 - Farewell My Concubine</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIy</span><br><span class="line">这个杀手不太冷 - Léon</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIz</span><br><span class="line">肖申克的救赎 - The Shawshank Redemption</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI0</span><br><span class="line">泰坦尼克号 - Titanic</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI1</span><br><span class="line">罗马假日 - Roman Holiday</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI2</span><br><span class="line">唐伯虎点秋香 - Flirting Scholar</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI3</span><br><span class="line">乱世佳人 - Gone with the Wind</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI4</span><br><span class="line">喜剧之王 - The King of Comedy</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI5</span><br><span class="line">楚门的世界 - The Truman Show</span><br><span class="line"><span class="regexp">/detail/</span>ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxMA==</span><br><span class="line">狮子王 - The Lion King</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="获取单个节点"><a href="#获取单个节点" class="headerlink" title="获取单个节点"></a>获取单个节点</h3>
                  <p>获取单个节点也有特定的方法，就是 query_selector，如果传入的选择器匹配到多个节点，那它只会返回第一个节点，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line">    page.goto(<span class="string">'https://spa6.scrape.center/'</span>)</span><br><span class="line">    page.wait_for_load_state(<span class="string">'networkidle'</span>)</span><br><span class="line">    element = page.query_selector(<span class="string">'a.name'</span>)</span><br><span class="line">    print(element.get_attribute(<span class="string">'href'</span>))</span><br><span class="line">    print(element.text_content())</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight gcode">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">/detail/ZWYz<span class="symbol">NCN0</span>ZXVxMGJ<span class="number">0</span>dWEjKC<span class="number">01</span><span class="symbol">N3</span>cxcTVv<span class="symbol">NS0</span>takA<span class="number">5</span>OHh<span class="number">5</span>Z<span class="number">2</span>ltbHlmeHMqLSFpLTAtbWIx</span><br><span class="line">霸王别姬 - Farewell My Co<span class="symbol">ncubine</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到这里只输出了第一个匹配节点的信息。</p>
                  <h3 id="网络劫持"><a href="#网络劫持" class="headerlink" title="网络劫持"></a>网络劫持</h3>
                  <p>最后再介绍一个实用的方法 route，利用 route 方法，我们可以实现一些网络劫持和修改操作，比如修改 request 的属性，修改 response 响应结果等。</p>
                  <p>看一个实例：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cancel_request</span><span class="params">(route, request)</span>:</span></span><br><span class="line">        route.abort()</span><br><span class="line"></span><br><span class="line">    page.route(re.compile(<span class="string">r"(\.png)|(\.jpg)"</span>), cancel_request)</span><br><span class="line">    page.goto(<span class="string">"https://spa6.scrape.center/"</span>)</span><br><span class="line">    page.wait_for_load_state(<span class="string">'networkidle'</span>)</span><br><span class="line">    page.screenshot(path=<span class="string">'no_picture.png'</span>)</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们调用了 route 方法，第一个参数通过正则表达式传入了匹配的 URL 路径，这里代表的是任何包含 <code>.png</code> 或 <code>.jpg</code> 的链接，遇到这样的请求，会回调 cancel_request 方法处理，cancel_request 方法可以接收两个参数，一个是 route，代表一个 CallableRoute 对象，另外一个是 request，代表 Request 对象。这里我们直接调用了 route 的 abort 方法，取消了这次请求，所以最终导致的结果就是图片的加载全部取消了。</p>
                  <p>观察下运行结果，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/eyisw.png" alt=""></p>
                  <p>可以看到图片全都加载失败了。</p>
                  <p>这个设置有什么用呢？其实是有用的，因为图片资源都是二进制文件，而我们在做爬取过程中可能并不想关心其具体的二进制文件的内容，可能只关心图片的 URL 是什么，所以在浏览器中是否把图片加载出来就不重要了。所以如此设置之后，我们可以提高整个页面的加载速度，提高爬取效率。</p>
                  <p>另外，利用这个功能，我们还可以将一些响应内容进行修改，比如直接修改 Response 的结果为自定义的文本文件内容。</p>
                  <p>首先这里定义一个 HTML 文本文件，命名为 custom_response.html，内容如下：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Hack Response<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Hack Response<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>代码编写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> playwright.sync_api <span class="keyword">import</span> sync_playwright</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sync_playwright() <span class="keyword">as</span> p:</span><br><span class="line">    browser = p.chromium.launch(headless=<span class="literal">False</span>)</span><br><span class="line">    page = browser.new_page()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">modify_response</span><span class="params">(route, request)</span>:</span></span><br><span class="line">        route.fulfill(path=<span class="string">"./custom_response.html"</span>)</span><br><span class="line"></span><br><span class="line">    page.route(<span class="string">'/'</span>, modify_response)</span><br><span class="line">    page.goto(<span class="string">"https://spa6.scrape.center/"</span>)</span><br><span class="line">    browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用 route 的 fulfill 方法指定了一个本地文件，就是刚才我们定义的 HTML 文件，运行结果如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/di2dp.png" alt=""></p>
                  <p>可以看到，Response 的运行结果就被我们修改了，URL 还是不变的，但是结果已经成了我们修改的 HTML 代码。</p>
                  <p>所以通过 route 方法，我们可以灵活地控制请求和响应的内容，从而在某些场景下达成某些目的。</p>
                  <h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2>
                  <p>本节介绍了 Playwright 的基本用法，其 API 强大又易于使用，同时具备很多 Selenium、Pyppeteer 不具备的更好用的 API，是新一代 JavaScript 渲染页面的爬取利器。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/PlaywrightTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/PlaywrightTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-17 20:11:31" itemprop="dateCreated datePublished" datetime="2022-02-17T20:11:31+08:00">2022-02-17</time>
                </span>
                <span id="/202262.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 新兴动态渲染工具 Playwright 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>18k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>16 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202253.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202253.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - Ajax 案例爬取实战</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在上一节中我们已经学习了 Ajax 的基本原理和分析方法，这一节我们来结合一个实际的案例来看一下 Ajax 分析和爬取页面的具体实现。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>在本节开始之前，我们需要做好如下准备工作：</p>
                  <ul>
                    <li>安装好 Python 3（最低为 3.6 版本），并成功运行 Python 3 程序。</li>
                    <li>了解 Python HTTP 请求库 requests 的基本用法。</li>
                    <li>了解 Ajax 基础知识和分析 Ajax 的基本方法。</li>
                  </ul>
                  <p>以上内容在前面的章节中均有讲解，如尚未准备好，建议先熟悉一下这些内容。</p>
                  <h2 id="2-爬取目标"><a href="#2-爬取目标" class="headerlink" title="2. 爬取目标"></a>2. 爬取目标</h2>
                  <p>本节我们以一个示例网站来试验一下 Ajax 的爬取，其链接为：<a href="https://spa1.scrape.center/，该示例网站的数据请求是通过" target="_blank" rel="noopener">https://spa1.scrape.center/，该示例网站的数据请求是通过</a> Ajax 完成的，页面的内容是通过 JavaScript 渲染出来的，页面如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/add5o.png" alt="image-20210705004644681"></p>
                  <p>可能大家看着这个页面似曾相识，心想这不就是上一个案例的网站吗？但其实不是。这个网站的后台实现逻辑和数据加载方式完全不同。只不过最后呈现的样式是一样的。</p>
                  <p>这个网站同样支持翻页，可以点击最下方的页码来切换到下一页，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/lyy4e.png" alt="image-20210705004704636"></p>
                  <p>点击每一个电影的链接进入详情页，页面结构也是完全一样的，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/y712p.png" alt="image-20210705004718813"></p>
                  <p>我们需要爬取的数据也是和原来相同的，包括电影的名称、封面、类别、上映日期、评分、剧情简介等信息。</p>
                  <p>本节中我们需要完成的目标如下。</p>
                  <ul>
                    <li>分析页面数据的加载逻辑。</li>
                    <li>用 requests 实现 Ajax 数据的爬取。</li>
                    <li>将每部电影的数据保存成一个 JSON 数据文件。</li>
                  </ul>
                  <p>由于本节主要讲解 Ajax，所以对于数据存储和加速部分就不再展开详细实现，主要是讲解 Ajax 的分析和爬取实现。</p>
                  <p>好，我们现在就开始吧。</p>
                  <h2 id="3-初步探索"><a href="#3-初步探索" class="headerlink" title="3. 初步探索"></a>3. 初步探索</h2>
                  <p>首先，我们先尝试用之前的 requests 来直接提取页面，看看会得到怎样的结果。用最简单的代码实现一下 requests 获取首页源码的过程，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://spa1.scrape.center/'</span></span><br><span class="line">html = requests.get(url).text</span><br><span class="line">print(html)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">en</span>&gt;</span><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">utf-8</span>&gt;</span><span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">X-UA-Compatible</span> <span class="attr">content</span>=<span class="string">"IE=edge"</span>&gt;</span><span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">viewport</span> <span class="attr">content</span>=<span class="string">"width=device-width,initial-scale=1"</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">icon</span> <span class="attr">href</span>=<span class="string">/favicon.ico</span>&gt;</span><span class="tag">&lt;<span class="name">title</span>&gt;</span>Scrape | Movie<span class="tag">&lt;/<span class="name">title</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/css/chunk-700f70e1.1126d090.css</span> <span class="attr">rel</span>=<span class="string">prefetch</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/css/chunk-d1db5eda.0ff76b36.css</span> <span class="attr">rel</span>=<span class="string">prefetch</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/js/chunk-700f70e1.0548e2b4.js</span> <span class="attr">rel</span>=<span class="string">prefetch</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/js/chunk-d1db5eda.b564504d.js</span> <span class="attr">rel</span>=<span class="string">prefetch</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/css/app.ea9d802a.css</span> <span class="attr">rel</span>=<span class="string">preload</span> <span class="attr">as</span>=<span class="string">style</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/js/app.1435ecd5.js</span> <span class="attr">rel</span>=<span class="string">preload</span> <span class="attr">as</span>=<span class="string">script</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/js/chunk-vendors.77daf991.js</span> <span class="attr">rel</span>=<span class="string">preload</span> <span class="attr">as</span>=<span class="string">script</span>&gt;</span><span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">/css/app.ea9d802a.css</span> <span class="attr">rel</span>=<span class="string">stylesheet</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">body</span>&gt;</span><span class="tag">&lt;<span class="name">noscript</span>&gt;</span><span class="tag">&lt;<span class="name">strong</span>&gt;</span>We're sorry but portal doesn't work properly without JavaScript enabled. Please enable it to continue.<span class="tag">&lt;/<span class="name">strong</span>&gt;</span><span class="tag">&lt;/<span class="name">noscript</span>&gt;</span><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">app</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">/js/chunk-vendors.77daf991.js</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">/js/app.1435ecd5.js</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span><span class="tag">&lt;/<span class="name">body</span>&gt;</span><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，爬取结果就只有这么一点 HTML 内容，而我们在浏览器中打开这个页面，却能看到如图所示的结果：</p>
                  <p><img src="https://cdn.cuiqingcai.com/p2u8d.png" alt="image-20210705004644681"></p>
                  <p>在 HTML 中，我们只能看到在源码中引用了一些 JavaScript 和 CSS 文件，并没有观察到有任何电影数据信息。</p>
                  <p>如果遇到这样的情况，这说明我们现在看到的整个页面便是 JavaScript 渲染得到的，浏览器执行了 HTML 中所引用的 JavaScript 文件，JavaScript 通过调用一些数据加载和页面渲染方法，才最终呈现了图中所示的结果。</p>
                  <p>在一般情况下，这些数据都是通过 Ajax 来加载的， JavaScript 在后台调用这些 Ajax 数据接口，得到数据之后，再把数据进行解析并渲染呈现出来，得到最终的页面。所以说，要想爬取这个页面，我们可以直接爬取 Ajax 接口获取数据就好了。</p>
                  <p>在上一节中，我们已经了解了 Ajax 分析的基本方法，下面我们就来分析一下 Ajax 接口的逻辑并实现数据爬取吧。</p>
                  <h2 id="4-爬取列表页"><a href="#4-爬取列表页" class="headerlink" title="4. 爬取列表页"></a>4. 爬取列表页</h2>
                  <p>首先我们来分析一下列表页的 Ajax 接口逻辑，打开浏览器开发者工具，切换到 Network 面板，勾选上 Preserve Log 并切换到 XHR 选项卡，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/dpns6.png" alt="image-20210705004826230"></p>
                  <p>接着重新刷新页面，再点击第二页、第三页、第四页的按钮，这时候可以观察到页面上的数据发生了变化，同时开发者工具下方就监听到了几个 Ajax 请求，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/x22b1.png" alt="image-20210705004904893"></p>
                  <p>由于我们切换了 4 页，每次翻页也出现了对应的 Ajax 请求，我们可以点击查看其请求详情。观察其请求的 URL 和参数以及响应内容是怎样的，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/1ro2i.png" alt="image-20210705004957327"></p>
                  <p>这里我们点开了最后个结果，观察到其 Ajax 接口请求的 URL 地址为：<a href="https://spa1.scrape.center/api/movie/?limit=10&amp;offset=40，这里有两个参数，一个是" target="_blank" rel="noopener">https://spa1.scrape.center/api/movie/?limit=10&amp;offset=40，这里有两个参数，一个是</a> <code>limit</code>，这里是 10；一个是 <code>offset</code>，这里也是 40。</p>
                  <p>通过多个 Ajax 接口的参数，我们可以观察到这么一个规律：<code>limit</code> 一直为 10，这就正好对应着每页 10 条数据；<code>offset</code> 在依次变大，页面每加 1 页，<code>offset</code> 就加 10，这就代表着页面的数据偏移量，比如第二页的 <code>offset</code> 为 10 则代表着跳过 10 条数据，返回从 11 条数据开始的结果，再加上 <code>limit</code> 的限制，那就是第 11 条至第 20 条数据的结果。</p>
                  <p>接着我们再观察一下响应的数据，切换到 Preview 选项卡，结果如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/0ldhk.png" alt="image-20210705005115792"></p>
                  <p>可以看到，结果就是一些 JSON 数据，它有一个 <code>results</code> 字段，是一个列表，列表中每一个元素都是一个字典。观察一下字典的内容，这里我们正好可以看到有对应的电影数据的字段了，如 <code>name</code>、<code>alias</code>、<code>cover</code>、<code>categories</code>，对比下浏览器中的真实数据，各个内容完全一致，而且这个数据已经非常结构化了，完全就是我们想要爬取的数据，真的是得来全不费工夫。</p>
                  <p>这样的话，我们只需要把所有页面的 Ajax 接口构造出来，所有列表页的数据我们都可以轻松获取到了。</p>
                  <p>我们先定义一些准备工作，导入一些所需的库并定义一些配置，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO,</span><br><span class="line">                    format=<span class="string">'%(asctime)s - %(levelname)s: %(message)s'</span>)</span><br><span class="line"></span><br><span class="line">INDEX_URL = <span class="string">'https://spa1.scrape.center/api/movie/?limit=&#123;limit&#125;&amp;offset=&#123;offset&#125;'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们引入了 requests 和 logging 库，并定义了 logging 的基本配置，接着我们定义了 <code>INDEX_URL</code>，这里把 <code>limit</code> 和 <code>offset</code> 预留出来了变成了占位符，可以动态传入参数构造一个完整的列表页 URL。</p>
                  <p>下面我们来实现一下详情页的爬取。还是和原来一样，我们先定义一个通用的爬取方法，其代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_api</span><span class="params">(url)</span>:</span></span><br><span class="line">    logging.info(<span class="string">'scraping %s...'</span>, url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.json()</span><br><span class="line">        logging.error(<span class="string">'get invalid status code %s while scraping %s'</span>, response.status_code, url)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException:</span><br><span class="line">        logging.error(<span class="string">'error occurred while scraping %s'</span>, url, exc_info=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 <code>scrape_api</code> 方法，和之前不同的是，这个方法专门用来处理 JSON 接口，最后的 <code>response</code> 调用的是 <code>json</code> 方法，它可以解析响应的内容并将其转化成 JSON 字符串。</p>
                  <p>接着在这个基础之上，我们定义一个爬取列表页的方法，其代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">LIMIT = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_index</span><span class="params">(page)</span>:</span></span><br><span class="line">    url = INDEX_URL.format(limit=LIMIT, offset=LIMIT * (page - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> scrape_api(url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 <code>scrape_index</code> 方法，它接收一个参数 <code>page</code>，该参数代表列表页的页码。</p>
                  <p>这里我们先构造了一个 <code>url</code>，通过字符串的 <code>format</code> 方法，传入 <code>limit</code> 和 <code>offset</code> 的值。这里 <code>limit</code> 就直接使用了全局变量 <code>LIMIT</code> 的值；<code>offset</code> 则是动态计算的，就是页码数减一再乘以 <code>limit</code>，比如第一页 <code>offset</code> 就是 0，第二页 <code>offset</code> 就是 10，以此类推。构造好了 <code>url</code> 之后，直接调用 <code>scrape_api</code> 方法并返回结果即可。</p>
                  <p>这样我们就完成了列表页的爬取，每次请求都会得到一页 10 部的电影数据。</p>
                  <p>由于这时爬取到的数据已经是 JSON 类型了，所以我们不用像之前那样去解析 HTML 代码来提取数据了，爬到的数据就是我们想要的结构化数据，因此解析这一步就可以直接省略啦。</p>
                  <p>到此为止，我们能成功爬取列表页并提取出电影列表信息了。</p>
                  <h2 id="5-爬取详情页"><a href="#5-爬取详情页" class="headerlink" title="5. 爬取详情页"></a>5. 爬取详情页</h2>
                  <p>这时候我们已经可以拿到每一页的电影数据了，但是看看这些数据实际上还缺少了一些我们想要的信息，如剧情简介等信息，所以需要进一步进入到详情页来获取这些内容。</p>
                  <p>这时候点击任意一部电影，如《教父》，进入其详情页，这时可以发现页面的 URL 已经变成了 <a href="https://spa1.scrape.center/detail/40" target="_blank" rel="noopener">https://spa1.scrape.center/detail/40</a>，页面也成功展示了详情页的信息，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/tuum6.png" alt="image-20210705005243372"></p>
                  <p>另外，我们也可以观察到在开发者工具中又出现了一个 Ajax 请求，其 URL 为 <a href="https://spa1.scrape.center/api/movie/40/" target="_blank" rel="noopener">https://spa1.scrape.center/api/movie/40/</a>，通过 Preview 选项卡也能看到 Ajax 请求对应响应的信息，如图 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/y1o4n.png" alt="image-20200601141202684"><br>稍加观察就可以发现，Ajax 请求的 URL 后面有一个参数是可变的，这个参数就是电影的 <code>id</code>，这里是 40，对应《教父》这部电影。</p>
                  <p>如果我们想要获取 <code>id</code> 为 50 的电影，只需要把 URL 最后的参数改成 50 即可，即 <a href="https://spa1.scrape.center/api/movie/50/" target="_blank" rel="noopener">https://spa1.scrape.center/api/movie/50/</a>，请求这个新的 URL 我们就能获取 <code>id</code> 为 50 的电影所对应的数据了。</p>
                  <p>同样，响应结果也是结构化的 JSON 数据，字段也非常规整，我们直接爬取即可。</p>
                  <p>现在分析好了详情页的数据提取逻辑，那么怎么和列表页关联起来呢？这个 <code>id</code> 哪里来呢？我们回过头来再看看列表页的接口返回数据，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/bzc8x.png" alt=""></p>
                  <p>可以看到，列表页原本的返回数据就带了 <code>id</code> 这个字段，所以我们只需要拿列表页结果中的 <code>id</code> 来构造详情页的 Ajax 请求的 URL 就好了。</p>
                  <p>接着，我们就先定义一个详情页的爬取逻辑，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">DETAIL_URL = <span class="string">'https://spa1.scrape.center/api/movie/&#123;id&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_detail</span><span class="params">(id)</span>:</span></span><br><span class="line">    url = DETAIL_URL.format(id=id)</span><br><span class="line">    <span class="keyword">return</span> scrape_api(url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 <code>scrape_detail</code> 方法，它接收一个参数 <code>id</code>。这里的实现也非常简单，先根据定义好的 <code>DETAIL_URL</code> 加 <code>id</code> 构造一个真实的详情页 Ajax 请求的 URL，然后直接调用 <code>scrape_api</code> 方法传入这个 <code>url</code> 即可。</p>
                  <p>接着，我们定义一个总的调用方法，将以上方法串联调用起来，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">TOTAL_PAGE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, TOTAL_PAGE + <span class="number">1</span>):</span><br><span class="line">        index_data = scrape_index(page)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> index_data.get(<span class="string">'results'</span>):</span><br><span class="line">            id = item.get(<span class="string">'id'</span>)</span><br><span class="line">            detail_data = scrape_detail(id)</span><br><span class="line">            logging.info(<span class="string">'detail data %s'</span>, detail_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 <code>main</code> 方法，首先遍历获取了页码 <code>page</code>，然后把 <code>page</code> 当参数传递给了 <code>scrape_index</code> 方法，得到列表页的数据。接着我们遍历每个列表页的每个结果，获取到每部电影的 <code>id</code>，然后把 <code>id</code> 当作参数传递给 <code>scrape_detail</code> 方法来爬取每部电影的详情数据，并将其赋值为 <code>detail_data</code>，输出即可。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight 1c">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">55</span>,<span class="number">981</span> - INFO: scraping https:<span class="comment">//spa1.scrape.center/api/movie/?limit=10&amp;offset=0...</span></span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">56</span>,<span class="number">446</span> - INFO: scraping https:<span class="comment">//spa1.scrape.center/api/movie/1...</span></span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">56</span>,<span class="number">638</span> - INFO: detail data &#123;'id': <span class="number">1</span>, 'name': '霸王别姬', 'alias': 'Farewell My Concubine', 'cover': 'https://p0.meituan.net/movie/ce4da3e03e655b5b88ed31b5cd<span class="number">7896</span>cf<span class="number">6247</span>2.jpg@464w_644h_1e_1c', 'categories': ['剧情', '爱情'], 'regions': ['中国大陆', '中国香港'], 'actors': [&#123;'name': '张国荣', 'role': '程蝶衣', ...&#125;, ...], 'directors': [&#123;'name': '陈凯歌', 'image': 'https://p0.meituan.net/movie/8f<span class="number">93722520500950</span>67e0e8d58ef3d<span class="number">93915640</span>7.jpg@128w_170h_1e_1c'&#125;], 'score': <span class="number">9.5</span>, 'rank': <span class="number">1</span>, 'minute': <span class="number">171</span>, 'drama': '影片借一出《霸王别姬》的京戏，牵扯出三个人之间一段随时代风云变幻的爱恨情仇。段小楼（张丰毅 饰）与程蝶衣（张国荣 饰）是一对打小一起长大的师兄弟，...', 'photos': [...], 'published_at': '<span class="number">1993-07-26</span>', 'updated_at': '<span class="number">2020-03-07</span>T16:31:36.<span class="number">967843</span>Z'&#125;</span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">56</span>,<span class="number">640</span> - INFO: scraping https:<span class="comment">//spa1.scrape.center/api/movie/2...</span></span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">56</span>,<span class="number">813</span> - INFO: detail data &#123;'id': <span class="number">2</span>, 'name': '这个杀手不太冷', 'alias': 'Léon', 'cover': 'https://p1.meituan.net/movie/6bea9af<span class="number">4524</span>dfbd0b668eaa7e187c3df<span class="number">767253</span>.jpg@464w_644h_1e_1c', 'categories': ['剧情', '动作', '犯罪'], 'regions': ['法国'], 'actors': [&#123;'name': '让·雷诺', 'role': '莱昂 Leon', ...&#125;, ...], 'directors': [&#123;'name': '吕克·贝松', 'image': 'https://p0.meituan.net/movie/0e7d67e343bd<span class="number">3372</span>a<span class="number">714093</span>e<span class="number">834002</span>8d<span class="number">4049</span>6.jpg@128w_170h_1e_1c'&#125;], 'score': <span class="number">9.5</span>, 'rank': <span class="number">3</span>, 'minute': <span class="number">110</span>, 'drama': '里昂（让·雷诺 饰）是名孤独的职业杀手，受人雇佣。一天，邻居家小姑娘马蒂尔德（纳塔丽·波特曼 饰）敲开他的房门，要求在他那里暂避杀身之祸。...', 'photos': [...], 'published_at': '<span class="number">1994-09-14</span>', 'updated_at': '<span class="number">2020-03-07</span>T16:31:43.<span class="number">826235</span>Z'&#125;</span><br><span class="line">...</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于内容较多，这里省略了部分内容。</p>
                  <p>可以看到，其实整个爬取工作就已经完成了，这里会顺次爬取每一页列表页 Ajax 接口，然后去顺次爬取每部电影的详情页 Ajax 接口，打印出每部电影的 Ajax 接口响应数据，而且都是 JSON 格式。这样，所有电影的详情数据都会被我们爬取到啦。</p>
                  <h2 id="6-保存数据"><a href="#6-保存数据" class="headerlink" title="6. 保存数据"></a>6. 保存数据</h2>
                  <p>好，成功提取到详情页信息之后，我们下一步就要把数据保存起来了。在前面我们学习了 MongoDB 的相关操作，接下来我们就把数据保存到 MongoDB 吧。</p>
                  <p>在这之前，请确保现在有一个可以正常连接和使用的 MongoDB 数据库，这里我就以本地 localhost 的 M 哦能够 DB 数据库为例来进行操作，其运行在 27017 端口上，无用户名和密码。</p>
                  <p>将数据导入 MongoDB 需要用到 PyMongo 这个库。接下来我们把它们引入一下，然后同时定义一下 MongoDB 的连接配置，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">MONGO_CONNECTION_STRING = <span class="string">'mongodb://localhost:27017'</span></span><br><span class="line">MONGO_DB_NAME = <span class="string">'movies'</span></span><br><span class="line">MONGO_COLLECTION_NAME = <span class="string">'movies'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">client = pymongo.MongoClient(MONGO_CONNECTION_STRING)</span><br><span class="line">db = client[<span class="string">'movies'</span>]</span><br><span class="line">collection = db[<span class="string">'movies'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们声明了几个变量，介绍如下：</p>
                  <ul>
                    <li>MONGO_CONNECTION_STRING：MongoDB 的连接字符串，里面定义了 MongoDB 的基本连接信息，如 host、port，还可以定义用户名密码等内容。</li>
                    <li>MONGO_DB_NAME：MongoDB 数据库的名称。</li>
                    <li>MONGO_COLLECTION_NAME：MongoDB 的集合名称。</li>
                  </ul>
                  <p>这里我们用 MongoClient 声明了一个连接对象，然后依次声明了存储的数据库和集合。</p>
                  <p>接下来，我们再实现一个将数据保存到 MongoDB 的方法，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    collection.update_one(&#123;</span><br><span class="line">        <span class="string">'name'</span>: data.get(<span class="string">'name'</span>)</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="string">'$set'</span>: data</span><br><span class="line">    &#125;, upsert=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们声明了一个 save_data 方法，它接收一个 data 参数，也就是我们刚才提取的电影详情信息。在方法里面，我们调用了 update_one 方法，第一个参数是查询条件，即根据 name 进行查询；第二个参数就是 data 对象本身，就是所有的数据，这里我们用 <code>$set</code> 操作符表示更新操作；第三个参数很关键，这里实际上是 upsert 参数，如果把这个设置为 True，则可以做到存在即更新，不存在即插入的功能，更新会根据第一个参数设置的 name 字段，所以这样可以防止数据库中出现同名的电影数据。</p>
                  <blockquote>
                    <p>注：实际上电影可能有同名，但该场景下的爬取数据没有同名情况，当然这里更重要的是实现 MongoDB 的去重操作。</p>
                  </blockquote>
                  <p>好的，那么接下来 main 方法稍微改写一下就好了，改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, TOTAL_PAGE + <span class="number">1</span>):</span><br><span class="line">        index_data = scrape_index(page)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> index_data.get(<span class="string">'results'</span>):</span><br><span class="line">            id = item.get(<span class="string">'id'</span>)</span><br><span class="line">            detail_data = scrape_detail(id)</span><br><span class="line">            logging.info(<span class="string">'detail data %s'</span>, detail_data)</span><br><span class="line">            save_data(detail_data)</span><br><span class="line">            logging.info(<span class="string">'data saved successfully'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里就是加了 save_data 方法的调用，并加了一些日志信息。</p>
                  <p>重新运行，我们看下输出结果：</p>
                  <figure class="highlight 1c">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">06</span>,<span class="number">323</span> - INFO: scraping https:<span class="comment">//spa1.scrape.center/api/movie/?limit=10&amp;offset=0...</span></span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">06</span>,<span class="number">440</span> - INFO: scraping https:<span class="comment">//spa1.scrape.center/api/movie/1...</span></span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">06</span>,<span class="number">551</span> - INFO: detail data &#123;'id': <span class="number">1</span>, 'name': '霸王别姬', 'alias': 'Farewell My Concubine', 'cover': 'https://p0.meituan.net/movie/ce4da3e03e655b5b88ed31b5cd<span class="number">7896</span>cf<span class="number">6247</span>2.jpg@464w_644h_1e_1c', 'categories': ['剧情', '爱情'], 'regions': ['中国大陆', '中国香港'], 'actors': [&#123;'name': '张国荣', 'role': '程蝶衣', 'image': 'https://p0.meituan.net/movie/5de69a492dcbd3f4b<span class="number">014503</span>d4e95d46c<span class="number">2883</span>7.jpg@128w_170h_1e_1c'&#125;, ..., &#123;'name': '方征', 'role': '嫖客', 'image': 'https://p1.meituan.net/movie/<span class="number">39687137</span>b23bc<span class="number">9727</span>b47fd24bdcc579b<span class="number">9761</span>8.jpg@128w_170h_1e_1c'&#125;], 'directors': [&#123;'name': '陈凯歌', 'image': 'https://p0.meituan.net/movie/8f<span class="number">93722520500950</span>67e0e8d58ef3d<span class="number">93915640</span>7.jpg@128w_170h_1e_1c'&#125;], 'score': <span class="number">9.5</span>, 'rank': <span class="number">1</span>, 'minute': <span class="number">171</span>, 'drama': '影片借一出《霸王别姬》的京戏，牵扯出三个人之间一段随时代风云变幻的爱恨情仇。段小楼（张丰毅 饰）与程蝶衣（张国荣 饰）是一对打小一起长大的师兄弟，两人一个演生，一个饰旦，一向配合天衣无缝，尤其一出《霸王别姬》，更是誉满京城，为此，两人约定合演一辈子《霸王别姬》。但两人对戏剧与人生关系的理解有本质不同，段小楼深知戏非人生，程蝶衣则是人戏不分。段小楼在认为该成家立业之时迎娶了名妓菊仙（巩俐 饰），致使程蝶衣认定菊仙是可耻的第三者，使段小楼做了叛徒，自此，三人围绕一出《霸王别姬》生出的爱恨情仇战开始随着时代风云的变迁不断升级，终酿成悲剧。', 'photos': ['https://p0.meituan.net/movie/45be<span class="number">438368</span>bb291e501dc<span class="number">523092</span>f0ac<span class="number">819342</span>4.jpg@106w_106h_1e_1c', ..., 'https://p0.meituan.net/movie/0d<span class="number">95210742</span>9db<span class="number">3029</span>b64bf4f25bd<span class="number">76266169</span>6.jpg@106w_106h_1e_1c'], 'published_at': '<span class="number">1993-07-26</span>', 'updated_at': '<span class="number">2020-03-07</span>T16:31:36.<span class="number">967843</span>Z'&#125;</span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">06</span>,<span class="number">583</span> - INFO: data saved successfully</span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">19</span> <span class="number">02</span>:<span class="number">51</span>:<span class="number">06</span>,<span class="number">583</span> - INFO: scraping https:<span class="comment">//spa1.scrape.center/api/movie/2...</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于输出内容较多，这里省略了部分内容。</p>
                  <p>我们可以看到这里我们成功爬取到了数据，并且提示了数据存储成功的信息，没有任何报错信息。</p>
                  <p>接下来我们使用 Robo 3T 连接 MongoDB 数据库看下爬取的结果，由于我使用的是本地的 MongoDB，所以在 Robo 3T 里面我直接输入 localhost 的连接信息即可，这里请替换成自己的 MongoDB 连接信息，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/imum3.png" alt=""></p>
                  <p>连接之后我们便可以在 movies 这个数据库，movies 这个集合下看到我们刚才爬取的数据了，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/ammal.png" alt=""></p>
                  <p>可以看到数据就是以 JSON 格式存储的，一条数据就对应一部电影的信息，各种嵌套信息也一目了然，同时第三列还有数据类型标识。</p>
                  <p>这样就证明我们的数据就成功存储到 MongoDB 里了。</p>
                  <h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2>
                  <p>本节中我们通过一个案例来体会了 Ajax 分析和爬取的基本流程，希望大家通过本节能够更加熟悉 Ajax 的分析和爬取实现。</p>
                  <p>另外，我们也观察到，由于 Ajax 接口大部分返回的是 JSON 数据，所以在一定程度上可以避免一些数据提取的工作，这也在一定程度上减轻了工作量。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/ScrapeSpa1。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapeSpa1。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-16 17:42:44" itemprop="dateCreated datePublished" datetime="2022-02-16T17:42:44+08:00">2022-02-16</time>
                </span>
                <span id="/202253.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - Ajax 案例爬取实战" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>11k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>10 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202251.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202251.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 什么是Ajax？</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>Ajax，全称为 Asynchronous JavaScript and XML，即异步的 JavaScript 和 XML。它不是一门编程语言，而是利用 JavaScript 在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。</p>
                  <p>对于传统的网页，如果想更新其内容，那么必须刷新整个页面，但有了 Ajax，便可以在页面不被全部刷新的情况下更新其内容。在这个过程中，页面实际上是在后台与服务器进行了数据交互，获取到数据之后，再利用 JavaScript 改变网页，这样网页内容就会更新了。</p>
                  <p>可以到 W3School 上体验几个示例感受一下：<a href="http://www.w3school.com.cn/ajax/ajax_xmlhttprequest_send.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/ajax/ajax_xmlhttprequest_send.asp</a>。</p>
                  <h2 id="1-实例引入"><a href="#1-实例引入" class="headerlink" title="1. 实例引入"></a>1. 实例引入</h2>
                  <p>浏览网页的时候，我们会发现很多网页都有下滑查看更多的选项。比如，拿微博来说，以我的主页为例：<a href="https://m.weibo.cn/u/2830678474" target="_blank" rel="noopener">https://m.weibo.cn/u/2830678474</a>，切换到微博页面，一直下滑，可以发现下滑几个微博之后，再向下就没有了，转而会出现一个加载的动画，不一会儿下方就继续出现了新的微博内容，这个过程其实就是 Ajax 加载的过程，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/2jqaq.png" alt=""></p>
                  <p>我们注意到页面其实并没有整个刷新，也就意味着页面的链接没有变化，但是网页中却多了新内容，也就是后面刷出来的新微博。这就是通过 Ajax 获取新数据并呈现的过程。</p>
                  <h2 id="2-基本原理"><a href="#2-基本原理" class="headerlink" title="2. 基本原理"></a>2. 基本原理</h2>
                  <p>初步了解了 Ajax 之后，我们再来详细了解它的基本原理。发送 Ajax 请求到网页更新的这个过程可以简单分为以下 3 步：</p>
                  <ol>
                    <li>发送请求</li>
                    <li>解析内容</li>
                    <li>渲染网页</li>
                  </ol>
                  <p>下面我们分别来详细介绍一下这几个过程。</p>
                  <h3 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h3>
                  <p>我们知道 JavaScript 可以实现页面的各种交互功能，Ajax 也不例外，它也是由 JavaScript 实现的，</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">var</span> xmlhttp;</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">window</span>.XMLHttpRequest) &#123;</span><br><span class="line">  <span class="comment">//code for IE7+, Firefox, Chrome, Opera, Safari</span></span><br><span class="line">  xmlhttp = <span class="keyword">new</span> XMLHttpRequest();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">//code for IE6, IE5</span></span><br><span class="line">  xmlhttp = <span class="keyword">new</span> ActiveXObject(<span class="string">"Microsoft.XMLHTTP"</span>);</span><br><span class="line">&#125;</span><br><span class="line">xmlhttp.onreadystatechange = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (xmlhttp.readyState == <span class="number">4</span> &amp;&amp; xmlhttp.status == <span class="number">200</span>) &#123;</span><br><span class="line">    <span class="built_in">document</span>.getElementById(<span class="string">"myDiv"</span>).innerHTML = xmlhttp.responseText;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">xmlhttp.open(<span class="string">"POST"</span>, <span class="string">"/ajax/"</span>, <span class="literal">true</span>);</span><br><span class="line">xmlhttp.send();</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这是 JavaScript 对 Ajax 最底层的实现，实际上就是新建了 <code>XMLHttpRequest</code> 对象，然后调用 <code>onreadystatechange</code> 属性设置了监听，然后调用 <code>open</code> 和 <code>send</code> 方法向某个链接（也就是服务器）发送了请求。前面用 Python 实现请求发送之后，可以得到响应结果，但这里请求的发送变成 JavaScript 来完成。由于设置了监听，所以当服务器返回响应时，<code>onreadystatechange</code> 对应的方法便会被触发，然后在这个方法里面解析响应内容即可。</p>
                  <h3 id="解析内容"><a href="#解析内容" class="headerlink" title="解析内容"></a>解析内容</h3>
                  <p>得到响应之后，<code>onreadystatechange</code> 属性对应的方法便会被触发，此时利用 <code>xmlhttp</code> 的 <code>responseText</code> 属性便可取到响应内容。这类似于 Python 中利用 requests 向服务器发起请求，然后得到响应的过程。那么返回内容可能是 HTML，可能是 JSON，接下来只需要在方法中用 JavaScript 进一步处理即可。比如，如果是 JSON 的话，可以进行解析和转化。</p>
                  <h3 id="渲染网页"><a href="#渲染网页" class="headerlink" title="渲染网页"></a>渲染网页</h3>
                  <p>JavaScript 有改变网页内容的能力，解析完响应内容之后，就可以调用 JavaScript 来针对解析完的内容对网页进行下一步处理了。比如，通过 <code>document.getElementById().innerHTML</code> 这样的操作，便可以对某个元素内的源代码进行更改，这样网页显示的内容就改变了，这样的操作也被称作 DOM 操作，即对网页文档进行操作，如更改、删除等。</p>
                  <p>上例中，<code>document.getElementById(&quot;myDiv&quot;).innerHTML=xmlhttp.responseText</code> 便将 ID 为 <code>myDiv</code> 的节点内部的 HTML 代码更改为服务器返回的内容，这样 <code>myDiv</code> 元素内部便会呈现出服务器返回的新数据，网页的部分内容看上去就更新了。</p>
                  <p>我们观察到，这 3 个步骤其实都是由 JavaScript 完成的，它完成了整个请求、解析和渲染的过程。</p>
                  <p>再回想微博的下拉刷新，这其实就是 JavaScript 向服务器发送了一个 Ajax 请求，然后获取新的微博数据，将其解析，并将其渲染在网页中。</p>
                  <p>因此，我们知道，真实的数据其实都是一次次 Ajax 请求得到的，如果想要抓取这些数据，需要知道这些请求到底是怎么发送的，发往哪里，发了哪些参数。如果我们知道了这些，不就可以用 Python 模拟这个发送操作，获取到其中的结果了吗？</p>
                  <h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2>
                  <p>本节我们简单了解了 Ajax 请求的基本原理和带来的页面加载效果，下一节我们来介绍下怎么来分析 Ajax 请求。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-16 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-16T12:42:44+08:00">2022-02-16</time>
                </span>
                <span id="/202251.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 什么是Ajax？" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>2.4k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202242.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202242.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 方便灵活的 JSON 文本文件存储</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>JSON，全称为 JavaScript Object Notation, 也就是 JavaScript 对象标记，它通过对象和数组的组合来表示数据，构造简洁但是结构化程度非常高，是一种轻量级的数据交换格式。</p>
                  <p>本节中，我们就来了解如何利用 Python 保存数据到 JSON 文件。</p>
                  <h2 id="1-对象和数组"><a href="#1-对象和数组" class="headerlink" title="1. 对象和数组"></a>1. 对象和数组</h2>
                  <p>在 JavaScript 语言中，一切都是对象。因此，任何支持的类型都可以通过 JSON 来表示，例如字符串、数字、对象、数组等，但是对象和数组是比较特殊且常用的两种类型，下面简要介绍一下它们。</p>
                  <ul>
                    <li>
                      <p>对象：它在 JavaScript 中是使用花括号 <code>{}</code> 包裹起来的内容，数据结构为 <code>{key1：value1, key2：value2, ...}</code> 的键值对结构。在面向对象的语言中，key 为对象的属性，value 为对应的值。键名可以使用整数和字符串来表示。值的类型可以是任意类型。</p>
                    </li>
                    <li>
                      <p>数组：数组在 JavaScript 中是方括号 <code>[]</code> 包裹起来的内容，数据结构为 <code>[&quot;java&quot;, &quot;javascript&quot;, &quot;vb&quot;, ...]</code> 的索引结构。在 JavaScript 中，数组是一种比较特殊的数据类型，它也可以像对象那样使用键值对，但还是索引用得多。同样，值的类型可以是任意类型。</p>
                    </li>
                  </ul>
                  <p>所以，一个 JSON 对象可以写为如下形式：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    name: <span class="string">"Bob"</span>,</span><br><span class="line">    gender: <span class="string">"male"</span>,</span><br><span class="line">    birthday: <span class="string">"1992-10-18"</span>,</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    name: <span class="string">"Selina"</span>,</span><br><span class="line">    gender: <span class="string">"female"</span>,</span><br><span class="line">    birthday: <span class="string">"1995-10-18"</span>,</span><br><span class="line">  &#125;,</span><br><span class="line">];</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由中括号包围的就相当于列表类型，列表中的每个元素可以是任意类型，这个示例中它是字典类型，由大括号包围。</p>
                  <p>JSON 可以由以上两种形式自由组合而成，可以无限次嵌套，结构清晰，是数据交换的极佳方式。</p>
                  <h2 id="2-读取-JSON"><a href="#2-读取-JSON" class="headerlink" title="2. 读取 JSON"></a>2. 读取 JSON</h2>
                  <p>Python 为我们提供了简单易用的 JSON 库来实现 JSON 文件的读写操作，我们可以调用 JSON 库的 loads 方法将 JSON 文本字符串转为 JSON 对象，实际上 JSON 对象为 Python 中的 list 和 dict 的嵌套和组合，这里称之为 JSON 对象。另外我们还可以通过 dumps 方法将 JSON 对象转为文本字符串。</p>
                  <p>例如，这里有一段 JSON 形式的字符串，它是 str 类型，我们用 Python 将其转换为可操作的数据结构，如列表或字典：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">str = <span class="string">'''</span></span><br><span class="line"><span class="string">[&#123;</span></span><br><span class="line"><span class="string">    "name": "Bob",</span></span><br><span class="line"><span class="string">    "gender": "male",</span></span><br><span class="line"><span class="string">    "birthday": "1992-10-18"</span></span><br><span class="line"><span class="string">&#125;, &#123;</span></span><br><span class="line"><span class="string">    "name": "Selina",</span></span><br><span class="line"><span class="string">    "gender": "female",</span></span><br><span class="line"><span class="string">    "birthday": "1995-10-18"</span></span><br><span class="line"><span class="string">&#125;]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">print(type(str))</span><br><span class="line">data = json.loads(str)</span><br><span class="line">print(data)</span><br><span class="line">print(type(data))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span>'<span class="title">str</span>'&gt;</span></span><br><span class="line">[&#123;'name': 'Bob', 'gender': 'male', 'birthday': '1992-10-18'&#125;, &#123;'name': 'Selina', 'gender': 'female', 'birthday': '1995-10-18'&#125;]</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里使用 loads 方法将字符串转为 JSON 对象。由于最外层是中括号，所以最终的类型是列表类型。</p>
                  <p>这样一来，我们就可以用索引来获取对应的内容了。例如，如果想取第一个元素里的 name 属性，就可以使用如下方式：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">data[<span class="number">0</span>][<span class="string">'name'</span>]</span><br><span class="line">data[<span class="number">0</span>].get(<span class="string">'name'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>得到的结果都是：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Bob</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过中括号加 0 索引，可以得到第一个字典元素，然后再调用其键名即可得到相应的键值。获取键值时有两种方式，一种是中括号加键名，另一种是通过 get 方法传入键名。这里推荐使用 get 方法，这样如果键名不存在，则不会报错，会返回 None。另外，get 方法还可以传入第二个参数（即默认值），示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">data[<span class="number">0</span>].get(<span class="string">'age'</span>)</span><br><span class="line">data[<span class="number">0</span>].get(<span class="string">'age'</span>, <span class="number">25</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">None</span></span><br><span class="line"><span class="number">25</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们尝试获取年龄 age，其实在原字典中该键名不存在，此时默认会返回 None。如果传入第二个参数（即默认值），那么在不存在的情况下返回该默认值。</p>
                  <p>值得注意的是，JSON 的数据需要用双引号来包围，不能使用单引号。例如，若使用如下形式表示，则会出现错误：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">str = <span class="string">'''</span></span><br><span class="line"><span class="string">[&#123;</span></span><br><span class="line"><span class="string">    'name': 'Bob',</span></span><br><span class="line"><span class="string">    'gender': 'male',</span></span><br><span class="line"><span class="string">    'birthday': '1992-10-18'</span></span><br><span class="line"><span class="string">&#125;]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">data = json.loads(str)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">json.decoder.JSONDecodeError: Expecting property name enclosed <span class="keyword">in</span> double quotes: line <span class="number">3</span> column <span class="number">5</span> (char <span class="number">8</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里会出现 JSON 解析错误的提示。这是因为这里数据用单引号来包围，请千万注意 JSON 字符串的表示需要用双引号，否则 loads 方法会解析失败。</p>
                  <p>如果从 JSON 文本中读取内容，例如这里有一个 data.json 文本文件，其内容是刚才定义的 JSON 字符串，我们可以先将文本文件内容读出，然后再利用 loads 方法转化：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    str = file.read()</span><br><span class="line">    data = json.loads(str)</span><br><span class="line">    print(data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&#123;<span class="string">'name'</span>: <span class="string">'Bob'</span>, <span class="string">'gender'</span>: <span class="string">'male'</span>, <span class="string">'birthday'</span>: <span class="string">'1992-10-18'</span>&#125;, &#123;<span class="string">'name'</span>: <span class="string">'Selina'</span>, <span class="string">'gender'</span>: <span class="string">'female'</span>, <span class="string">'birthday'</span>: <span class="string">'1995-10-18'</span>&#125;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们用 open 方法读取了文本文件，同时使用了默认的读模式，编码指定为 utf-8，文件操作对象赋值为 file。接着我们调用了 file 对象的 read 方法读取了文本的所有内容，赋值为 str。然后再调用 loads 方法解析 JSON 字符串，将其转化为 JSON 对象。</p>
                  <p>这里其实也有更简便的写法，我们可以直接使用 load 方法传入文件操作对象，同样也可以将文本转化为 JSON 对象，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">data = json.load(open(<span class="string">'data.json'</span>, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">print(data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>注意这里使用的是 load 方法，而不是 loads 方法。前者的参数是一个文件操作对象，后者的参数是一个 JSON 字符串。</p>
                  <p>这两种写法的运行结果也是完全一样的。只不过 load 方法是将整个文件的内容转化为 JSON 对象，而使用 loads 方法可以更灵活地控制要转化的内容。两种方法可以在适当的场景下使用。</p>
                  <h2 id="3-输出-JSON"><a href="#3-输出-JSON" class="headerlink" title="3. 输出 JSON"></a>3. 输出 JSON</h2>
                  <p>另外，我们还可以调用 dumps 方法将 JSON 对象转化为字符串。例如，将上例中的列表重新写入文本：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">data = [&#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span>,</span><br><span class="line">    <span class="string">'birthday'</span>: <span class="string">'1992-10-18'</span></span><br><span class="line">&#125;]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(json.dumps(data))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>利用 dumps 方法，我们可以将 JSON 对象转为字符串，然后再调用文件的 write 方法写入文本，结果如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/puyn5.png" alt="写入结果"></p>
                  <p>另外，如果想保存 JSON 的格式缩进，可以再加一个参数 indent，代表缩进字符个数。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(json.dumps(data, indent=<span class="number">2</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此时写入结果如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/ryvux.png" alt="写入结果"></p>
                  <p>这样得到的内容会自动带缩进，格式会更加清晰。</p>
                  <p>另外，如果 JSON 中包含中文字符，会怎么样呢？例如，我们将之前的 JSON 的部分值改为中文，再用之前的方法写入到文本：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">data = [&#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'王伟'</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'男'</span>,</span><br><span class="line">    <span class="string">'birthday'</span>: <span class="string">'1992-10-18'</span></span><br><span class="line">&#125;]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(json.dumps(data, indent=<span class="number">2</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>写入结果如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/m571c.png" alt="写入结果"></p>
                  <p>可以看到，中文字符都变成了 Unicode 字符，这并不是我们想要的结果。</p>
                  <p>为了输出中文，还需要指定参数 ensure_ascii 为 False，另外还要规定文件输出的编码：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(json.dumps(data, indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>写入结果如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/6naa7.png" alt="写入结果"></p>
                  <p>可以发现，这样就可以输出 JSON 为中文了。</p>
                  <p>同样地，类比 loads 与 load 方法，dumps 也有对应的 dump 方法，它可以直接将 JSON 对象全部写入到文件中，因此上述的写法也可以写为如下形式：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">json.dump(data, open(<span class="string">'data.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>), indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里第一个参数就是 JSON 对象，第二个参数可以传入文件操作对象，其他的 indent、ensure_ascii 对象还是保持不变，运行效果是一样的。</p>
                  <h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2>
                  <p>本节中，我们了解了用 Python 进行 JSON 文件读写的方法，后面做数据解析时经常会用到，建议熟练掌握。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/FileStorageTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/FileStorageTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-15 17:42:44" itemprop="dateCreated datePublished" datetime="2022-02-15T17:42:44+08:00">2022-02-15</time>
                </span>
                <span id="/202242.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 方便灵活的 JSON 文本文件存储" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.4k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202232.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202232.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - parsel 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>前文我们了解了 lxml 使用 XPath 和 pyquery 使用 CSS Selector 来提取页面内容的方法，不论是 XPath 还是 CSS Selector，对于绝大多数的内容提取都足够了，大家可以选择适合自己的库来做内容提取。</p>
                  <p>不过这时候有人可能会问：我能不能二者穿插使用呀？有时候做内容提取的时候觉得 XPath 写起来比较方便，有时候觉得 CSS Selector 写起来比较方便，能不能二者结合起来使用呢？答案是可以的。</p>
                  <p>这里我们就介绍另一个解析库，叫做 parsel。</p>
                  <blockquote>
                    <p>注意：如果你用过 Scrapy 框架（后文会介绍）的话，你会发现 parsel 的 API 和 Scrapy 选择器的 API 极其相似，这是因为 Scrapy 的选择器就是基于 parsel 做了二次封装，因此学会了这个库的用法，后文 Scrapy 选择器的用法就融会贯通了。</p>
                  </blockquote>
                  <h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2>
                  <p>parsel 这个库可以对 HTML 和 XML 进行解析，并支持使用 XPath 和 CSS Selector 对内容进行提取和修改，同时它还融合了正则表达式提取的功能。功能灵活而又强大，同时它也是 Python 最流行爬虫框架 Scrapy 的底层支持。</p>
                  <h2 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h2>
                  <p>在本节开始之前，请确保已经安装好了 parsel 库，如尚未安装，可以使用 pip3 进行安装即可：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> parsel</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更详细的安装说明可以参考：<a href="https://setup.scrape.center/parsel。" target="_blank" rel="noopener">https://setup.scrape.center/parsel。</a></p>
                  <p>安装好之后，我们便可以开始本节的学习了。</p>
                  <h2 id="3-初始化"><a href="#3-初始化" class="headerlink" title="3. 初始化"></a>3. 初始化</h2>
                  <p>首先我们还是用上一节的示例 HTML，声明 html 变量如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接着，一般我们会用 parsel 的 Selector 这个类来声明一个 Selector 对象，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> parsel <span class="keyword">import</span> Selector</span><br><span class="line">selector = Selector(text=html)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们创建了一个 Selector 对象，传入了 text 参数，内容就是刚才声明的 HTML 字符串，赋值为 selector 变量。</p>
                  <p>有了 Selector 对象之后，我们可以使用 css 和 xpath 方法分别传入 CSS Selector 和 XPath 进行内容的提取，比如这里我们提取 class 包含 item-0 的节点，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">items = selector.css(<span class="string">'.item-0'</span>)</span><br><span class="line">print(len(items), type(items), items)</span><br><span class="line">items2 = selector.xpath(<span class="string">'//li[contains(@class, "item-0")]'</span>)</span><br><span class="line">print(len(items2), type(items), items2)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们先用 css 方法进行了节点提取，输出了提取结果的长度和内容，xpath 方法也是一样的写法，运行结果如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">3 &lt;class 'parsel.selector.SelectorList'&gt; [&lt;Selector xpath="descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' item-0 ')]" data='&lt;li class="item-0"&gt;first item&lt;/li&gt;'&gt;, &lt;Selector xpath="descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' item-0 ')]" data='&lt;li class="item-0 active"&gt;&lt;a href="li...'&gt;, &lt;Selector xpath="descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' item-0 ')]" data='&lt;li class="item-0"&gt;&lt;a href="link5.htm...'&gt;]</span><br><span class="line">3 &lt;class 'parsel.selector.SelectorList'&gt; [&lt;Selector xpath='//li[contains(@class, "item-0")]' data='&lt;li class="item-0"&gt;first item&lt;/li&gt;'&gt;, &lt;Selector xpath='//li[contains(@class, "item-0")]' data='&lt;li class="item-0 active"&gt;&lt;a href="li...'&gt;, &lt;Selector xpath='//li[contains(@class, "item-0")]' data='&lt;li class="item-0"&gt;&lt;a href="link5.htm...'&gt;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到两个结果都是 SelectorList 对象，它其实是一个可迭代对象。另外可以用 len 方法获取它的长度，都是 3，提取结果代表的节点其实也是一样的，都是第 1、3、5 个 li 节点，每个节点还是以 Selector 对象的形式返回了，其中每个 Selector 对象的 data 属性里面包含了提取节点的 HTML 代码。</p>
                  <p>不过这里可能大家有个疑问，第一次我们不是用 css 方法来提取的节点吗？为什么结果中的 Selector 对象还输出了 xpath 属性而不是 css 属性呢？这是因为 css 方法背后，我们传入的 CSS Selector 首先被转成了 XPath，XPath 才真正被用作节点提取。其中 CSS Selector 转换为 XPath 这个过程是在底层用 cssselect 这个库实现的，比如 <code>.item-0</code> 这个 CSS Selector 转换为 XPath 的结果就是 <code>descendant-or-self::*[@class and contains(concat(&#39; &#39;, normalize-space(@class), &#39; &#39;), &#39; item-0 &#39;)]</code>，因此输出的 Selector 对象有了 xpath 属性了。不过这个大家不用担心，这个对提取结果是没有影响的，仅仅是换了一个表示方法而已。</p>
                  <h2 id="4-提取文本"><a href="#4-提取文本" class="headerlink" title="4. 提取文本"></a>4. 提取文本</h2>
                  <p>好，既然刚才提取的结果是一个可迭代对象 SelectorList，那么要获取提取到的所有 li 节点的文本内容就要对结果进行遍历了，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> parsel <span class="keyword">import</span> Selector</span><br><span class="line">selector = Selector(text=html)</span><br><span class="line">items = selector.css(<span class="string">'.item-0'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">    text = item.xpath(<span class="string">'.//text()'</span>).get()</span><br><span class="line">    print(text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们遍历了 items 变量，赋值为 item，那么这里 item 又变成了一个 Selector 对象，那么此时我们又可以调用其 css 或 xpath 方法进行内容提取了，比如这里我们就用 <code>.//text()</code> 这个 XPath 写法提取了当前节点的所有内容，此时如果不再调用其他方法，其返回结果应该依然为 Selector 构成的可迭代对象 SelectorList。SelectorList 有一个 get 方法，get 方法可以将 SelectorList 包含的 Selector 对象中的内容提取出来。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">first</span> <span class="built_in">item</span></span><br><span class="line"><span class="keyword">third</span> <span class="built_in">item</span></span><br><span class="line"><span class="keyword">fifth</span> <span class="built_in">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 get 方法的作用是从 SelectorList 里面提取第一个 Selector 对象，然后输出其中的结果。</p>
                  <p>我们再看一个实例：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = selector.xpath(<span class="string">'//li[contains(@class, "item-0")]//text()'</span>).get()</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如下：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">first</span> <span class="built_in">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其实这里我们使用 <code>//li[contains(@class, &quot;item-0&quot;)]//text()</code> 选取了所有 class 包含 item-0 的 li 节点的文本内容。应该来说，返回结果 SelectorList 应该对应三个 li 对象，而这里 get 方法仅仅返回了第一个 li 对象的文本内容，因为其实它会只提取第一个 Selector 对象的结果。</p>
                  <p>那有没有能提取所有 Selector 的对应内容的方法呢？有，那就是 getall 方法。</p>
                  <p>所以如果要提取所有对应的 li 节点的文本内容的话，写法可以改写为如下内容：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = selector.xpath(<span class="string">'//li[contains(@class, "item-0")]//text()'</span>).getall()</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'first item'</span>, <span class="string">'third item'</span>, <span class="string">'fifth item'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候，我们就能得到列表类型结果了，和 Selector 对象是一一对应的。</p>
                  <p>因此，如果要提取 SelectorList 里面对应的结果，可以使用 get 或 getall 方法，前者会获取第一个 Selector 对象里面的内容，后者会依次获取每个 Selector 对象对应的结果。</p>
                  <p>另外上述案例中，xpath 方法改写成 css 方法，可以这么实现：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = selector.css(<span class="string">'.item-0 *::text'</span>).getall()</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里<code>*</code> 用来提取所有子节点（包括纯文本节点），提取文本需要再加上<code>::text</code>，最终的运行结果是一样的。</p>
                  <p>到这里我们就简单了解了文本提取的方法。</p>
                  <h2 id="5-提取属性"><a href="#5-提取属性" class="headerlink" title="5. 提取属性"></a>5. 提取属性</h2>
                  <p>刚才我们演示了 HTML 中文本的提取，直接在 XPath 中加入 <code>//text()</code> 即可，那提取属性怎么做呢？类似的方式，也直接在 XPath 或者 CSS Selector 中表示出来就好了。</p>
                  <p>比如我们提取第三个 li 节点内部的 a 节点的 href 属性，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> parsel <span class="keyword">import</span> Selector</span><br><span class="line">selector = Selector(text=html)</span><br><span class="line">result = selector.css(<span class="string">'.item-0.active a::attr(href)'</span>).get()</span><br><span class="line">print(result)</span><br><span class="line">result = selector.xpath(<span class="string">'//li[contains(@class, "item-0") and contains(@class, "active")]/a/@href'</span>).get()</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们实现了两种写法，分别用 css 和 xpath 方法实现。我们根据同时包含 item-0 和 active 这两个 class 为依据来选取第三个 li 节点，然后进一步选取了里面的 a 节点，对于 CSS Selector，选取属性需要加 <code>::attr()</code> 并传入对应的属性名称来选取，对于 XPath，直接用 <code>/@</code> 再加属性名称即可选取。最后统一用 get 方法提取结果即可。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">link3</span><span class="selector-class">.html</span></span><br><span class="line"><span class="selector-tag">link3</span><span class="selector-class">.html</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到两种方法都正确提取到了对应的 href 属性。</p>
                  <h2 id="6-正则提取"><a href="#6-正则提取" class="headerlink" title="6. 正则提取"></a>6. 正则提取</h2>
                  <p>除了常用的 css 和 xpath 方法，Selector 对象还提供了正则表达式提取方法，我们用一个实例来了解下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> parsel <span class="keyword">import</span> Selector</span><br><span class="line">selector = Selector(text=html)</span><br><span class="line">result = selector.css(<span class="string">'.item-0'</span>).re(<span class="string">'link.*'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们先用 css 方法提取了所有 class 包含 item-0 的节点，然后使用 re 方法，传入了 <code>link.*</code>，用来匹配包含 link 的所有结果。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;'</span>, <span class="string">'link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，re 方法在这里遍历了所有提取到的 Selector 对象，然后根据传入的正则表达式查找出符合规则的节点源码并以列表的形式返回。</p>
                  <p>当然如果在调用 css 方法时已经提取了进一步的结果，比如提取了节点文本值，那么 re 方法就只会针对节点文本值进行提取：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> parsel <span class="keyword">import</span> Selector</span><br><span class="line">selector = Selector(text=html)</span><br><span class="line">result = selector.css(<span class="string">'.item-0 *::text'</span>).re(<span class="string">'.*item'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight scheme">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="symbol">'first</span> item', <span class="symbol">'third</span> item', <span class="symbol">'fifth</span> item']</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外我们也可以利用 re_first 方法来提取第一个符合规则的结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> parsel <span class="keyword">import</span> Selector</span><br><span class="line">selector = Selector(text=html)</span><br><span class="line">result = selector.css(<span class="string">'.item-0'</span>).re_first(<span class="string">'&lt;span class="bold"&gt;(.*?)&lt;/span&gt;'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里调用了 re_first 方法，这里提取的是被 span 标签包含的文本值，提取结果用小括号括起来表示一个提取分组，最后输出的结果就是小括号部分对应的结果，运行结果如下：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">third</span> <span class="built_in">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过这几个例子我们知道了正则匹配的一些使用方法，re 对应多个结果，re_first 对应单个结果，可以在不同情况下选择对应的方法进行提取。</p>
                  <h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2>
                  <p>parsel 是一个融合了 XPath、CSS Selector 和正则表达式的提取库，功能强大又灵活，建议好好学习一下，同时也可以为后文学习 Scrapy 框架打下基础，有关 parsel 更多的用法可以参考其官方文档：<a href="https://parsel.readthedocs.io/。" target="_blank" rel="noopener">https://parsel.readthedocs.io/。</a></p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/ParselTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ParselTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-15 08:33:32" itemprop="dateCreated datePublished" datetime="2022-02-15T08:33:32+08:00">2022-02-15</time>
                </span>
                <span id="/202232.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - parsel 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>6.4k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>6 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202252.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202252.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - Ajax 分析方法</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>这里还以前面的微博为例，我们知道拖动刷新的内容由 Ajax 加载，而且页面的 URL 没有变化，那么应该到哪里去查看这些 Ajax 请求呢？</p>
                  <h2 id="1-分析案例"><a href="#1-分析案例" class="headerlink" title="1. 分析案例"></a>1. 分析案例</h2>
                  <p>这里还需要借助浏览器的开发者工具，下面以 Chrome 浏览器为例来介绍。</p>
                  <p>首先，用 Chrome 浏览器打开微博的链接 <a href="https://m.weibo.cn/u/2830678474" target="_blank" rel="noopener">https://m.weibo.cn/u/2830678474</a>，随后在页面中点击鼠标右键，从弹出的快捷菜单中选择，随后在页面中点击鼠标右键，从弹出的快捷菜单中选择) “检查” 选项，此时便会弹出开发者工具，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/7040b.png" alt=""></p>
                  <p>前面也提到过，这里其实就是在页面加载过程中浏览器与服务器之间发送请求和接收响应的所有记录。</p>
                  <p>Ajax 其实有其特殊的请求类型，它叫作 xhr。在图中我们可以发现一个名称以 getIndex 开头的请求，其 Type 为 xhr，这就是一个 Ajax 请求。用鼠标点击这个请求，可以查看这个请求的详细信息。</p>
                  <p><img src="https://cdn.cuiqingcai.com/1kiqe.png" alt=""></p>
                  <p>在右侧可以观察到其 Request Headers、URL 和 Response Headers 等信息。其中 Request Headers 中有一个信息为 <code>X-Requested-With:XMLHttpRequest</code>，这就标记了此请求是 Ajax 请求，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/t4hm0.png" alt=""></p>
                  <p>随后点击一下 Preview，即可看到响应的内容，它是 JSON 格式的。这里 Chrome 为我们自动做了解析，点击箭头即可展开和收起相应内容。</p>
                  <p>观察可以发现，这里的返回结果是我的个人信息，如昵称、简介、头像等，这也是用来渲染个人主页所使用的数据。JavaScript 接收到这些数据之后，再执行相应的渲染方法，整个页面就渲染出来了。</p>
                  <p><img src="https://cdn.cuiqingcai.com/kah0s.png" alt=""></p>
                  <p>另外，也可以切换到 Response 选项卡，从中观察到真实的返回数据，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/l1z1j.png" alt=""></p>
                  <p>接下来，切回到第一个请求，观察一下它的 Response 是什么，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/yfn4s.png" alt=""></p>
                  <p>这是最原始的链接 <a href="https://m.weibo.cn/u/2830678474" target="_blank" rel="noopener">https://m.weibo.cn/u/2830678474</a> 返回的结果，其代码只有不到 50 行，结构也非常简单，只是执行了一些 JavaScript。</p>
                  <p>所以说，我们看到的微博页面的真实数据并不是最原始的页面返回的，而是后来执行 JavaScript 后再次向后台发送了 Ajax 请求，浏览器拿到数据后再进一步渲染出来的。</p>
                  <h2 id="2-过滤请求"><a href="#2-过滤请求" class="headerlink" title="2. 过滤请求"></a>2. 过滤请求</h2>
                  <p>接下来，再利用 Chrome 开发者工具的筛选功能筛选出所有的 Ajax 请求。在请求的上方有一层筛选栏，直接点击 XHR，此时在下方显示的所有请求便都是 Ajax 请求了，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/0xqyh.png" alt=""></p>
                  <p>接下来，不断滑动页面，可以看到页面底部有一条条新的微博被刷出，而开发者工具下方也一个个地出现 Ajax 请求，这样我们就可以捕获到所有的 Ajax 请求了。</p>
                  <p>随意点开一个条目，都可以清楚地看到其 Request URL、Request Headers、Response Headers、Response Body 等内容，此时想要模拟请求和提取就非常简单了。</p>
                  <p>下图所示的内容便是我的某一页微博的列表信息：</p>
                  <p><img src="https://cdn.cuiqingcai.com/3gv1x.png" alt=""></p>
                  <p>到现在为止，我们已经可以分析出 Ajax 请求的一些详细信息了，接下来只需要用程序模拟这些 Ajax 请求，就可以轻松提取我们所需要的信息了。</p>
                  <h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2>
                  <p>本节我们介绍了 Ajax 的基本原理和分析方法，在下一节中，我们用一个正式的实例来实现一下 Ajax 数据的爬取。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-14 21:11:31" itemprop="dateCreated datePublished" datetime="2022-02-14T21:11:31+08:00">2022-02-14</time>
                </span>
                <span id="/202252.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - Ajax 分析方法" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202241.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202241.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 简易的TXT纯文本文件存储</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>将数据保存到 TXT 文本的操作非常简单，而且 TXT 文本几乎兼容任何平台，但是这有个缺点，那就是不利于检索。所以如果对检索和数据结构要求不高，追求方便第一的话，可以采用 TXT 文本存储。</p>
                  <p>本节中，我们就来看下利用 Python 保存 TXT 文本文件的方法。</p>
                  <h2 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h2>
                  <p>本节我们以电影示例网站 <a href="https://ssr1.scrape.center/" target="_blank" rel="noopener">https://ssr1.scrape.center/</a> 为例，爬取首页 10 部电影的数据，然后将相关信息存储为 TXT 文本格式。</p>
                  <h2 id="2-基本实例"><a href="#2-基本实例" class="headerlink" title="2. 基本实例"></a>2. 基本实例</h2>
                  <p>首先，可以用 requests 将网页源代码获取下来，然后使用 pyquery 解析库解析，接下来将提取的电影名称、类别、上映时间等信息保存到 TXT 文本中，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://ssr1.scrape.center/'</span></span><br><span class="line">html = requests.get(url).text</span><br><span class="line">doc = pq(html)</span><br><span class="line">items = doc(<span class="string">'.el-card'</span>).items()</span><br><span class="line"></span><br><span class="line">file = open(<span class="string">'movies.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">    <span class="comment"># 电影名称</span></span><br><span class="line">    name = item.find(<span class="string">'a &gt; h2'</span>).text()</span><br><span class="line">    file.write(<span class="string">f'名称: <span class="subst">&#123;name&#125;</span>\n'</span>)</span><br><span class="line">    <span class="comment"># 类别</span></span><br><span class="line">    categories = [item.text() <span class="keyword">for</span> item <span class="keyword">in</span> item.find(<span class="string">'.categories button span'</span>).items()]</span><br><span class="line">    file.write(<span class="string">f'类别: <span class="subst">&#123;categories&#125;</span>\n'</span>)</span><br><span class="line">    <span class="comment"># 上映时间</span></span><br><span class="line">    published_at = item.find(<span class="string">'.info:contains(上映)'</span>).text()</span><br><span class="line">    published_at = re.search(<span class="string">'(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)'</span>, published_at).group(<span class="number">1</span>) \</span><br><span class="line">        <span class="keyword">if</span> published_at <span class="keyword">and</span> re.search(<span class="string">'\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;'</span>, published_at) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    file.write(<span class="string">f'上映时间: <span class="subst">&#123;published_at&#125;</span>\n'</span>)</span><br><span class="line">    <span class="comment"># 评分</span></span><br><span class="line">    score = item.find(<span class="string">'p.score'</span>).text()</span><br><span class="line">    file.write(<span class="string">f'评分: <span class="subst">&#123;score&#125;</span>\n'</span>)</span><br><span class="line">    file.write(<span class="string">f'<span class="subst">&#123;<span class="string">"="</span> * <span class="number">50</span>&#125;</span>\n'</span>)</span><br><span class="line">file.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里主要是为了演示文件保存的方式，因此 requests 异常处理部分在此省去。首先，用 requests 提取首页的 HTML 代码，然后利用 pyquery 将电影的名称、类别、上映时间、评分信息提取出来。</p>
                  <p>利用 Python 提供的 open 方法打开一个文本文件，获取一个文件操作对象，这里赋值为 file，每提取一部分信息，就利用 file 对象的 write 方法将提取的内容写入文件。</p>
                  <p>全部提取完毕之后，最后调用 close 方法将其关闭，这样抓取的内容即可成功写入文本中了。</p>
                  <p>运行程序，可以发现在本地生成了一个 movies.txt 文件，其内容如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/uwg48.png" alt="image-20200531171232808"></p>
                  <p>这样电影信息的内容就被保存成文本形式了。</p>
                  <p>回过头来我们看下本节重点需要了解的内容，就是文本写入操作，其实就是 open、write、close 这三个方法的用法。</p>
                  <p>这里 open 方法的第一个参数即要保存的目标文件名称；第二个参数为 w，代表以覆盖写入的方式写入文本；另外，我们还指定了文件的编码为 utf-8。最后，写入完成后，还需要调用 close 方法来关闭文件对象。</p>
                  <h2 id="3-打开方式"><a href="#3-打开方式" class="headerlink" title="3. 打开方式"></a>3. 打开方式</h2>
                  <p>在刚才的实例中，open 方法的第二个参数设置成了 w，这样在每次写入文本时会清空源文件，然后将新的内容写入文件，这是一种文件打开方式。关于文件的打开方式，其实还有其他几种，这里简要介绍一下。</p>
                  <ul>
                    <li>r：以只读方式打开文件，意思就是只能读取文件内容，不能写入文件内容。这是默认模式。</li>
                    <li>rb：以二进制只读方式打开一个文件，通常用于打开二进制文件，比如音频、图片、视频等等。</li>
                    <li>r+：以读写方式打开一个文件，既可以读文件又可以写文件。</li>
                    <li>rb+：以二进制读写方式打开一个文件，同样既可以读又可以写，但读取和写入的都是二进制数据。</li>
                    <li>w：以写入方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li>
                    <li>wb：以二进制写入方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li>
                    <li>w+：以读写方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li>
                    <li>wb+：以二进制读写格式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li>
                    <li>a：以追加方式打开一个文件。如果该文件已存在，文件指针将会放在文件结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，则创建新文件来写入。</li>
                    <li>ab：以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，则创建新文件来写入。</li>
                    <li>a+：以读写方式打开一个文件。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，则创建新文件来读写。</li>
                    <li>ab+：以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾。如果该文件不存在，则创建新文件用于读写。</li>
                  </ul>
                  <h2 id="4-简化写法"><a href="#4-简化写法" class="headerlink" title="4. 简化写法"></a>4. 简化写法</h2>
                  <p>另外，文件写入还有一种简写方法，那就是使用 with as 语法。在 with 控制块结束时，文件会自动关闭，所以就不需要再调用 close 方法了。</p>
                  <p>这种保存方式可以简写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">with</span> open(<span class="string">'movies.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>):</span><br><span class="line">    file.write(<span class="string">f'名称: <span class="subst">&#123;name&#125;</span>\n'</span>)</span><br><span class="line">    file.write(<span class="string">f'类别: <span class="subst">&#123;categories&#125;</span>\n'</span>)</span><br><span class="line">    file.write(<span class="string">f'上映时间: <span class="subst">&#123;published_at&#125;</span>\n'</span>)</span><br><span class="line">    file.write(<span class="string">f'评分: <span class="subst">&#123;score&#125;</span>\n'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>上面便是利用 Python 将结果保存为 TXT 文件的方法，这种方法简单易用，操作高效，是一种最基本的保存数据的方法。</p>
                  <h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2>
                  <p>本节我们了解了基本 TXT 文件存储的实现方式，建议熟练掌握。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/FileStorageTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/FileStorageTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-14 15:33:32" itemprop="dateCreated datePublished" datetime="2022-02-14T15:33:32+08:00">2022-02-14</time>
                </span>
                <span id="/202241.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 简易的TXT纯文本文件存储" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202231.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202231.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 网页解析利器 XPath</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>XPath，全称是 XML Path Language，即 XML 路径语言，它是一门在 XML 文档中查找信息的语言。它最初是用来搜寻 XML 文档的，但是它同样适用于 HTML 文档的搜索。</p>
                  <p>所以在做爬虫时，我们完全可以使用 XPath 来做相应的信息抽取。本节我们就来了解下 XPath 的基本用法。</p>
                  <h2 id="1-XPath-概览"><a href="#1-XPath-概览" class="headerlink" title="1. XPath 概览"></a>1. XPath 概览</h2>
                  <p>XPath 的选择功能十分强大，它提供了非常简洁明了的路径选择表达式。另外，它还提供了超过 100 个内建函数，用于字符串、数值、时间的匹配以及节点、序列的处理等。几乎所有我们想要定位的节点，都可以用 XPath 来选择。</p>
                  <p>XPath 于 1999 年 11 月 16 日成为 W3C 标准，它被设计为供 XSLT、XPointer 以及其他 XML 解析软件使用，更多的文档可以访问其官方网站：<a href="https://www.w3.org/TR/xpath/" target="_blank" rel="noopener">https://www.w3.org/TR/xpath/</a>。</p>
                  <h2 id="2-XPath-常用规则"><a href="#2-XPath-常用规则" class="headerlink" title="2. XPath 常用规则"></a>2. XPath 常用规则</h2>
                  <p>下表列举了 XPath 的几个常用规则。</p>
                  <p>表 XPath 常用规则</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">表　达　式</th>
                          <th style="text-align:left">描　　述</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>nodename</code></td>
                          <td style="text-align:left">选取此节点的所有子节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>/</code></td>
                          <td style="text-align:left">从当前节点选取直接子节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>//</code></td>
                          <td style="text-align:left">从当前节点选取子孙节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>.</code></td>
                          <td style="text-align:left">选取当前节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>..</code></td>
                          <td style="text-align:left">选取当前节点的父节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>@</code></td>
                          <td style="text-align:left">选取属性</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>这里列出了 XPath 的常用匹配规则，示例如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#x2F;&#x2F;title[@lang&#x3D;&#39;eng&#39;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是一个 XPath 规则，它代表选择所有名称为 <code>title</code>，同时属性 <code>lang</code> 的值为 <code>eng</code> 的节点。</p>
                  <p>后面会通过 Python 的 lxml 库，利用 XPath 进行 HTML 的解析。</p>
                  <h2 id="3-准备工作"><a href="#3-准备工作" class="headerlink" title="3. 准备工作"></a>3. 准备工作</h2>
                  <p>使用之前，首先要确保安装好 lxml 库。如尚未安装，可以使用 pip3 来安装：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> lxml</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更详细的安装说明可以参考：<a href="https://setup.scrape.center/lxml。" target="_blank" rel="noopener">https://setup.scrape.center/lxml。</a></p>
                  <p>安装完成之后，我们就可以进行接下来的学习了。</p>
                  <h2 id="4-实例引入"><a href="#4-实例引入" class="headerlink" title="4. 实例引入"></a>4. 实例引入</h2>
                  <p>现在通过实例来感受一下使用 XPath 对网页进行解析的过程，相关代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = etree.tostring(html)</span><br><span class="line">print(result.decode(<span class="string">'utf-8'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先导入 lxml 库的 etree 模块，然后声明了一段 HTML 文本，调用 HTML 类进行初始化，这样就成功构造了一个 XPath 解析对象。这里需要注意的是，HTML 文本中的最后一个 <code>li</code> 节点是没有闭合的，但是 etree 模块可以自动修正 HTML 文本。</p>
                  <p>这里我们调用 <code>tostring</code> 方法即可输出修正后的 HTML 代码，但是结果是 <code>bytes</code> 类型。这里利用 <code>decode</code> 方法将其转成 <code>str</code> 类型，结果如下：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link1.html"</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-1"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link2.html"</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-inactive"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link3.html"</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-1"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link4.html"</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link5.html"</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，经过处理之后，<code>li</code> 节点标签被补全，并且还自动添加了 <code>body</code>、<code>html</code> 节点。</p>
                  <p>另外，也可以直接读取文本文件进行解析，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = etree.tostring(html)</span><br><span class="line">print(result.decode(<span class="string">'utf-8'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中 test.html 的内容就是上面例子中的 HTML 代码，内容如下：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link1.html"</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-1"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link2.html"</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-inactive"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link3.html"</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-1"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link4.html"</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link5.html"</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次的输出结果略有不同，多了一个 <code>DOCTYPE</code> 声明，不过对解析无任何影响，结果如下：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span> <span class="meta-keyword">PUBLIC</span> <span class="meta-string">"-//W3C//DTD HTML 4.0 Transitional//EN"</span> <span class="meta-string">"http://www.w3.org/TR/REC-html40/loose.dtd"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link1.html"</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-1"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link2.html"</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-inactive"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link3.html"</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-1"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link4.html"</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link5.html"</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h2 id="5-所有节点"><a href="#5-所有节点" class="headerlink" title="5. 所有节点"></a>5. 所有节点</h2>
                  <p>我们一般会用 <code>//</code> 开头的 XPath 规则来选取所有符合要求的节点。这里以前面的 HTML 文本为例，如果要选取所有节点，可以这样实现：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//*'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&lt;Element html at <span class="number">0x10510d9c8</span>&gt;, &lt;Element body at <span class="number">0x10510da08</span>&gt;, &lt;Element div at <span class="number">0x10510da48</span>&gt;, &lt;Element ul at <span class="number">0x10510da88</span>&gt;, &lt;Element li at <span class="number">0x10510dac8</span>&gt;, &lt;Element a at <span class="number">0x10510db48</span>&gt;, &lt;Element li at <span class="number">0x10510db88</span>&gt;, &lt;Element a at <span class="number">0x10510dbc8</span>&gt;, &lt;Element li at <span class="number">0x10510dc08</span>&gt;, &lt;Element a at <span class="number">0x10510db08</span>&gt;, &lt;Element li at <span class="number">0x10510dc48</span>&gt;, &lt;Element a at <span class="number">0x10510dc88</span>&gt;, &lt;Element li at <span class="number">0x10510dcc8</span>&gt;, &lt;Element a at <span class="number">0x10510dd08</span>&gt;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里使用 <code>*</code> 代表匹配所有节点，也就是整个 HTML 文本中的所有节点都会被获取。可以看到，返回形式是一个列表，每个元素是 <code>Element</code> 类型，其后跟了节点的名称，如 <code>html</code>、<code>body</code>、<code>div</code>、<code>ul</code>、<code>li</code>、<code>a</code> 等，所有节点都包含在列表中了。</p>
                  <p>当然，此处匹配也可以指定节点名称。如果想获取所有 <code>li</code> 节点，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li'</span>)</span><br><span class="line">print(result)</span><br><span class="line">print(result[<span class="number">0</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里要选取所有 <code>li</code> 节点，可以使用 <code>//</code>，然后直接加上节点名称即可，调用时直接使用 <code>xpath</code> 方法即可。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&lt;Element li at <span class="number">0x105849208</span>&gt;, &lt;Element li at <span class="number">0x105849248</span>&gt;, &lt;Element li at <span class="number">0x105849288</span>&gt;, &lt;Element li at <span class="number">0x1058492c8</span>&gt;, &lt;Element li at <span class="number">0x105849308</span>&gt;]</span><br><span class="line">&lt;Element li at <span class="number">0x105849208</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里可以看到，提取结果是一个列表形式，其中每个元素都是一个 <code>Element</code> 对象。如果要取出其中一个对象，可以直接用中括号加索引，如 <code>[0]</code>。</p>
                  <h2 id="6-子节点"><a href="#6-子节点" class="headerlink" title="6. 子节点"></a>6. 子节点</h2>
                  <p>我们通过 <code>/</code> 或 <code>//</code> 即可查找元素的子节点或子孙节点。假如现在想选择 <code>li</code> 节点的所有直接子节点 <code>a</code>，可以这样实现：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里通过追加 <code>/a</code> 即选择了所有 <code>li</code> 节点的所有直接子节点 <code>a</code>。因为 <code>//li</code> 用于选中所有 <code>li</code> 节点，<code>/a</code> 用于选中 <code>li</code> 节点的所有直接子节点 <code>a</code>，二者组合在一起即获取所有 <code>li</code> 节点的所有直接子节点 <code>a</code>。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&lt;Element a at <span class="number">0x106ee8688</span>&gt;, &lt;Element a at <span class="number">0x106ee86c8</span>&gt;, &lt;Element a at <span class="number">0x106ee8708</span>&gt;, &lt;Element a at <span class="number">0x106ee8748</span>&gt;, &lt;Element a at <span class="number">0x106ee8788</span>&gt;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此处的 <code>/</code> 用于选取直接子节点，如果要获取所有子孙节点，就可以使用 <code>//</code>。例如，要获取 <code>ul</code> 节点下的所有子孙节点 <code>a</code>，可以这样实现：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//ul//a'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果是相同的。</p>
                  <p>但是如果这里用 <code>//ul/a</code>，就无法获取任何结果了。因为 <code>/</code> 用于获取直接子节点，而在 <code>ul</code> 节点下没有直接的 <code>a</code> 子节点，只有 <code>li</code> 节点，所以无法获取任何匹配结果，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//ul/a'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>因此，这里我们要注意 <code>/</code> 和 <code>//</code> 的区别，其中 <code>/</code> 用于获取直接子节点，<code>//</code> 用于获取子孙节点。</p>
                  <h2 id="7-父节点"><a href="#7-父节点" class="headerlink" title="7. 父节点"></a>7. 父节点</h2>
                  <p>我们知道通过连续的 <code>/</code> 或 <code>//</code> 可以查找子节点或子孙节点，那么假如我们知道了子节点，怎样来查找父节点呢？这可以用 <code>..</code> 来实现。</p>
                  <p>比如，现在首先选中 <code>href</code> 属性为 <code>link4.html</code> 的 <code>a</code> 节点，然后获取其父节点，再获取其 <code>class</code> 属性，相关代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/../@class'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&#39;item-1&#39;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>检查一下结果发现，这正是我们获取的目标 <code>li</code> 节点的 <code>class</code> 属性。</p>
                  <p>同时，我们也可以通过 <code>parent::</code> 来获取父节点，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/parent::*/@class'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h2 id="8-属性匹配"><a href="#8-属性匹配" class="headerlink" title="8. 属性匹配"></a>8. 属性匹配</h2>
                  <p>在选取的时候，我们还可以用 <code>@</code> 符号进行属性过滤。比如，这里如果要选取 <code>class</code> 为 <code>item-0</code> 的 <code>li</code> 节点，可以这样实现:</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们通过加入 <code>[@class=&quot;item-0&quot;]</code>，限制了节点的 <code>class</code> 属性为 <code>item-0</code>，而 HTML 文本中符合条件的 <code>li</code> 节点有两个，所以结果应该返回两个匹配到的元素。结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;Element li at <span class="number">0x10a399288</span>&gt;, &lt;Element li at <span class="number">0x10a3992c8</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可见，匹配结果正是两个，至于是不是那正确的两个，后面再验证。</p>
                  <h2 id="9-文本获取"><a href="#9-文本获取" class="headerlink" title="9. 文本获取"></a>9. 文本获取</h2>
                  <p>我们用 XPath 中的 <code>text</code> 方法获取节点中的文本，接下来尝试获取前面 <code>li</code> 节点中的文本，相关代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]/text()'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&#39;\n     &#39;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>奇怪的是，我们并没有获取到任何文本，只获取到了一个换行符，这是为什么呢？因为 XPath 中 <code>text</code> 方法前面是 <code>/</code>，而此处 <code>/</code> 的含义是选取直接子节点，很明显 <code>li</code> 的直接子节点都是 <code>a</code> 节点，文本都是在 <code>a</code> 节点内部的，所以这里匹配到的结果就是被修正的 <code>li</code> 节点内部的换行符，因为自动修正的 <code>li</code> 节点的尾标签换行了。</p>
                  <p>即选中的是这两个节点：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link1.html"</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"item-0"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"link5.html"</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中一个节点因为自动修正，<code>li</code> 节点的尾标签添加的时候换行了，所以提取文本得到的唯一结果就是 <code>li</code> 节点的尾标签和 <code>a</code> 节点的尾标签之间的换行符。</p>
                  <p>因此，如果想获取 <code>li</code> 节点内部的文本，就有两种方式，一种是先选取 <code>a</code> 节点再获取文本，另一种就是使用 <code>//</code>。接下来，我们来看下二者的区别。</p>
                  <p>首先，选取 <code>a</code> 节点再获取文本，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&#39;first item&#39;, &#39;fifth item&#39;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里的返回值是两个，内容都是属性为 <code>item-0</code> 的 <code>li</code> 节点的文本，这也印证了前面属性匹配的结果是正确的。</p>
                  <p>这里我们是逐层选取的，先选取了 <code>li</code> 节点，又利用 <code>/</code> 选取了其直接子节点 <code>a</code>，然后再选取其文本，得到的结果恰好是符合我们预期的两个结果。</p>
                  <p>再来看下用另一种方式（即使用 <code>//</code>）选取的结果，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]//text()'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'first item'</span>, <span class="string">'fifth item'</span>, <span class="string">'\n     '</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>不出所料，这里的返回结果是 3 个。可想而知，这里是选取所有子孙节点的文本，其中前两个就是 <code>li</code> 的子节点 <code>a</code> 内部的文本，另外一个就是最后一个 <code>li</code> 节点内部的文本，即换行符。</p>
                  <p>所以说，如果要想获取子孙节点内部的所有文本，可以直接用 <code>//</code> 加 <code>text</code> 方法的方式，这样可以保证获取到最全面的文本信息，但是可能会夹杂一些换行符等特殊字符。如果想获取某些特定子孙节点下的所有文本，可以先选取到特定的子孙节点，然后再调用 <code>text</code> 方法获取其内部文本，这样可以保证获取的结果是整洁的。</p>
                  <h2 id="10-属性获取"><a href="#10-属性获取" class="headerlink" title="10. 属性获取"></a>10. 属性获取</h2>
                  <p>我们知道用 <code>text</code> 方法可以获取节点内部文本，那么节点属性该怎样获取呢？其实还是用 <code>@</code> 符号就可以。例如，我们想获取所有 <code>li</code> 节点下所有 <code>a</code> 节点的 <code>href</code> 属性，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a/@href'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们通过 <code>@href</code> 即可获取节点的 <code>href</code> 属性。注意，此处和属性匹配的方法不同，属性匹配是中括号加属性名和值来限定某个属性，如 <code>[@href=&quot;link1.html&quot;]</code>，而此处的 <code>@href</code> 指的是获取节点的某个属性，二者需要做好区分。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'link1.html'</span>, <span class="string">'link2.html'</span>, <span class="string">'link3.html'</span>, <span class="string">'link4.html'</span>, <span class="string">'link5.html'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，我们成功获取了所有 <code>li</code> 节点下 <code>a</code> 节点的 <code>href</code> 属性，它们以列表形式返回。</p>
                  <h2 id="11-属性多值匹配"><a href="#11-属性多值匹配" class="headerlink" title="11. 属性多值匹配"></a>11. 属性多值匹配</h2>
                  <p>有时候，某些节点的某个属性可能有多个值，例如：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="li"]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 HTML 文本中 <code>li</code> 节点的 <code>class</code> 属性有两个值 <code>li</code> 和 <code>li-first</code>，此时如果还想用之前的属性匹配获取，就无法匹配了，此时的运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时就需要用 <code>contains</code> 方法了，代码可以改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li")]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样通过 <code>contains</code> 方法，给其第一个参数传入属性名称，第二个参数传入属性值，只要此属性包含所传入的属性值，就可以完成匹配了。</p>
                  <p>此时运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&#39;first item&#39;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此种方式在某个节点的某个属性有多个值时经常用到，如某个节点的 <code>class</code> 属性通常有多个。</p>
                  <h2 id="12-多属性匹配"><a href="#12-多属性匹配" class="headerlink" title="12. 多属性匹配"></a>12. 多属性匹配</h2>
                  <p>另外，我们可能还遇到一种情况，那就是根据多个属性确定一个节点，这时就需要同时匹配多个属性。此时可以使用运算符 <code>and</code> 来连接，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li") and @name="item"]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的 <code>li</code> 节点又增加了一个属性 <code>name</code>。要确定这个节点，需要同时根据 <code>class</code> 和 <code>name</code> 属性来选择，一个条件是 <code>class</code> 属性里面包含 <code>li</code> 字符串，另一个条件是 <code>name</code> 属性为 <code>item</code> 字符串，二者需要同时满足，需要用 <code>and</code> 操作符相连，相连之后置于中括号内进行条件筛选。运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&#39;first item&#39;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的 <code>and</code> 其实是 XPath 中的运算符。另外，还有很多运算符，如 <code>or</code>、<code>mod</code> 等，在此总结为表 3-。</p>
                  <p>表 3- 运算符及其介绍</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">运算符</th>
                          <th style="text-align:left">描　　述</th>
                          <th style="text-align:left">实　　例</th>
                          <th style="text-align:left">返　回　值</th>
                          <th style="text-align:left"></th>
                          <th style="text-align:left"></th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>or</code></td>
                          <td style="text-align:left">或</td>
                          <td style="text-align:left"><code>age=19 or age=20</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 19，则返回 <code>true</code>。如果 <code>age</code> 是 21，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>and</code></td>
                          <td style="text-align:left">与</td>
                          <td style="text-align:left"><code>age&gt;19 and age&lt;21</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 20，则返回 <code>true</code>。如果 <code>age</code> 是 18，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>mod</code></td>
                          <td style="text-align:left">计算除法的余数</td>
                          <td style="text-align:left"><code>5 mod 2</code></td>
                          <td style="text-align:left">1</td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left">`</td>
                          <td style="text-align:left">`</td>
                          <td style="text-align:left">计算两个节点集</td>
                          <td style="text-align:left">`//book</td>
                          <td style="text-align:left">//cd`</td>
                          <td style="text-align:left">返回所有拥有 <code>book</code> 和 <code>cd</code> 元素的节点集</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>+</code></td>
                          <td style="text-align:left">加法</td>
                          <td style="text-align:left"><code>6 + 4</code></td>
                          <td style="text-align:left">10</td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>-</code></td>
                          <td style="text-align:left">减法</td>
                          <td style="text-align:left"><code>6 - 4</code></td>
                          <td style="text-align:left">2</td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>*</code></td>
                          <td style="text-align:left">乘法</td>
                          <td style="text-align:left"><code>6 * 4</code></td>
                          <td style="text-align:left">24</td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>div</code></td>
                          <td style="text-align:left">除法</td>
                          <td style="text-align:left"><code>8 div 4</code></td>
                          <td style="text-align:left">2</td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>=</code></td>
                          <td style="text-align:left">等于</td>
                          <td style="text-align:left"><code>age=19</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 19，则返回 <code>true</code>。如果 <code>age</code> 是 20，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>!=</code></td>
                          <td style="text-align:left">不等于</td>
                          <td style="text-align:left"><code>age!=19</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 18，则返回 <code>true</code>。如果 <code>age</code> 是 19，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>&lt;</code></td>
                          <td style="text-align:left">小于</td>
                          <td style="text-align:left"><code>age&lt;19</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 18，则返回 <code>true</code>。如果 <code>age</code> 是 19，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>&lt;=</code></td>
                          <td style="text-align:left">小于或等于</td>
                          <td style="text-align:left"><code>&lt;=19</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 19，则返回 <code>true</code>。如果 <code>age</code> 是 <code>20</code>，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>&gt;</code></td>
                          <td style="text-align:left">大于</td>
                          <td style="text-align:left"><code>age&gt;19</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 20，则返回 <code>true</code>。如果 <code>age</code> 是 19，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                          <td style="text-align:left"></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>&gt;=</code></td>
                          <td style="text-align:left">大于或等于</td>
                          <td style="text-align:left"><code>age&gt;=19</code></td>
                          <td style="text-align:left">如果 <code>age</code> 是 19，则返回 <code>true</code>。如果 <code>age</code> 是 18，则返回 <code>false</code></td>
                          <td style="text-align:left"></td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>此表参考来源：<a href="http://www.w3school.com.cn/xpath/xpath_operators.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_operators.asp</a>。</p>
                  <h2 id="13-按序选择"><a href="#13-按序选择" class="headerlink" title="13. 按序选择"></a>13. 按序选择</h2>
                  <p>有时候，我们在选择的时候某些属性可能同时匹配了多个节点，但是只想要其中的某个节点，如第二个节点或者最后一个节点，这时该怎么办呢？</p>
                  <p>这时可以利用中括号传入索引的方法获取特定次序的节点，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/a/text()'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[last()]/a/text()'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[position()&lt;3]/a/text()'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[last()-2]/a/text()'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一次选择时，我们选取了第一个 <code>li</code> 节点，中括号中传入数字 1 即可。注意，这里和代码中不同，序号是以 1 开头的，不是以 0 开头。</p>
                  <p>第二次选择时，我们选取了最后一个 <code>li</code> 节点，中括号中调用 <code>last</code> 方法即可。</p>
                  <p>第三次选择时，我们选取了位置小于 3 的 <code>li</code> 节点，也就是位置序号为 1 和 2 的节点，得到的结果就是前两个 <code>li</code> 节点。</p>
                  <p>第四次选择时，我们选取了倒数第三个 <code>li</code> 节点，中括号中调用 <code>last</code> 方法再减去 2 即可。因为 <code>last</code> 方法代表最后一个，在此基础减 2 就是倒数第三个。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'first item'</span>]</span><br><span class="line">[<span class="string">'fifth item'</span>]</span><br><span class="line">[<span class="string">'first item'</span>, <span class="string">'second item'</span>]</span><br><span class="line">[<span class="string">'third item'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们使用了 <code>last</code>、<code>position</code> 等方法。在 XPath 中，提供了 100 多个方法，包括存取、数值、字符串、逻辑、节点、序列等处理功能，它们的具体作用可以参考：<a href="http://www.w3school.com.cn/xpath/xpath_functions.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_functions.asp</a>。</p>
                  <h2 id="14-节点轴选择"><a href="#14-节点轴选择" class="headerlink" title="14. 节点轴选择"></a>14. 节点轴选择</h2>
                  <p>XPath 提供了很多节点轴选择方法，包括获取子元素、兄弟元素、父元素、祖先元素等，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;&lt;span&gt;first item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::*'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::div'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/attribute::*'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/child::a[@href="link1.html"]'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/descendant::span'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following::*[2]'</span>)</span><br><span class="line">print(result)</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following-sibling::*'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[&lt;Element html at <span class="number">0x107941808</span>&gt;, &lt;Element body at <span class="number">0x1079418c8</span>&gt;, &lt;Element div at <span class="number">0x107941908</span>&gt;, &lt;Element ul at <span class="number">0x107941948</span>&gt;]</span><br><span class="line">[&lt;Element div at <span class="number">0x107941908</span>&gt;]</span><br><span class="line">[<span class="string">'item-0'</span>]</span><br><span class="line">[&lt;Element a at <span class="number">0x1079418c8</span>&gt;]</span><br><span class="line">[&lt;Element span at <span class="number">0x107941948</span>&gt;]</span><br><span class="line">[&lt;Element a at <span class="number">0x1079418c8</span>&gt;]</span><br><span class="line">[&lt;Element li at <span class="number">0x107941948</span>&gt;, &lt;Element li at <span class="number">0x107941988</span>&gt;, &lt;Element li at <span class="number">0x1079419c8</span>&gt;, &lt;Element li at <span class="number">0x107941a08</span>&gt;]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一次选择时，我们调用了 <code>ancestor</code> 轴，可以获取所有祖先节点。其后需要跟两个冒号，然后是节点的选择器，这里我们直接使用 <code>*</code>，表示匹配所有节点，因此返回结果是第一个 <code>li</code> 节点的所有祖先节点，包括 <code>html</code>、<code>body</code>、<code>div</code> 和 <code>ul</code>。</p>
                  <p>第二次选择时，我们又加了限定条件，这次在冒号后面加了 <code>div</code>，这样得到的结果就只有 <code>div</code> 这个祖先节点了。</p>
                  <p>第三次选择时，我们调用了 <code>attribute</code> 轴，可以获取所有属性值，其后跟的选择器还是 <code>*</code>，这代表获取节点的所有属性，返回值就是 <code>li</code> 节点的所有属性值。</p>
                  <p>第四次选择时，我们调用了 <code>child</code> 轴，可以获取所有直接子节点。这里我们又加了限定条件，选取 <code>href</code> 属性为 <code>link1.html</code> 的 <code>a</code> 节点。</p>
                  <p>第五次选择时，我们调用了 <code>descendant</code> 轴，可以获取所有子孙节点。这里我们又加了限定条件获取 <code>span</code> 节点，所以返回的结果只包含 <code>span</code> 节点而不包含 <code>a</code> 节点。</p>
                  <p>第六次选择时，我们调用了 <code>following</code> 轴，可以获取当前节点之后的所有节点。这里我们虽然使用的是 <code>*</code> 匹配，但又加了索引选择，所以只获取了第二个后续节点。</p>
                  <p>第七次选择时，我们调用了 <code>following-sibling</code> 轴，可以获取当前节点之后的所有同级节点。这里我们使用 <code>*</code> 匹配，所以获取了所有后续同级节点。</p>
                  <p>以上是 XPath 轴的简单用法，更多轴的用法可以参考：<a href="http://www.w3school.com.cn/xpath/xpath_axes.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_axes.asp</a>。</p>
                  <h2 id="15-总结"><a href="#15-总结" class="headerlink" title="15. 总结"></a>15. 总结</h2>
                  <p>到现在为止，我们基本上把可能用到的 XPath 选择器介绍完了。XPath 功能非常强大，内置函数非常多，熟练使用之后，可以大大提升 HTML 信息的提取效率。</p>
                  <p>如果想查询更多 XPath 的用法，可以查看：<a href="http://www.w3school.com.cn/xpath/index.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/index.asp</a>。</p>
                  <p>如果想查询更多 Python lxml 库的用法，可以查看 <a href="http://lxml.de/" target="_blank" rel="noopener">http://lxml.de/</a>。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/XPathTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/XPathTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-14 15:33:32" itemprop="dateCreated datePublished" datetime="2022-02-14T15:33:32+08:00">2022-02-14</time>
                </span>
                <span id="/202231.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 网页解析利器 XPath" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>14k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>13 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202243.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202243.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 高效实用的 MongoDB 文档存储</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>NoSQL，全称 Not Only SQL，意为不仅仅是 SQL，泛指非关系型数据库。NoSQL 是基于键值对的，而且不需要经过 SQL 层的解析，数据之间没有耦合性，性能非常高。</p>
                  <p>非关系型数据库又可细分如下：</p>
                  <ul>
                    <li>键值存储数据库：其代表有 Redis、Voldemort 和 Oracle BDB 等。</li>
                    <li>列存储数据库：其代表有 Cassandra、HBase 和 Riak 等。</li>
                    <li>文档型数据库：其代表有 CouchDB 和 MongoDB 等。</li>
                    <li>键值存储数据库：其代表有 Redis、Voldemort 和 Oracle BDB 等。</li>
                    <li>图形数据库：其代表有 Neo4J、InfoGrid 和 Infinite Graph 等。</li>
                  </ul>
                  <p>对于爬虫的数据存储来说，一条数据可能存在某些字段提取失败而缺失的情况，而且数据可能随时调整。另外，数据之间还存在嵌套关系。如果使用关系型数据库存储，一是需要提前建表，二是如果存在数据嵌套关系的话，需要进行序列化操作才可以存储，这非常不方便。如果用了非关系型数据库，就可以避免一些麻烦，更简单、高效。</p>
                  <p>本节中，我们主要介绍 MongoDB 存储操作。</p>
                  <p>MongoDB 是由 C++ 语言编写的非关系型数据库，是一个基于分布式文件存储的开源数据库系统，其内容存储形式类似 JSON 对象，它的字段值可以包含其他文档、数组及文档数组，非常灵活。在这一节中，我们就来看看 Python 3 下 MongoDB 的存储操作。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>在开始之前，请确保已经安装好了 MongoDB 并启动了其服务，安装方式可以参考：<a href="https://setup.scrape.center/mongodb。" target="_blank" rel="noopener">https://setup.scrape.center/mongodb。</a></p>
                  <p>除了安装好 MongoDB 数据库，我们还需要安装好 Python 的 PyMongo 库，如尚未安装，可以使用 pip3 来安装：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> pymongo</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更详细的安装说明可以参考：<a href="https://setup.scrape.center/pymongo。" target="_blank" rel="noopener">https://setup.scrape.center/pymongo。</a></p>
                  <p>安装好 MongoDB 数据库和 PyMongo 库之后，我们便可以开始本节的学习了。</p>
                  <h2 id="2-连接-MongoDB"><a href="#2-连接-MongoDB" class="headerlink" title="2. 连接 MongoDB"></a>2. 连接 MongoDB</h2>
                  <p>连接 MongoDB 时，我们需要使用 PyMongo 库里面的 <code>MongoClient</code>。一般来说，传入 MongoDB 的 IP 及端口即可，其中第一个参数为地址 <code>host</code>，第二个参数为端口 <code>port</code>（如果不给它传递参数，默认是 27017）：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就可以创建 MongoDB 的连接对象了。</p>
                  <p>另外，<code>MongoClient</code> 的第一个参数 <code>host</code> 还可以直接传入 MongoDB 的连接字符串，它以 <code>mongodb</code> 开头，例如：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">client = MongoClient(<span class="string">'mongodb://localhost:27017/'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这也可以达到同样的连接效果。</p>
                  <h2 id="3-指定数据库"><a href="#3-指定数据库" class="headerlink" title="3. 指定数据库"></a>3. 指定数据库</h2>
                  <p>在 MongoDB 中，可以建立多个数据库，接下来我们需要指定操作哪个数据库。这里我们以 test 数据库为例来说明，下一步需要在程序中指定要使用的数据库：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">db = client.test</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里调用 <code>client</code> 的 <code>test</code> 属性即可返回 test 数据库。当然，我们也可以这样指定：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">db = client[<span class="string">'test'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这两种方式是等价的。</p>
                  <h2 id="4-指定集合"><a href="#4-指定集合" class="headerlink" title="4. 指定集合"></a>4. 指定集合</h2>
                  <p>MongoDB 的每个数据库又包含许多集合（collection），它们类似于关系型数据库中的表。</p>
                  <p>下一步需要指定要操作的集合，这里指定一个集合名称为 <code>students</code>。与指定数据库类似，指定集合也有两种方式：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">collection = db.students</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">collection = db[<span class="string">'students'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们便声明了一个集合对象。</p>
                  <h2 id="5-插入数据"><a href="#5-插入数据" class="headerlink" title="5. 插入数据"></a>5. 插入数据</h2>
                  <p>接下来，便可以插入数据了。对于 <code>students</code> 这个集合，新建一条学生数据，这条数据以字典形式表示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">student = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170101'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Jordan'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里指定了学生的学号、姓名、年龄和性别。接下来，直接调用 <code>collection</code> 的 <code>insert</code> 方法即可插入数据，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = collection.insert(student)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 MongoDB 中，每条数据其实都有一个 <code>_id</code> 属性来唯一标识。如果没有显式指明该属性，MongoDB 会自动产生一个 <code>ObjectId</code> 类型的 <code>_id</code> 属性。<code>insert</code> 方法会在执行后返回 <code>_id</code> 值。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">5932</span>a68615c2606814c91f3d</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，我们也可以同时插入多条数据，只需要以列表形式传递即可，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">student1 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170101'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Jordan'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">student2 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170202'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Mike'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">result = collection.insert([student1, student2])</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回结果是对应的 <code>_id</code> 的集合：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[ObjectId(<span class="string">'5932a80115c2606a59e8a048'</span>), ObjectId(<span class="string">'5932a80115c2606a59e8a049'</span>)]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>实际上，在 PyMongo 3.x 版本中，官方已经不推荐使用 <code>insert</code> 方法了。当然，继续使用也没有什么问题。官方推荐使用 <code>insert_one</code> 和 <code>insert_many</code> 方法来分别插入单条记录和多条记录，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">student = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170101'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Jordan'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">result = collection.insert_one(student)</span><br><span class="line">print(result)</span><br><span class="line">print(result.inserted_id)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;pymongo.results.InsertOneResult object at <span class="number">0x10d68b558</span>&gt;</span><br><span class="line"><span class="number">5932</span>ab0f15c2606f0c1cf6c5</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>与 <code>insert</code> 方法不同，这次返回的是 <code>InsertOneResult</code> 对象，我们可以调用其 <code>inserted_id</code> 属性获取 <code>_id</code>。</p>
                  <p>对于 <code>insert_many</code> 方法，我们可以将数据以列表形式传递，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">student1 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170101'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Jordan'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">student2 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170202'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Mike'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">result = collection.insert_many([student1, student2])</span><br><span class="line">print(result)</span><br><span class="line">print(result.inserted_ids)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;pymongo.results.InsertManyResult object at <span class="number">0x101dea558</span>&gt;</span><br><span class="line">[ObjectId(<span class="string">'5932abf415c2607083d3b2ac'</span>), ObjectId(<span class="string">'5932abf415c2607083d3b2ad'</span>)]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>该方法返回的是 <code>InsertManyResult</code> 类型的对象，调用 <code>inserted_ids</code> 属性可以获取插入数据的 <code>_id</code> 列表。</p>
                  <h2 id="6-查询"><a href="#6-查询" class="headerlink" title="6. 查询"></a>6. 查询</h2>
                  <p>插入数据后，我们可以利用 <code>find_one</code> 或 <code>find</code> 方法进行查询，其中 <code>find_one</code> 查询得到的是单个结果，<code>find</code> 则返回一个生成器对象。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = collection.find_one(&#123;<span class="string">'name'</span>: <span class="string">'Mike'</span>&#125;)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们查询 <code>name</code> 为 <code>Mike</code> 的数据，它的返回结果是字典类型，运行结果如下：</p>
                  <figure class="highlight">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">dict</span>'&gt;</span></span><br><span class="line">&#123;'_id': ObjectId('5932a80115c2606a59e8a049'), 'id': '20170202', 'name': 'Mike', 'age': 21, 'gender': 'male'&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，它多了 <code>_id</code> 属性，这就是 MongoDB 在插入过程中自动添加的。</p>
                  <p>此外，我们也可以根据 <code>ObjectId</code> 来查询，此时需要使用 bson 库里面的 <code>objectid</code>：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</span><br><span class="line"></span><br><span class="line">result = collection.find_one(&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'593278c115c2602667ec6bae'</span>)&#125;)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其查询结果依然是字典类型，具体如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'593278c115c2602667ec6bae'</span>), <span class="string">'id'</span>: <span class="string">'20170101'</span>, <span class="string">'name'</span>: <span class="string">'Jordan'</span>, <span class="string">'age'</span>: <span class="number">20</span>, <span class="string">'gender'</span>: <span class="string">'male'</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，如果查询结果不存在，则会返回 <code>None</code>。</p>
                  <p>对于多条数据的查询，我们可以使用 <code>find</code> 方法。例如，这里查找年龄为 20 的数据，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: <span class="number">20</span>&#125;)</span><br><span class="line">print(results)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;pymongo.cursor.Cursor object at <span class="number">0x1032d5128</span>&gt;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'593278c115c2602667ec6bae'</span>), <span class="string">'id'</span>: <span class="string">'20170101'</span>, <span class="string">'name'</span>: <span class="string">'Jordan'</span>, <span class="string">'age'</span>: <span class="number">20</span>, <span class="string">'gender'</span>: <span class="string">'male'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'593278c815c2602678bb2b8d'</span>), <span class="string">'id'</span>: <span class="string">'20170102'</span>, <span class="string">'name'</span>: <span class="string">'Kevin'</span>, <span class="string">'age'</span>: <span class="number">20</span>, <span class="string">'gender'</span>: <span class="string">'male'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'593278d815c260269d7645a8'</span>), <span class="string">'id'</span>: <span class="string">'20170103'</span>, <span class="string">'name'</span>: <span class="string">'Harden'</span>, <span class="string">'age'</span>: <span class="number">20</span>, <span class="string">'gender'</span>: <span class="string">'male'</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回结果是 <code>Cursor</code> 类型，它相当于一个生成器，我们需要遍历取到所有的结果，其中每个结果都是字典类型。</p>
                  <p>如果要查询年龄大于 20 的数据，则写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$gt'</span>: <span class="number">20</span>&#125;&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里查询的条件键值已经不是单纯的数字了，而是一个字典，其键名为比较符号 <code>$gt</code>，意思是大于，键值为 20。</p>
                  <p>这里将比较符号归纳为表 5-3。</p>
                  <p>表 5-3 比较符号</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">符　　号</th>
                          <th style="text-align:left">含　　义</th>
                          <th style="text-align:left">示　　例</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>$lt</code></td>
                          <td style="text-align:left">小于</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$lt&#39;: 20}}</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$gt</code></td>
                          <td style="text-align:left">大于</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$gt&#39;: 20}}</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$lte</code></td>
                          <td style="text-align:left">小于等于</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$lte&#39;: 20}}</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$gte</code></td>
                          <td style="text-align:left">大于等于</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$gte&#39;: 20}}</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$ne</code></td>
                          <td style="text-align:left">不等于</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$ne&#39;: 20}}</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$in</code></td>
                          <td style="text-align:left">在范围内</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$in&#39;: [20, 23]}}</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$nin</code></td>
                          <td style="text-align:left">不在范围内</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$nin&#39;: [20, 23]}}</code></td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>另外，还可以进行正则匹配查询。例如，查询名字以 M 开头的学生数据，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = collection.find(&#123;<span class="string">'name'</span>: &#123;<span class="string">'$regex'</span>: <span class="string">'^M.*'</span>&#125;&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里使用 <code>$regex</code> 来指定正则匹配，<code>^M.*</code> 代表以 M 开头的正则表达式。</p>
                  <p>这里将一些功能符号再归类为下表。</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">符　　号</th>
                          <th style="text-align:left">含　　义</th>
                          <th style="text-align:left">示　　例</th>
                          <th style="text-align:left">示例含义</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>$regex</code></td>
                          <td style="text-align:left">匹配正则表达式</td>
                          <td style="text-align:left"><code>{&#39;name&#39;: {&#39;$regex&#39;: &#39;^M.*&#39;}}</code></td>
                          <td style="text-align:left"><code>name</code> 以 M 开头</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$exists</code></td>
                          <td style="text-align:left">属性是否存在</td>
                          <td style="text-align:left">{<code>&#39;name&#39;: {&#39;$exists&#39;: True}}</code></td>
                          <td style="text-align:left"><code>name</code> 属性存在</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$type</code></td>
                          <td style="text-align:left">类型判断</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$type&#39;: &#39;int&#39;}}</code></td>
                          <td style="text-align:left"><code>age</code> 的类型为 <code>int</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$mod</code></td>
                          <td style="text-align:left">数字模操作</td>
                          <td style="text-align:left"><code>{&#39;age&#39;: {&#39;$mod&#39;: [5, 0]}}</code></td>
                          <td style="text-align:left">年龄模 5 余 0</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$text</code></td>
                          <td style="text-align:left">文本查询</td>
                          <td style="text-align:left"><code>{&#39;$text&#39;: {&#39;$search&#39;: &#39;Mike&#39;}}</code></td>
                          <td style="text-align:left"><code>text</code> 类型的属性中包含 <code>Mike</code> 字符串</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$where</code></td>
                          <td style="text-align:left">高级条件查询</td>
                          <td style="text-align:left"><code>{&#39;$where&#39;: &#39;obj.fans_count == obj.follows_count&#39;}</code></td>
                          <td style="text-align:left">自身粉丝数等于关注数</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>关于这些操作的更详细用法，可以在 MongoDB 官方文档找到： <a href="https://docs.mongodb.com/manual/reference/operator/query/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/operator/query/</a>。</p>
                  <h2 id="7-计数"><a href="#7-计数" class="headerlink" title="7. 计数"></a>7. 计数</h2>
                  <p>要统计查询结果有多少条数据，可以调用 <code>count</code> 方法。比如，统计所有数据条数：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">count = collection.find().count()</span><br><span class="line">print(count)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>或者统计符合某个条件的数据：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">count = collection.find(&#123;<span class="string">'age'</span>: <span class="number">20</span>&#125;).count()</span><br><span class="line">print(count)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果是一个数值，即符合条件的数据条数。</p>
                  <h2 id="8-排序"><a href="#8-排序" class="headerlink" title="8. 排序"></a>8. 排序</h2>
                  <p>排序时，直接调用 <code>sort</code> 方法，并在其中传入排序的字段及升降序标志即可。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING)</span><br><span class="line">print([result[<span class="string">'name'</span>] <span class="keyword">for</span> result <span class="keyword">in</span> results])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'Harden'</span>, <span class="string">'Jordan'</span>, <span class="string">'Kevin'</span>, <span class="string">'Mark'</span>, <span class="string">'Mike'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们调用 <code>pymongo.ASCENDING</code> 指定升序。如果要降序排列，可以传入 <code>pymongo.DESCENDING</code>。</p>
                  <h2 id="9-偏移"><a href="#9-偏移" class="headerlink" title="9. 偏移"></a>9. 偏移</h2>
                  <p>在某些情况下，我们可能想只取某几个元素，这时可以利用 <code>skip</code> 方法偏移几个位置，比如偏移 2，就忽略前两个元素，得到第三个及以后的元素：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING).skip(<span class="number">2</span>)</span><br><span class="line">print([result[<span class="string">'name'</span>] <span class="keyword">for</span> result <span class="keyword">in</span> results])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'Kevin'</span>, <span class="string">'Mark'</span>, <span class="string">'Mike'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外，还可以用 <code>limit</code> 方法指定要取的结果个数，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING).skip(<span class="number">2</span>).limit(<span class="number">2</span>)</span><br><span class="line">print([result[<span class="string">'name'</span>] <span class="keyword">for</span> result <span class="keyword">in</span> results])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'Kevin'</span>, <span class="string">'Mark'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果不使用 <code>limit</code> 方法，原本会返回三个结果，加了限制后，会截取两个结果返回。</p>
                  <p>值得注意的是，在数据库数量非常庞大的时候，如千万、亿级别，最好不要使用大的偏移量来查询数据，因为这样很可能导致内存溢出。此时可以使用类似如下操作来查询：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</span><br><span class="line">collection.find(&#123;<span class="string">'_id'</span>: &#123;<span class="string">'$gt'</span>: ObjectId(<span class="string">'593278c815c2602678bb2b8d'</span>)&#125;&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时需要记录好上次查询的 <code>_id</code>。</p>
                  <h2 id="10-更新"><a href="#10-更新" class="headerlink" title="10. 更新"></a>10. 更新</h2>
                  <p>对于数据更新，我们可以使用 <code>update</code> 方法，指定更新的条件和更新后的数据即可。例如：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">condition = &#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;</span><br><span class="line">student = collection.find_one(condition)</span><br><span class="line">student[<span class="string">'age'</span>] = <span class="number">25</span></span><br><span class="line">result = collection.update(condition, student)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们要更新 <code>name</code> 为 <code>Kevin</code> 的数据的年龄：首先指定查询条件，然后将数据查询出来，修改年龄后调用 <code>update</code> 方法将原条件和修改后的数据传入。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'ok'</span>: <span class="number">1</span>, <span class="string">'nModified'</span>: <span class="number">1</span>, <span class="string">'n'</span>: <span class="number">1</span>, <span class="string">'updatedExisting'</span>: <span class="literal">True</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回结果是字典形式，<code>ok</code> 代表执行成功，<code>nModified</code> 代表影响的数据条数。</p>
                  <p>另外，我们也可以使用 <code>$set</code> 操作符对数据进行更新，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = collection.update(condition, &#123;<span class="string">'$set'</span>: student&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样可以只更新 <code>student</code> 字典内存在的字段。如果原先还有其他字段，则不会更新，也不会删除。而如果不用 <code>$set</code> 的话，则会把之前的数据全部用 <code>student</code> 字典替换；如果原本存在其他字段，则会被删除。</p>
                  <p>另外，<code>update</code> 方法其实也是官方不推荐使用的方法。这里也分为 <code>update_one</code> 方法和 <code>update_many</code> 方法，用法更加严格，它们的第二个参数需要使用 <code>$</code> 类型操作符作为字典的键名，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">condition = &#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;</span><br><span class="line">student = collection.find_one(condition)</span><br><span class="line">student[<span class="string">'age'</span>] = <span class="number">26</span></span><br><span class="line">result = collection.update_one(condition, &#123;<span class="string">'$set'</span>: student&#125;)</span><br><span class="line">print(result)</span><br><span class="line">print(result.matched_count, result.modified_count)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里调用了 <code>update_one</code> 方法，其第二个参数不能再直接传入修改后的字典，而是需要使用 <code>{&#39;$set&#39;: student}</code> 这样的形式，其返回结果是 <code>UpdateResult</code> 类型。然后分别调用 <code>matched_count</code> 和 <code>modified_count</code> 属性，获得匹配的数据条数和影响的数据条数。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;pymongo.results.UpdateResult object at <span class="number">0x10d17b678</span>&gt;</span><br><span class="line"><span class="number">1</span> <span class="number">0</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们再看一个例子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">condition = &#123;<span class="string">'age'</span>: &#123;<span class="string">'$gt'</span>: <span class="number">20</span>&#125;&#125;</span><br><span class="line">result = collection.update_one(condition, &#123;<span class="string">'$inc'</span>: &#123;<span class="string">'age'</span>: <span class="number">1</span>&#125;&#125;)</span><br><span class="line">print(result)</span><br><span class="line">print(result.matched_count, result.modified_count)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里指定查询条件为年龄大于 20，然后更新条件为 <code>{&#39;$inc&#39;: {&#39;age&#39;: 1}}</code>，也就是年龄加 1，执行之后会将第一条符合条件的数据年龄加 1。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;pymongo.results.UpdateResult object at <span class="number">0x10b8874c8</span>&gt;</span><br><span class="line"><span class="number">1</span> <span class="number">1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到匹配条数为 1 条，影响条数也为 1 条。</p>
                  <p>如果调用 <code>update_many</code> 方法，则会将所有符合条件的数据都更新，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">condition = &#123;<span class="string">'age'</span>: &#123;<span class="string">'$gt'</span>: <span class="number">20</span>&#125;&#125;</span><br><span class="line">result = collection.update_many(condition, &#123;<span class="string">'$inc'</span>: &#123;<span class="string">'age'</span>: <span class="number">1</span>&#125;&#125;)</span><br><span class="line">print(result)</span><br><span class="line">print(result.matched_count, result.modified_count)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时匹配条数就不再为 1 条了，运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;pymongo.results.UpdateResult object at <span class="number">0x10c6384c8</span>&gt;</span><br><span class="line"><span class="number">3</span> <span class="number">3</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这时所有匹配到的数据都会被更新。</p>
                  <h2 id="11-删除"><a href="#11-删除" class="headerlink" title="11. 删除"></a>11. 删除</h2>
                  <p>删除操作比较简单，直接调用 <code>remove</code> 方法指定删除的条件即可，此时符合条件的所有数据均会被删除。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = collection.remove(&#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'ok'</span>: <span class="number">1</span>, <span class="string">'n'</span>: <span class="number">1</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外，这里依然存在两个新的推荐方法 —— <code>delete_one</code> 和 <code>delete_many</code>。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = collection.delete_one(&#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;)</span><br><span class="line">print(result)</span><br><span class="line">print(result.deleted_count)</span><br><span class="line">result = collection.delete_many(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$lt'</span>: <span class="number">25</span>&#125;&#125;)</span><br><span class="line">print(result.deleted_count)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;pymongo.results.DeleteResult object at <span class="number">0x10e6ba4c8</span>&gt;</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">4</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p><code>delete_one</code> 即删除第一条符合条件的数据，<code>delete_many</code> 即删除所有符合条件的数据。它们的返回结果都是 <code>DeleteResult</code> 类型，可以调用 <code>deleted_count</code> 属性获取删除的数据条数。</p>
                  <h2 id="12-其他操作"><a href="#12-其他操作" class="headerlink" title="12. 其他操作"></a>12. 其他操作</h2>
                  <p>另外，PyMongo 还提供了一些组合方法，如 <code>find_one_and_delete</code>、<code>find_one_and_replace</code> 和 <code>find_one_and_update</code>，它们是查找后删除、替换和更新操作，其用法与上述方法基本一致。</p>
                  <p>另外，还可以对索引进行操作，相关方法有 <code>create_index</code>、<code>create_indexes</code> 和 <code>drop_index</code> 等。</p>
                  <p>关于 PyMongo 的详细用法，可以参见官方文档：<a href="http://api.mongodb.com/python/current/api/pymongo/collection.html" target="_blank" rel="noopener">http://api.mongodb.com/python/current/api/pymongo/collection.html</a>。</p>
                  <p>另外，还有对数据库和集合本身等的一些操作，这里不再一一讲解，可以参见官方文档：<a href="http://api.mongodb.com/python/current/api/pymongo/" target="_blank" rel="noopener">http://api.mongodb.com/python/current/api/pymongo/</a>。</p>
                  <h2 id="13-总结"><a href="#13-总结" class="headerlink" title="13. 总结"></a>13. 总结</h2>
                  <p>本节讲解了使用 PyMongo 操作 MongoDB 进行数据增删改查的方法，后面我们会在实战案例中应用这些操作进行数据存储。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/MongoDBTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/MongoDBTest</a>。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-14 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-14T12:42:44+08:00">2022-02-14</time>
                </span>
                <span id="/202243.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 高效实用的 MongoDB 文档存储" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>9.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>9 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202224.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202224.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 基础爬虫案例爬取实战</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在前面我们已经学习了 requests、正则表达式的基本用法，但我们还没有完整地实现一个爬取案例，这一节，我们就来实现一个完整的网站爬虫，把前面学习的知识点串联起来，同时加深对这些知识点的理解。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>在本节开始之前，我们需要做好如下的准备工作：</p>
                  <ul>
                    <li>安装好 Python3，最低为 3.6 版本，并能成功运行 Python3 程序。</li>
                    <li>了解 Python HTTP 请求库 requests 的基本用法。</li>
                    <li>了解正则表达式的用法和 Python 中正则表达式库 re 的基本用法。</li>
                  </ul>
                  <p>以上内容在前面的章节中均有讲解，如尚未准备好建议先熟悉一下这些内容。</p>
                  <h2 id="2-爬取目标"><a href="#2-爬取目标" class="headerlink" title="2. 爬取目标"></a>2. 爬取目标</h2>
                  <p>本节我们以一个基本的静态网站作为案例进行爬取，需要爬取的链接为 <a href="https://ssr1.scrape.center/" target="_blank" rel="noopener">https://ssr1.scrape.center/</a>，这个网站里面包含了一些电影信息，界面如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/ub4f5.png" alt=""></p>
                  <p>这里首页展示了一个个电影的列表，每部电影包含了它的封面、名称、分类、上映时间、评分等内容，同时列表页还支持翻页，点击相应的页码我们就能进入到对应的新列表页。</p>
                  <p>如果我们点开其中一部电影，会进入到电影的详情页面，比如我们把第一个电影《霸王别姬》打开，会得到如下的页面：</p>
                  <p><img src="https://cdn.cuiqingcai.com/km8us.png" alt=""></p>
                  <p>这里显示的内容更加丰富、包括剧情简介、导演、演员等信息。</p>
                  <p>我们本节要完成的目标是：</p>
                  <ul>
                    <li>用 requests 爬取这个站点每一页的电影列表，顺着列表再爬取每个电影的详情页。</li>
                    <li>用 pyquery 和正则表达式提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等内容。</li>
                    <li>把以上爬取的内容存入 MongoDB 数据库。</li>
                    <li>使用多进程实现爬取的加速。</li>
                  </ul>
                  <p>好，那我们现在就开始吧。</p>
                  <h2 id="3-爬取列表页"><a href="#3-爬取列表页" class="headerlink" title="3. 爬取列表页"></a>3. 爬取列表页</h2>
                  <p>好，第一步的爬取我们肯定要从列表页入手，我们首先观察一下列表页的结构和翻页规则。在浏览器中访问 <a href="https://ssr1.scrape.center/" target="_blank" rel="noopener">https://ssr1.scrape.center/</a>，然后打开浏览器开发者工具，我们观察每一个电影信息区块对应的 HTML 以及进入到详情页的 URL 是怎样的，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/q295l.png" alt=""></p>
                  <p>可以看到每部电影对应的区块都是一个 div 节点，它的 class 属性都有 el-card 这个值。每个列表页有 10 个这样的 div 节点，也就对应着 10 部电影的信息。</p>
                  <p>好，我们再分析下从列表页是怎么进入到详情页的，我们选中电影的名称，看下结果：</p>
                  <p><img src="https://cdn.cuiqingcai.com/gi0nt.png" alt=""></p>
                  <p>可以看到这个名称实际上是一个 h2 节点，其内部的文字就是电影的标题。再看，h2 节点的外面包含了一个 a 节点，这个 a 节点带有 href 属性，这就是一个超链接，其中 href 的值为 <code>/detail/1</code>，这是一个相对网站的根 URL <a href="https://ssr1.scrape.center/" target="_blank" rel="noopener">https://ssr1.scrape.center/</a> 的路径，加上网站的根 URL 就构成了 <a href="https://ssr1.scrape.center/detail/1" target="_blank" rel="noopener">https://ssr1.scrape.center/detail/1</a> ，也就是这部电影的详情页的 URL。这样我们只需要提取到这个 href 属性就能构造出详情页的 URL 并接着爬取了。</p>
                  <p>好的，那接下来我们来分析下翻页的逻辑，我们拉到页面的最下方，可以看到分页页码，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/97yn6.png" alt=""></p>
                  <p>这里观察到一共有 100 条数据，10 页的内容，因此页码最多是 10。</p>
                  <p>接着我们点击第二页，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/r63xq.png" alt=""></p>
                  <p>可以看到网页的 URL 变成了 <a href="https://ssr1.scrape.center/page/2" target="_blank" rel="noopener">https://ssr1.scrape.center/page/2</a>，相比根 URL 多了 <code>/page/2</code> 这部分内容。网页的结构还是和原来一模一样，可以和第一页一样处理。</p>
                  <p>接着我们查看第三页、第四页等内容，可以发现有这么一个规律，其 URL 最后分别变成了 <code>/page/3</code>、<code>/page/4</code>。所以，<code>/page</code> 后面跟的就是列表页的页码，当然第一页也是一样，我们在根 URL 后面加上 <code>/page/1</code> 也是能访问的，只不过网站做了一下处理，默认的页码是 1，所以显示第一页的内容。</p>
                  <p>好，分析到这里，逻辑基本就清晰了。</p>
                  <p>所以，我们要完成列表页的爬取，可以这么实现：</p>
                  <ul>
                    <li>遍历页码构造 10 页的索引页 URL。</li>
                    <li>从每个索引页分析提取出每个电影的详情页 URL。</li>
                  </ul>
                  <p>好，那么我们写代码来实现一下吧。</p>
                  <p>首先，我们需要先定义一些基础的变量，并引入一些必要的库，写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO,</span><br><span class="line">                    format=<span class="string">'%(asctime)s - %(levelname)s: %(message)s'</span>)</span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">'https://ssr1.scrape.center'</span></span><br><span class="line">TOTAL_PAGE = <span class="number">10</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们引入了 requests 用来爬取页面，logging 用来输出信息，re 用来实现正则表达式解析，urljoin 用来做 URL 的拼接。</p>
                  <p>接着我们定义了下日志输出级别和输出格式，接着定义了 BASE_URL 为当前站点的根 URL，TOTAL_PAGE 为需要爬取的总页码数量。</p>
                  <p>好，定义好了之后，我们来实现一个页面爬取的方法吧，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    logging.info(<span class="string">'scraping %s...'</span>, url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        logging.error(<span class="string">'get invalid status code %s while scraping %s'</span>, response.status_code, url)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException:</span><br><span class="line">        logging.error(<span class="string">'error occurred while scraping %s'</span>, url, exc_info=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>考虑到我们不仅要爬取列表页，还要爬取详情页，所以在这里我们定义一个较通用的爬取页面的方法，叫做 scrape_page，它接收一个 url 参数，返回页面的 html 代码。这里首先判断了状态码是不是 200，如果是，则直接返回页面的 HTML 代码，如果不是，则会输出错误日志信息。另外这里实现了 requests 的异常处理，如果出现了爬取异常，则会输出对应的错误日志信息，我们将 logging 的 error 方法的 exc_info 参数设置为 True 则可以打印出 Traceback 错误堆栈信息。</p>
                  <p>好了，有了 scrape_page 方法之后，我们给这个方法传入一个 url，正常情况下它就可以返回页面的 HTML 代码了。</p>
                  <p>接着在这个基础上，我们来定义列表页的爬取方法吧，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_index</span><span class="params">(page)</span>:</span></span><br><span class="line">    index_url = <span class="string">f'<span class="subst">&#123;BASE_URL&#125;</span>/page/<span class="subst">&#123;page&#125;</span>'</span></span><br><span class="line">    <span class="keyword">return</span> scrape_page(index_url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>方法名称叫做 scrape_index，这个实现就很简单了，这个方法会接收一个 page 参数，即列表页的页码，我们在方法里面实现列表页的 URL 拼接，然后调用 scrape_page 方法爬取即可，这样就能得到列表页的 HTML 代码了。</p>
                  <p>获取了 HTML 代码之后，下一步就是解析列表页，并得到每部电影的详情页的 URL 了，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(html)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">'&lt;a.*?href="(.*?)".*?class="name"&gt;'</span>)</span><br><span class="line">    items = re.findall(pattern, html)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> items:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        detail_url = urljoin(BASE_URL, item)</span><br><span class="line">        logging.info(<span class="string">'get detail url %s'</span>, detail_url)</span><br><span class="line">        <span class="keyword">yield</span> detail_url</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们定义了 parse_index 方法，它接收一个 html 参数，即列表页的 HTML 代码。</p>
                  <p>在 parse_index 方法里面，我们首先定义了一个提取标题超链接 href 属性的正则表达式，内容为：</p>
                  <figure class="highlight cs">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;a.*?href=<span class="string">"(.*?)"</span>.*?<span class="keyword">class</span>=<span class="string">"name"</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们使用非贪婪通用匹配正则表达式 <code>.*?</code> 来匹配任意字符，同时在 href 属性的引号之间使用了分组匹配 <code>(.*?)</code> 正则表达式，这样 href 的属性值我们便能在匹配结果里面获取到了。紧接着，正则表达式后面紧跟了 <code>class=&quot;name&quot;</code> 来标示这个 <code>&lt;a&gt;</code> 节点是代表电影名称的节点。</p>
                  <p>好，现在有了正则表达式，那么怎么提取列表页所有的 href 值呢？使用 re 的 findall 方法就好了，第一个参数传入这个正则表达式构造的 pattern 对象，第二个参数传入 html，这样 findall 方法便会搜索 html 中所有能匹配该正则表达式的内容，然后把匹配到的结果返回，最后赋值为 items。</p>
                  <p>如果 items 为空，那么我们可以直接返回空的列表，如果 items 不为空，那么我们直接遍历处理即可。</p>
                  <p>遍历 items 得到的 item 就是我们在上文所说的类似 <code>/detail/1</code> 这样的结果。由于这并不是一个完整的 URL，所以我们需要借助 urljoin 方法把 BASE_URL 和 href 拼接起来，获得详情页的完整 URL，得到的结果就类似 <a href="https://ssr1.scrape.center/detail/1" target="_blank" rel="noopener">https://ssr1.scrape.center/detail/1</a> 这样的完整的 URL 了，最后 yield 返回即可。</p>
                  <p>这样我们通过调用 parse_index 方法并传入列表页的 HTML 代码就可以获得该列表页所有电影的详情页 URL 了。</p>
                  <p>好，接下来我们把上面的方法串联调用一下，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, TOTAL_PAGE + <span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index(index_html)</span><br><span class="line">        logging.info(<span class="string">'detail urls %s'</span>, list(detail_urls))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了 main 方法来完成上面所有方法的调用，首先使用 range 方法遍历了一下页码，得到的 page 就是 1-10，接着把 page 变量传给 scrape_index 方法，得到列表页的 HTML，赋值为 index_html 变量。接下来再将 index_html 变量传给 parse_index 方法，得到列表页所有电影的详情页 URL，赋值为 detail_urls，结果是一个生成器，我们调用 list 方法就可以将其输出出来。</p>
                  <p>好，我们运行一下上面的代码，结果如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">50</span>,<span class="number">505</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/page/1...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">949</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/1</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/2</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/3</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/4</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/5</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/6</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/7</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/8</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/9</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">950</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/10</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">951</span> - INFO: detail urls [<span class="string">'https://ssr1.scrape.center/detail/1'</span>, <span class="string">'https://ssr1.scrape.center/detail/2'</span>, <span class="string">'https://ssr1.scrape.center/detail/3'</span>, <span class="string">'https://ssr1.scrape.center/detail/4'</span>, <span class="string">'https://ssr1.scrape.center/detail/5'</span>, <span class="string">'https://ssr1.scrape.center/detail/6'</span>, <span class="string">'https://ssr1.scrape.center/detail/7'</span>, <span class="string">'https://ssr1.scrape.center/detail/8'</span>, <span class="string">'https://ssr1.scrape.center/detail/9'</span>, <span class="string">'https://ssr1.scrape.center/detail/10'</span>]</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">951</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/page/2...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">842</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/11</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">842</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/12</span></span><br><span class="line">...</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于输出内容比较多，这里只贴了一部分。</p>
                  <p>可以看到，这里程序首先爬取了第一页列表页，然后得到了对应详情页的每个 URL，接着再接着爬第二页、第三页，一直到第十页，依次输出了每一页的详情页 URL。这样，我们就成功获取到了所有电影的详情页 URL 啦。</p>
                  <h2 id="4-爬取详情页"><a href="#4-爬取详情页" class="headerlink" title="4. 爬取详情页"></a>4. 爬取详情页</h2>
                  <p>现在我们已经可以成功获取所有详情页 URL 了，那么下一步当然就是解析详情页并提取出我们想要的信息了。</p>
                  <p>我们首先观察一下详情页的 HTML 代码吧，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/6j55t.png" alt=""></p>
                  <p>经过分析，我们想要提取的内容和对应的节点信息如下：</p>
                  <ul>
                    <li>封面：是一个 img 节点，其 class 属性为 cover。</li>
                    <li>名称：是一个 h2 节点，其内容便是名称。</li>
                    <li>类别：是 span 节点，其内容便是类别内容，其外侧是 button 节点，再外侧则是 class 为 categories 的 div 节点。</li>
                    <li>上映时间：是 span 节点，其内容包含了上映时间，其外侧是包含了 class 为 info 的 div 节点。另外提取结果中还多了「上映」二字，我们可以用正则表达式把日期提取出来。</li>
                    <li>评分：是一个 p 节点，其内容便是评分，p 节点的 class 属性为 score。</li>
                    <li>剧情简介：是一个 p 节点，其内容便是剧情简介，其外侧是 class 为 drama 的 div 节点。</li>
                  </ul>
                  <p>看似有点复杂是吧，不用担心，有了正则表达式，我们可以轻松搞定。</p>
                  <p>接着我们来实现一下代码吧。</p>
                  <p>刚才我们已经成功获取了详情页 URL，接着当然是定义一个详情页的爬取方法了，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_detail</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> scrape_page(url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了一个 scrape_detail 方法，接收一个 url 参数，并通过调用 scrape_page 方法获得网页源代码。由于我们刚才已经实现了 scrape_page 方法，所以在这里我们不用再写一遍页面爬取的逻辑了，直接调用即可，做到了代码复用。</p>
                  <p>另外有人会说，这个 scrape_detail 方法里面只调用了 scrape_page 方法，别没有别的功能，那爬取详情页直接用 scrape_page 方法不就好了，还有必要再单独定义 scrape_detail 方法吗？有必要，单独定义一个 scrape_detail 方法在逻辑上会显得更清晰，而且以后如果我们想要对 scrape_detail 方法进行改动，比如添加日志输出，比如增加预处理，都可以在 scrape_detail 里面实现，而不用改动 scrape_page 方法，灵活性会更好。</p>
                  <p>好了，详情页的爬取方法已经实现了，接着就是详情页的解析了，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(html)</span>:</span></span><br><span class="line">    cover_pattern = re.compile(<span class="string">'class="item.*?&lt;img.*?src="(.*?)".*?class="cover"&gt;'</span>, re.S)</span><br><span class="line">    name_pattern = re.compile(<span class="string">'&lt;h2.*?&gt;(.*?)&lt;/h2&gt;'</span>)</span><br><span class="line">    categories_pattern = re.compile(<span class="string">'&lt;button.*?category.*?&lt;span&gt;(.*?)&lt;/span&gt;.*?&lt;/button&gt;'</span>, re.S)</span><br><span class="line">    published_at_pattern = re.compile(<span class="string">'(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)\s?上映'</span>)</span><br><span class="line">    drama_pattern = re.compile(<span class="string">'&lt;div.*?drama.*?&gt;.*?&lt;p.*?&gt;(.*?)&lt;/p&gt;'</span>, re.S)</span><br><span class="line">    score_pattern = re.compile(<span class="string">'&lt;p.*?score.*?&gt;(.*?)&lt;/p&gt;'</span>, re.S)</span><br><span class="line">    cover = re.search(cover_pattern, html).group(<span class="number">1</span>).strip() <span class="keyword">if</span> re.search(cover_pattern, html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    name = re.search(name_pattern, html).group(<span class="number">1</span>).strip() <span class="keyword">if</span> re.search(name_pattern, html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    categories = re.findall(categories_pattern, html) <span class="keyword">if</span> re.findall(categories_pattern, html) <span class="keyword">else</span> []</span><br><span class="line">    published_at = re.search(published_at_pattern, html).group(<span class="number">1</span>) <span class="keyword">if</span> re.search(published_at_pattern, html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    drama = re.search(drama_pattern, html).group(<span class="number">1</span>).strip() <span class="keyword">if</span> re.search(drama_pattern, html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    score = float(re.search(score_pattern, html).group(<span class="number">1</span>).strip()) <span class="keyword">if</span> re.search(score_pattern, html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'cover'</span>: cover,</span><br><span class="line">        <span class="string">'name'</span>: name,</span><br><span class="line">        <span class="string">'categories'</span>: categories,</span><br><span class="line">        <span class="string">'published_at'</span>: published_at,</span><br><span class="line">        <span class="string">'drama'</span>: drama,</span><br><span class="line">        <span class="string">'score'</span>: score</span><br><span class="line">    &#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了 parse_detail 方法用于解析详情页，它接收一个参数为 html，解析其中的内容，并以字典的形式返回结果。每个字段的解析情况如下所述：</p>
                  <ul>
                    <li>cover：封面，其值是带有 cover 这个 class 的 img 节点的 src 属性的值 ，所有直接 src 的内容使用 <code>(.*?)</code> 来表示即可，在 img 节点的前面我们再加上一些区分位置的标识符，如 item。由于结果只有一个，写好正则表达式后用 search 方法提取即可。</li>
                    <li>name：名称，其值是 h2 节点的文本值，我们直接在 h2 标签的中间使用 <code>(.*?)</code> 表示即可。由于结果只有一个，写好正则表达式后同样用 search 方法提取即可。</li>
                    <li>categories：类别，我们注意到每个 category 的值都是 button 节点里面的 span 节点的值，所以我们写好表示 button 节点的正则表达式后，再直接在其内部的 span 标签的中间使用 <code>(.*?)</code> 表示即可。由于结果有多个，所以这里使用 findall 方法提取，结果是一个列表。</li>
                    <li>published_at：上映时间，由于每个上映时间信息都包含了「上映」二字，另外日期又都是一个规整的格式，所以对于这个上映时间的提取，我们直接使用标准年月日的正则表达式 <code>(\d{4}-\d{2}-\d{2})</code> 表示即可。由于结果只有一个，直接使用 search 方法提取即可。</li>
                    <li>drama：直接提取 class 为 drama 的节点内部的 p 节点的文本即可，同样用 search 方法可以提取。</li>
                    <li>score：直接提取 class 为 score 的 p 节点的文本即可，但由于提取结果是字符串，所以我们还需要把它转成浮点数，即 float 类型。</li>
                  </ul>
                  <p>最后，上述的字段提取完毕之后，构造一个字典返回即可。</p>
                  <p>这样，我们就成功完成了详情页的提取和分析了。</p>
                  <p>最后，main 方法稍微改写一下，增加这两个方法的调用，改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, TOTAL_PAGE + <span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index(index_html)</span><br><span class="line">        <span class="keyword">for</span> detail_url <span class="keyword">in</span> detail_urls:</span><br><span class="line">            detail_html = scrape_detail(detail_url)</span><br><span class="line">            data = parse_detail(detail_html)</span><br><span class="line">            logging.info(<span class="string">'get detail data %s'</span>, data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们首先遍历了 detail_urls，获取了每个详情页的 URL，然后依次调用了 scrape_detail 和 parse_detail 方法，最后得到了每个详情页的提取结果，赋值为 data 并输出。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">936</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/page/1...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">36</span>,<span class="number">833</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/1</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">36</span>,<span class="number">833</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/detail/1...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">39</span>,<span class="number">985</span> - INFO: <span class="keyword">get</span> detail data &#123;<span class="string">'cover'</span>: <span class="string">'https://p0.meituan.net/movie/ce4da3e03e655b5b88ed31b5cd7896cf62472.jpg@464w_644h_1e_1c'</span>, <span class="string">'name'</span>: <span class="string">'霸王别姬 - Farewell My Concubine'</span>, <span class="string">'categories'</span>: [<span class="string">'剧情'</span>, <span class="string">'爱情'</span>], <span class="string">'published_at'</span>: <span class="string">'1993-07-26'</span>, <span class="string">'drama'</span>: <span class="string">'影片借一出《霸王别姬》的京戏，牵扯出三个人之间一段随时代风云变幻的爱恨情仇。段小楼（张丰毅 饰）与程蝶衣（张国荣 饰）是一对打小一起长大的师兄弟，两人一个演生，一个饰旦，一向配合天衣无缝，尤其一出《霸王别姬》，更是誉满京城，为此，两人约定合演一辈子《霸王别姬》。但两人对戏剧与人生关系的理解有本质不同，段小楼深知戏非人生，程蝶衣则是人戏不分。段小楼在认为该成家立业之时迎娶了名妓菊仙（巩俐 饰），致使程蝶衣认定菊仙是可耻的第三者，使段小楼做了叛徒，自此，三人围绕一出《霸王别姬》生出的爱恨情仇战开始随着时代风云的变迁不断升级，终酿成悲剧。'</span>, <span class="string">'score'</span>: <span class="number">9.5</span>&#125;</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">39</span>,<span class="number">985</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/2</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">39</span>,<span class="number">985</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/detail/2...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">41</span>,<span class="number">061</span> - INFO: <span class="keyword">get</span> detail data &#123;<span class="string">'cover'</span>: <span class="string">'https://p1.meituan.net/movie/6bea9af4524dfbd0b668eaa7e187c3df767253.jpg@464w_644h_1e_1c'</span>, <span class="string">'name'</span>: <span class="string">'这个杀手不太冷 - Léon'</span>, <span class="string">'categories'</span>: [<span class="string">'剧情'</span>, <span class="string">'动作'</span>, <span class="string">'犯罪'</span>], <span class="string">'published_at'</span>: <span class="string">'1994-09-14'</span>, <span class="string">'drama'</span>: <span class="string">'里昂（让·雷诺 饰）是名孤独的职业杀手，受人雇佣。一天，邻居家小姑娘马蒂尔德（纳塔丽·波特曼 饰）敲开他的房门，要求在他那里暂避杀身之祸。原来邻居家的主人是警方缉毒组的眼线，只因贪污了一小包毒品而遭恶警（加里·奥德曼 饰）杀害全家的惩罚。马蒂尔德 得到里昂的留救，幸免于难，并留在里昂那里。里昂教小女孩使枪，她教里昂法文，两人关系日趋亲密，相处融洽。 女孩想着去报仇，反倒被抓，里昂及时赶到，将女孩救回。混杂着哀怨情仇的正邪之战渐次升级，更大的冲突在所难免……'</span>, <span class="string">'score'</span>: <span class="number">9.5</span>&#125;</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-08</span> <span class="number">23</span>:<span class="number">37</span>:<span class="number">41</span>,<span class="number">062</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/3</span></span><br><span class="line">...</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于内容较多，这里省略了后续内容。</p>
                  <p>可以看到，这里我们就成功提取出来了每部电影的基本信息了，包括封面、名称、类别等等。</p>
                  <h2 id="5-保存数据"><a href="#5-保存数据" class="headerlink" title="5. 保存数据"></a>5. 保存数据</h2>
                  <p>好，成功提取到详情页信息之后，我们下一步就要把数据保存起来了。由于我们到现在我们还没有学习数据库的存储，所以现在我们临时先将数据保存成文本格式，在这里我们可以一个条目一个 JSON 文本。</p>
                  <p>定义保存数据的方法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> makedirs</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> exists</span><br><span class="line"></span><br><span class="line">RESULTS_DIR = <span class="string">'results'</span></span><br><span class="line">exists(RESULTS_DIR) <span class="keyword">or</span> makedirs(RESULTS_DIR)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    name = data.get(<span class="string">'name'</span>)</span><br><span class="line">    data_path = <span class="string">f'<span class="subst">&#123;RESULTS_DIR&#125;</span>/<span class="subst">&#123;name&#125;</span>.json'</span></span><br><span class="line">    json.dump(data, open(data_path, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>), ensure_ascii=<span class="literal">False</span>, indent=<span class="number">2</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们首先定义了数据保存的文件夹 RESULTS_DIR，然后判断了下这个文件夹是否存在，如果不存在则创建。</p>
                  <p>接着，我们定义了保存数据的方法 save_data，首先我们获取了数据的 name 字段，即电影的名称，我们将电影的名称当做 JSON 文件的名称，接着构造了 JSON 文件的路径，然后用 json 的 dump 方法将数据保存成文本格式。在 dump 的方法设置了两个参数，一个是 ensure_ascii 设置为 False，可以保证的中文字符在文件中能以正常的中文文本呈现，而不是 unicode 字符；另一个 indent 为 2，则是设置了 JSON 数据的结果有两行缩进，让 JSON 数据的格式显得更加美观。</p>
                  <p>好的，那么接下来 main 方法稍微改写一下就好了，改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, TOTAL_PAGE + <span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index(index_html)</span><br><span class="line">        <span class="keyword">for</span> detail_url <span class="keyword">in</span> detail_urls:</span><br><span class="line">            detail_html = scrape_detail(detail_url)</span><br><span class="line">            data = parse_detail(detail_html)</span><br><span class="line">            logging.info(<span class="string">'get detail data %s'</span>, data)</span><br><span class="line">            logging.info(<span class="string">'saving data to json file'</span>)</span><br><span class="line">            save_data(data)</span><br><span class="line">            logging.info(<span class="string">'data saved successfully'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里就是加了 save_data 方法的调用，并加了一些日志信息。</p>
                  <p>重新运行，我们看下输出结果：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">27</span>,<span class="number">094</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/page/1...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">28</span>,<span class="number">019</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/1</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">28</span>,<span class="number">019</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/detail/1...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">29</span>,<span class="number">183</span> - INFO: <span class="keyword">get</span> detail data &#123;<span class="string">'cover'</span>: <span class="string">'https://p0.meituan.net/movie/ce4da3e03e655b5b88ed31b5cd7896cf62472.jpg@464w_644h_1e_1c'</span>, <span class="string">'name'</span>: <span class="string">'霸王别姬 - Farewell My Concubine'</span>, <span class="string">'categories'</span>: [<span class="string">'剧情'</span>, <span class="string">'爱情'</span>], <span class="string">'published_at'</span>: <span class="string">'1993-07-26'</span>, <span class="string">'drama'</span>: <span class="string">'影片借一出《霸王别姬》的京戏，牵扯出三个人之间一段随时代风云变幻的爱恨情仇。段小楼（张丰毅 饰）与程蝶衣（张国荣 饰）是一对打小一起长大的师兄弟，两人一个演生，一个饰旦，一向配合天衣无缝，尤其一出《霸王别姬》，更是誉满京城，为此，两人约定合演一辈子《霸王别姬》。但两人对戏剧与人生关系的理解有本质不同，段小楼深知戏非人生，程蝶衣则是人戏不分。段小楼在认为该成家立业之时迎娶了名妓菊仙（巩俐 饰），致使程蝶衣认定菊仙是可耻的第三者，使段小楼做了叛徒，自此，三人围绕一出《霸王别姬》生出的爱恨情仇战开始随着时代风云的变迁不断升级，终酿成悲剧。'</span>, <span class="string">'score'</span>: <span class="number">9.5</span>&#125;</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">29</span>,<span class="number">183</span> - INFO: saving data to json file</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">29</span>,<span class="number">288</span> - INFO: data saved successfully</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">29</span>,<span class="number">288</span> - INFO: <span class="keyword">get</span> detail url https:<span class="comment">//ssr1.scrape.center/detail/2</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">29</span>,<span class="number">288</span> - INFO: scraping https:<span class="comment">//ssr1.scrape.center/detail/2...</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">30</span>,<span class="number">250</span> - INFO: <span class="keyword">get</span> detail data &#123;<span class="string">'cover'</span>: <span class="string">'https://p1.meituan.net/movie/6bea9af4524dfbd0b668eaa7e187c3df767253.jpg@464w_644h_1e_1c'</span>, <span class="string">'name'</span>: <span class="string">'这个杀手不太冷 - Léon'</span>, <span class="string">'categories'</span>: [<span class="string">'剧情'</span>, <span class="string">'动作'</span>, <span class="string">'犯罪'</span>], <span class="string">'published_at'</span>: <span class="string">'1994-09-14'</span>, <span class="string">'drama'</span>: <span class="string">'里昂（让·雷诺 饰）是名孤独的职业杀手，受人雇佣。一天，邻居家小姑娘马蒂尔德（纳塔丽·波特曼 饰）敲开他的房门，要求在他那里暂避杀身之祸。原来邻居家的主人是警方缉毒组的眼线，只因贪污了一小包毒品而遭恶警（加里·奥德曼 饰）杀害全家的惩罚。马蒂尔德 得到里昂的留救，幸免于难，并留在里昂那里。里昂教小女孩使枪，她教里昂法文，两人关系日趋亲密，相处融洽。 女孩想着去报仇，反倒被抓，里昂及时赶到，将女孩救回。混杂着哀怨情仇的正邪之战渐次升级，更大的冲突在所难免……'</span>, <span class="string">'score'</span>: <span class="number">9.5</span>&#125;</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">30</span>,<span class="number">250</span> - INFO: saving data to json file</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-09</span> <span class="number">01</span>:<span class="number">10</span>:<span class="number">30</span>,<span class="number">253</span> - INFO: data saved successfully</span><br><span class="line">...</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过运行结果可以发现，这里成功输出了将数据存储到 JSON 文件的信息。</p>
                  <p>运行完毕之后我们可以观察下本地的结果，可以看到 results 文件夹下就多了 100 个 JSON 文件，每部电影数据都是一个 JSON 文件，文件名就是电影名，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/yse4w.png" alt=""></p>
                  <h2 id="6-多进程加速"><a href="#6-多进程加速" class="headerlink" title="6. 多进程加速"></a>6. 多进程加速</h2>
                  <p>由于整个的爬取是单进程的，而且只能逐条爬取，速度稍微有点慢，我们有没有方法来对整个爬取过程进行加速呢？</p>
                  <p>在前面我们讲了多进程的基本原理和使用方法，下面我们就来实践一下多进程的爬取吧。</p>
                  <p>由于一共有 10 页详情页，这 10 页内容是互不干扰的，所以我们可以一页开一个进程来爬取。而且由于这 10 个列表页页码正好可以提前构造成一个列表，所以我们可以选用多进程里面的进程池 Pool 来实现这个过程。</p>
                  <p>这里我们需要改写下 main 方法的调用，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    index_html = scrape_index(page)</span><br><span class="line">    detail_urls = parse_index(index_html)</span><br><span class="line">    <span class="keyword">for</span> detail_url <span class="keyword">in</span> detail_urls:</span><br><span class="line">        detail_html = scrape_detail(detail_url)</span><br><span class="line">        data = parse_detail(detail_html)</span><br><span class="line">        logging.info(<span class="string">'get detail data %s'</span>, data)</span><br><span class="line">        logging.info(<span class="string">'saving data to json data'</span>)</span><br><span class="line">        save_data(data)</span><br><span class="line">        logging.info(<span class="string">'data saved successfully'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = multiprocessing.Pool()</span><br><span class="line">    pages = range(<span class="number">1</span>, TOTAL_PAGE + <span class="number">1</span>)</span><br><span class="line">    pool.map(main, pages)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们首先给 main 方法添加了一个参数 page，用以表示列表页的页码。接着我们声明了一个进程池，并声明了 pages 为所有需要遍历的页码，即 1-10。最后调用 map 方法，第一个参数就是需要被调用的参数，第二个参数就是 pages，即需要遍历的页码。</p>
                  <p>这样 pages 就会被依次遍历，把 1-10 这 10 个页码分别传递给 main 方法，并把每次的调用变成一个进程，加入到进程池中执行，进程池会根据当前运行环境来决定运行多少进程。比如我的机器的 CPU 有 8 个核，那么进程池的大小会默认设定为 8，这样就会同时有 8 个进程并行执行。</p>
                  <p>运行输出结果和之前类似，但是可以明显看到加了多进程执行之后，爬取速度快了非常多。可以清空一下之前的爬取数据，可以发现数据依然可以被正常保存成 JSON 文件。</p>
                  <h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2>
                  <p>好了，到现在为止，我们就完成了全站电影数据的爬取并实现了存储和优化。</p>
                  <p>我们本节用到的库有 requests、multiprocessing、re、logging 等，通过这个案例实战，我们把前面学习到的知识都串联了起来，其中的一些实现方法可以好好思考和体会，也希望这个案例能够让你对爬虫的实现有更实际的了解。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/ScrapeSsr1。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapeSsr1。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-14 09:23:56" itemprop="dateCreated datePublished" datetime="2022-02-14T09:23:56+08:00">2022-02-14</time>
                </span>
                <span id="/202224.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 基础爬虫案例爬取实战" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>15k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>14 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202222.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202222.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 方便好用的 requests</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>上一节中，我们了解了 urllib 的基本用法，但是其中确实有不方便的地方，比如处理网页验证和 Cookie 时，需要写 Opener 和 Handler 来处理。另外我们要实现 POST、PUT 等请求时写法也不太方便。</p>
                  <p>为了更加方便地实现这些操作，就有了更为强大的库 requests，有了它，Cookie、登录验证、代理设置等操作都不是事儿。</p>
                  <p>接下来，让我们领略一下它的强大之处吧。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>在开始之前，请确保已经正确安装好了 requests 库，如尚未安装可以使用 pip3 来安装：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> requests</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更加详细的安装说明可以参考 <a href="https://setup.scrape.center/requests。" target="_blank" rel="noopener">https://setup.scrape.center/requests。</a></p>
                  <h2 id="2-实例引入"><a href="#2-实例引入" class="headerlink" title="2. 实例引入"></a>2. 实例引入</h2>
                  <p>urllib 库中的 urlopen 方法实际上是以 GET 方式请求网页，而 requests 中相应的方法就是 get 方法，是不是感觉表达更明确一些？下面通过实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com/'</span>)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r.status_code)</span><br><span class="line">print(type(r.text))</span><br><span class="line">print(r.text[:<span class="number">100</span>])</span><br><span class="line">print(r.cookies)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">requests</span>.<span class="title">models</span>.<span class="title">Response</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;!<span class="title">DOCTYPE</span> <span class="title">html</span>&gt;</span></span><br><span class="line"><span class="class">&lt;!--<span class="title">STATUS</span> <span class="title">OK</span>--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charse</span></span><br><span class="line"><span class="class">&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们调用 get 方法实现与 urlopen 相同的操作，得到一个 Response 对象，然后分别输出了 Response 的类型、状态码、响应体的类型、内容以及 Cookie。</p>
                  <p>通过运行结果可以发现，它的返回类型是 <code>requests.models.Response</code>，响应体的类型是字符串 str，Cookie 的类型是 RequestsCookieJar。</p>
                  <p>使用 get 方法成功实现一个 GET 请求，这倒不算什么，更方便之处在于其他的请求类型依然可以用一句话来完成，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>)</span><br><span class="line">r = requests.post(<span class="string">'https://httpbin.org/post'</span>)</span><br><span class="line">r = requests.put(<span class="string">'https://httpbin.org/put'</span>)</span><br><span class="line">r = requests.delete(<span class="string">'https://httpbin.org/delete'</span>)</span><br><span class="line">r = requests.patch(<span class="string">'https://httpbin.org/patch'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里分别用 post、put、delete 等方法实现了 POST、PUT、DELETE 等请求。是不是比 urllib 简单太多了？</p>
                  <p>其实这只是冰山一角，更多的还在后面。</p>
                  <h2 id="3-GET-请求"><a href="#3-GET-请求" class="headerlink" title="3. GET 请求"></a>3. GET 请求</h2>
                  <p>HTTP 中最常见的请求之一就是 GET 请求，下面首先来详细了解一下利用 requests 构建 GET 请求的方法。</p>
                  <h4 id="基本实例"><a href="#基本实例" class="headerlink" title="基本实例"></a>基本实例</h4>
                  <p>首先，构建一个最简单的 GET 请求，请求的链接为 <a href="https://httpbin.org/get" target="_blank" rel="noopener">https://httpbin.org/get</a>，该网站会判断如果客户端发起的是 GET 请求的话，它返回相应的请求信息：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;,</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*&#x2F;*&quot;,</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests&#x2F;2.22.0&quot;,</span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root&#x3D;1-5e6e3a2e-6b1a28288d721c9e425a462a&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;origin&quot;: &quot;17.20.233.237&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;https:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，我们成功发起了 GET 请求，返回结果中包含请求头、URL、IP 等信息。</p>
                  <p>那么，对于 GET 请求，如果要附加额外的信息，一般怎样添加呢？比如现在想添加两个参数，其中 name 是 germey，age 是 25，URL 就可以写成如下内容：</p>
                  <figure class="highlight avrasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">https:</span>//httpbin<span class="meta">.org</span>/get?name=germey&amp;age=<span class="number">25</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>要构造这个请求链接，是不是要直接写成这样呢？</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get?name=germey&amp;age=25'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样也可以，但是是不是有点不人性化呢？这些参数还需要我们手动去拼接，实现起来有点不优雅。</p>
                  <p>一般情况下，这种信息我们利用 params 这个参数就可以直接传递了，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>, params=data)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;</span><br><span class="line">    <span class="string">"age"</span>: <span class="string">"25"</span>,</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>,</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"python-requests/2.10.0"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"122.4.215.33"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://httpbin.org/get?age=22&amp;name=germey"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们把 URL 参数通过一个字典的形式传给 get 方法的 params 参数，通过返回信息我们可以判断，请求的链接自动被构造成了：<a href="https://httpbin.org/get?age=22&amp;name=germey" target="_blank" rel="noopener">https://httpbin.org/get?age=22&amp;name=germey</a>，这样我们就不用再去自己构造 URL 了，非常方便。</p>
                  <p>另外，网页的返回类型实际上是 str 类型，但是它很特殊，是 JSON 格式的。所以，如果想直接解析返回结果，得到一个 JSON 格式的数据的话，可以直接调用 json 方法。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>)</span><br><span class="line">print(type(r.text))</span><br><span class="line">print(r.json())</span><br><span class="line">print(type(r.json()))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span>'<span class="title">str</span>'&gt;</span></span><br><span class="line">&#123;'headers': &#123;'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.10.0'&#125;, 'url': 'http://httpbin.org/get', 'args': &#123;&#125;, 'origin': '182.33.248.131'&#125;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">dict</span>'&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，调用 json 方法，就可以将返回结果是 JSON 格式的字符串转化为字典。</p>
                  <p>但需要注意的是，如果返回结果不是 JSON 格式，便会出现解析错误，抛出 json.decoder.JSONDecodeError 异常。</p>
                  <h4 id="抓取网页"><a href="#抓取网页" class="headerlink" title="抓取网页"></a>抓取网页</h4>
                  <p>上面的请求链接返回的是 JSON 形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容了。下面以一个实例页面 <a href="https://ssr1.scrape.center/" target="_blank" rel="noopener">https://ssr1.scrape.center/</a> 来试一下，我们再加上一点提取信息的逻辑，将代码完善成如下的样子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://ssr1.scrape.center/'</span>)</span><br><span class="line">pattern = re.compile(<span class="string">'&lt;h2.*?&gt;(.*?)&lt;/h2&gt;'</span>, re.S)</span><br><span class="line">titles = re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这个例子中我们用到了最基础的正则表达式来匹配出所有的问题内容。关于正则表达式的相关内容，我们会在下一节详细介绍，这里作为实例来配合讲解。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="string">'肖申克的救赎 - The Shawshank Redemption'</span>, <span class="string">'霸王别姬 - Farewell My Concubine'</span>, <span class="string">'泰坦尼克号 - Titanic'</span>, <span class="string">'罗马假日 - Roman Holiday'</span>, <span class="string">'这个杀手不太冷 - Léon'</span>, <span class="string">'魂断蓝桥 - Waterloo Bridge'</span>, <span class="string">'唐伯虎点秋香 - Flirting Scholar'</span>, <span class="string">'喜剧之王 - The King of Comedy'</span>, <span class="string">'楚门的世界 - The Truman Show'</span>, <span class="string">'活着 - To Live'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们发现，这里成功提取出了所有的电影标题，一个最基本的抓取和提取流程就完成了。</p>
                  <h4 id="抓取二进制数据"><a href="#抓取二进制数据" class="headerlink" title="抓取二进制数据"></a>抓取二进制数据</h4>
                  <p>在上面的例子中，我们抓取的是网站的一个页面，实际上它返回的是一个 HTML 文档。如果想抓取图片、音频、视频等文件，应该怎么办呢？</p>
                  <p>图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制数据。</p>
                  <p>下面以示例网站的站点图标为例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://scrape.center/favicon.ico'</span>)</span><br><span class="line">print(r.text)</span><br><span class="line">print(r.content)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/1sfy2.png" alt="image-20210704202919308"></p>
                  <p>这里打印了 Response 对象的两个属性，一个是 text，另一个是 content。</p>
                  <p>运行结果如图所示，分别是 r.text 和 r.content 的结果。</p>
                  <p><img src="https://cdn.cuiqingcai.com/vexbl.png" alt="image-20210704203039567"></p>
                  <p><img src="https://cdn.cuiqingcai.com/xfo4w.png" alt="image-20210704202959490"></p>
                  <p>可以注意到，前者出现了乱码，后者结果前带有一个 b，这代表是 bytes 类型的数据。由于图片是二进制数据，所以前者在打印时转化为 str 类型，也就是图片直接转化为字符串，这理所当然会出现乱码。</p>
                  <p>上面返回的结果我们并不能看懂，它实际上是图片的二进制数据，没关系，我们将刚才提取到的信息保存下来就好了，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://scrape.center/favicon.ico'</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里用了 open 方法，它的第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据。</p>
                  <p>运行结束之后，可以发现在文件夹中出现了名为 favicon.ico 的图标，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/16oto.png" alt="image-20210704203204899"></p>
                  <p>这样，我们就把二进制数据成功保存成一张图片了，这个小图标就被我们成功爬取下来了。</p>
                  <p>同样地，音频和视频文件我们也可以用这种方法获取。</p>
                  <h4 id="添加-headers"><a href="#添加-headers" class="headerlink" title="添加 headers"></a>添加 headers</h4>
                  <p>我们知道，在发起一个 HTTP 请求的时候，会有一个请求头 Request Headers，那么这个怎么来设置呢？</p>
                  <p>很简单，我们使用 headers 参数就可以完成了。</p>
                  <p>在刚才的实例中，实际上我们是没有设置 Request Headers 信息的，如果不设置，某些网站会发现这不是一个正常的浏览器发起的请求，网站可能会返回异常的结果，导致网页抓取失败。</p>
                  <p>要添加 Headers 信息，比如我们这里想添加一个 User-Agent 字段，我们可以这么来写：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://ssr1.scrape.center/'</span>, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，我们可以在 headers 这个参数中任意添加其他的字段信息。</p>
                  <h2 id="4-POST-请求"><a href="#4-POST-请求" class="headerlink" title="4. POST 请求"></a>4. POST 请求</h2>
                  <p>前面我们了解了最基本的 GET 请求，另外一种比较常见的请求方式是 POST。使用 requests 实现 POST 请求同样非常简单，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="string">'25'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">"https://httpbin.org/post"</span>, data=data)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里还是请求 <a href="https://httpbin.org/post" target="_blank" rel="noopener">https://httpbin.org/post</a>，该网站可以判断如果请求是 POST 方式，就把相关请求信息返回。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;,</span><br><span class="line">  &quot;data&quot;: &quot;&quot;,</span><br><span class="line">  &quot;files&quot;: &#123;&#125;,</span><br><span class="line">  &quot;form&quot;: &#123;</span><br><span class="line">    &quot;age&quot;: &quot;25&quot;,</span><br><span class="line">    &quot;name&quot;: &quot;germey&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*&#x2F;*&quot;,</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,</span><br><span class="line">    &quot;Content-Length&quot;: &quot;18&quot;,</span><br><span class="line">    &quot;Content-Type&quot;: &quot;application&#x2F;x-www-form-urlencoded&quot;,</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests&#x2F;2.22.0&quot;,</span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root&#x3D;1-5e6e3b52-0f36782ea980fce53c8c6524&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;json&quot;: null,</span><br><span class="line">  &quot;origin&quot;: &quot;17.20.232.237&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;https:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，我们成功获得了返回结果，其中 form 部分就是提交的数据，这就证明 POST 请求成功发送了。</p>
                  <h2 id="5-响应"><a href="#5-响应" class="headerlink" title="5. 响应"></a>5. 响应</h2>
                  <p>发送请求后，得到的自然就是响应。在上面的实例中，我们使用 text 和 content 获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookie 等。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://ssr1.scrape.center/'</span>)</span><br><span class="line">print(type(r.status_code), r.status_code)</span><br><span class="line">print(type(r.headers), r.headers)</span><br><span class="line">print(type(r.cookies), r.cookies)</span><br><span class="line">print(type(r.url), r.url)</span><br><span class="line">print(type(r.history), r.history)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里分别打印输出 status_code 属性得到状态码，输出 headers 属性得到响应头，输出 cookies 属性得到 Cookie，输出 url 属性得到 URL，输出 history 属性得到请求历史。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">int</span>'&gt; 200</span></span><br><span class="line">&lt;class 'requests.structures.CaseInsensitiveDict'&gt; &#123;'Server': 'nginx/1.17.8', 'Date': 'Sat, 30 May 2020 16:56:40 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'X-Frame-Options': 'DENY', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains', 'Content-Encoding': 'gzip'&#125;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">requests</span>.<span class="title">cookies</span>.<span class="title">RequestsCookieJar</span>'&gt; &lt;<span class="title">RequestsCookieJar</span>[]&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt; <span class="title">https</span>:</span>//ssr1.scrape.center/</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt; []</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，headers 和 cookies 这两个属性得到的结果分别是 CaseInsensitiveDict 和 RequestsCookieJar 类型。</p>
                  <p>在第一章我们知道，状态码是用来表示响应状态的，比如返回 200 代表我们得到的响应是没问题的，上面的例子正好输出的结果也是 200，所以我们可以通过判断 Response 的状态码来知道爬取是否爬取成功。</p>
                  <p>requests 还提供了一个内置的状态码查询对象 requests.codes，用法示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://ssr1.scrape.center/'</span>)</span><br><span class="line">exit() <span class="keyword">if</span> <span class="keyword">not</span> r.status_code == requests.codes.ok <span class="keyword">else</span> print(<span class="string">'Request Successfully'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用 requests.codes.ok 得到的是成功的状态码 200。</p>
                  <p>这样的话，我们就不用再在程序里面写状态吗对应的数字了，用字符串表示状态码会显得更加直观。</p>
                  <p>当然，肯定不能只有 ok 这个条件码。</p>
                  <p>下面列出了返回码和相应的查询条件：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 信息性状态码</span></span><br><span class="line"><span class="number">100</span>: (<span class="string">'continue'</span>,),</span><br><span class="line"><span class="number">101</span>: (<span class="string">'switching_protocols'</span>,),</span><br><span class="line"><span class="number">102</span>: (<span class="string">'processing'</span>,),</span><br><span class="line"><span class="number">103</span>: (<span class="string">'checkpoint'</span>,),</span><br><span class="line"><span class="number">122</span>: (<span class="string">'uri_too_long'</span>, <span class="string">'request_uri_too_long'</span>),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 成功状态码</span></span><br><span class="line"><span class="number">200</span>: (<span class="string">'ok'</span>, <span class="string">'okay'</span>, <span class="string">'all_ok'</span>, <span class="string">'all_okay'</span>, <span class="string">'all_good'</span>, <span class="string">'\\o/'</span>, <span class="string">'✓'</span>),</span><br><span class="line"><span class="number">201</span>: (<span class="string">'created'</span>,),</span><br><span class="line"><span class="number">202</span>: (<span class="string">'accepted'</span>,),</span><br><span class="line"><span class="number">203</span>: (<span class="string">'non_authoritative_info'</span>, <span class="string">'non_authoritative_information'</span>),</span><br><span class="line"><span class="number">204</span>: (<span class="string">'no_content'</span>,),</span><br><span class="line"><span class="number">205</span>: (<span class="string">'reset_content'</span>, <span class="string">'reset'</span>),</span><br><span class="line"><span class="number">206</span>: (<span class="string">'partial_content'</span>, <span class="string">'partial'</span>),</span><br><span class="line"><span class="number">207</span>: (<span class="string">'multi_status'</span>, <span class="string">'multiple_status'</span>, <span class="string">'multi_stati'</span>, <span class="string">'multiple_stati'</span>),</span><br><span class="line"><span class="number">208</span>: (<span class="string">'already_reported'</span>,),</span><br><span class="line"><span class="number">226</span>: (<span class="string">'im_used'</span>,),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重定向状态码</span></span><br><span class="line"><span class="number">300</span>: (<span class="string">'multiple_choices'</span>,),</span><br><span class="line"><span class="number">301</span>: (<span class="string">'moved_permanently'</span>, <span class="string">'moved'</span>, <span class="string">'\\o-'</span>),</span><br><span class="line"><span class="number">302</span>: (<span class="string">'found'</span>,),</span><br><span class="line"><span class="number">303</span>: (<span class="string">'see_other'</span>, <span class="string">'other'</span>),</span><br><span class="line"><span class="number">304</span>: (<span class="string">'not_modified'</span>,),</span><br><span class="line"><span class="number">305</span>: (<span class="string">'use_proxy'</span>,),</span><br><span class="line"><span class="number">306</span>: (<span class="string">'switch_proxy'</span>,),</span><br><span class="line"><span class="number">307</span>: (<span class="string">'temporary_redirect'</span>, <span class="string">'temporary_moved'</span>, <span class="string">'temporary'</span>),</span><br><span class="line"><span class="number">308</span>: (<span class="string">'permanent_redirect'</span>,</span><br><span class="line">      <span class="string">'resume_incomplete'</span>, <span class="string">'resume'</span>,), <span class="comment"># These 2 to be removed in 3.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 客户端错误状态码</span></span><br><span class="line"><span class="number">400</span>: (<span class="string">'bad_request'</span>, <span class="string">'bad'</span>),</span><br><span class="line"><span class="number">401</span>: (<span class="string">'unauthorized'</span>,),</span><br><span class="line"><span class="number">402</span>: (<span class="string">'payment_required'</span>, <span class="string">'payment'</span>),</span><br><span class="line"><span class="number">403</span>: (<span class="string">'forbidden'</span>,),</span><br><span class="line"><span class="number">404</span>: (<span class="string">'not_found'</span>, <span class="string">'-o-'</span>),</span><br><span class="line"><span class="number">405</span>: (<span class="string">'method_not_allowed'</span>, <span class="string">'not_allowed'</span>),</span><br><span class="line"><span class="number">406</span>: (<span class="string">'not_acceptable'</span>,),</span><br><span class="line"><span class="number">407</span>: (<span class="string">'proxy_authentication_required'</span>, <span class="string">'proxy_auth'</span>, <span class="string">'proxy_authentication'</span>),</span><br><span class="line"><span class="number">408</span>: (<span class="string">'request_timeout'</span>, <span class="string">'timeout'</span>),</span><br><span class="line"><span class="number">409</span>: (<span class="string">'conflict'</span>,),</span><br><span class="line"><span class="number">410</span>: (<span class="string">'gone'</span>,),</span><br><span class="line"><span class="number">411</span>: (<span class="string">'length_required'</span>,),</span><br><span class="line"><span class="number">412</span>: (<span class="string">'precondition_failed'</span>, <span class="string">'precondition'</span>),</span><br><span class="line"><span class="number">413</span>: (<span class="string">'request_entity_too_large'</span>,),</span><br><span class="line"><span class="number">414</span>: (<span class="string">'request_uri_too_large'</span>,),</span><br><span class="line"><span class="number">415</span>: (<span class="string">'unsupported_media_type'</span>, <span class="string">'unsupported_media'</span>, <span class="string">'media_type'</span>),</span><br><span class="line"><span class="number">416</span>: (<span class="string">'requested_range_not_satisfiable'</span>, <span class="string">'requested_range'</span>, <span class="string">'range_not_satisfiable'</span>),</span><br><span class="line"><span class="number">417</span>: (<span class="string">'expectation_failed'</span>,),</span><br><span class="line"><span class="number">418</span>: (<span class="string">'im_a_teapot'</span>, <span class="string">'teapot'</span>, <span class="string">'i_am_a_teapot'</span>),</span><br><span class="line"><span class="number">421</span>: (<span class="string">'misdirected_request'</span>,),</span><br><span class="line"><span class="number">422</span>: (<span class="string">'unprocessable_entity'</span>, <span class="string">'unprocessable'</span>),</span><br><span class="line"><span class="number">423</span>: (<span class="string">'locked'</span>,),</span><br><span class="line"><span class="number">424</span>: (<span class="string">'failed_dependency'</span>, <span class="string">'dependency'</span>),</span><br><span class="line"><span class="number">425</span>: (<span class="string">'unordered_collection'</span>, <span class="string">'unordered'</span>),</span><br><span class="line"><span class="number">426</span>: (<span class="string">'upgrade_required'</span>, <span class="string">'upgrade'</span>),</span><br><span class="line"><span class="number">428</span>: (<span class="string">'precondition_required'</span>, <span class="string">'precondition'</span>),</span><br><span class="line"><span class="number">429</span>: (<span class="string">'too_many_requests'</span>, <span class="string">'too_many'</span>),</span><br><span class="line"><span class="number">431</span>: (<span class="string">'header_fields_too_large'</span>, <span class="string">'fields_too_large'</span>),</span><br><span class="line"><span class="number">444</span>: (<span class="string">'no_response'</span>, <span class="string">'none'</span>),</span><br><span class="line"><span class="number">449</span>: (<span class="string">'retry_with'</span>, <span class="string">'retry'</span>),</span><br><span class="line"><span class="number">450</span>: (<span class="string">'blocked_by_windows_parental_controls'</span>, <span class="string">'parental_controls'</span>),</span><br><span class="line"><span class="number">451</span>: (<span class="string">'unavailable_for_legal_reasons'</span>, <span class="string">'legal_reasons'</span>),</span><br><span class="line"><span class="number">499</span>: (<span class="string">'client_closed_request'</span>,),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务端错误状态码</span></span><br><span class="line"><span class="number">500</span>: (<span class="string">'internal_server_error'</span>, <span class="string">'server_error'</span>, <span class="string">'/o\\'</span>, <span class="string">'✗'</span>),</span><br><span class="line"><span class="number">501</span>: (<span class="string">'not_implemented'</span>,),</span><br><span class="line"><span class="number">502</span>: (<span class="string">'bad_gateway'</span>,),</span><br><span class="line"><span class="number">503</span>: (<span class="string">'service_unavailable'</span>, <span class="string">'unavailable'</span>),</span><br><span class="line"><span class="number">504</span>: (<span class="string">'gateway_timeout'</span>,),</span><br><span class="line"><span class="number">505</span>: (<span class="string">'http_version_not_supported'</span>, <span class="string">'http_version'</span>),</span><br><span class="line"><span class="number">506</span>: (<span class="string">'variant_also_negotiates'</span>,),</span><br><span class="line"><span class="number">507</span>: (<span class="string">'insufficient_storage'</span>,),</span><br><span class="line"><span class="number">509</span>: (<span class="string">'bandwidth_limit_exceeded'</span>, <span class="string">'bandwidth'</span>),</span><br><span class="line"><span class="number">510</span>: (<span class="string">'not_extended'</span>,),</span><br><span class="line"><span class="number">511</span>: (<span class="string">'network_authentication_required'</span>, <span class="string">'network_auth'</span>, <span class="string">'network_authentication'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>比如，如果想判断结果是不是 404 状态，可以用 <code>requests.codes.not_found</code> 来比对。</p>
                  <h2 id="6-高级用法"><a href="#6-高级用法" class="headerlink" title="6. 高级用法"></a>6. 高级用法</h2>
                  <p>前面我们了解了 requests 的基本用法，如基本的 GET、POST 请求以及 Response 对象。下面我们再来了解下 requests 的一些高级用法，如文件上传、Cookie 设置、代理设置等。</p>
                  <h3 id="文件上传"><a href="#文件上传" class="headerlink" title="文件上传"></a>文件上传</h3>
                  <p>我们知道 requests 可以模拟提交一些数据。假如有的网站需要上传文件，我们也可以用它来实现，这非常简单，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">'file'</span>: open(<span class="string">'favicon.ico'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">'https://httpbin.org/post'</span>, files=files)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在前一节中我们保存了一个文件 favicon.ico，这次用它来模拟文件上传的过程。需要注意的是，favicon.ico 需要和当前脚本在同一目录下。如果有其他文件，当然也可以使用其他文件来上传，更改下代码即可。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"files"</span>: &#123;</span><br><span class="line">    <span class="string">"file"</span>: <span class="string">"data:application/octet-stream;base64,AAABAAI..."</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"form"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>,</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"6665"</span>,</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"multipart/form-data; boundary=41fc691282cc894f8f06adabb24f05fb"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"python-requests/2.22.0"</span>,</span><br><span class="line">    <span class="string">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5e6e3c0b-45b07bdd3a922e364793ef48"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"json"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"16.20.232.237"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上省略部分内容，这个网站会返回响应，里面包含 files 这个字段，而 form 字段是空的，这证明文件上传部分会单独有一个 files 字段来标识。</p>
                  <h3 id="Cookie-设置"><a href="#Cookie-设置" class="headerlink" title="Cookie 设置"></a>Cookie 设置</h3>
                  <p>前面我们使用 urllib 处理过 Cookie，写法比较复杂，而有了 requests，获取和设置 Cookie 只需一步即可完成。</p>
                  <p>我们先用一个实例看一下获取 Cookie 的过程：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(r.cookies)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> r.cookies.items():</span><br><span class="line">    print(key + <span class="string">'='</span> + value)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;RequestsCookieJar[&lt;Cookie BDORZ=<span class="number">27315</span> <span class="keyword">for</span> .baidu.com/&gt;]&gt;</span><br><span class="line">BDORZ=<span class="number">27315</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们首先调用 cookies 属性即可成功得到 Cookie，可以发现它是 RequestCookieJar 类型。然后用 items 方法将其转化为元组组成的列表，遍历输出每一个 Cookie 条目的名称和值，实现 Cookie 的遍历解析。</p>
                  <p>当然，我们也可以直接用 Cookie 来维持登录状态，下面我们以 GitHub 为例来说明一下，首先我们登录 GitHub，然后将 Headers 中的 Cookie 内容复制下来，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/odrdk.png" alt="image-20200301214840166"></p>
                  <p>这里可以替换成你自己的 Cookie，将其设置到 Headers 里面，然后发送请求，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'_octo=GH1.1.1849343058.1576602081; _ga=GA1.2.90460451.1576602111; __Host-user_session_same_site=nbDv62kHNjp4N5KyQNYZ208waeqsmNgxFnFC88rnV7gTYQw_; _device_id=a7ca73be0e8f1a81d1e2ebb5349f9075; user_session=nbDv62kHNjp4N5KyQNYZ208waeqsmNgxFnFC88rnV7gTYQw_; logged_in=yes; dotcom_user=Germey; tz=Asia%2FShanghai; has_recent_activity=1; _gat=1; _gh_sess=your_session_info'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>,</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://github.com/'</span>, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们发现，结果中包含了登录后才能包含的结果，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/3j50b.png" alt="image-20200301215251376"></p>
                  <p>可以看到这里包含了我的 GitHub 用户名信息，你如果尝试之后同样可以得到你的用户信息。</p>
                  <p>得到这样类似的结果，就说明我们用 Cookie 就成功模拟了登录状态，这样我们就能爬取登录之后才能看到的页面了。</p>
                  <p>当然，我们也可以通过 cookies 参数来设置 Cookie 的信息，这里我们可以构造一个 RequestsCookieJar 对象，然后把刚才复制的 Cookie 处理下并赋值，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">cookies = <span class="string">'_octo=GH1.1.1849343058.1576602081; _ga=GA1.2.90460451.1576602111; __Host-user_session_same_site=nbDv62kHNjp4N5KyQNYZ208waeqsmNgxFnFC88rnV7gTYQw_; _device_id=a7ca73be0e8f1a81d1e2ebb5349f9075; user_session=nbDv62kHNjp4N5KyQNYZ208waeqsmNgxFnFC88rnV7gTYQw_; logged_in=yes; dotcom_user=Germey; tz=Asia%2FShanghai; has_recent_activity=1; _gat=1; _gh_sess=your_session_info'</span></span><br><span class="line">jar = requests.cookies.RequestsCookieJar()</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookies.split(<span class="string">';'</span>):</span><br><span class="line">    key, value = cookie.split(<span class="string">'='</span>, <span class="number">1</span>)</span><br><span class="line">    jar.set(key, value)</span><br><span class="line">r = requests.get(<span class="string">'https://github.com/'</span>, cookies=jar, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们首先新建了一个 RequestCookieJar 对象，然后将复制下来的 cookies 利用 split 方法分割，接着利用 set 方法设置好每个 Cookie 的 key 和 value，然后通过调用 requests 的 get 方法并传递给 cookies 参数即可。</p>
                  <p>测试后，发现同样可以正常登录。</p>
                  <h3 id="Session-维持"><a href="#Session-维持" class="headerlink" title="Session 维持"></a>Session 维持</h3>
                  <p>在 requests 中，如果直接利用 get 或 post 等方法的确可以做到模拟网页的请求，但是这实际上是相当于不同的 Session，也就是说相当于你用了两个浏览器打开了不同的页面。</p>
                  <p>设想这样一个场景，第一个请求利用 requests 的 post 方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，又用了一次 requests 的 get 方法去请求个人信息页面。</p>
                  <p>实际上，这相当于打开了两个浏览器，是两个完全独立的操作，对应两个完全不相关的 Session，能成功获取个人信息吗？那当然不能。</p>
                  <p>有人可能说了，我在两次请求时设置一样的 Cookies 不就行了？可以，但这样做起来显得很烦琐，我们有更简单的解决方法。</p>
                  <p>其实解决这个问题的主要方法就是维持同一个 Session，也就是相当于打开一个新的浏览器选项卡而不是新开一个浏览器。但是我又不想每次设置 Cookies，那该怎么办呢？这时候就有了新的利器 —— Session 对象。</p>
                  <p>利用它，我们可以方便地维护一个 Session，而且不用担心 Cookie 的问题，它会帮我们自动处理好。</p>
                  <p>我们先做一个小实验吧，如果沿用之前的写法，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'https://httpbin.org/cookies/set/number/123456789'</span>)</span><br><span class="line">r = requests.get(<span class="string">'https://httpbin.org/cookies'</span>)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们请求了一个测试网址 <a href="https://httpbin.org/cookies/set/number/123456789" target="_blank" rel="noopener">https://httpbin.org/cookies/set/number/123456789</a>。请求这个网址时，可以设置一个 Cookie 条目，名称叫作 number，内容是 123456789，随后又请求了 <a href="https://httpbin.org/cookies" target="_blank" rel="noopener">https://httpbin.org/cookies</a>，此网址可以获取当前的 Cookie 信息。</p>
                  <p>这样能成功获取到设置的 Cookie 吗？试试看。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"cookies"</span>: &#123;&#125;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这并不行。</p>
                  <p>这时候，我们再用刚才所说的 Session 试试看：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">'https://httpbin.org/cookies/set/number/123456789'</span>)</span><br><span class="line">r = s.get(<span class="string">'https://httpbin.org/cookies'</span>)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>再看下运行结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"cookies"</span>: &#123;<span class="string">"number"</span>: <span class="string">"123456789"</span>&#125;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这些可以看到 Cookies 被成功获取了！这下能体会到同一个会话和不同会话的区别了吧！</p>
                  <p>所以，利用 Session，可以做到模拟同一个会话而不用担心 Cookie 的问题。它通常用于模拟登录成功之后再进行下一步的操作。</p>
                  <p>Session 在平常用得非常广泛，可以用于模拟在一个浏览器中打开同一站点的不同页面，后面会有专门的章节来讲解这部分内容。</p>
                  <h3 id="SSL-证书验证"><a href="#SSL-证书验证" class="headerlink" title="SSL 证书验证"></a>SSL 证书验证</h3>
                  <p>现在很多网站都要求使用 HTTPS 协议，但是有些网站可能并没有设置好 HTTPS 证书，或者网站的 HTTPS 证书可能并不被 CA 机构认可，这时候，这些网站可能就会出现 SSL 证书错误的提示。</p>
                  <p>比如这个示例网站：<a href="https://ssr2.scrape.center/" target="_blank" rel="noopener">https://ssr2.scrape.center/</a>，如果我们用 Chrome 浏览器打开这个 URL，则会提示「您的连接不是私密连接」这样的错误，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/gyo9f.png" alt="image-20210704204017465"></p>
                  <p>我们可以在浏览器中通过一些设置来忽略证书的验证。</p>
                  <p>但是如果我们想用 requests 来请求这类网站，会遇到什么问题呢？我们用代码来试一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://ssr2.scrape.center/'</span>)</span><br><span class="line">print(response.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">requests.exceptions.SSLError: HTTPSConnectionPool(host=<span class="string">'ssr2.scrape.center'</span>, port=<span class="number">443</span>): Max retries exceeded <span class="keyword">with</span> url: / (Caused by SSLError(SSLCertVerificationError(<span class="number">1</span>, <span class="string">'[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)'</span>)))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里直接抛出了 SSLError 错误，原因就是因为我们请求的 URL 的证书是无效的。</p>
                  <p>那如果我们一定要爬取这个网站怎么办呢？我们可以使用 verify 参数控制是否验证证书，如果将其设置为 False，在请求时就不会再验证证书是否有效。如果不加 verify 参数的话，默认值是 True，会自动验证。</p>
                  <p>我们改写代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://ssr2.scrape.center/'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就会打印出请求成功的状态码：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">/usr/local/lib/python3<span class="number">.7</span>/site-packages/urllib3/connectionpool.py:<span class="number">857</span>: InsecureRequestWarning: Unverified HTTPS request <span class="keyword">is</span> being made. Adding certificate verification <span class="keyword">is</span> strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html<span class="comment">#ssl-warnings</span></span><br><span class="line">  InsecureRequestWarning)</span><br><span class="line"><span class="number">200</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>不过我们发现报了一个警告，它建议我们给它指定证书。我们可以通过设置忽略警告的方式来屏蔽这个警告：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line"></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://ssr2.scrape.center/'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>或者通过捕获警告到日志的方式忽略警告：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">logging.captureWarnings(<span class="literal">True</span>)</span><br><span class="line">response = requests.get(<span class="string">'https://ssr2.scrape.center/'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，我们也可以指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://ssr2.scrape.center/'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/server.key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，上面的代码是演示实例，我们需要有 crt 和 key 文件，并且指定它们的路径。另外注意，本地私有证书的 key 必须是解密状态，加密状态的 key 是不支持的。</p>
                  <h3 id="超时设置"><a href="#超时设置" class="headerlink" title="超时设置"></a>超时设置</h3>
                  <p>在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能收到响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错。这需要用到 timeout 参数。这个时间的计算是发出请求到服务器返回响应的时间。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(r.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>通过这样的方式，我们可以将超时时间设置为 1 秒，如果 1 秒内没有响应，那就抛出异常。</p>
                  <p>实际上，请求分为两个阶段，即连接（connect）和读取（read）。</p>
                  <p>上面设置的 timeout 将用作连接和读取这二者的 timeout 总和。</p>
                  <p>如果要分别指定，就可以传入一个元组：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>, timeout=(<span class="number">5</span>, <span class="number">30</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果想永久等待，可以直接将 timeout 设置为 None，或者不设置直接留空，因为默认是 None。这样的话，如果服务器还在运行，但是响应特别慢，那就慢慢等吧，它永远不会返回超时错误的。其用法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="literal">None</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>或直接不加参数：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">r = requests.get(<span class="string">'https://httpbin.org/get'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="身份认证"><a href="#身份认证" class="headerlink" title="身份认证"></a>身份认证</h3>
                  <p>在上一节我们讲到，在访问启用了基本身份认证的网站时，我们会首先遇到一个认证窗口，例如：<a href="https://ssr3.scrape.center/" target="_blank" rel="noopener">https://ssr3.scrape.center/</a>，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/4cha6.png" alt="image-20210704202140395"></p>
                  <p>这个网站就是启用了基本身份认证，在上一节中我们可以利用 urllib 来实现身份的校验，但实现起来相对繁琐。那在 reqeusts 中怎么做呢？当然也有办法。</p>
                  <p>我们可以使用 requests 自带的身份认证功能，通过 auth 参数即可设置，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://ssr3.scrape.center/'</span>, auth=HTTPBasicAuth(<span class="string">'admin'</span>, <span class="string">'admin'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个示例网站的用户名和密码都是 admin，在这里我们可以直接设置。</p>
                  <p>如果用户名和密码正确的话，请求时就会自动认证成功，会返回 200 状态码；如果认证失败，则返回 401 状态码。</p>
                  <p>当然，如果参数都传一个 HTTPBasicAuth 类，就显得有点烦琐了，所以 requests 提供了一个更简单的写法，可以直接传一个元组，它会默认使用 HTTPBasicAuth 这个类来认证。</p>
                  <p>所以上面的代码可以直接简写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://ssr3.scrape.center/'</span>, auth=(<span class="string">'admin'</span>, <span class="string">'admin'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此外，requests 还提供了其他认证方式，如 OAuth 认证，不过此时需要安装 oauth 包，安装命令如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 install requests_oauthlib</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>使用 OAuth1 认证的示例方法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_oauthlib <span class="keyword">import</span> OAuth1</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://api.twitter.com/1.1/account/verify_credentials.json'</span></span><br><span class="line">auth = OAuth1(<span class="string">'YOUR_APP_KEY'</span>, <span class="string">'YOUR_APP_SECRET'</span>,</span><br><span class="line">              <span class="string">'USER_OAUTH_TOKEN'</span>, <span class="string">'USER_OAUTH_TOKEN_SECRET'</span>)</span><br><span class="line">requests.get(url, auth=auth)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更多详细的功能就可以参考 requests_oauthlib 的官方文档：<a href="https://requests-oauthlib.readthedocs.org/" target="_blank" rel="noopener">https://requests-oauthlib.readthedocs.org/</a>，在此就不再赘述了。</p>
                  <h3 id="代理设置"><a href="#代理设置" class="headerlink" title="代理设置"></a>代理设置</h3>
                  <p>对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的 IP，导致一定时间段内无法访问。</p>
                  <p>那么，为了防止这种情况发生，我们需要设置代理来解决这个问题，这就需要用到 proxies 参数。可以用这样的方式设置：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">'http'</span>: <span class="string">'http://10.10.10.10:1080'</span>,</span><br><span class="line">  <span class="string">'https'</span>: <span class="string">'http://10.10.10.10:1080'</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://httpbin.org/get'</span>, proxies=proxies)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，直接运行这个实例可能不行，因为这个代理可能是无效的，可以直接搜索寻找有效的代理并替换试验一下。</p>
                  <p>若代理需要使用上文所述的身份认证，可以使用类似 <a href="http://user:password@host:port">http://user:password@host:port</a> 这样的语法来设置代理，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;<span class="string">'https'</span>: <span class="string">'http://user:password@10.10.10.10:1080/'</span>,&#125;</span><br><span class="line">requests.get(<span class="string">'https://httpbin.org/get'</span>, proxies=proxies)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>除了基本的 HTTP 代理外，requests 还支持 SOCKS 协议的代理。</p>
                  <p>首先，需要安装 socks 这个库：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 install &quot;requests[socks]&quot;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后就可以使用 SOCKS 协议代理了，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://user:password@host:port'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://user:password@host:port'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://httpbin.org/get'</span>, proxies=proxies)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="Prepared-Request"><a href="#Prepared-Request" class="headerlink" title="Prepared Request"></a>Prepared Request</h3>
                  <p>我们使用 requests 库的 get 和 post 方法当然直接可以发送请求，但有没有想过，这个请求在 requests 内部是怎么实现的呢？</p>
                  <p>实际上，requests 在发送请求的时候，是在内部构造了一个 Request 对象，并给这个对象赋予了各种参数，包括 url、headers、data 等等，然后直接把这个 Request 对象发送出去，请求成功后会再得到一个 Response 对象，再解析即可。</p>
                  <p>那么这个 Request 是什么类型呢？实际上它就是 Prepared Request。</p>
                  <p>我们深入一下，不用 get 方法，直接构造一个 Prepared Request 对象来试试，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://httpbin.org/post'</span></span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</span><br><span class="line">prepped = s.prepare_request(req)</span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们引入了 Request 这个类，然后用 url、data 和 headers 参数构造了一个 Request 对象，这时需要再调用 Session 的 prepare_request 方法将其转换为一个 Prepared Request 对象，然后调用 send 方法发送，运行结果如下：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"form"</span>: &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>,</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"11"</span>,</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36"</span>,</span><br><span class="line">    <span class="string">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5e5bd6a9-6513c838f35b06a0751606d8"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"json"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"167.220.232.237"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"http://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，我们达到了同样的 POST 请求效果。</p>
                  <p>有了 Request 这个对象，就可以将请求当作独立的对象来看待，这样在一些场景中我们可以直接操作这个 Request 对象，更灵活地实现请求的调度和各种操作。</p>
                  <p>更多的用法可以参考 requests 的官方文档：<a href="http://docs.python-requests.org/" target="_blank" rel="noopener">http://docs.python-requests.org/</a>。</p>
                  <h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2>
                  <p>本节的 requests 库的基本用法就介绍到这里了，怎么样？有没有感觉它比 urllib 使用起来更为方便。本节内容需要好好掌握，在后文我们会在实战中使用 requests 完成一个网站的爬取，巩固 requests 的相关知识。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/RequestsTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/RequestsTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-13 08:56:31" itemprop="dateCreated datePublished" datetime="2022-02-13T08:56:31+08:00">2022-02-13</time>
                </span>
                <span id="/202222.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 方便好用的 requests" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>22k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>20 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202221.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202221.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - urllib 爬虫初体验</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>首先我们介绍一个 Python 库，叫做 urllib，利用它我们可以实现 HTTP 请求的发送，而不用去关心 HTTP 协议本身甚至更低层的实现。我们只需要指定请求的 URL、请求头、请求体等信息即可实现 HTTP 请求的发送，同时 urllib 还可以把服务器返回的响应转化为 Python 对象，通过该对象我们便可以方便地获取响应的相关信息了，如响应状态码、响应头、响应体等等。</p>
                  <blockquote>
                    <p>注意：在 Python 2 中，有 urllib 和 urllib2 两个库来实现请求的发送。而在 Python 3 中，已经不存在 urllib2 这个库了，统一为 urllib，其官方文档链接为：<a href="https://docs.python.org/3/library/urllib.html" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.html</a>。</p>
                  </blockquote>
                  <p>首先，我们来了解一下 urllib 库的使用方法，它是 Python 内置的 HTTP 请求库，也就是说不需要额外安装即可使用。它包含如下 4 个模块。</p>
                  <ul>
                    <li><strong>request</strong>：它是最基本的 HTTP 请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只需要给库方法传入 URL 以及额外的参数，就可以模拟实现这个过程了。</li>
                    <li><strong>error</strong>：异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。</li>
                    <li><strong>parse</strong>：一个工具模块，提供了许多 URL 处理方法，比如拆分、解析和合并等。</li>
                    <li><strong>robotparser</strong>：主要用来识别网站的 robots.txt 文件，然后判断哪些网站可以爬，哪些网站不可以爬，它其实用得比较少。</li>
                  </ul>
                  <h2 id="1-发送请求"><a href="#1-发送请求" class="headerlink" title="1. 发送请求"></a>1. 发送请求</h2>
                  <p>使用 urllib 的 request 模块，我们可以方便地实现请求的发送并得到响应。我们先来看下它的具体用法。</p>
                  <h3 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a><code>urlopen</code></h3>
                  <p>urllib.request 模块提供了最基本的构造 HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理授权验证（Authentication）、重定向（Redirection)、浏览器 Cookie 以及其他内容。</p>
                  <p>下面我们来看一下它的强大之处。这里以 Python 官网为例，我们来把这个网页抓下来：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/yqq2r.png" alt="image-20200315212839610"></p>
                  <p>图 运行结果</p>
                  <p>这里我们只用了两行代码，便完成了 Python 官网的抓取，输出了网页的源代码。得到源代码之后呢？我们想要的链接、图片地址、文本信息不就都可以提取出来了吗？</p>
                  <p>接下来，看看它返回的到底是什么。利用 <code>type</code> 方法输出响应的类型：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(type(response))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">http</span>.<span class="title">client</span>.<span class="title">HTTPResponse</span>'&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，它是一个 <code>HTTPResposne</code> 类型的对象，主要包含 <code>read</code>、<code>readinto</code>、<code>getheader</code>、<code>getheaders</code>、<code>fileno</code> 等方法，以及 <code>msg</code>、<code>version</code>、<code>status</code>、<code>reason</code>、<code>debuglevel</code>、<code>closed</code> 等属性。</p>
                  <p>得到这个对象之后，我们把它赋值为 <code>response</code> 变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。</p>
                  <p>例如，调用 <code>read</code> 方法可以得到返回的网页内容，调用 <code>status</code> 属性可以得到返回结果的状态码，如 200 代表请求成功，404 代表网页未找到等。</p>
                  <p>下面再通过一个实例来看看：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(response.status)</span><br><span class="line">print(response.getheaders())</span><br><span class="line">print(response.getheader(<span class="string">'Server'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">200</span></span><br><span class="line">[(<span class="string">'Server'</span>, <span class="string">'nginx'</span>), (<span class="string">'Content-Type'</span>, <span class="string">'text/html; charset=utf-8'</span>), (<span class="string">'X-Frame-Options'</span>, <span class="string">'DENY'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 vegur'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 varnish'</span>), (<span class="string">'Content-Length'</span>, <span class="string">'48775'</span>), (<span class="string">'Accept-Ranges'</span>, <span class="string">'bytes'</span>), (<span class="string">'Date'</span>, <span class="string">'Sun, 15 Mar 2020 13:29:01 GMT'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 varnish'</span>), (<span class="string">'Age'</span>, <span class="string">'708'</span>), (<span class="string">'Connection'</span>, <span class="string">'close'</span>), (<span class="string">'X-Served-By'</span>, <span class="string">'cache-bwi5120-BWI, cache-tyo19943-TYO'</span>), (<span class="string">'X-Cache'</span>, <span class="string">'HIT, HIT'</span>), (<span class="string">'X-Cache-Hits'</span>, <span class="string">'2, 518'</span>), (<span class="string">'X-Timer'</span>, <span class="string">'S1584278942.717942,VS0,VE0'</span>), (<span class="string">'Vary'</span>, <span class="string">'Cookie'</span>), (<span class="string">'Strict-Transport-Security'</span>, <span class="string">'max-age=63072000; includeSubDomains'</span>)]</span><br><span class="line">nginx</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可见，前两个输出分别输出了响应的状态码和响应的头信息，最后一个输出通过调用 <code>getheader</code> 方法并传递一个参数 <code>Server</code> 获取了响应头中的 <code>Server</code> 值，结果是 <code>nginx</code>，意思是服务器是用 Nginx 搭建的。</p>
                  <p>利用最基本的 <code>urlopen</code> 方法，可以完成最基本的简单网页的 GET 请求抓取。</p>
                  <p>如果想给链接传递一些参数，该怎么实现呢？首先看一下 <code>urlopen</code> 方法的 API：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, [timeout,]*, cafile=<span class="literal">None</span>, capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，除了第一个参数可以传递 URL 之外，我们还可以传递其他内容，比如 <code>data</code>（附加数据）、<code>timeout</code>（超时时间）等。</p>
                  <p>下面我们详细说明这几个参数的用法。</p>
                  <h4 id="data-参数"><a href="#data-参数" class="headerlink" title="data 参数"></a><code>data</code> 参数</h4>
                  <p><code>data</code> 参数是可选的。如果要添加该参数，需要使用 <code>bytes</code> 方法将参数转化为字节流编码格式的内容，即 <code>bytes</code> 类型。另外，如果传递了这个参数，则它的请求方式就不再是 GET 方式，而是 POST 方式。</p>
                  <p>下面用实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'name'</span>: <span class="string">'germey'</span>&#125;), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们传递了一个参数 <code>word</code>，值是 <code>hello</code>。它需要被转码成 <code>bytes</code>（字节流）类型。其中转字节流采用了 <code>bytes</code> 方法，该方法的第一个参数需要是 <code>str</code>（字符串）类型，需要用 <code>urllib.parse</code> 模块里的 <code>urlencode</code> 方法来将参数字典转化为字符串；第二个参数指定编码格式，这里指定为 <code>utf-8</code>。</p>
                  <p>这里请求的站点是 httpbin.org，它可以提供 HTTP 请求测试。本次我们请求的 URL 为 <a href="https://httpbin.org/post" target="_blank" rel="noopener">https://httpbin.org/post</a>，这个链接可以用来测试 POST 请求，它可以输出 Request 的一些信息，其中就包含我们传递的 <code>data</code> 参数。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"form"</span>: &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"identity"</span>,</span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"11"</span>,</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Python-urllib/3.7"</span>,</span><br><span class="line">    <span class="string">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5ed27e43-9eee361fec88b7d3ce9be9db"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"json"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"17.220.233.154"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们传递的参数出现在了 <code>form</code> 字段中，这表明是模拟了表单提交的方式，以 POST 方式传输数据。</p>
                  <h4 id="timeout-参数"><a href="#timeout-参数" class="headerlink" title="timeout 参数"></a><code>timeout</code> 参数</h4>
                  <p><code>timeout</code> 参数用于设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。它支持 HTTP、HTTPS、FTP 请求。</p>
                  <p>下面用实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line">print(response.read())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果可能如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">During handling of the above exception, another exception occurred:</span><br><span class="line">Traceback (most recent call last): File <span class="string">"/var/py/python/urllibtest.py"</span>, line <span class="number">4</span>, <span class="keyword">in</span> &lt;module&gt; response =</span><br><span class="line">urllib.request.urlopen(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line">...</span><br><span class="line">urllib.error.URLError: &lt;urlopen error _ssl.c:<span class="number">1059</span>: The handshake operation timed out&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们设置的超时时间是 1 秒。程序运行 1 秒过后，服务器依然没有响应，于是抛出了 <code>URLError</code> 异常。该异常属于 <code>urllib.error</code> 模块，错误原因是超时。</p>
                  <p>因此，可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用 <code>try…except</code> 语句来实现，相关代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们请求了 <a href="https://httpbin.org/get" target="_blank" rel="noopener">https://httpbin.org/get</a> 这个测试链接，设置的超时时间是 0.1 秒，然后捕获了 <code>URLError</code> 这个异常，然后判断异常类型是 <code>socket.timeout</code>，意思就是超时异常。因此，得出它确实是因为超时而报错，打印输出了 <code>TIME OUT</code>。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">TIME OUT</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>按照常理来说，0.1 秒内基本不可能得到服务器响应，因此输出了 <code>TIME OUT</code> 的提示。</p>
                  <p>通过设置 <code>timeout</code> 这个参数来实现超时处理，有时还是很有用的。</p>
                  <h4 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h4>
                  <p>除了 <code>data</code> 参数和 <code>timeout</code> 参数外，还有 <code>context</code> 参数，它必须是 <code>ssl.SSLContext</code> 类型，用来指定 SSL 设置。</p>
                  <p>此外，<code>cafile</code> 和 <code>capath</code> 这两个参数分别指定 CA 证书和它的路径，这个在请求 HTTPS 链接时会有用。</p>
                  <p><code>cadefault</code> 参数现在已经弃用了，其默认值为 <code>False</code>。</p>
                  <p>前面讲解了 <code>urlopen</code> 方法的用法，通过这个最基本的方法，我们可以完成简单的请求和网页抓取。若需更加详细的信息，可以参见官方文档：<a href="https://docs.python.org/3/library/urllib.request.html" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html</a>。</p>
                  <h3 id="Request"><a href="#Request" class="headerlink" title="Request"></a><code>Request</code></h3>
                  <p>我们知道利用 <code>urlopen</code> 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求。如果请求中需要加入 <code>Headers</code> 等信息，就可以利用更强大的 <code>Request</code> 类来构建。</p>
                  <p>首先，我们用实例来感受一下 <code>Request</code> 类的用法：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(<span class="string">'https://python.org'</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，我们依然用 <code>urlopen</code> 方法来发送这个请求，只不过这次该方法的参数不再是 URL，而是一个 <code>Request</code> 类型的对象。通过构造这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可更加丰富和灵活地配置参数。</p>
                  <p>下面我们看一下 <code>Request</code> 可以通过怎样的参数来构造，它的构造方法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中，第一个参数 <code>url</code> 用于请求 URL，这是必传参数，其他都是可选参数。</p>
                  <p>第二个参数 <code>data</code> 如果要传，必须传 <code>bytes</code>（字节流）类型的。如果它是字典，可以先用 <code>urllib.parse</code> 模块里的 <code>urlencode()</code> 编码。</p>
                  <p>第三个参数 <code>headers</code> 是一个字典，它就是请求头。我们在构造请求时，既可以通过 <code>headers</code> 参数直接构造，也可以通过调用请求实例的 <code>add_header()</code> 方法添加。</p>
                  <p>添加请求头最常用的方法就是通过修改 <code>User-Agent</code> 来伪装浏览器。默认的 <code>User-Agent</code> 是 <code>Python-urllib</code>，我们可以通过修改它来伪装浏览器。比如要伪装火狐浏览器，你可以把它设置为：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Mozilla&#x2F;5.0 (X11; U; Linux i686) Gecko&#x2F;20071127 Firefox&#x2F;2.0.0.11</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第四个参数 <code>origin_req_host</code> 指的是请求方的 host 名称或者 IP 地址。</p>
                  <p>第五个参数 <code>unverifiable</code> 表示这个请求是否是无法验证的，默认是 <code>False</code>，意思就是说用户没有足够权限来选择接收这个请求的结果。例如，我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 <code>unverifiable</code> 的值就是 <code>True</code>。</p>
                  <p>第六个参数 <code>method</code> 是一个字符串，用来指示请求使用的方法，比如 GET、POST 和 PUT 等。</p>
                  <p>下面我们传入多个参数来构建请求：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们通过 4 个参数构造了一个请求，其中 <code>url</code> 即请求 URL，<code>headers</code> 中指定了 <code>User-Agent</code> 和 <code>Host</code>，参数 <code>data</code> 用 <code>urlencode</code> 和 <code>bytes</code> 方法转成字节流。另外，指定了请求方式为 POST。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"form"</span>: &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"identity"</span>,</span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"11"</span>,</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)"</span>,</span><br><span class="line">    <span class="string">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5ed27f77-884f503a2aa6760df7679f05"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"json"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"17.220.233.154"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>观察结果可以发现，我们成功设置了 <code>data</code>、<code>headers</code> 和 <code>method</code>。</p>
                  <p>另外，<code>headers</code> 也可以用 <code>add_header</code> 方法来添加：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">req = request.Request(url=url, data=data, method=<span class="string">'POST'</span>)</span><br><span class="line">req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如此一来，我们就可以更加方便地构造请求，实现请求的发送啦。</p>
                  <h3 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h3>
                  <p>在上面的过程中，我们虽然可以构造请求，但是对于一些更高级的操作（比如 Cookies 处理、代理设置等），该怎么办呢？</p>
                  <p>接下来，就需要更强大的工具 Handler 登场了。简而言之，我们可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookie 的，有处理代理设置的。利用它们，我们几乎可以做到 HTTP 请求中所有的事情。</p>
                  <p>首先，介绍一下 <code>urllib.request</code> 模块里的 <code>BaseHandler</code> 类，它是所有其他 Handler 的父类，它提供了最基本的方法，例如 <code>default_open</code>、<code>protocol_request</code> 等。</p>
                  <p>接下来，就有各种 Handler 子类继承这个 <code>BaseHandler</code> 类，举例如下。</p>
                  <ul>
                    <li><code>HTTPDefaultErrorHandler</code> 用于处理 HTTP 响应错误，错误都会抛出 <code>HTTPError</code> 类型的异常。</li>
                    <li><code>HTTPRedirectHandler</code> 用于处理重定向。</li>
                    <li><code>HTTPCookieProcessor</code> 用于处理 Cookies。</li>
                    <li><code>ProxyHandler</code> 用于设置代理，默认代理为空。</li>
                    <li><code>HTTPPasswordMgr</code> 用于管理密码，它维护了用户名和密码的表。</li>
                    <li><code>HTTPBasicAuthHandler</code> 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。</li>
                  </ul>
                  <p>另外，还有其他的 Handler 类，这里就不一一列举了，详情可以参考官方文档： <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a>。</p>
                  <p>关于怎么使用它们，现在先不用着急，后面会有实例演示。</p>
                  <p>另一个比较重要的类就是 <code>OpenerDirector</code>，我们可以称为 Opener。我们之前用过 <code>urlopen</code> 这个方法，实际上它就是 urllib 为我们提供的一个 Opener。</p>
                  <p>那么，为什么要引入 Opener 呢？因为需要实现更高级的功能。之前使用的 <code>Request</code> 和 <code>urlopen</code> 相当于类库为你封装好了极其常用的请求方法，利用它们可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以需要深入一层进行配置，使用更底层的实例来完成操作，所以这里就用到了 Opener。</p>
                  <p>Opener 可以使用 <code>open</code> 方法，返回的类型和 <code>urlopen</code> 如出一辙。那么，它和 Handler 有什么关系呢？简而言之，就是利用 Handler 来构建 Opener。</p>
                  <p>下面用几个实例来看看它们的用法。</p>
                  <h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4>
                  <p>在访问某些设置了身份认证的网站时，例如 <a href="https://ssr3.scrape.center/，我们可能会遇到这样的认证窗口，如图" target="_blank" rel="noopener">https://ssr3.scrape.center/，我们可能会遇到这样的认证窗口，如图</a> 2- 所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/fries.png" alt="image-20210704202140395"></p>
                  <p>图 2- 认证窗口</p>
                  <p>如果遇到了这种情况，那么这个网站就是启用了基本身份认证，英文叫作 HTTP Basic Access Authentication，它是一种用来允许网页浏览器或其他客户端程序在请求时提供用户名和口令形式的身份凭证的一种登录验证方式。</p>
                  <p>那么，如果要请求这样的页面，该怎么办呢？借助 <code>HTTPBasicAuthHandler</code> 就可以完成，相关代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'admin'</span></span><br><span class="line">password = <span class="string">'admin'</span></span><br><span class="line">url = <span class="string">'https://ssr3.scrape.center/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先实例化 <code>HTTPBasicAuthHandler</code> 对象，其参数是 <code>HTTPPasswordMgrWithDefaultRealm</code> 对象，它利用 <code>add_password</code> 方法添加进去用户名和密码，这样就建立了一个处理验证的 Handler。</p>
                  <p>接下来，利用这个 Handler 并使用 <code>build_opener</code> 方法构建一个 Opener，这个 Opener 在发送请求时就相当于已经验证成功了。</p>
                  <p>接下来，利用 Opener 的 <code>open</code> 方法打开链接，就可以完成验证了。这里获取到的结果就是验证后的页面源码内容。</p>
                  <h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4>
                  <p>在做爬虫的时候，免不了要使用代理，如果要添加代理，可以这样做：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:8080'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:8080'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们在本地需要先事先搭建一个 HTTP 代理，运行在 8080 端口上。</p>
                  <p>这里使用了 <code>ProxyHandler</code>，其参数是一个字典，键名是协议类型（比如 HTTP 或者 HTTPS 等），键值是代理链接，可以添加多个代理。</p>
                  <p>然后，利用这个 Handler 及 <code>build_opener</code> 方法构造一个 Opener，之后发送请求即可。</p>
                  <h4 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h4>
                  <p>Cookie 的处理就需要相关的 Handler 了。</p>
                  <p>我们先用实例来看看怎样将网站的 Cookie 获取下来，相关代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name + <span class="string">"="</span> + item.value)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先，我们必须声明一个 <code>CookieJar</code> 对象。接下来，就需要利用 <code>HTTPCookieProcessor</code> 来构建一个 Handler，最后利用 <code>build_opener</code> 方法构建出 Opener，执行 <code>open</code> 函数即可。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">BAIDUID&#x3D;A09E6C4E38753531B9FB4C60CE9FDFCB:FG&#x3D;1</span><br><span class="line">BIDUPSID&#x3D;A09E6C4E387535312F8AA46280C6C502</span><br><span class="line">H_PS_PSSID&#x3D;31358_1452_31325_21088_31110_31253_31605_31271_31463_30823</span><br><span class="line">PSTM&#x3D;1590854698</span><br><span class="line">BDSVRTM&#x3D;10</span><br><span class="line">BD_HOME&#x3D;1</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里输出了每个 Cookie 条目的名称和值。</p>
                  <p>不过既然能输出，那可不可以输出成文件格式呢？我们知道 Cookie 实际上也是以文本形式保存的。</p>
                  <p>答案当然是肯定的，这里通过下面的实例来看看：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.request, http.cookiejar</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'cookie.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时 <code>CookieJar</code> 就需要换成 <code>MozillaCookieJar</code>，它在生成文件时会用到，是 <code>CookieJar</code> 的子类，可以用来处理 Cookie 和文件相关的事件，比如读取和保存 Cookie，可以将 Cookie 保存成 Mozilla 型浏览器的 Cookie 格式。</p>
                  <p>运行之后，可以发现生成了一个 cookie.txt 文件，其内容如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"># Netscape HTTP Cookie File</span><br><span class="line"># http:&#x2F;&#x2F;curl.haxx.se&#x2F;rfc&#x2F;cookie_spec.html</span><br><span class="line"># This is a generated file!  Do not edit.</span><br><span class="line"></span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE	1622390755	BAIDUID	0B4A68D74B0C0E53E5B82AFD9BF9178F:FG&#x3D;1</span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE	3738338402	BIDUPSID	0B4A68D74B0C0E53471FA6329280FA58</span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE		H_PS_PSSID	31262_1438_31325_21127_31110_31596_31673_31464_30823_26350</span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE	3738338402	PSTM	1590854754</span><br><span class="line">www.baidu.com	FALSE	&#x2F;	FALSE		BDSVRTM	0</span><br><span class="line">www.baidu.com	FALSE	&#x2F;	FALSE		BD_HOME	1</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外，<code>LWPCookieJar</code> 同样可以读取和保存 Cookie，但是保存的格式和 <code>MozillaCookieJar</code> 不一样，它会保存成 libwww-perl（LWP）格式的 Cookie 文件。</p>
                  <p>要保存成 LWP 格式的 Cookie 文件，可以在声明时就改为：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此时生成的内容如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">#LWP-Cookies-2.0</span><br><span class="line">Set-Cookie3: BAIDUID&#x3D;&quot;1F30EEDA35C7A94320275F991CA5B3A5:FG&#x3D;1&quot;; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; expires&#x3D;&quot;2021-05-30 16:06:39Z&quot;; comment&#x3D;bd; version&#x3D;0</span><br><span class="line">Set-Cookie3: BIDUPSID&#x3D;1F30EEDA35C7A9433C97CF6245CBC383; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; expires&#x3D;&quot;2088-06-17 19:20:46Z&quot;; version&#x3D;0</span><br><span class="line">Set-Cookie3: H_PS_PSSID&#x3D;31626_1440_21124_31069_31254_31594_30841_31673_31464_31715_30823; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; discard; version&#x3D;0</span><br><span class="line">Set-Cookie3: PSTM&#x3D;1590854799; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; expires&#x3D;&quot;2088-06-17 19:20:46Z&quot;; version&#x3D;0</span><br><span class="line">Set-Cookie3: BDSVRTM&#x3D;11; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;www.baidu.com&quot;; path_spec; discard; version&#x3D;0</span><br><span class="line">Set-Cookie3: BD_HOME&#x3D;1; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;www.baidu.com&quot;; path_spec; discard; version&#x3D;0</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由此看来，生成的格式还是有比较大差异的。</p>
                  <p>那么，生成了 Cookie 文件后，怎样从文件中读取并利用呢？</p>
                  <p>下面我们以 <code>LWPCookieJar</code> 格式为例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> urllib.request, http.cookiejar</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(<span class="string">'cookie.txt'</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里调用 <code>load</code> 方法来读取本地的 Cookie 文件，获取到了 Cookie 的内容。不过前提是我们首先生成了 <code>LWPCookieJar</code> 格式的 Cookie，并保存成文件，然后读取 Cookie 之后使用同样的方法构建 Handler 和 Opener 即可完成操作。</p>
                  <p>运行结果正常的话，会输出百度网页的源代码。</p>
                  <p>通过上面的方法，我们可以实现绝大多数请求功能的设置了。</p>
                  <p>这便是 urllib 库中 request 模块的基本用法，如果想实现更多的功能，可以参考官方文档的说明：<a href="https://docs.python.org/3/library/urllib.request.html#basehandler-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#basehandler-objects</a>。</p>
                  <h2 id="2-处理异常"><a href="#2-处理异常" class="headerlink" title="2. 处理异常"></a>2. 处理异常</h2>
                  <p>在前一节中，我们了解了请求的发送过程，但是在网络不好的情况下，如果出现了异常，该怎么办呢？这时如果不处理这些异常，程序很可能因报错而终止运行，所以异常处理还是十分有必要的。</p>
                  <p>urllib 的 error 模块定义了由 request 模块产生的异常。如果出现了问题，request 模块便会抛出 error 模块中定义的异常。</p>
                  <h3 id="URLError"><a href="#URLError" class="headerlink" title="URLError"></a><code>URLError</code></h3>
                  <p><code>URLError</code> 类来自 urllib 库的 error 模块，它继承自 <code>OSError</code> 类，是 error 异常模块的基类，由 request 模块产生的异常都可以通过捕获这个类来处理。</p>
                  <p>它具有一个属性 <code>reason</code>，即返回错误的原因。</p>
                  <p>下面用一个实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'https://cuiqingcai.com/404'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了 <code>URLError</code> 这个异常，运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Not Found</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>程序没有直接报错，而是输出了如上内容，这样就可以避免程序异常终止，同时异常得到了有效处理。</p>
                  <h3 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a><code>HTTPError</code></h3>
                  <p>它是 <code>URLError</code> 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等。它有如下 3 个属性。</p>
                  <ul>
                    <li><code>code</code>：返回 HTTP 状态码，比如 404 表示网页不存在，500 表示服务器内部错误等。</li>
                    <li><code>reason</code>：同父类一样，用于返回错误的原因。</li>
                    <li><code>headers</code>：返回请求头。</li>
                  </ul>
                  <p>下面我们用几个实例来看看：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'https://cuiqingcai.com/404'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Not Found</span><br><span class="line">404</span><br><span class="line">Server: nginx&#x2F;1.10.3 (Ubuntu)</span><br><span class="line">Date: Sat, 30 May 2020 16:08:42 GMT</span><br><span class="line">Content-Type: text&#x2F;html; charset&#x3D;UTF-8</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Connection: close</span><br><span class="line">Set-Cookie: PHPSESSID&#x3D;kp1a1b0o3a0pcf688kt73gc780; path&#x3D;&#x2F;</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Vary: Cookie</span><br><span class="line">Expires: Wed, 11 Jan 1984 05:00:00 GMT</span><br><span class="line">Cache-Control: no-cache, must-revalidate, max-age&#x3D;0</span><br><span class="line">Link: &lt;https:&#x2F;&#x2F;cuiqingcai.com&#x2F;wp-json&#x2F;&gt;; rel&#x3D;&quot;https:&#x2F;&#x2F;api.w.org&#x2F;&quot;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>依然是同样的网址，这里捕获了 <code>HTTPError</code> 异常，输出了 <code>reason</code>、<code>code</code> 和 <code>headers</code> 属性。</p>
                  <p>因为 <code>URLError</code> 是 <code>HTTPError</code> 的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码的更好写法如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'https://cuiqingcai.com/404'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就可以做到先捕获 <code>HTTPError</code>，获取它的错误原因、状态码、<code>headers</code> 等信息。如果不是 <code>HTTPError</code> 异常，就会捕获 <code>URLError</code> 异常，输出错误原因。最后，用 <code>else</code> 来处理正常的逻辑。这是一个较好的异常处理写法。</p>
                  <p>有时候，<code>reason</code> 属性返回的不一定是字符串，也可能是一个对象。再看下面的实例：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'https://www.baidu.com'</span>, timeout=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(type(e.reason))</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们直接设置超时时间来强制抛出 <code>timeout</code> 异常。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span>'<span class="title">socket</span>.<span class="title">timeout</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">TIME</span> <span class="title">OUT</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，<code>reason</code> 属性的结果是 <code>socket.timeout</code> 类。所以，这里我们可以用 <code>isinstance</code> 方法来判断它的类型，作出更详细的异常判断。</p>
                  <p>本节中，我们讲述了 error 模块的相关用法，通过合理地捕获异常可以做出更准确的异常判断，使程序更加稳健。</p>
                  <h2 id="3-解析链接"><a href="#3-解析链接" class="headerlink" title="3. 解析链接"></a>3. 解析链接</h2>
                  <p>前面说过，urllib 库里还提供了 parse 模块，它定义了处理 URL 的标准接口，例如实现 URL 各部分的抽取、合并以及链接转换。它支持如下协议的 URL 处理：<code>file</code>、<code>ftp</code>、<code>gopher</code>、<code>hdl</code>、<code>http</code>、<code>https</code>、<code>imap</code>、<code>mailto</code>、<code>mms</code>、<code>news</code>、<code>nntp</code>、<code>prospero</code>、<code>rsync</code>、<code>rtsp</code>、<code>rtspu</code>、<code>sftp</code>、<code>sip</code>、<code>sips</code>、<code>snews</code>、<code>svn</code>、<code>svn+ssh</code>、<code>telnet</code> 和 <code>wais</code>。本节中，我们介绍一下该模块中常用的方法来看一下它的便捷之处。</p>
                  <h3 id="urlparse"><a href="#urlparse" class="headerlink" title="urlparse"></a><code>urlparse</code></h3>
                  <p>该方法可以实现 URL 的识别和分段，这里先用一个实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们利用 <code>urlparse</code> 方法进行了一个 URL 的解析。首先，输出了解析结果的类型，然后将结果也输出出来。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">urllib</span>.<span class="title">parse</span>.<span class="title">ParseResult</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">ParseResult</span><span class="params">(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，返回结果是一个 <code>ParseResult</code> 类型的对象，它包含 6 个部分，分别是 <code>scheme</code>、<code>netloc</code>、<code>path</code>、<code>params</code>、<code>query</code> 和 <code>fragment</code>。</p>
                  <p>观察一下该实例的 URL：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;index.html;user?id&#x3D;5#comment</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，<code>urlparse</code> 方法将其拆分成了 6 个部分。大体观察可以发现，解析时有特定的分隔符。比如，<code>://</code> 前面的就是 <code>scheme</code>，代表协议；第一个 <code>/</code> 符号前面便是 <code>netloc</code>，即域名，后面是 <code>path</code>，即访问路径；分号<code>;</code>后面是 <code>params</code>，代表参数；问号 <code>?</code> 后面是查询条件 <code>query</code>，一般用作 GET 类型的 URL；井号 <code>#</code> 后面是锚点，用于直接定位页面内部的下拉位置。</p>
                  <p>所以，可以得出一个标准的链接格式，具体如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scheme:&#x2F;&#x2F;netloc&#x2F;path;params?query#fragment</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>一个标准的 URL 都会符合这个规则，利用 <code>urlparse</code> 方法可以将它拆分开来。</p>
                  <p>除了这种最基本的解析方式外，<code>urlparse</code> 方法还有其他配置吗？接下来，看一下它的 API 用法：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，它有 3 个参数。</p>
                  <ul>
                    <li><code>urlstring</code>：这是必填项，即待解析的 URL。</li>
                    <li><code>scheme</code>：它是默认的协议（比如 <code>http</code> 或 <code>https</code> 等）。假如这个链接没有带协议信息，会将这个作为默认的协议。我们用实例来看一下：</li>
                  </ul>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ParseResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">''</span>, path=<span class="string">'www.baidu.com/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，我们提供的 URL 没有包含最前面的 <code>scheme</code> 信息，但是通过默认的 <code>scheme</code> 参数，返回的结果是 <code>https</code>。</p>
                  <p>假设我们带上了 <code>scheme</code>：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>则结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ParseResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可见，<code>scheme</code> 参数只有在 URL 中不包含 <code>scheme</code> 信息时才生效。如果 URL 中有 <code>scheme</code> 信息，就会返回解析出的 <code>scheme</code>。</p>
                  <ul>
                    <li><code>allow_fragments</code>：即是否忽略 <code>fragment</code>。如果它被设置为 <code>False</code>，<code>fragment</code> 部分就会被忽略，它会被解析为 <code>path</code>、<code>parameters</code> 或者 <code>query</code> 的一部分，而 <code>fragment</code> 部分为空。</li>
                  </ul>
                  <p>下面我们用实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ParseResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5#comment'</span>, fragment=<span class="string">''</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>假设 URL 中不包含 <code>params</code> 和 <code>query</code>，我们再通过实例看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ParseResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html#comment'</span>, params=<span class="string">''</span>, query=<span class="string">''</span>, fragment=<span class="string">''</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，当 URL 中不包含 <code>params</code> 和 <code>query</code> 时，<code>fragment</code> 便会被解析为 <code>path</code> 的一部分。</p>
                  <p>返回结果 <code>ParseResult</code> 实际上是一个元组，我们既可以用索引顺序来获取，也可以用属性名获取。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result.scheme, result[<span class="number">0</span>], result.netloc, result[<span class="number">1</span>], sep=<span class="string">'\n'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们分别用索引和属性名获取了 <code>scheme</code> 和 <code>netloc</code>，其运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https</span><br><span class="line">https</span><br><span class="line">www.baidu.com</span><br><span class="line">www.baidu.com</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，二者的结果是一致的，两种方法都可以成功获取。</p>
                  <h3 id="urlunparse"><a href="#urlunparse" class="headerlink" title="urlunparse"></a><code>urlunparse</code></h3>
                  <p>有了 <code>urlparse</code> 方法，相应地就有了它的对立方法 <code>urlunparse</code>。它接收的参数是一个可迭代对象，但是它的长度必须是 6，否则会抛出参数数量不足或者过多的问题。先用一个实例看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line">data = [<span class="string">'https'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里参数 <code>data</code> 用了列表类型。当然，你也可以用其他类型，比如元组或者特定的数据结构。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;index.html;user?a&#x3D;6#comment</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就成功实现了 URL 的构造。</p>
                  <h3 id="urlsplit"><a href="#urlsplit" class="headerlink" title="urlsplit"></a><code>urlsplit</code></h3>
                  <p>这个方法和 <code>urlparse</code> 方法非常相似，只不过它不再单独解析 <code>params</code> 这一部分，只返回 5 个结果。上面例子中的 <code>params</code> 会合并到 <code>path</code> 中。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">SplitResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html;user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，返回结果是 <code>SplitResult</code>，它其实也是一个元组类型，既可以用属性获取值，也可以用索引来获取。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result.scheme, result[<span class="number">0</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https https</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="urlunsplit"><a href="#urlunsplit" class="headerlink" title="urlunsplit"></a><code>urlunsplit</code></h3>
                  <p>与 <code>urlunparse</code> 方法类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是长度必须为 5。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunsplit</span><br><span class="line"></span><br><span class="line">data = [<span class="string">'https'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunsplit(data))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;index.html?a&#x3D;6#comment</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="urljoin"><a href="#urljoin" class="headerlink" title="urljoin"></a><code>urljoin</code></h3>
                  <p>有了 <code>urlunparse</code> 和 <code>urlunsplit</code> 方法，我们可以完成链接的合并，不过前提是必须要有特定长度的对象，链接的每一部分都要清晰分开。</p>
                  <p>此外，生成链接还有另一个方法，那就是 <code>urljoin</code> 方法。我们可以提供一个 <code>base_url</code>（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析 <code>base_url</code> 的 <code>scheme</code>、<code>netloc</code> 和 <code>path</code> 这 3 个内容并对新链接缺失的部分进行补充，最后返回结果。</p>
                  <p>下面通过几个实例看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com'</span>, <span class="string">'FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html?question=2'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com?wd=abc'</span>, <span class="string">'https://cuiqingcai.com/index.php'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com#comment'</span>, <span class="string">'?category=2'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;FAQ.html</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;FAQ.html</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;FAQ.html</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;FAQ.html?question&#x3D;2</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;index.php</span><br><span class="line">https:&#x2F;&#x2F;www.baidu.com?category&#x3D;2#comment</span><br><span class="line">www.baidu.com?category&#x3D;2#comment</span><br><span class="line">www.baidu.com?category&#x3D;2</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以发现，<code>base_url</code> 提供了三项内容 <code>scheme</code>、<code>netloc</code> 和 <code>path</code>。如果这 3 项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而 <code>base_url</code> 中的 <code>params</code>、<code>query</code> 和 <code>fragment</code> 是不起作用的。</p>
                  <p>通过 <code>urljoin</code> 方法，我们可以轻松实现链接的解析、拼合与生成。</p>
                  <h3 id="urlencode"><a href="#urlencode" class="headerlink" title="urlencode"></a><code>urlencode</code></h3>
                  <p>这里我们再介绍一个常用的方法 —— <code>urlencode</code>，它在构造 GET 请求参数的时候非常有用，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'https://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先声明一个字典来将参数表示出来，然后调用 <code>urlencode</code> 方法将其序列化为 GET 请求参数。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com?name&#x3D;germey&amp;age&#x3D;25</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，参数成功地由字典类型转化为 GET 请求参数了。</p>
                  <p>这个方法非常常用。有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为 URL 的参数时，只需要调用该方法即可。</p>
                  <h3 id="parse-qs"><a href="#parse-qs" class="headerlink" title="parse_qs"></a><code>parse_qs</code></h3>
                  <p>有了序列化，必然就有反序列化。如果我们有一串 GET 请求参数，利用 <code>parse_qs</code> 方法，就可以将它转回字典，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=25'</span></span><br><span class="line">print(parse_qs(query))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="string">'name'</span>: [<span class="string">'germey'</span>], <span class="string">'age'</span>: [<span class="string">'25'</span>]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这样就成功转回为字典类型了。</p>
                  <h3 id="parse-qsl"><a href="#parse-qsl" class="headerlink" title="parse_qsl"></a><code>parse_qsl</code></h3>
                  <p>另外，还有一个 <code>parse_qsl</code> 方法，它用于将参数转化为元组组成的列表，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=25'</span></span><br><span class="line">print(parse_qsl(query))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[(<span class="string">'name'</span>, <span class="string">'germey'</span>), (<span class="string">'age'</span>, <span class="string">'25'</span>)]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，运行结果是一个列表，而列表中的每一个元素都是一个元组，元组的第一个内容是参数名，第二个内容是参数值。</p>
                  <h3 id="quote"><a href="#quote" class="headerlink" title="quote"></a><code>quote</code></h3>
                  <p>该方法可以将内容转化为 URL 编码的格式。URL 中带有中文参数时，有时可能会导致乱码的问题，此时可以用这个方法可以将中文字符转化为 URL 编码，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们声明了一个中文的搜索文字，然后用 <code>quote</code> 方法对其进行 URL 编码，最后得到的结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;%E5%A3%81%E7%BA%B8</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="unquote"><a href="#unquote" class="headerlink" title="unquote"></a><code>unquote</code></h3>
                  <p>有了 <code>quote</code> 方法，当然还有 <code>unquote</code> 方法，它可以进行 URL 解码，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'</span></span><br><span class="line">print(unquote(url))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这是上面得到的 URL 编码后的结果，这里利用 <code>unquote</code> 方法还原，结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;壁纸</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，利用 <code>unquote</code> 方法可以方便地实现解码。</p>
                  <p>本节中，我们介绍了 parse 模块的一些常用 URL 处理方法。有了这些方法，我们可以方便地实现 URL 的解析和构造，建议熟练掌握。</p>
                  <h2 id="4-分析-Robots-协议"><a href="#4-分析-Robots-协议" class="headerlink" title="4. 分析 Robots 协议"></a>4. 分析 Robots 协议</h2>
                  <p>利用 urllib 的 robotparser 模块，我们可以实现网站 Robots 协议的分析。本节中，我们来简单了解一下该模块的用法。</p>
                  <h3 id="1-Robots-协议"><a href="#1-Robots-协议" class="headerlink" title="1. Robots 协议"></a>1. Robots 协议</h3>
                  <p>Robots 协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作 robots.txt 的文本文件，一般放在网站的根目录下。</p>
                  <p>当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在 robots.txt 文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。</p>
                  <p>下面我们看一个 robots.txt 的样例：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br><span class="line">Allow: &#x2F;public&#x2F;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这实现了对所有搜索爬虫只允许爬取 public 目录的功能，将上述内容保存成 robots.txt 文件，放在网站的根目录下，和网站的入口文件（比如 index.php、index.html 和 index.jsp 等）放在一起。</p>
                  <p>上面的 <code>User-agent</code> 描述了搜索爬虫的名称，这里将其设置为 <code>*</code> 则代表该协议对任何爬取爬虫有效。比如，我们可以设置：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">User-agent: Baiduspider</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就代表我们设置的规则对百度爬虫是有效的。如果有多条 <code>User-agent</code> 记录，就有多个爬虫会受到爬取限制，但至少需要指定一条。</p>
                  <p><code>Disallow</code> 指定了不允许抓取的目录，比如上例子中设置为 <code>/</code> 则代表不允许抓取所有页面。</p>
                  <p><code>Allow</code> 一般和 <code>Disallow</code> 一起使用，一般不会单独使用，用来排除某些限制。上例中我们设置为 <code>/public/</code>，则表示所有页面不允许抓取，但可以抓取 public 目录。</p>
                  <p>下面我们再来看几个例子。禁止所有爬虫访问任何目录的代码如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>允许所有爬虫访问任何目录的代码如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">User-agent: *</span><br><span class="line">Disallow:</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外，直接把 robots.txt 文件留空也是可以的。</p>
                  <p>禁止所有爬虫访问网站某些目录的代码如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;private&#x2F;</span><br><span class="line">Disallow: &#x2F;tmp&#x2F;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>只允许某一个爬虫访问的代码如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">User-agent: WebCrawler</span><br><span class="line">Disallow:</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这些是 robots.txt 的一些常见写法。</p>
                  <h3 id="爬虫名称"><a href="#爬虫名称" class="headerlink" title="爬虫名称"></a>爬虫名称</h3>
                  <p>大家可能会疑惑，爬虫名是从哪儿来的？为什么就叫这个名？其实它是有固定名字的了，比如百度的就叫作 BaiduSpider。表 2- 列出了一些常见搜索爬虫的名称及对应的网站。</p>
                  <p>表 一些常见搜索爬虫的名称及其对应的网站</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">爬虫名称</th>
                          <th style="text-align:left">名称</th>
                          <th style="text-align:left">网站</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left">BaiduSpider</td>
                          <td style="text-align:left">百度</td>
                          <td style="text-align:left">www.baidu.com</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">Googlebot</td>
                          <td style="text-align:left">谷歌</td>
                          <td style="text-align:left">www.google.com</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">360Spider</td>
                          <td style="text-align:left">360 搜索</td>
                          <td style="text-align:left">www.so.com</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">YodaoBot</td>
                          <td style="text-align:left">有道</td>
                          <td style="text-align:left">www.youdao.com</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">ia_archiver</td>
                          <td style="text-align:left">Alexa</td>
                          <td style="text-align:left">www.alexa.cn</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">Scooter</td>
                          <td style="text-align:left">altavista</td>
                          <td style="text-align:left">www.altavista.com</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">Bingbot</td>
                          <td style="text-align:left">必应</td>
                          <td style="text-align:left">www.bing.com</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <h3 id="robotparser"><a href="#robotparser" class="headerlink" title="robotparser"></a>robotparser</h3>
                  <p>了解 Robots 协议之后，我们就可以使用 robotparser 模块来解析 robots.txt 了。该模块提供了一个类 <code>RobotFileParser</code>，它可以根据某网站的 robots.txt 文件来判断一个爬虫是否有权限来爬取这个网页。</p>
                  <p>该类用起来非常简单，只需要在构造方法里传入 robots.txt 的链接即可。首先看一下它的声明：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">urllib.robotparser.RobotFileParser(url=<span class="string">''</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，也可以在声明时不传入，默认为空，最后再使用 <code>set_url</code> 方法设置一下即可。</p>
                  <p>下面列出了这个类常用的几个方法。</p>
                  <ul>
                    <li><code>set_url</code>：用来设置 robots.txt 文件的链接。如果在创建 <code>RobotFileParser</code> 对象时传入了链接，那么就不需要再使用这个方法设置了。</li>
                    <li><code>read</code>：读取 robots.txt 文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 <code>False</code>，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。</li>
                    <li><code>parse</code>：用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。</li>
                    <li><code>can_fetch</code>：该方法用两个参数，第一个是 <code>User-Agent</code>，第二个是要抓取的 URL。返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 <code>True</code> 或 <code>False</code>。</li>
                    <li><code>mtime</code>：返回的是上次抓取和分析 robots.txt 的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。</li>
                    <li><code>modified</code>：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。</li>
                  </ul>
                  <p>下面我们用实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'https://www.baidu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Googlebot'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里以百度为例，首先创建 <code>RobotFileParser</code> 对象，然后通过 <code>set_url</code> 方法设置了 robots.txt 的链接。当然，不用这个方法的话，可以在声明时直接用如下方法设置：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">rp = RobotFileParser(<span class="string">'https://www.baidu.com/robots.txt'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接着利用 <code>can_fetch</code> 方法判断网页是否可以被抓取。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里同样可以使用 <code>parse</code> 方法执行读取和分析，示例如下：<br>可以看到这里我们利用 Baiduspider 可以抓取百度等首页以及 homepage 页面，但是 Googlebot 就不能抓取 homepage 页面。</p>
                  <p>打开百度的 robots.txt 文件看下，可以看到如下的信息：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">User-agent: Baiduspider</span><br><span class="line">Disallow: &#x2F;baidu</span><br><span class="line">Disallow: &#x2F;s?</span><br><span class="line">Disallow: &#x2F;ulink?</span><br><span class="line">Disallow: &#x2F;link?</span><br><span class="line">Disallow: &#x2F;home&#x2F;news&#x2F;data&#x2F;</span><br><span class="line">Disallow: &#x2F;bh</span><br><span class="line"></span><br><span class="line">User-agent: Googlebot</span><br><span class="line">Disallow: &#x2F;baidu</span><br><span class="line">Disallow: &#x2F;s?</span><br><span class="line">Disallow: &#x2F;shifen&#x2F;</span><br><span class="line">Disallow: &#x2F;homepage&#x2F;</span><br><span class="line">Disallow: &#x2F;cpro</span><br><span class="line">Disallow: &#x2F;ulink?</span><br><span class="line">Disallow: &#x2F;link?</span><br><span class="line">Disallow: &#x2F;home&#x2F;news&#x2F;data&#x2F;</span><br><span class="line">Disallow: &#x2F;bh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由此我们可以看到，Baiduspider 没有限制 homepage 页面的抓取，而 Googlebot 则限制了 homepage 页面的抓取。</p>
                  <p>这里同样可以使用 parse 方法执行读取和分析，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.parse(urlopen(<span class="string">'https://www.baidu.com/robots.txt'</span>).read().decode(<span class="string">'utf-8'</span>).split(<span class="string">'\n'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Googlebot'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果一样：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>本节介绍了 robotparser 模块的基本用法和实例，利用它，我们可以方便地判断哪些页面可以抓取，哪些页面不可以抓取。</p>
                  <h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2>
                  <p>本节内容比较多，我们介绍了 urllib 的 request、error、parse、robotparser 模块的基本用法。这些是一些基础模块，其中有一些模块的实用性还是很强的，比如我们可以利用 parse 模块来进行 URL 的各种处理。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/UrllibTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/UrllibTest</a>。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-13 07:44:31" itemprop="dateCreated datePublished" datetime="2022-02-13T07:44:31+08:00">2022-02-13</time>
                </span>
                <span id="/202221.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - urllib 爬虫初体验" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>27k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>24 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202223.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202223.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 强大灵活的正则表达式</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在上一节中，我们已经可以用 requests 来获取网页的源代码，得到 HTML 代码。但我们真正想要的数据是包含在 HTML 代码之中的，怎么才能从 HTML 代码中获取我们想要的信息呢？正则表达式就是其中一个有效的方法。</p>
                  <p>本节中，我们了解一下正则表达式的相关用法。正则表达式是处理字符串的强大工具，它有自己特定的语法结构，有了它，实现字符串的检索、替换、匹配验证都不在话下。</p>
                  <p>当然，对于爬虫来说，有了它，从 HTML 里提取想要的信息就非常方便了。</p>
                  <h3 id="1-实例引入"><a href="#1-实例引入" class="headerlink" title="1. 实例引入"></a>1. 实例引入</h3>
                  <p>说了这么多，可能我们对它到底是个什么还是比较模糊，下面就用几个实例来看一下正则表达式的用法。</p>
                  <p>打开开源中国提供的正则表达式测试工具 <a href="http://tool.oschina.net/regex/" target="_blank" rel="noopener">http://tool.oschina.net/regex/</a>，输入待匹配的文本，然后选择常用的正则表达式，就可以得出相应的匹配结果了。例如，这里输入待匹配的文本，具体如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Hello, my phone number is 010-86432100 and email is cqc@cuiqingcai.com, and my website is https:&#x2F;&#x2F;cuiqingcai.com</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这段字符串中包含了一个电话号码和一个电子邮件和一个 URL，接下来就尝试用正则表达式提取出来，如图所示。</p>
                  <p>在网页右侧选择「匹配 Email 地址」，就可以看到下方出现了文本中的 E-mail。</p>
                  <p><img src="https://cdn.cuiqingcai.com/lsdsz.png" alt=""></p>
                  <p>如果选择「匹配网址 URL」，就可以看到下方出现了文本中的 URL。</p>
                  <p>在网页右侧选择 “匹配 Email 地址”，就可以看到下方出现了文本中的 E-mail。如果选择 “匹配网址 URL”，就可以看到下方出现了文本中的 URL。是不是非常神奇？</p>
                  <p><img src="https://cdn.cuiqingcai.com/ijr13.png" alt=""></p>
                  <p>其实，这里就是用了正则表达式匹配，也就是用一定的规则将特定的文本提取出来。比如，电子邮件开头是一段字符串，然后是一个 <code>@</code> 符号，最后是某个域名，这是有特定的组成格式的。另外，对于 URL，开头是协议类型，然后是冒号加双斜线，最后是域名加路径。</p>
                  <p>对于 URL 来说，可以用下面的正则表达式匹配：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[a-zA-z]+:&#x2F;&#x2F;[^\s]*</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>用这个正则表达式去匹配一个字符串，如果这个字符串中包含类似 URL 的文本，就会被提取出来。</p>
                  <p>这个正则表达式看上去是乱糟糟的一团，其实不然，这里面都是有特定的语法规则的。比如，<code>a-z</code> 代表匹配任意的小写字母，<code>\s</code> 表示匹配任意的空白字符，<code>*</code> 就代表匹配前面的字符任意多个，这一长串的正则表达式就是这么多匹配规则的组合。</p>
                  <p>写好正则表达式后，就可以拿它去一个长字符串里匹配查找了。不论这个字符串里面有什么，只要符合我们写的规则，统统可以找出来。对于网页来说，如果想找出网页源代码里有多少 URL，用匹配 URL 的正则表达式去匹配即可。</p>
                  <p>上面我们说了几个匹配规则，表 2- 列出了常用的匹配规则。</p>
                  <p>表 2- 常用的匹配规则</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">模　　式</th>
                          <th style="text-align:left">描　　述</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>\w</code></td>
                          <td style="text-align:left">匹配字母、数字及下划线</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\W</code></td>
                          <td style="text-align:left">匹配不是字母、数字及下划线的字符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\s</code></td>
                          <td style="text-align:left">匹配任意空白字符，等价于 <code>[\t\n\r\f]</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\S</code></td>
                          <td style="text-align:left">匹配任意非空字符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\d</code></td>
                          <td style="text-align:left">匹配任意数字，等价于 <code>[0-9]</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\D</code></td>
                          <td style="text-align:left">匹配任意非数字的字符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\A</code></td>
                          <td style="text-align:left">匹配字符串开头</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\Z</code></td>
                          <td style="text-align:left">匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\z</code></td>
                          <td style="text-align:left">匹配字符串结尾，如果存在换行，同时还会匹配换行符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\G</code></td>
                          <td style="text-align:left">匹配最后匹配完成的位置</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\n</code></td>
                          <td style="text-align:left">匹配一个换行符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>\t</code></td>
                          <td style="text-align:left">匹配一个制表符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>^</code></td>
                          <td style="text-align:left">匹配一行字符串的开头</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>$</code></td>
                          <td style="text-align:left">匹配一行字符串的结尾</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>.</code></td>
                          <td style="text-align:left">匹配任意字符，除了换行符，当 <code>re.DOTALL</code> 标记被指定时，则可以匹配包括换行符的任意字符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[...]</code></td>
                          <td style="text-align:left">用来表示一组字符，单独列出，比如 <code>[amk]</code> 匹配 <code>a</code>、<code>m</code> 或 <code>k</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[^...]</code></td>
                          <td style="text-align:left">不在 <code>[]</code> 中的字符，比如 匹配除了 <code>a</code>、<code>b</code>、<code>c</code> 之外的字符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>*</code></td>
                          <td style="text-align:left">匹配 0 个或多个表达式</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>+</code></td>
                          <td style="text-align:left">匹配 1 个或多个表达式</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>?</code></td>
                          <td style="text-align:left">匹配 0 个或 1 个前面的正则表达式定义的片段，非贪婪方式</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>{n}</code></td>
                          <td style="text-align:left">精确匹配 n 个前面的表达式</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>{n, m}</code></td>
                          <td style="text-align:left">匹配 n 到 m 次由前面正则表达式定义的片段，贪婪方式</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">`a</td>
                          <td style="text-align:left">b`</td>
                          <td>匹配 a 或 b</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>()</code></td>
                          <td style="text-align:left">匹配括号内的表达式，也表示一个组</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>看完了之后，可能有点晕晕的吧，不过不用担心，后面我们会详细讲解一些常见规则的用法。</p>
                  <p>其实正则表达式不是 Python 独有的，它也可以用在其他编程语言中。Python 的 re 库提供了整个正则表达式的实现，利用这个库，可以在 Python 中使用正则表达式。在 Python 中写正则表达式几乎都用这个库，下面就来了解它的一些常用方法。</p>
                  <h2 id="2-match"><a href="#2-match" class="headerlink" title="2. match"></a>2. <code>match</code></h2>
                  <p>这里首先介绍第一个常用的匹配方法 —— <code>match</code>，向它传入要匹配的字符串以及正则表达式，就可以检测这个正则表达式是否匹配字符串。</p>
                  <p><code>match</code> 方法会尝试从字符串的起始位置匹配正则表达式，如果匹配，就返回匹配成功的结果；如果不匹配，就返回 <code>None</code>。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 123 4567 World_This is a Regex Demo'</span></span><br><span class="line">print(len(content))</span><br><span class="line">result = re.match(<span class="string">'^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;'</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.span())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">41</span></span><br><span class="line">&lt;_sre.SRE_Match object; span=(<span class="number">0</span>, <span class="number">25</span>), match=<span class="string">'Hello 123 4567 World_This'</span>&gt;</span><br><span class="line">Hello <span class="number">123</span> <span class="number">4567</span> World_This</span><br><span class="line">(<span class="number">0</span>, <span class="number">25</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先声明了一个字符串，其中包含英文字母、空白字符、数字等。接下来，我们写一个正则表达式：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>用它来匹配这个长字符串。开头的 <code>^</code> 是匹配字符串的开头，也就是以 <code>Hello</code> 开头；然后 <code>\s</code> 匹配空白字符，用来匹配目标字符串的空格；<code>\d</code> 匹配数字，3 个 <code>\d</code> 匹配 <code>123</code>；然后再写 1 个 <code>\s</code> 匹配空格；后面还有 <code>4567</code>，我们其实可以依然用 4 个 <code>\d</code> 来匹配，但是这么写比较烦琐，所以后面可以跟 <code>{4}</code> 以代表匹配前面的规则 4 次，也就是匹配 4 个数字；后面再紧接 1 个空白字符，最后的 <code>\w{10}</code> 匹配 10 个字母及下划线。我们注意到，这里其实并没有把目标字符串匹配完，不过这样依然可以进行匹配，只不过匹配结果短一点而已。</p>
                  <p>而在 <code>match</code> 方法中，第一个参数传入了正则表达式，第二个参数传入了要匹配的字符串。</p>
                  <p>打印输出结果，可以看到结果是 <code>SRE_Match</code> 对象，这证明成功匹配。该对象有两个方法：<code>group</code> 方法可以输出匹配到的内容，结果是 <code>Hello 123 4567 World_This</code>，这恰好是正则表达式规则所匹配的内容；<code>span</code> 方法可以输出匹配的范围，结果是 <code>(0, 25)</code>，这就是匹配到的结果字符串在原字符串中的位置范围。</p>
                  <p>通过上面的例子，我们基本了解了如何在 Python 中使用正则表达式来匹配一段文字。</p>
                  <h3 id="匹配目标"><a href="#匹配目标" class="headerlink" title="匹配目标"></a>匹配目标</h3>
                  <p>刚才我们用 <code>match</code> 方法得到匹配到的字符串内容，但是如果想从字符串中提取一部分内容，该怎么办呢？就像最前面的实例一样，从一段文本中提取出邮件或电话号码等内容。</p>
                  <p>这里可以使用括号 <code>()</code> 将想提取的子字符串括起来。<code>()</code> 实际上标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，调用 <code>group</code> 方法传入分组的索引即可获取提取的结果。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></span><br><span class="line">result = re.match(<span class="string">'^Hello\s(\d+)\sWorld'</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br><span class="line">print(result.span())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们想把字符串中的 <code>1234567</code> 提取出来，此时可以将数字部分的正则表达式用 <code>()</code> 括起来，然后调用了 <code>group(1)</code> 获取匹配结果。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;_sre.SRE_Match object; span=(<span class="number">0</span>, <span class="number">19</span>), match=<span class="string">'Hello 1234567 World'</span>&gt;</span><br><span class="line">Hello <span class="number">1234567</span> World</span><br><span class="line"><span class="number">1234567</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">19</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，我们成功得到了 <code>1234567</code>。这里用的是 <code>group(1)</code>，它与 <code>group()</code> 有所不同，后者会输出完整的匹配结果，而前者会输出第一个被 <code>()</code> 包围的匹配结果。假如正则表达式后面还有 <code>()</code> 包括的内容，那么可以依次用 <code>group(2)</code>、<code>group(3)</code> 等来获取。</p>
                  <h3 id="通用匹配"><a href="#通用匹配" class="headerlink" title="通用匹配"></a>通用匹配</h3>
                  <p>刚才我们写的正则表达式其实比较复杂，出现空白字符我们就写 <code>\s</code> 匹配，出现数字我们就用 <code>\d</code> 匹配，这样的工作量非常大。其实完全没必要这么做，因为还有一个万能匹配可以用，那就是 <code>.*</code>。其中 <code>.</code> 可以匹配任意字符（除换行符），<code>*</code> 代表匹配前面的字符无限次，所以它们组合在一起就可以匹配任意字符了。有了它，我们就不用挨个字符匹配了。</p>
                  <p>接着上面的例子，我们可以改写一下正则表达式：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 123 4567 World_This is a Regex Demo'</span></span><br><span class="line">result = re.match(<span class="string">'^Hello.*Demo$'</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.span())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们将中间部分直接省略，全部用 <code>.*</code> 来代替，最后加一个结尾字符串就好了。运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;_sre.SRE_Match object; span=(<span class="number">0</span>, <span class="number">41</span>), match=<span class="string">'Hello 123 4567 World_This is a Regex Demo'</span>&gt;</span><br><span class="line">Hello <span class="number">123</span> <span class="number">4567</span> World_This <span class="keyword">is</span> a Regex Demo</span><br><span class="line">(<span class="number">0</span>, <span class="number">41</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，<code>group</code> 方法输出了匹配的全部字符串，也就是说我们写的正则表达式匹配到了目标字符串的全部内容；<code>span</code> 方法输出 <code>(0, 41)</code>，这是整个字符串的长度。</p>
                  <p>因此，我们可以使用 <code>.*</code> 简化正则表达式的书写。</p>
                  <h3 id="贪婪与非贪婪"><a href="#贪婪与非贪婪" class="headerlink" title="贪婪与非贪婪"></a>贪婪与非贪婪</h3>
                  <p>使用上面的通用匹配 <code>.*</code> 时，可能有时候匹配到的并不是我们想要的结果。看下面的例子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></span><br><span class="line">result = re.match(<span class="string">'^He.*(\d+).*Demo$'</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们依然想获取中间的数字，所以中间依然写的是 <code>(\d+)</code>。而数字两侧由于内容比较杂乱，所以想省略来写，都写成 <code>.*</code>。最后，组成 <code>^He.*(\d+).*Demo$</code>，看样子并没有什么问题。我们看下运行结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;_sre.SRE_Match object; span=(<span class="number">0</span>, <span class="number">40</span>), match=<span class="string">'Hello 1234567 World_This is a Regex Demo'</span>&gt;</span><br><span class="line"><span class="number">7</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>奇怪的事情发生了，我们只得到了 7 这个数字，这是怎么回事呢？</p>
                  <p>这里就涉及贪婪匹配与非贪婪匹配的问题了。在贪婪匹配下，<code>.*</code> 会匹配尽可能多的字符。正则表达式中 <code>.*</code> 后面是 <code>\d+</code>，也就是至少一个数字，并没有指定具体多少个数字，因此，<code>.*</code> 就尽可能匹配多的字符，这里就把 <code>123456</code> 匹配了，给 <code>\d+</code> 留下一个可满足条件的数字 7，最后得到的内容就只有数字 7 了。</p>
                  <p>但这很明显会给我们带来很大的不便。有时候，匹配结果会莫名其妙少了一部分内容。其实，这里只需要使用非贪婪匹配就好了。非贪婪匹配的写法是 <code>.*?</code>，多了一个 <code>?</code>，那么它可以达到怎样的效果？我们再用实例看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></span><br><span class="line">result = re.match(<span class="string">'^He.*?(\d+).*Demo$'</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们只是将第一个<code>.*</code> 改成了 <code>.*?</code>，转变为非贪婪匹配。结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;_sre.SRE_Match object; span=(<span class="number">0</span>, <span class="number">40</span>), match=<span class="string">'Hello 1234567 World_This is a Regex Demo'</span>&gt;</span><br><span class="line"><span class="number">1234567</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此时就可以成功获取 <code>1234567</code> 了。原因可想而知，贪婪匹配是尽可能匹配多的字符，非贪婪匹配就是尽可能匹配少的字符。当 <code>.*?</code> 匹配到 <code>Hello</code> 后面的空白字符时，再往后的字符就是数字了，而 <code>\d+</code> 恰好可以匹配，那么这里 <code>.*?</code> 就不再进行匹配，交给 <code>\d+</code> 去匹配后面的数字。所以这样 <code>.*?</code> 匹配了尽可能少的字符，<code>\d+</code> 的结果就是 <code>1234567</code> 了。</p>
                  <p>所以说，在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用 <code>.*?</code> 来代替 <code>.*</code>，以免出现匹配结果缺失的情况。</p>
                  <p>但这里需要注意，如果匹配的结果在字符串结尾，<code>.*?</code> 就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。例如：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'http://weibo.com/comment/kEraCN'</span></span><br><span class="line">result1 = re.match(<span class="string">'http.*?comment/(.*?)'</span>, content)</span><br><span class="line">result2 = re.match(<span class="string">'http.*?comment/(.*)'</span>, content)</span><br><span class="line">print(<span class="string">'result1'</span>, result1.group(<span class="number">1</span>))</span><br><span class="line">print(<span class="string">'result2'</span>, result2.group(<span class="number">1</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result1</span><br><span class="line">result2 kEraCN</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以观察到，<code>.*?</code> 没有匹配到任何结果，而 <code>.*</code> 则尽量匹配多的内容，成功得到了匹配结果。</p>
                  <h3 id="修饰符"><a href="#修饰符" class="headerlink" title="修饰符"></a>修饰符</h3>
                  <p>正则表达式可以包含一些可选标志修饰符来控制匹配模式。修饰符被指定为一个可选的标志。我们用实例来看一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'''Hello 1234567 World_This</span></span><br><span class="line"><span class="string">is a Regex Demo</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">result = re.match(<span class="string">'^He.*?(\d+).*?Demo$'</span>, content)</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>和上面的例子相仿，我们在字符串中加了换行符，正则表达式还是一样的，用来匹配其中的数字。看一下运行结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">AttributeError Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-18</span>-c7d232b39645&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">      <span class="number">5</span> <span class="string">'''</span></span><br><span class="line"><span class="string">      6 result = re.match('^He.*?(\d+).*?Demo$', content)</span></span><br><span class="line"><span class="string">----&gt; 7 print(result.group(1))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">AttributeError: 'NoneType' object has no attribute 'group'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行直接报错，也就是说正则表达式没有匹配到这个字符串，返回结果为 <code>None</code>，而我们又调用了 <code>group</code> 方法导致 <code>AttributeError</code>。</p>
                  <p>那么，为什么加了一个换行符，就匹配不到了呢？这是因为。匹配的是除换行符之外的任意字符，当遇到换行符时，<code>.*?</code> 就不能匹配了，所以导致匹配失败。这里只需加一个修饰符 <code>re.S</code>，即可修正这个错误：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = re.match(<span class="string">'^He.*?(\d+).*?Demo$'</span>, content, re.S)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个修饰符的作用是使。匹配包括换行符在内的所有字符。此时运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">1234567</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个 <code>re.S</code> 在网页匹配中经常用到。因为 HTML 节点经常会有换行，加上它，就可以匹配节点与节点之间的换行了。</p>
                  <p>另外，还有一些修饰符，在必要的情况下也可以使用，如表 2- 所示。</p>
                  <p>表 2- 修饰符及其描述</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">修饰符</th>
                          <th style="text-align:left">描　　述</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>re.I</code></td>
                          <td style="text-align:left">使匹配对大小写不敏感</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>re.L</code></td>
                          <td style="text-align:left">做本地化识别（locale-aware）匹配</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>re.M</code></td>
                          <td style="text-align:left">多行匹配，影响 <code>^</code> 和 <code>$</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>re.S</code></td>
                          <td style="text-align:left">使。匹配包括换行符在内的所有字符</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>re.U</code></td>
                          <td style="text-align:left">根据 Unicode 字符集解析字符。这个标志影响 <code>\w</code>、<code>\W</code>、<code>\b</code> 和 <code>\B</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>re.X</code></td>
                          <td style="text-align:left">该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>在网页匹配中，较为常用的有 <code>re.S</code> 和 <code>re.I</code>。</p>
                  <h3 id="转义匹配"><a href="#转义匹配" class="headerlink" title="转义匹配"></a>转义匹配</h3>
                  <p>我们知道正则表达式定义了许多匹配模式，如 <code>.</code> 匹配除换行符以外的任意字符，但是如果目标字符串里面就包含 <code>.</code>，那该怎么办呢？</p>
                  <p>这里就需要用到转义匹配了，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'(百度) www.baidu.com'</span></span><br><span class="line">result = re.match(<span class="string">'\(百度 \) www\.baidu\.com'</span>, content)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当遇到用于正则匹配模式的特殊字符时，在前面加反斜线转义一下即可。例如可以用 <code>\.</code> 来匹配 <code>.</code>，运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;_sre.SRE_Match object; span=(<span class="number">0</span>, <span class="number">17</span>), match=<span class="string">'(百度) www.baidu.com'</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里成功匹配到了原字符串。</p>
                  <p>这些是写正则表达式常用的几个知识点，熟练掌握它们对后面写正则表达式非常有帮助。</p>
                  <h2 id="3-search"><a href="#3-search" class="headerlink" title="3. search"></a>3. <code>search</code></h2>
                  <p>前面提到过，<code>match</code> 方法是从字符串的开头开始匹配的，一旦开头不匹配，那么整个匹配就失败了。我们看下面的例子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'</span></span><br><span class="line">result = re.match(<span class="string">'Hello.*?(\d+).*?Demo'</span>, content)</span><br><span class="line">print(result)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的字符串以 <code>Extra</code> 开头，但是正则表达式以 <code>Hello</code> 开头，整个正则表达式是字符串的一部分，但是这样匹配是失败的。运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">None</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>因为 <code>match</code> 方法在使用时需要考虑到开头的内容，这在做匹配时并不方便。它更适合用来检测某个字符串是否符合某个正则表达式的规则。</p>
                  <p>这里就有另外一个方法 <code>search</code>，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果。也就是说，正则表达式可以是字符串的一部分，在匹配时，<code>search</code> 方法会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容，如果搜索完了还没有找到，就返回 <code>None</code>。</p>
                  <p>我们把上面代码中的 <code>match</code> 方法修改成 <code>search</code>，再看一下运行结果：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;_sre.SRE_Match object; span=(<span class="number">13</span>, <span class="number">53</span>), match=<span class="string">'Hello 1234567 World_This is a Regex Demo'</span>&gt;</span><br><span class="line"><span class="number">1234567</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时就得到了匹配结果。</p>
                  <p>因此，为了匹配方便，我们可以尽量使用 <code>search</code> 方法。</p>
                  <p>下面再用几个实例来看看 <code>search</code> 方法的用法。</p>
                  <p>首先，这里有一段待匹配的 HTML 文本，接下来写几个正则表达式实例来实现相应信息的提取：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">html = '''</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"songs-list"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>经典老歌<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"introduction"</span>&gt;</span>经典老歌列表<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">ul</span> <span class="attr">id</span>=<span class="string">"list"</span> <span class="attr">class</span>=<span class="string">"list-group"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">"2"</span>&gt;</span>一路上有你<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">"7"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/2.mp3"</span> <span class="attr">singer</span>=<span class="string">"任贤齐"</span>&gt;</span>沧海一声笑<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">"4"</span> <span class="attr">class</span>=<span class="string">"active"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/3.mp3"</span> <span class="attr">singer</span>=<span class="string">"齐秦"</span>&gt;</span>往事随风<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">"6"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/4.mp3"</span> <span class="attr">singer</span>=<span class="string">"beyond"</span>&gt;</span>光辉岁月<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">"5"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/5.mp3"</span> <span class="attr">singer</span>=<span class="string">"陈慧琳"</span>&gt;</span>记事本<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">"5"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/6.mp3"</span> <span class="attr">singer</span>=<span class="string">"邓丽君"</span>&gt;</span>但愿人长久<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">'''</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以观察到，<code>ul</code> 节点里有许多 <code>li</code> 节点，其中 <code>li</code> 节点中有的包含 <code>a</code> 节点，有的不包含 <code>a</code> 节点，<code>a</code> 节点还有一些相应的属性 —— 超链接和歌手名。</p>
                  <p>首先，我们尝试提取 <code>class</code> 为 <code>active</code> 的 <code>li</code> 节点内部的超链接包含的歌手名和歌名，此时需要提取第三个 <code>li</code> 节点下 <code>a</code> 节点的 <code>singer</code> 属性和文本。</p>
                  <p>此时正则表达式可以以 <code>li</code> 开头，然后寻找一个标志符 <code>active</code>，中间的部分可以用 <code>.*?</code> 来匹配。接下来，要提取 <code>singer</code> 这个属性值，所以还需要写入 <code>singer=&quot;(.*?)&quot;</code>，这里需要提取的部分用小括号括起来，以便用 <code>group</code> 方法提取出来，它的两侧边界是双引号。然后还需要匹配 <code>a</code> 节点的文本，其中它的左边界是 <code>&gt;</code>，右边界是 <code>&lt;/a&gt;</code>。然后目标内容依然用 <code>(.*?)</code> 来匹配，所以最后的正则表达式就变成了：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;li.*?active.*?singer&#x3D;&quot;(.*?)&quot;&gt;(.*?)&lt;&#x2F;a&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后再调用 <code>search</code> 方法，它会搜索整个 HTML 文本，找到符合正则表达式的第一个内容返回。</p>
                  <p>另外，由于代码有换行，所以这里第三个参数需要传入 <code>re.S</code>。整个匹配代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = re.search(<span class="string">'&lt;li.*?active.*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</span><br><span class="line"><span class="keyword">if</span> result:</span><br><span class="line">    print(result.group(<span class="number">1</span>), result.group(<span class="number">2</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于需要获取的歌手和歌名都已经用小括号包围，所以可以用 <code>group</code> 方法获取。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">齐秦 往事随风</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这正是 <code>class</code> 为 <code>active</code> 的 <code>li</code> 节点内部的超链接包含的歌手名和歌名。</p>
                  <p>如果正则表达式不加 <code>active</code>（也就是匹配不带 <code>class</code> 为 <code>active</code> 的节点内容），那会怎样呢？我们将正则表达式中的 <code>active</code> 去掉，代码改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = re.search(<span class="string">'&lt;li.*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</span><br><span class="line"><span class="keyword">if</span> result:</span><br><span class="line">    print(result.group(<span class="number">1</span>), result.group(<span class="number">2</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于 <code>search</code> 方法会返回第一个符合条件的匹配目标，这里结果就变了：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">任贤齐 沧海一声笑</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>把 <code>active</code> 标签去掉后，从字符串开头开始搜索，此时符合条件的节点就变成了第二个 <code>li</code> 节点，后面的就不再匹配，所以运行结果就变成第二个 <code>li</code> 节点中的内容了。</p>
                  <p>注意，在上面的两次匹配中，<code>search</code> 方法的第三个参数都加了 <code>re.S</code>，这使得 <code>.*?</code> 可以匹配换行，所以含有换行符的 <code>li</code> 节点被匹配到了。如果我们将其去掉，结果会是什么？代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">result = re.search(<span class="string">'&lt;li.*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html)</span><br><span class="line"><span class="keyword">if</span> result:</span><br><span class="line">    print(result.group(<span class="number">1</span>), result.group(<span class="number">2</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">beyond 光辉岁月</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，结果变成了第四个 <code>li</code> 节点的内容。这是因为第二个和第三个 <code>li</code> 节点都包含了换行符，去掉 <code>re.S</code> 之后，<code>.*?</code> 已经不能匹配换行符，所以正则表达式不会匹配到第二个和第三个 <code>li</code> 节点，而第四个 <code>li</code> 节点中不包含换行符，所以成功匹配。</p>
                  <p>由于绝大部分的 HTML 文本都包含了换行符，所以尽量都需要加上 <code>re.S</code> 修饰符，以免出现匹配不到的问题。</p>
                  <h2 id="4-findall"><a href="#4-findall" class="headerlink" title="4. findall"></a>4. <code>findall</code></h2>
                  <p>前面我们介绍了 <code>search</code> 方法的用法，它可以返回匹配正则表达式的第一个内容，但是如果想要获取匹配正则表达式的所有内容，那该怎么办呢？这时就要借助 <code>findall</code> 方法了。该方法会搜索整个字符串，然后返回匹配正则表达式的所有内容。</p>
                  <p>还是上面的 HTML 文本，如果想获取所有 <code>a</code> 节点的超链接、歌手和歌名，就可以将 <code>search</code> 方法换成 <code>findall</code> 方法。如果有返回结果的话，就是列表类型，所以需要遍历一下来依次获取每组内容。代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = re.findall(<span class="string">'&lt;li.*?href="(.*?)".*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</span><br><span class="line">print(results)</span><br><span class="line">print(type(results))</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br><span class="line">    print(result[<span class="number">0</span>], result[<span class="number">1</span>], result[<span class="number">2</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[(&#39;&#x2F;2.mp3&#39;, &#39; 任贤齐 &#39;, &#39; 沧海一声笑 &#39;), (&#39;&#x2F;3.mp3&#39;, &#39; 齐秦 &#39;, &#39; 往事随风 &#39;), (&#39;&#x2F;4.mp3&#39;, &#39;beyond&#39;, &#39; 光辉岁月 &#39;), (&#39;&#x2F;5.mp3&#39;, &#39; 陈慧琳 &#39;, &#39; 记事本 &#39;), (&#39;&#x2F;6.mp3&#39;, &#39; 邓丽君 &#39;, &#39; 但愿人长久 &#39;)]</span><br><span class="line">&lt;class &#39;list&#39;&gt;</span><br><span class="line">(&#39;&#x2F;2.mp3&#39;, &#39; 任贤齐 &#39;, &#39; 沧海一声笑 &#39;)</span><br><span class="line">&#x2F;2.mp3 任贤齐 沧海一声笑</span><br><span class="line">(&#39;&#x2F;3.mp3&#39;, &#39; 齐秦 &#39;, &#39; 往事随风 &#39;)</span><br><span class="line">&#x2F;3.mp3 齐秦 往事随风</span><br><span class="line">(&#39;&#x2F;4.mp3&#39;, &#39;beyond&#39;, &#39; 光辉岁月 &#39;)</span><br><span class="line">&#x2F;4.mp3 beyond 光辉岁月</span><br><span class="line">(&#39;&#x2F;5.mp3&#39;, &#39; 陈慧琳 &#39;, &#39; 记事本 &#39;)</span><br><span class="line">&#x2F;5.mp3 陈慧琳 记事本</span><br><span class="line">(&#39;&#x2F;6.mp3&#39;, &#39; 邓丽君 &#39;, &#39; 但愿人长久 &#39;)</span><br><span class="line">&#x2F;6.mp3 邓丽君 但愿人长久</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，返回的列表中的每个元素都是元组类型，我们用对应的索引依次取出即可。</p>
                  <p>如果只是获取第一个内容，可以用 <code>search</code> 方法。当需要提取多个内容时，可以用 <code>findall</code> 方法。</p>
                  <h2 id="5-sub"><a href="#5-sub" class="headerlink" title="5. sub"></a>5. <code>sub</code></h2>
                  <p>除了使用正则表达式提取信息外，有时候还需要借助它来修改文本。比如，想要把一串文本中的所有数字都去掉，如果只用字符串的 <code>replace</code> 方法，那就太烦琐了，这时可以借助 <code>sub</code> 方法。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'54aK54yr5oiR54ix5L2g'</span></span><br><span class="line">content = re.sub(<span class="string">'\d+'</span>, <span class="string">''</span>, content)</span><br><span class="line">print(content)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">aKyroiRixLg</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里只需要给第一个参数传入 <code>\d+</code> 来匹配所有的数字，第二个参数为替换成的字符串（如果去掉该参数的话，可以赋值为空），第三个参数是原字符串。</p>
                  <p>在上面的 HTML 文本中，如果想获取所有 <code>li</code> 节点的歌名，直接用正则表达式来提取可能比较烦琐。比如，可以写成这样子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">results = re.findall(<span class="string">'&lt;li.*?&gt;\s*?(&lt;a.*?&gt;)?(\w+)(&lt;/a&gt;)?\s*?&lt;/li&gt;'</span>, html, re.S)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result[<span class="number">1</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">一路上有你</span><br><span class="line">沧海一声笑</span><br><span class="line">往事随风</span><br><span class="line">光辉岁月</span><br><span class="line">记事本</span><br><span class="line">但愿人长久</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此时借助 <code>sub</code> 方法就比较简单了。可以先用 <code>sub</code> 方法将 <code>a</code> 节点去掉，只留下文本，然后再利用 <code>findall</code> 提取就好了：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">html = re.sub(<span class="string">'&lt;a.*?&gt;|&lt;/a&gt;'</span>, <span class="string">''</span>, html)</span><br><span class="line">print(html)</span><br><span class="line">results = re.findall(<span class="string">'&lt;li.*?&gt;(.*?)&lt;/li&gt;'</span>, html, re.S)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result.strip())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;div id=<span class="string">"songs-list"</span>&gt;</span><br><span class="line">    &lt;h2 class="title"&gt; 经典老歌 &lt;/h2&gt;</span><br><span class="line">    &lt;p class="introduction"&gt;</span><br><span class="line">        经典老歌列表</span><br><span class="line">    &lt;/p&gt;</span><br><span class="line">    &lt;ul id="list" class="list-group"&gt;</span><br><span class="line">        &lt;li data-view="2"&gt; 一路上有你 &lt;/li&gt;</span><br><span class="line">        &lt;li data-view=<span class="string">"7"</span>&gt;</span><br><span class="line">            沧海一声笑</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li data-view="4" class="active"&gt;</span><br><span class="line">            往事随风</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li data-view="6"&gt; 光辉岁月 &lt;/li&gt;</span><br><span class="line">        &lt;li data-view="5"&gt; 记事本 &lt;/li&gt;</span><br><span class="line">        &lt;li data-view=<span class="string">"5"</span>&gt;</span><br><span class="line">            但愿人长久</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">一路上有你</span><br><span class="line">沧海一声笑</span><br><span class="line">往事随风</span><br><span class="line">光辉岁月</span><br><span class="line">记事本</span><br><span class="line">但愿人长久</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，<code>a</code> 节点经过 <code>sub</code> 方法处理后就没有了，然后再通过 <code>findall</code> 方法直接提取即可。可以看到，在适当的时候，借助 <code>sub</code> 方法可以起到事半功倍的效果。</p>
                  <h2 id="6-compile"><a href="#6-compile" class="headerlink" title="6. compile"></a>6. <code>compile</code></h2>
                  <p>前面所讲的方法都是用来处理字符串的方法，最后再介绍一下 <code>compile</code> 方法，这个方法可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用。示例代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content1 = <span class="string">'2019-12-15 12:00'</span></span><br><span class="line">content2 = <span class="string">'2019-12-17 12:55'</span></span><br><span class="line">content3 = <span class="string">'2019-12-22 13:21'</span></span><br><span class="line">pattern = re.compile(<span class="string">'\d&#123;2&#125;:\d&#123;2&#125;'</span>)</span><br><span class="line">result1 = re.sub(pattern, <span class="string">''</span>, content1)</span><br><span class="line">result2 = re.sub(pattern, <span class="string">''</span>, content2)</span><br><span class="line">result3 = re.sub(pattern, <span class="string">''</span>, content3)</span><br><span class="line">print(result1, result2, result3)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>例如，这里有 3 个日期，我们想分别将 3 个日期中的时间去掉，这时可以借助 <code>sub</code> 方法。该方法的第一个参数是正则表达式，但是这里没有必要重复写 3 个同样的正则表达式，此时可以借助 <code>compile</code> 方法将正则表达式编译成一个正则表达式对象，以便复用。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2019</span><span class="number">-12</span><span class="number">-15</span>  <span class="number">2019</span><span class="number">-12</span><span class="number">-17</span>  <span class="number">2019</span><span class="number">-12</span><span class="number">-22</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外，<code>compile</code> 还可以传入修饰符，例如 <code>re.S</code> 等修饰符，这样在 <code>search</code>、<code>findall</code> 等方法中就不需要额外传了。所以，<code>compile</code> 方法可以说是给正则表达式做了一层封装，以便我们更好地复用。</p>
                  <h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2>
                  <p>到此为止，正则表达式的基本用法就介绍完了，后面会通过具体的实例来讲解正则表达式的用法。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/RegexTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/RegexTest。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-13 07:44:31" itemprop="dateCreated datePublished" datetime="2022-02-13T07:44:31+08:00">2022-02-13</time>
                </span>
                <span id="/202223.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 强大灵活的正则表达式" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>13k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>12 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202215.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202215.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 1.5 代理的基本原理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>我们在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么美好，然而一杯茶的功夫可能就会出现错误，比如 403 Forbidden，这时打开网页一看，可能会看到 “您的 IP 访问频率太高” 这样的提示。出现这种现象的原因是网站采取了一些反爬虫措施。比如，服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封 IP。</p>
                  <p>既然服务器检测的是某个 IP 单位时间的请求次数，那么借助某种方式来伪装我们的 IP，让服务器识别不出是由我们本机发起的请求，不就可以成功防止封 IP 了吗？</p>
                  <p>一种有效的方式就是使用代理，后面会详细说明代理的用法。在这之前，需要先了解下代理的基本原理，它是怎样实现伪装 IP 的呢？</p>
                  <h2 id="1-基本原理"><a href="#1-基本原理" class="headerlink" title="1. 基本原理"></a>1. 基本原理</h2>
                  <p>代理实际上指的就是代理服务器，英文叫作 Proxy Server，它的功能是代理网络用户去取得网络信息。形象地说，它是网络信息的中转站。在我们正常请求一个网站时，是发送了请求给 Web 服务器，Web 服务器把响应传回给我们。如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向 Web 服务器发起请求，而是向代理服务器发出请求，请求会发送给代理服务器，然后由代理服务器再发送给 Web 服务器，接着由代理服务器再把 Web 服务器返回的响应转发给本机。这样我们同样可以正常访问网页，但这个过程中 Web 服务器识别出的真实 IP 就不再是我们本机的 IP 了，就成功实现了 IP 伪装，这就是代理的基本原理。</p>
                  <h2 id="2-代理的作用"><a href="#2-代理的作用" class="headerlink" title="2. 代理的作用"></a>2. 代理的作用</h2>
                  <p>那么，代理有什么作用呢？我们可以简单列举如下。</p>
                  <ul>
                    <li>突破自身 IP 访问限制，访问一些平时不能访问的站点。</li>
                    <li>访问一些单位或团体内部资源。比如，使用教育网内地址段的免费代理服务器，就可以下载和上传对教育网开放的各类 FTP，以及查询、共享各类资料等。</li>
                    <li>提高访问速度。通常，代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，会同时将其保存到缓冲区中，而当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。</li>
                    <li>隐藏真实 IP。上网者也可以通过这种方法隐藏自己的 IP，免受攻击。对于爬虫来说，我们用代理就是为了隐藏自身的 IP，防止自身的 IP 被封锁。</li>
                  </ul>
                  <h2 id="3-爬虫代理"><a href="#3-爬虫代理" class="headerlink" title="3. 爬虫代理"></a>3. 爬虫代理</h2>
                  <p>对于爬虫来说，由于爬虫爬取速度过快，在爬取过程中可能遇到同一个 IP 访问过于频繁的问题，此时网站就会让我们输入验证码登录或者直接封锁 IP，这样会给爬取带来极大的不便。</p>
                  <p>使用代理隐藏真实的 IP，让服务器误以为是代理服务器在请求自己。这样在爬取过程中通过不断更换代理，就不会被封锁，可以达到很好的爬取效果。</p>
                  <h2 id="4-代理分类"><a href="#4-代理分类" class="headerlink" title="4. 代理分类"></a>4. 代理分类</h2>
                  <p>对代理进行分类时，既可以根据协议区分，也可以根据其匿名程度区分，下面总结如下。</p>
                  <h3 id="根据协议区分"><a href="#根据协议区分" class="headerlink" title="根据协议区分"></a>根据协议区分</h3>
                  <p>根据代理的协议，代理可以分为如下类别。</p>
                  <ul>
                    <li><strong>FTP 代理服务器</strong>。主要用于访问 FTP 服务器，一般有上传、下载以及缓存功能，端口一般为 21、2121 等。</li>
                    <li><strong>HTTP 代理服务器</strong>。主要用于访问网页，一般有内容过滤和缓存功能，端口一般为 80、8080、3128 等。</li>
                    <li><strong>SSL/TLS 代理</strong>。主要用于访问加密网站，一般有 SSL 或 TLS 加密功能（最高支持 128 位加密强度），端口一般为 443。</li>
                    <li><strong>RTSP 代理</strong>。主要用于 Realplayer 访问 Real 流媒体服务器，一般有缓存功能，端口一般为 554。</li>
                    <li><strong>Telnet 代理</strong>。主要用于 Telnet 远程控制（黑客入侵计算机时常用于隐藏身份），端口一般为 23。</li>
                    <li><strong>POP3/SMTP 代理</strong>。主要用于 POP3/SMTP 方式收发邮件，一般有缓存功能，端口一般为 110/25。</li>
                    <li><strong>SOCKS 代理</strong>。只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为 1080。SOCKS 代理协议又分为 SOCKS4 和 SOCKS5，SOCKS4 协议只支持 TCP，而 SOCKS5 协议支持 TCP 和 UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCKS4 能做到的 SOCKS5 都可以做到，但 SOCKS5 能做到的 SOCKS4 不一定能做到。</li>
                  </ul>
                  <h3 id="根据匿名程度区分"><a href="#根据匿名程度区分" class="headerlink" title="根据匿名程度区分"></a>根据匿名程度区分</h3>
                  <p>根据代理的匿名程度，代理可以分为如下类别。</p>
                  <ul>
                    <li><strong>高度匿名代理</strong>：高度匿名代理会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的 IP 是代理服务器的 IP。</li>
                    <li><strong>普通匿名代理</strong>：普通匿名代理会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实 IP。代理服务器通常会加入的 HTTP 头有 <code>HTTP_VIA</code> 和 <code>HTTP_X_FORWARDED_FOR</code>。</li>
                    <li><strong>透明代理</strong>：透明代理不但改动了数据包，还会告诉服务器客户端的真实 IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网中的硬件防火墙。</li>
                    <li><strong>间谍代理</strong>：间谍代理指组织或个人创建的，用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。</li>
                  </ul>
                  <h2 id="5-常见代理设置"><a href="#5-常见代理设置" class="headerlink" title="5. 常见代理设置"></a>5. 常见代理设置</h2>
                  <p>常见的代理设置如下：</p>
                  <ul>
                    <li>使用网上的免费代理，最好使用高匿代理，使用前抓取下来并筛选一下可用代理，也可以进一步维护一个代理池。</li>
                    <li>使用付费代理服务，互联网上存在许多代理商，可以付费使用，其质量比免费代理好很多。</li>
                    <li>ADSL 拨号，拨一次号换一次 IP，稳定性高，也是一种比较有效的解决方案。</li>
                    <li>蜂窝代理，即用 4G 或 5G 网卡等制作的代理。由于蜂窝网络用作代理的情形较少，因此整体被封锁的几率会较低，但搭建蜂窝代理的成本较高。</li>
                  </ul>
                  <p>在后面，我们会详细介绍一些代理的使用方式。</p>
                  <h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2>
                  <p>本文介绍了代理的相关知识，这对后文我们进行一些反爬绕过的实现有很大的帮助，同时也为后文的一些抓包操作打下基础，需要好好理解。</p>
                  <p>本节由于涉及一些专业名词，本节的部分内容参考来源如下：</p>
                  <ul>
                    <li>文档 - 代理服务器 - 维基百科：<a href="https://github.com/Germey/Python3WebSpider2/tree/3dc0dc1092305e1040f86c4e72a40f627b095897/[https:/zh.wikipedia.org/wiki/%20代理服务器/README.md" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/ 代理服务器</a></li>
                    <li>文档 - 代理 - 百度百科：<a href="https://baike.baidu.com/item/%20代理%20/%203242667" target="_blank" rel="noopener">https://baike.baidu.com/item/代理/3242667</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-12 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-12T12:42:44+08:00">2022-02-12</time>
                </span>
                <span id="/202215.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 1.5 代理的基本原理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>2.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202244.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202244.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 关系型数据库 MySQL 存储</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>关系型数据库是基于关系模型的数据库，而关系模型是通过二维表来保存的，所以它的存储方式就是行列组成的表，每一列是一个字段，每一行是一条记录。表可以看作某个实体的集合，而实体之间存在联系，这就需要表与表之间的关联关系来体现，如主键外键的关联关系。多个表组成一个数据库，也就是关系型数据库。</p>
                  <p>关系型数据库有多种，如 SQLite、MySQL、Oracle、SQL Server、DB2 等，本节我们主要来了解下 MySQL 数据库的存储操作。</p>
                  <p>在 Python 2 中，连接 MySQL 的库大多是使用 MySQLdb，但是此库的官方并不支持 Python 3，所以这里推荐使用的库是 PyMySQL。本节中，我们就来讲解使用 PyMySQL 操作 MySQL 数据库的方法。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>在开始之前，请确保已经安装好了 MySQL 数据库并保证它能正常运行，安装方式可以参考：<a href="https://setup.scrape.center/mysql。" target="_blank" rel="noopener">https://setup.scrape.center/mysql。</a></p>
                  <p>除了安装好 MySQL 数据外，还需要安装好 PyMySQL 库，如尚未安装 PyMySQL，可以使用 pip3 来安装：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> pymysql</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更详细的安装方式可以参考：<a href="https://setup.scrape.center/pymysql" target="_blank" rel="noopener">https://setup.scrape.center/pymysql</a></p>
                  <p>二者都安装好了之后，我们就可以开始本节的学习了。</p>
                  <h2 id="2-连接数据库"><a href="#2-连接数据库" class="headerlink" title="2. 连接数据库"></a>2. 连接数据库</h2>
                  <p>这里首先尝试连接一下数据库。假设当前的 MySQL 运行在本地，用户名为 root，密码为 123456，运行端口为 3306。这里利用 PyMySQL 先连接 MySQL，然后创建一个新的数据库，名字叫作 spiders，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">cursor.execute(<span class="string">'SELECT VERSION()'</span>)</span><br><span class="line">data = cursor.fetchone()</span><br><span class="line">print(<span class="string">'Database version:'</span>, data)</span><br><span class="line">cursor.execute(<span class="string">"CREATE DATABASE spiders DEFAULT CHARACTER SET utf8mb4"</span>)</span><br><span class="line">db.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Database version: (<span class="string">'8.0.19'</span>,)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里通过 PyMySQL 的 <code>connect</code> 方法声明一个 MySQL 连接对象 <code>db</code>，此时需要传入 MySQL 运行的 <code>host</code>（即 IP）。由于 MySQL 在本地运行，所以传入的是 <code>localhost</code>。如果 MySQL 在远程运行，则传入其公网 IP 地址。后续的参数 <code>user</code> 即用户名，<code>password</code> 即密码，<code>port</code> 即端口（默认为 3306）。</p>
                  <p>连接成功后，需要再调用 <code>cursor</code> 方法获得 MySQL 的操作游标，利用游标来执行 SQL 语句。这里我们执行了两句 SQL，直接用 <code>execute</code> 方法执行即可。第一句 SQL 用于获得 MySQL 的当前版本，然后调用 <code>fetchone</code> 方法获得第一条数据，也就得到了版本号。第二句 SQL 执行创建数据库的操作，数据库名叫作 spiders，默认编码为 UTF-8。由于该语句不是查询语句，所以直接执行后就成功创建了数据库 spiders。接着，再利用这个数据库进行后续的操作。</p>
                  <h2 id="3-创建表"><a href="#3-创建表" class="headerlink" title="3. 创建表"></a>3. 创建表</h2>
                  <p>一般来说，创建数据库的操作只需要执行一次就好了。当然，我们也可以手动创建数据库。以后，我们的操作都在 spiders 数据库上执行。</p>
                  <p>创建数据库后，在连接时需要额外指定一个参数 <code>db</code>。</p>
                  <p>接下来，新创建一个数据表 students，此时执行创建表的 SQL 语句即可。这里指定 3 个字段，结构如表 4- 所示。</p>
                  <p>表 4- 数据表 students</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">字　段　名</th>
                          <th style="text-align:left">含　　义</th>
                          <th style="text-align:left">类　　型</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>id</code></td>
                          <td style="text-align:left">学号</td>
                          <td style="text-align:left"><code>varchar</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>name</code></td>
                          <td style="text-align:left">姓名</td>
                          <td style="text-align:left"><code>varchar</code></td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>age</code></td>
                          <td style="text-align:left">年龄</td>
                          <td style="text-align:left"><code>int</code></td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>创建该表的示例代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS students (id VARCHAR(255) NOT NULL, name VARCHAR(255) NOT NULL, age INT NOT NULL, PRIMARY KEY (id))'</span></span><br><span class="line">cursor.execute(sql)</span><br><span class="line">db.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后，我们便创建了一个名为 students 的数据表。</p>
                  <p>当然，为了演示，这里只指定了最简单的几个字段。实际上，在爬虫过程中，我们会根据爬取结果设计特定的字段。</p>
                  <h2 id="4-插入数据"><a href="#4-插入数据" class="headerlink" title="4. 插入数据"></a>4. 插入数据</h2>
                  <p>下一步就是向数据库中插入数据了。例如，这里爬取了一个学生信息，学号为 20120001，名字为 Bob，年龄为 20，那么如何将该条数据插入数据库呢？示例代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">id = <span class="string">'20120001'</span></span><br><span class="line">user = <span class="string">'Bob'</span></span><br><span class="line">age = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'INSERT INTO students(id, name, age) values(%s, %s, %s)'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql, (id, user, age))</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先构造了一个 SQL 语句，其值没有用字符串拼接的方式来构造，如：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">sql = <span class="string">'INSERT INTO students(id, name, age) values('</span> + id + <span class="string">', '</span> + name + <span class="string">', '</span> + age + <span class="string">')'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的写法烦琐而且不直观，所以我们选择直接用格式化符 <code>%s</code> 来实现。有几个 <code>value</code> 写几个 <code>%s</code>，我们只需要在 <code>execute</code> 方法的第一个参数传入该 SQL 语句，<code>value</code> 值用统一的元组传过来就好了。这样的写法既可以避免字符串拼接的麻烦，又可以避免引号冲突的问题。</p>
                  <p>之后值得注意的是，需要执行 <code>db</code> 对象的 <code>commit</code> 方法才可以实现数据插入，这个方法才是真正将语句提交到数据库执行的方法。对于数据插入、更新、删除操作，都需要调用该方法才能生效。</p>
                  <p>接下来，我们加了一层异常处理。如果执行失败，则调用 <code>rollback</code> 执行数据回滚，相当于什么都没有发生过。</p>
                  <p>这里涉及事务的问题。事务机制可以确保数据的一致性，也就是这件事要么发生了，要么没有发生。比如插入一条数据，不会存在插入一半的情况，要么全部插入，要么都不插入，这就是事务的原子性。另外，事务还有 3 个属性 —— 一致性、隔离性和持久性。这 4 个属性通常称为 ACID 特性，具体如表 4- 所示。</p>
                  <p>表 4- 事务的 4 个属性</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">属　　性</th>
                          <th style="text-align:left">解　　释</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left">原子性（atomicity）</td>
                          <td style="text-align:left">事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">一致性（consistency）</td>
                          <td style="text-align:left">事务必须使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">隔离性（isolation）</td>
                          <td style="text-align:left">一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">持久性（durability）</td>
                          <td style="text-align:left">持续性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>插入、更新和删除操作都是对数据库进行更改的操作，而更改操作都必须为一个事务，所以这些操作的标准写法就是：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样便可以保证数据的一致性。这里的 <code>commit</code> 和 <code>rollback</code> 方法就为事务的实现提供了支持。</p>
                  <p>上面数据插入的操作是通过构造 SQL 语句实现的，但是很明显，这有一个极其不方便的地方，比如突然增加了性别字段 <code>gender</code>，此时 SQL 语句就需要改成：</p>
                  <figure class="highlight sql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> students(<span class="keyword">id</span>, <span class="keyword">name</span>, age, gender) <span class="keyword">values</span>(%s, %s, %s, %s)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>相应的元组参数则需要改成：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">(id, name, age, gender)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这显然不是我们想要的。在很多情况下，我们要达到的效果是插入方法无须改动，做成一个通用方法，只需要传入一个动态变化的字典就好了。比如，构造这样一个字典：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20120001'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后 SQL 语句会根据字典动态构造，元组也动态构造，这样才能实现通用的插入方法。所以，这里我们需要改写一下插入方法：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20120001'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span></span><br><span class="line">&#125;</span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">sql = <span class="string">'INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;)'</span>.format(table=table, keys=keys, values=values)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">   <span class="keyword">if</span> cursor.execute(sql, tuple(data.values())):</span><br><span class="line">       print(<span class="string">'Successful'</span>)</span><br><span class="line">       db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Failed'</span>)</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们传入的数据是字典，并将其定义为 <code>data</code> 变量。表名也定义成变量 <code>table</code>。接下来，就需要构造一个动态的 SQL 语句了。</p>
                  <p>首先，需要构造插入的字段 <code>id</code>、<code>name</code> 和 <code>age</code>。这里只需要将 <code>data</code> 的键名拿过来，然后用逗号分隔即可。所以 <code>&#39;, &#39;.join(data.keys())</code> 的结果就是 <code>id, name, age</code>，然后需要构造多个 <code>%ss</code> 当作占位符，有几个字段构造几个即可。比如，这里有三个字段，就需要构造 <code>%s, %s, %s</code>。这里首先定义了长度为 1 的数组 <code>[&#39;%s&#39;]</code>，然后用乘法将其扩充为 <code>[&#39;%s&#39;, &#39;%s&#39;, &#39;%s&#39;]</code>，再调用 <code>join</code> 方法，最终变成 <code>%s, %s, %s</code>。最后，我们再利用字符串的 <code>format</code> 方法将表名、字段名和占位符构造出来。最终的 SQL 语句就被动态构造成了：</p>
                  <figure class="highlight sql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> students(<span class="keyword">id</span>, <span class="keyword">name</span>, age) <span class="keyword">VALUES</span> (%s, %s, %s)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>最后，为 <code>execute</code> 方法的第一个参数传入 <code>sql</code> 变量，第二个参数传入 <code>data</code> 的键值构造的元组，就可以成功插入数据了。</p>
                  <p>如此以来，我们便实现了传入一个字典来插入数据的方法，不需要再去修改 SQL 语句和插入操作了。</p>
                  <h2 id="5-更新数据"><a href="#5-更新数据" class="headerlink" title="5. 更新数据"></a>5. 更新数据</h2>
                  <p>数据更新操作实际上也是执行 SQL 语句，最简单的方式就是构造一个 SQL 语句，然后执行：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">sql = <span class="string">'UPDATE students SET age = %ss WHERE name = %s'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">   cursor.execute(sql, (<span class="number">25</span>, <span class="string">'Bob'</span>))</span><br><span class="line">   db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">   db.rollback()</span><br><span class="line">db.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里同样用占位符的方式构造 SQL，然后执行 <code>execute</code> 方法，传入元组形式的参数，同样执行 <code>commit</code> 方法执行操作。如果要做简单的数据更新的话，完全可以使用此方法。</p>
                  <p>但是在实际的数据抓取过程中，大部分情况下需要插入数据，但是我们关心的是会不会出现重复数据，如果出现了，我们希望更新数据而不是重复保存一次。另外，就像前面所说的动态构造 SQL 的问题，所以这里可以再实现一种去重的方法，如果数据存在，则更新数据；如果数据不存在，则插入数据。另外，这种做法支持灵活的字典传值。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20120001'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;) ON DUPLICATE KEY UPDATE '</span>.format(table=table, keys=keys, values=values)</span><br><span class="line">update = <span class="string">','</span>.join([<span class="string">"&#123;key&#125; = %s"</span>.format(key=key) <span class="keyword">for</span> key <span class="keyword">in</span> data])</span><br><span class="line">sql += update</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> cursor.execute(sql, tuple(data.values())*<span class="number">2</span>):</span><br><span class="line">        print(<span class="string">'Successful'</span>)</span><br><span class="line">        db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Failed'</span>)</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里构造的 SQL 语句其实是插入语句，但是我们在后面加了 <code>ON DUPLICATE KEY UPDATE</code>。这行代码的意思是如果主键已经存在，就执行更新操作。比如，我们传入的数据 <code>id</code> 仍然为 20120001，但是年龄有所变化，由 20 变成了 21，此时这条数据不会被插入，而是直接更新 id 为 20120001 的数据。完整的 SQL 构造出来是这样的：</p>
                  <figure class="highlight sql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> students(<span class="keyword">id</span>, <span class="keyword">name</span>, age) <span class="keyword">VALUES</span> (%s, %s, %s) <span class="keyword">ON</span> <span class="keyword">DUPLICATE</span> <span class="keyword">KEY</span> <span class="keyword">UPDATE</span> <span class="keyword">id</span> = %s, <span class="keyword">name</span> = %s, age = %s</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里就变成了 6 个 <code>%ss</code>。所以在后面的 <code>execute</code> 方法的第二个参数元组就需要乘以 2 变成原来的 2 倍。</p>
                  <p>如此一来，我们就可以实现主键不存在便插入数据，存在则更新数据的功能了。</p>
                  <h2 id="6-删除数据"><a href="#6-删除数据" class="headerlink" title="6. 删除数据"></a>6. 删除数据</h2>
                  <p>删除操作相对简单，直接使用 <code>DELETE</code> 语句即可，只是需要指定要删除的目标表名和删除条件，而且仍然需要使用 <code>db</code> 的 <code>commit</code> 方法才能生效。示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">table = <span class="string">'students'</span></span><br><span class="line">condition = <span class="string">'age &gt; 20'</span></span><br><span class="line"></span><br><span class="line">sql = <span class="string">'DELETE FROM  &#123;table&#125; WHERE &#123;condition&#125;'</span>.format(table=table, condition=condition)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br><span class="line"></span><br><span class="line">db.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>因为删除条件多种多样，运算符有大于、小于、等于、<code>LIKE</code> 等，条件连接符有 <code>AND</code>、<code>OR</code> 等，所以不再继续构造复杂的判断条件。这里直接将条件当作字符串来传递，以实现删除操作。</p>
                  <h2 id="7-查询数据"><a href="#7-查询数据" class="headerlink" title="7. 查询数据"></a>7. 查询数据</h2>
                  <p>说完插入、修改和删除等操作，还剩下非常重要的一个操作，那就是查询。查询会用到 <code>SELECT</code> 语句，示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">sql = <span class="string">'SELECT * FROM students WHERE age &gt;= 20'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    print(<span class="string">'Count:'</span>, cursor.rowcount)</span><br><span class="line">    one = cursor.fetchone()</span><br><span class="line">    print(<span class="string">'One:'</span>, one)</span><br><span class="line">    results = cursor.fetchall()</span><br><span class="line">    print(<span class="string">'Results:'</span>, results)</span><br><span class="line">    print(<span class="string">'Results Type:'</span>, type(results))</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> results:</span><br><span class="line">        print(row)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Error'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Count: <span class="number">4</span></span><br><span class="line">One: (<span class="string">'20120001'</span>, <span class="string">'Bob'</span>, <span class="number">25</span>)</span><br><span class="line">Results: ((<span class="string">'20120011'</span>, <span class="string">'Mary'</span>, <span class="number">21</span>), (<span class="string">'20120012'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>), (<span class="string">'20120013'</span>, <span class="string">'James'</span>, <span class="number">22</span>))</span><br><span class="line">Results Type: &lt;<span class="class"><span class="keyword">class</span> '<span class="title">tuple</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="params">(<span class="string">'20120011'</span>, <span class="string">'Mary'</span>, <span class="number">21</span>)</span></span></span><br><span class="line"><span class="class"><span class="params">(<span class="string">'20120012'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>)</span></span></span><br><span class="line"><span class="class"><span class="params">(<span class="string">'20120013'</span>, <span class="string">'James'</span>, <span class="number">22</span>)</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们构造了一条 SQL 语句，将年龄 20 岁及以上的学生查询出来，然后将其传给 <code>execute</code> 方法。注意，这里不再需要 <code>db</code> 的 <code>commit</code> 方法。接着，调用 <code>cursor</code> 的 <code>rowcount</code> 属性获取查询结果的条数，当前示例中是 4 条。</p>
                  <p>然后我们调用了 <code>fetchone</code> 方法，这个方法可以获取结果的第一条数据，返回结果是元组形式，元组的元素顺序跟字段一一对应，即第一个元素就是第一个字段 <code>id</code>，第二个元素就是第二个字段 <code>name</code>，以此类推。随后，我们又调用了 <code>fetchall</code> 方法，它可以得到结果的所有数据。然后将其结果和类型打印出来，它是二重元组，每个元素都是一条记录，我们将其遍历输出出来。</p>
                  <p>但是这里需要注意一个问题，这里显示的是 3 条数据而不是 4 条，<code>fetchall</code> 方法不是获取所有数据吗？这是因为它的内部实现有一个偏移指针用来指向查询结果，最开始偏移指针指向第一条数据，取一次之后，指针偏移到下一条数据，这样再取的话，就会取到下一条数据了。我们最初调用了一次 <code>fetchone</code> 方法，这样结果的偏移指针就指向下一条数据，<code>fetchall</code> 方法返回的是偏移指针指向的数据一直到结束的所有数据，所以该方法获取的结果就只剩 3 个了。</p>
                  <p>此外，我们还可以用 <code>while</code> 循环加 <code>fetchone</code> 方法来获取所有数据，而不是用 <code>fetchall</code> 全部一起获取出来。<code>fetchall</code> 会将结果以元组形式全部返回，如果数据量很大，那么占用的开销会非常高。因此，推荐使用如下方法来逐条取数据：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">sql = <span class="string">'SELECT * FROM students WHERE age &gt;= 20'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    print(<span class="string">'Count:'</span>, cursor.rowcount)</span><br><span class="line">    row = cursor.fetchone()</span><br><span class="line">    <span class="keyword">while</span> row:</span><br><span class="line">        print(<span class="string">'Row:'</span>, row)</span><br><span class="line">        row = cursor.fetchone()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Error'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样每循环一次，指针就会偏移一条数据，随用随取，简单高效。</p>
                  <h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2>
                  <p>本节我们了解了如何使用 PyMySQL 操作 MySQL 数据库以及一些 SQL 语句的构造方法，后面我们会在实战案例中应用这些操作来存储数据。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/MySQLTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/MySQLTest</a>。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-12 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-12T12:42:44+08:00">2022-02-12</time>
                </span>
                <span id="/202244.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 关系型数据库 MySQL 存储" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>7.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>7 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202245.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202245.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 当爬虫遇见 RabbitMQ 消息队列</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在数据爬取过程中，我们可能需要进行一些任务间通信机制的实现。比如说：</p>
                  <ul>
                    <li>一个进程负责构造爬取请求，另一个进程负责执行爬取请求。</li>
                    <li>某个爬取任务进程完成了，通知另外一个进程进行数据处理。</li>
                    <li>某个进程新建了一个爬取任务，就通知另外一个进程开始数据爬取。</li>
                  </ul>
                  <p>所以，为了降低这些进程的耦合度，就需要一个类似消息队列的中间件来存储和分发这些消息实现进程间的通信。</p>
                  <p>有了消息队列中间件之后，以上的两个任务就可以独立运行，通过消息队列来通信即可：</p>
                  <ul>
                    <li>
                      <p>一个进程将需要爬取的任务构造请求对象放入队列，另一个进程从队列中取出请求对象并执行爬取。</p>
                    </li>
                    <li>
                      <p>某个爬取任务进程完成了，完成时就向消息队列发一个消息，另一个进程监听到这类消息，那就开始数据处理。</p>
                    </li>
                    <li>某个进程新建了一个爬取任务，那就向消息队列发一个消息，另一个负责爬取的进程监听到这类消息，那就开始数据爬取。</li>
                  </ul>
                  <p>那这个消息队列用什么来实现呢？业界比较流行的有 RabbitMQ、RocketMQ、Kafka 等等，RabbitMQ 作为一个开源、可靠、灵活的消息队列中间件倍受青睐，本节我们就来了解下 RabbitMQ 的用法。</p>
                  <blockquote>
                    <p>注意：前面我们了解了一些数据存储库的用法，基本都用于持久化存储一些数据。但本节介绍的是一个消息队列组件，虽然其主要应用于数据消息通信，但由于其也有存储信息的能力，所以将其归类于本章进行介绍。</p>
                  </blockquote>
                  <h2 id="1-RabbitMQ-介绍"><a href="#1-RabbitMQ-介绍" class="headerlink" title="1. RabbitMQ 介绍"></a>1. RabbitMQ 介绍</h2>
                  <p>RabbitMQ 是使用 Erlang 语言开发的开源消息队列系统，基于 AMQP 协议实现。AMQP 的全称是 Advanced Message Queue，即高级消息队列协议，它的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。</p>
                  <p>RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括：</p>
                  <ul>
                    <li>可靠性（Reliability）：RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。</li>
                    <li>灵活的路由（Flexible Routing）：在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。</li>
                    <li>消息集群（Clustering）：多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。</li>
                    <li>高可用（Highly Available Queues）：队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。</li>
                    <li>多种协议支持（Multi-protocol）：RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。</li>
                    <li>多语言客户端（Many Clients）：RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。</li>
                    <li>管理界面（Management UI）：RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。</li>
                    <li>跟踪机制（Tracing）：如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。</li>
                    <li>插件机制（Plugin System）：RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。</li>
                  </ul>
                  <h2 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h2>
                  <p>在本节开始之前，请确保已经正确安装好了 RabbitMQ，安装方式可以参考：<a href="https://setup.scrape.center/rabbitmq，确保其可以在本地正常运行。" target="_blank" rel="noopener">https://setup.scrape.center/rabbitmq，确保其可以在本地正常运行。</a></p>
                  <p>除了安装 RabbitMQ 之外，我们还需要安装一个操作 RabbitMQ 的 Python 库，叫做 pika，使用 pip3 安装即可：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> pika</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更详细的安装说明可以参考：<a href="https://setup.scrape.center/pika。" target="_blank" rel="noopener">https://setup.scrape.center/pika。</a></p>
                  <p>以上二者都安装好之后，我们就可以开始本节的学习了。</p>
                  <h2 id="3-基本使用"><a href="#3-基本使用" class="headerlink" title="3. 基本使用"></a>3. 基本使用</h2>
                  <p>首先，RabbitMQ 提供的是队列的功能，我们要实现进程间通信，其本质上就是实现一个生产者-消费者模型，即一个进程作为生产者放入消息，另外一个进程作为消费者监听并处理消息，实现过程主要有 3 个关键点：</p>
                  <ul>
                    <li>声明队列：通过指定队列的一些参数，将队列创建出来。</li>
                    <li>生产内容：生产者根据队列的连接信息连接队列，向队列中放入对应的内容。</li>
                    <li>消费内容：消费者根据队列的连接信息连接队列，从队列中取出对应的内容。</li>
                  </ul>
                  <p>下面我们先来声明一个队列，相关代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape'</span></span><br><span class="line">connection = pika.BlockingConnection(pika.ConnectionParameters(<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line">channel.queue_declare(queue=QUEUE_NAME)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于 RabbitMQ 运行在本地，所以这里直接使用 <code>localhost</code> 即可连接 RabbitMQ 服务，得到一个连接对象 <code>connection</code>。接下来我们需要声明一个频道，即 <code>channel</code>，利用它我们可以操作队列内容的生产和消费，接着我们调用 <code>channel</code> 方法的 <code>queue_declare</code> 来声明一个队列，队列名称叫作 <code>scrape</code>。</p>
                  <p>接着我们尝试向队列中添加一个内容：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">channel.basic_publish(exchange=<span class="string">''</span>,</span><br><span class="line">                      routing_key=QUEUE_NAME,</span><br><span class="line">                      body=<span class="string">'Hello World!'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们调用了 <code>channel</code> 的 <code>basic_publish</code> 方法，向队列发布了一个内容，其中 <code>routing_key</code> 就是指队列的名称，<code>body</code> 就是真实的内容。</p>
                  <p>以上代码可以写入一个文件，取名为 producer.py，即生产者。</p>
                  <p>现在前两步——声明队列、生产内容其实已经完成了，接下来就是消费者从队列中获取内容了。</p>
                  <p>其实也很简单。消费者用同样的方式连接到队列，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape'</span></span><br><span class="line">connection = pika.BlockingConnection(pika.ConnectionParameters(<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line">channel.queue_declare(queue=QUEUE_NAME)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后从队列中获取数据，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span><span class="params">(ch, method, properties, body)</span>:</span></span><br><span class="line">    print(<span class="string">f"Get <span class="subst">&#123;body&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">channel.basic_consume(queue=<span class="string">'scrape'</span>,</span><br><span class="line">                      auto_ack=<span class="literal">True</span>,</span><br><span class="line">                      on_message_callback=callback)</span><br><span class="line">channel.start_consuming()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们调用了 <code>channel</code> 的 <code>basic_consume</code> 进行消费，同时指定了回调方法 <code>on_message_callback</code> 的名称为 <code>callback</code>，另外还指定了 <code>auto_ack</code> 为 <code>True</code>，这代表消费者获取信息之后会自动通知消息队列，表示这个消息已经被处理了，当前消息可以从队列中移除。</p>
                  <p>最后将以上代码保存为 consumer.py 并运行，它会监听 <code>scrape</code> 这个队列的变动，如果有消息进入，就会获取并进行消费，回调 <code>callback</code> 方法，打印输出结果。</p>
                  <p>然后运行一下 producer.py，运行之后会连接刚才的队列，同时在该队列中加入一条消息，内容为 <code>Hello World!</code>。</p>
                  <p>这时候我们再返回 <code>consumer</code>，可以发现输出如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="builtin-name">Get</span> Hello World!</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就说明消费者成功收到了生产者发送的消息，消息成功被生产者放入消息队列，然后被消费者捕获并输出。</p>
                  <p>另外我们再次运行 producer.py，每运行一次，生产者都会向其中放入一个消息，消费者便会收到该消息并输出。</p>
                  <p>以上便是最基本的 RabbitMQ 的用法。</p>
                  <h2 id="4-随用随取"><a href="#4-随用随取" class="headerlink" title="4. 随用随取"></a>4. 随用随取</h2>
                  <p>接下来我们来尝试实现一个简单的爬取队列，即一个进程负责构造爬取请求并将请求放入队列，另一个进程从队列中取出请求并执行爬取。</p>
                  <p>刚才我们仅仅是完成了基于 RabbitMQ 的最简单的生产者和消费者的通信，但是这种实现如果用在爬虫上是不太现实的，因为这里我们把消费者实现为了“订阅”的模式，也就是说，消费者会一直监听队列的变化，一旦队列中有了消息，消费者便要立马进行处理，消费者是无法主动控制它取用消息的时机的。</p>
                  <p>但实际上，假如我们要基于 RabbitMQ 实现一个爬虫的爬取队列的话，RabbitMQ 存的会是待执行/爬取的请求对象，生产者往里面放置请求对象，消费者获取到请求对象之后就执行这个请求，发起 HTTP 请求到服务器获取响应。但发起到获取响应的过程所消耗的时间是消费者无法控制的，这取决于服务器返回时间的长短。因此，消费者并不一定能够很快地将消息处理完，所以，消费者应该也有权利来控制取用的频率，这就是随用随取。</p>
                  <p>所以，这里我们可以稍微对前面的代码进行改写，生产者可以自行控制向消息队列中放入请求对象的频率，消费者也根据自己的处理能力控制自己从队列中取出请求对象的频率。如果生产者放置速度比消费者取用速度更快，那队列中就会缓存一些请求对象，反之队列则有时候会处于闲置状态。</p>
                  <p>但总的来说，消息队列充当了缓冲的作用，使得生产者和消费者可以按照自己的节奏来工作。</p>
                  <p>好，我们先实现下刚才所述的随用随取机制，队列中的消息暂且先用字符串来表示，后面我们可以再将其更换为请求对象。</p>
                  <p>这里我们可以将生产者实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape'</span></span><br><span class="line">connection = pika.BlockingConnection(</span><br><span class="line">    pika.ConnectionParameters(host=<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line">channel.queue_declare(queue=QUEUE_NAME)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    data = input()</span><br><span class="line">    channel.basic_publish(exchange=<span class="string">''</span>,</span><br><span class="line">                          routing_key=QUEUE_NAME,</span><br><span class="line">                          body=data)</span><br><span class="line">    print(<span class="string">f'Put <span class="subst">&#123;data&#125;</span>'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里生产者的数据我们还是使用 input 方法来获取，输入的内容就是字符串，输入之后该内容会直接被放置到队列中，然后打印输出到控制台。</p>
                  <p>先运行下生产者，然后回车输入几个内容：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">foo</span></span><br><span class="line"><span class="symbol">Put</span> foo</span><br><span class="line"><span class="keyword">bar</span></span><br><span class="line"><span class="keyword">Put </span><span class="keyword">bar</span></span><br><span class="line"><span class="keyword">baz</span></span><br><span class="line"><span class="keyword">Put </span><span class="keyword">baz</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们输入了 foo、bar、baz 三个内容，然后控制台也有对应的输出结果。</p>
                  <p>然后消费者实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape'</span></span><br><span class="line">connection = pika.BlockingConnection(</span><br><span class="line">    pika.ConnectionParameters(host=<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    input()</span><br><span class="line">    method_frame, header, body = channel.basic_get(</span><br><span class="line">        queue=QUEUE_NAME, auto_ack=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> body:</span><br><span class="line">        print(<span class="string">f'Get <span class="subst">&#123;body&#125;</span>'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>消费者也是一样的，我们这里也是可以通过 input 方法控制何时取用下一个，获取的方法就是 basic_get ，返回一个元组，其中 body 就是真正的数据。</p>
                  <p>运行消费者，回车几下，就可以看到每次回车都可以看到从消息队列中获取了一个新的数据：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="builtin-name">Get</span> b<span class="string">'foo'</span></span><br><span class="line"><span class="builtin-name">Get</span> b<span class="string">'bar'</span></span><br><span class="line"><span class="builtin-name">Get</span> b<span class="string">'baz'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就可以实现消费者的随用随取了。</p>
                  <h2 id="5-优先级队列"><a href="#5-优先级队列" class="headerlink" title="5. 优先级队列"></a>5. 优先级队列</h2>
                  <p>刚才我们仅仅是了解了最基本的队列的用法，RabbitMQ 还有一些高级功能。比如说，如果我们要想生产者发送的消息有优先级的区分，希望高优先级的队列被优先接收到，这个怎么实现呢？</p>
                  <p>其实很简单，我们只需要在声明队列的时候增加一个属性即可：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">MAX_PRIORITY = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">channel.queue_declare(queue=QUEUE_NAME, arguments=&#123;</span><br><span class="line">    <span class="string">'x-max-priority'</span>: MAX_PRIORITY</span><br><span class="line">&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里在声明队列的时候，我们增加了一个参数，叫做 x-max-priority，指定一个最大优先级，这样整个队列就支持优先级了。</p>
                  <p>这里改写下生产者，在发送消息的时候指定一个 properties 参数为 BasicProperties 对象，BasicProperties 对象里面通过 priority 参数指定了对应消息的优先级，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"></span><br><span class="line">MAX_PRIORITY = <span class="number">100</span></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape'</span></span><br><span class="line"></span><br><span class="line">connection = pika.BlockingConnection(</span><br><span class="line">    pika.ConnectionParameters(host=<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line">channel.queue_declare(queue=QUEUE_NAME, arguments=&#123;</span><br><span class="line">    <span class="string">'x-max-priority'</span>: MAX_PRIORITY</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    data, priority = input().split()</span><br><span class="line">    channel.basic_publish(exchange=<span class="string">''</span>,</span><br><span class="line">                          routing_key=QUEUE_NAME,</span><br><span class="line">                          properties=pika.BasicProperties(</span><br><span class="line">                              priority=int(priority),</span><br><span class="line">                          ),</span><br><span class="line">                          body=data)</span><br><span class="line">    print(<span class="string">f'Put <span class="subst">&#123;data&#125;</span>'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里优先级我们也可以手动输入，输入的内容我们需要分为两部分，用空格隔开，运行结果如下：</p>
                  <figure class="highlight properties">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">foo</span> <span class="string">40</span></span><br><span class="line"><span class="attr">Put</span> <span class="string">foo</span></span><br><span class="line"><span class="attr">bar</span> <span class="string">20</span></span><br><span class="line"><span class="attr">Put</span> <span class="string">bar</span></span><br><span class="line"><span class="attr">baz</span> <span class="string">50</span></span><br><span class="line"><span class="attr">Put</span> <span class="string">baz</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们输入了三次内容，比如第一次输入的就是 <code>foo 40</code>，代表 foo 这个消息的优先级是 40，然后输入 bar 这个消息，优先级是 20，最后输入 baz 这个消息，优先级是 50。</p>
                  <p>然后重新运行消费者，按几次回车，可以看到如下输出结果：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="builtin-name">Get</span> b<span class="string">'baz'</span></span><br><span class="line"></span><br><span class="line"><span class="builtin-name">Get</span> b<span class="string">'foo'</span></span><br><span class="line"></span><br><span class="line"><span class="builtin-name">Get</span> b<span class="string">'bar'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们可以看到结果就按照优先级取出来了，baz 这个优先级是最高的，所以就被最先取出来。bar 这个优先级是最低的，所以被最后取出来。</p>
                  <h2 id="6-队列持久化"><a href="#6-队列持久化" class="headerlink" title="6. 队列持久化"></a>6. 队列持久化</h2>
                  <p>除了设置优先级，我们还可以队列的持久化存储，如果不设置持久化存储，RabbitMQ 重启之后数据就没有了。</p>
                  <p>如果要开启持久化存储，可以在声明队列时指定 durable 为 True，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">channel.queue_declare(queue=QUEUE_NAME, arguments=&#123;</span><br><span class="line">    <span class="string">'x-max-priority'</span>: MAX_PRIORITY</span><br><span class="line">&#125;, durable=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>同时在添加消息的时候需要指定 BasicProperties 对象的 delivery_mode 为 2，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">properties=pika.BasicProperties(priority=int(priority), delivery_mode=<span class="number">2</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>所以，这时候生产者的写法就改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"></span><br><span class="line">MAX_PRIORITY = <span class="number">100</span></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape'</span></span><br><span class="line"></span><br><span class="line">connection = pika.BlockingConnection(</span><br><span class="line">    pika.ConnectionParameters(host=<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line">channel.queue_declare(queue=QUEUE_NAME, arguments=&#123;</span><br><span class="line">    <span class="string">'x-max-priority'</span>: MAX_PRIORITY</span><br><span class="line">&#125;, durable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    data, priority = input().split()</span><br><span class="line">    channel.basic_publish(exchange=<span class="string">''</span>,</span><br><span class="line">                          routing_key=QUEUE_NAME,</span><br><span class="line">                          properties=pika.BasicProperties(</span><br><span class="line">                              priority=int(priority),</span><br><span class="line">                              delivery_mode=<span class="number">2</span>,</span><br><span class="line">                          ),</span><br><span class="line">                          body=data)</span><br><span class="line">    print(<span class="string">f'Put <span class="subst">&#123;data&#125;</span>'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就可以实现队列的持久化存储了。</p>
                  <h2 id="7-实战"><a href="#7-实战" class="headerlink" title="7. 实战"></a>7. 实战</h2>
                  <p>最后，我们将消息改写成前面所描述的请求对象，这里我们借助于 requests 库中的 Request 类来表示一个请求对象。</p>
                  <p>构造请求对象时，我们传入请求方法、请求 URL 即可，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">request = requests.Request(<span class="string">'GET'</span>, url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就构造了一个 GET 请求，然后可以通过 pickle 进行序列化然后发送到 RabbitMQ 中。</p>
                  <p>生产者实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">MAX_PRIORITY = <span class="number">100</span></span><br><span class="line">TOTAL = <span class="number">100</span></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape_queue'</span></span><br><span class="line"></span><br><span class="line">connection = pika.BlockingConnection(</span><br><span class="line">    pika.ConnectionParameters(host=<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line">channel.queue_declare(queue=QUEUE_NAME, durable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, TOTAL + <span class="number">1</span>):</span><br><span class="line">    url = <span class="string">f'https://ssr1.scrape.center/detail/<span class="subst">&#123;i&#125;</span>'</span></span><br><span class="line">    request = requests.Request(<span class="string">'GET'</span>, url)</span><br><span class="line">    channel.basic_publish(exchange=<span class="string">''</span>,</span><br><span class="line">                          routing_key=QUEUE_NAME,</span><br><span class="line">                          properties=pika.BasicProperties(</span><br><span class="line">                              delivery_mode=<span class="number">2</span>,</span><br><span class="line">                          ),</span><br><span class="line">                          body=pickle.dumps(request))</span><br><span class="line">    print(<span class="string">f'Put request of <span class="subst">&#123;url&#125;</span>'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们运行下生产者，就构造了 100 个请求对象并发送到 RabbitMQ 中了。</p>
                  <p>对于消费者来说，可以设置一个循环，一直不断地从队列中取出这些请求对象，取出一个就执行一次爬取，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> pika</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">MAX_PRIORITY = <span class="number">100</span></span><br><span class="line">QUEUE_NAME = <span class="string">'scrape_queue'</span></span><br><span class="line"></span><br><span class="line">connection = pika.BlockingConnection(</span><br><span class="line">    pika.ConnectionParameters(host=<span class="string">'localhost'</span>))</span><br><span class="line">channel = connection.channel()</span><br><span class="line">session = requests.Session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape</span><span class="params">(request)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = session.send(request.prepare())</span><br><span class="line">        print(<span class="string">f'success scraped <span class="subst">&#123;response.url&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException:</span><br><span class="line">        print(<span class="string">f'error occurred when scraping <span class="subst">&#123;request.url&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    method_frame, header, body = channel.basic_get(</span><br><span class="line">        queue=QUEUE_NAME, auto_ack=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> body:</span><br><span class="line">        request = pickle.loads(body)</span><br><span class="line">        print(<span class="string">f'Get <span class="subst">&#123;request&#125;</span>'</span>)</span><br><span class="line">        scrape(request)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里消费者调用 basic_get 方法获取了消息，然后通过 pickle 反序列化还原成一个请求对象，然后使用 session 的 send 方法执行了该请求，进行了数据爬取，爬取成功就打印爬取成功的消息。</p>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Get &lt;Request [GET]&gt;</span><br><span class="line">success scraped https://ssr1.scrape.center/detail/<span class="number">1</span></span><br><span class="line">Get &lt;Request [GET]&gt;</span><br><span class="line">success scraped https://ssr1.scrape.center/detail/<span class="number">2</span></span><br><span class="line">...</span><br><span class="line">Get  &lt;Request [GET]&gt;</span><br><span class="line">success scraped https://ssr1.scrape.center/detail/<span class="number">100</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，消费者依次取出了爬取对象，然后成功完成了一个个爬取任务。</p>
                  <h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2>
                  <p>本节介绍了 RabbitMQ 的基本使用方法，有了它，爬虫任务间的消息通信就变得非常简单了，另外后文我们还会基于 RabbitMQ 实现分布式的爬取实战，所以本节的内容需要好好掌握。</p>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/RabbitMQTest。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/RabbitMQTest。</a></p>
                  <p>本节部分内容参考来源：</p>
                  <ul>
                    <li>文档 - RabbitMQ：<a href="https://www.rabbitmq.com/documentation.html" target="_blank" rel="noopener">https://www.rabbitmq.com/documentation.html</a></li>
                    <li>文档 - Pika：<a href="https://pika.readthedocs.io/" target="_blank" rel="noopener">https://pika.readthedocs.io/</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-12 12:42:44" itemprop="dateCreated datePublished" datetime="2022-02-12T12:42:44+08:00">2022-02-12</time>
                </span>
                <span id="/202245.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 当爬虫遇见 RabbitMQ 消息队列" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>9.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>8 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202214.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202214.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - Session 和 Cookie</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在浏览网站的过程中，我们经常会遇到需要登录的情况，有些页面只有登录之后才可以访问，而且登录之后可以连续访问很多次网站，但是有时候过一段时间就需要重新登录。还有一些网站，在打开浏览器时就自动登录了，而且很长时间都不会失效，这种情况又是为什么？其实这里面涉及 Session 和 Cookie 的相关知识，本节就来揭开它们的神秘面纱。</p>
                  <h2 id="1-静态网页和动态网页"><a href="#1-静态网页和动态网页" class="headerlink" title="1. 静态网页和动态网页"></a>1. 静态网页和动态网页</h2>
                  <p>在开始之前，我们需要先了解一下静态网页和动态网页的概念。这里还是前面的示例代码，内容如下：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>Hello World<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"text"</span>&gt;</span>Hello, this is a paragraph.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这是最基本的 HTML 代码，我们将其保存为一个 test.html 文件，然后把它放在某台具有固定公网 IP 的主机上，主机上装上 Apache 或 Nginx 等服务器，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面，这就搭建了一个最简单的网站。</p>
                  <p>这种网页的内容是 HTML 代码编写的，文字、图片等内容均通过写好的 HTML 代码来指定，这种页面叫作静态网页。它加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据 URL 灵活多变地显示内容等。例如，我们想要给这个网页的 URL 传入一个 <code>name</code> 参数，让其在网页中显示出来，是无法做到的。</p>
                  <p>因此，动态网页应运而生，它可以动态解析 URL 中参数的变化，关联数据库并动态呈现不同的页面内容，非常灵活多变。我们现在遇到的大多数网站都是动态网站，它们不再是一个简单的 HTML，而是可能由 JSP、PHP、Python 等语言编写的，其功能比静态网页强大、丰富太多了。此外，动态网站还可以实现用户登录和注册的功能。</p>
                  <p>再回到开头提到的问题，很多页面是需要登录之后才可以查看的。按照一般的逻辑来说，输入用户名和密码登录之后，肯定是拿到了一种类似凭证的东西，有了它，我们才能保持登录状态，访问登录之后才能看到的页面。</p>
                  <p>那么，这种神秘的凭证到底是什么呢？其实它就是 Session 和 Cookie 共同产生的结果，下面我们来一探究竟。</p>
                  <h2 id="2-无状态-HTTP"><a href="#2-无状态-HTTP" class="headerlink" title="2. 无状态 HTTP"></a>2. 无状态 HTTP</h2>
                  <p>在了解 Session 和 Cookie 之前，我们还需要了解 HTTP 的一个特点，叫作无状态。</p>
                  <p>HTTP 的无状态是指 HTTP 协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。当我们向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。这意味着如果后续需要处理前面的信息，则必须重传，这导致需要额外传递一些前面的重复请求，才能获取后续响应，然而这种效果显然不是我们想要的。为了保持前后状态，我们肯定不能将前面的请求全部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。</p>
                  <p>这时两个用于保持 HTTP 连接状态的技术就出现了，它们分别是 Session 和 Cookie。Session 在服务端，也就是网站的服务器，用来保存用户的 Session 信息；Cookie 在客户端，也可以理解为浏览器端，有了 Cookie，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookie 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的响应。</p>
                  <p>我们可以理解为 Cookie 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookie 发送请求而不必重新输入用户名、密码等信息重新登录了。</p>
                  <p>因此，在爬虫中，有时候处理需要登录才能访问的页面时，我们一般会直接将登录成功后获取的 Cookie 放在请求头里面直接请求，而不必重新模拟登录。</p>
                  <p>好了，了解 Session 和 Cookie 的概念之后，我们在来详细剖析它们的原理。</p>
                  <h2 id="3-Session"><a href="#3-Session" class="headerlink" title="3. Session"></a>3. Session</h2>
                  <p>Session，中文称为会话，其本来的含义是指有始有终的一系列动作 / 消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个 Session。</p>
                  <p>而在 Web 中，Session 对象用来存储特定用户 Session 所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户 Session 中一直存在下去。当用户请求来自应用程序的 Web 页时，如果该用户还没有 Session，则 Web 服务器将自动创建一个 Session 对象。当 Session 过期或被放弃后，服务器将终止该 Session。</p>
                  <h2 id="4-Cookie"><a href="#4-Cookie" class="headerlink" title="4. Cookie"></a>4. Cookie</h2>
                  <p>Cookie，也常用其复数形式 Cookies，Cookie 指某些网站为了辨别用户身份、进行 Session 跟踪而存储在用户本地终端上的数据。</p>
                  <h4 id="Session-维持"><a href="#Session-维持" class="headerlink" title="Session 维持"></a>Session 维持</h4>
                  <p>那么，我们怎样利用 Cookies 保持状态呢？当客户端第一次请求服务器时，服务器会返回一个响应头中带有 Set-Cookie 字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把 Cookies 保存起来。当浏览器下一次再请求该网站时，浏览器会把此 Cookies 放到请求头一起提交给服务器，Cookies 携带了 Session ID 信息，服务器检查该 Cookies 即可找到对应的 Session 是什么，然后再判断 Session 来辨认用户状态。</p>
                  <p>在成功登录某个网站时，服务器会告诉客户端设置哪些 Cookies 信息。在后续访问页面时，客户端会把 Cookies 发送给服务器，服务器再找到对应的 Session 加以判断。如果 Session 中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。</p>
                  <p>反之，如果传给服务器的 Cookies 是无效的，或者 Session 已经过期了，我们将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。</p>
                  <p>所以，Cookies 和 Session 需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录 Session 控制。</p>
                  <h4 id="属性结构"><a href="#属性结构" class="headerlink" title="属性结构"></a>属性结构</h4>
                  <p>接下来，我们来看看 Cookies 都有哪些内容。这里以知乎为例，在浏览器开发者工具中打开 Application 选项卡，然后在左侧会有一个 Storage 部分，最后一项即为 Cookies，将其点开，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/5yb6e.jpg" alt=""></p>
                  <p>Cookies 列表</p>
                  <p>可以看到，这里有很多条目，其中每个条目可以称为 Cookie。它有如下几个属性。</p>
                  <ul>
                    <li>Name，即该 Cookie 的名称。Cookie 一旦创建，名称便不可更改。</li>
                    <li>Value，即该 Cookie 的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。</li>
                    <li>Domain，即可以访问该 Cookie 的域名。例如如果设置为 .zhihu.com，则所有以 zhihu.com 结尾的域名都可以访问该 Cookie。</li>
                    <li>Path，即该 Cookie 的使用路径。如果设置为 /path/，则只有路径为 /path/ 的页面可以访问该 Cookie。如果设置为 /，则本域名下的所有页面都可以访问该 Cookie。</li>
                    <li>Max-Age，即该 Cookie 失效的时间，单位为秒，常和 Expires 一起使用，通过它可以计算出其有效时间。Max-Age 如果为正数，则该 Cookie 在 Max-Age 秒之后失效。如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。</li>
                    <li>Size 字段，即此 Cookie 的大小。</li>
                    <li>HTTP 字段，即 Cookie 的 <code>httponly</code> 属性。若此属性为 <code>true</code>，则只有在 HTTP Headers 中会带有此 Cookie 的信息，而不能通过 <code>document.cookie</code> 来访问此 Cookie。</li>
                    <li>Secure，即该 Cookie 是否仅被使用安全协议传输。安全协议有 HTTPS 和 SSL 等，在网络上传输数据之前先将数据加密。其默认值为 <code>false</code>。</li>
                  </ul>
                  <h4 id="会话-Cookie-和持久-Cookie"><a href="#会话-Cookie-和持久-Cookie" class="headerlink" title="会话 Cookie 和持久 Cookie"></a>会话 Cookie 和持久 Cookie</h4>
                  <p>从表面意思来说，会话 Cookie 就是把 Cookie 放在浏览器内存里，浏览器在关闭之后该 Cookie 即失效；持久 Cookie 则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。</p>
                  <p>其实严格来说，没有会话 Cookie 和持久 Cookie 之分，只是由 Cookie 的 Max-Age 或 Expires 字段决定了过期的时间。</p>
                  <p>因此，一些持久化登录的网站其实就是把 Cookie 的有效时间和 Session 有效期设置得比较长，下次我们再访问页面时仍然携带之前的 Cookie，就可以直接保持登录状态。</p>
                  <h2 id="5-常见误区"><a href="#5-常见误区" class="headerlink" title="5. 常见误区"></a>5. 常见误区</h2>
                  <p>在谈论 Session 机制的时候，常常听到这样一种误解 ——“只要关闭浏览器，Session 就消失了”。可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。对 Session 来说，也一样，除非程序通知服务器删除一个 Session，否则服务器会一直保留。比如，程序一般都是在我们做注销操作时才去删除 Session。</p>
                  <p>但是当我们关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。之所以会有这种错觉，是因为大部分网站都使用会话 Cookie 来保存 Session ID 信息，而关闭浏览器后 Cookies 就消失了，再次连接服务器时，也就无法找到原来的 Session 了。如果服务器设置的 Cookies 保存到硬盘上，或者使用某种手段改写浏览器发出的 HTTP 请求头，把原来的 Cookies 发送给服务器，则再次打开浏览器，仍然能够找到原来的 Session ID，依旧还是可以保持登录状态的。</p>
                  <p>而且恰恰是由于关闭浏览器不会导致 Session 被删除，这就需要服务器为 Session 设置一个失效时间，当距离客户端上一次使用 Session 的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把 Session 删除以节省存储空间。</p>
                  <h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2>
                  <p>本节介绍了 Session 和 Cookie 的基本概念，这对后文进行网络爬虫的开发有很大的帮助，需要好好掌握。</p>
                  <p>由于涉及一些专业名词知识，本节部分内容的参考来源如下：</p>
                  <ul>
                    <li>文档 - Session - 百度百科：<a href="https://baike.baidu.com/item/session/479100" target="_blank" rel="noopener">https://baike.baidu.com/item/session/479100</a></li>
                    <li>文档 - Cookie - 百度百科：<a href="https://baike.baidu.com/item/cookie/1119" target="_blank" rel="noopener">https://baike.baidu.com/item/cookie/1119</a></li>
                    <li>文档 - HTTP Cookie 维基百科：<a href="https://en.wikipedia.org/wiki/HTTP_cookie" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/HTTP_cookie</a></li>
                    <li>博客 - Session 和几种状态保持方案理解：<a href="http://www.mamicode.com/info-detail-46545.html" target="_blank" rel="noopener">http://www.mamicode.com/info-detail-46545.html</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-12 09:38:31" itemprop="dateCreated datePublished" datetime="2022-02-12T09:38:31+08:00">2022-02-12</time>
                </span>
                <span id="/202214.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - Session 和 Cookie" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.8k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202211.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202211.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - 爬虫是什么？</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>简而言之，爬虫可以帮助我们快速把网站上的信息快速提取并保存下来。</p>
                  <p>我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，就能把网页上的信息提取出来。我们可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。</p>
                  <h2 id="1-爬虫有什么用？"><a href="#1-爬虫有什么用？" class="headerlink" title="1. 爬虫有什么用？"></a>1. 爬虫有什么用？</h2>
                  <p>通过上面的话，你可能已经初步知道了爬虫是做了什么事情，但一般要学一个东西，我们得知道学来干什么用吧？</p>
                  <p>其实，爬虫的用处可大了去了。</p>
                  <ul>
                    <li>比如，我们想要研究最近各大网站头条都有什么热点，那我们就可以用爬虫把这些网站的热门新闻用爬虫爬下来，这样我们就可以分析其中的标题、内容等知道热点关键词了。</li>
                    <li>比如，我们想要对一些天气、金融、体育、公司等各种信息进行整理和分析，但这些内容都分布在各种不同的网站上，那我们就可以用爬虫把这些网站上的数据爬取下来，整理成我们想要的数据保存下来，就可以对其进行分析了。</li>
                    <li>比如，我们在网上看到了很多美图，比如风景、美食、美女，或者一些资料、文章，想保存到电脑上，但一次次右键保存、复制粘贴显然非常费时费力，那我们就可以利用爬虫将这些图片或资源快速爬取下来，极大地节省时间和精力。</li>
                  </ul>
                  <p>另外还有很多其他的，比如黄牛抢票、自助抢课、网站排名等等各种技术也都和爬虫分不开，爬虫的用处可谓是非常大，可以说人人都应该会点爬虫。</p>
                  <p>另外学爬虫还可以帮助我们顺便学好 Python。学爬虫，个人首推的就是 Python 语言，如果你对 Python 还不太熟，没关系，爬虫就非常适合作为入门 Python 的方向来学习，一边学爬虫，一边学 Python，最后一举两得。</p>
                  <p>不仅如此，爬虫技术和其他领域的几乎都有交集，比如前后端 Web 开发、数据库、数据分析、人工智能、运维、安全等等领域都和爬虫有所沾边，所以学好了爬虫，就相当于为其他的领域也铺好了一个台阶，以后想进军其他领域都可以更轻松地衔接。Python 爬虫可谓是学习计算机的一个很好的入门方向之一。</p>
                  <h2 id="2-爬虫的流程"><a href="#2-爬虫的流程" class="headerlink" title="2. 爬虫的流程"></a>2. 爬虫的流程</h2>
                  <p>简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，下面概要介绍一下。</p>
                  <h3 id="1-获取网页"><a href="#1-获取网页" class="headerlink" title="(1) 获取网页"></a>(1) 获取网页</h3>
                  <p>爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。</p>
                  <p>我们用浏览器浏览网页时，其实浏览器就帮我们模拟了这个过程，浏览器向服务器发送了一个个请求，返回的响应体便是网页源代码，然后浏览器将其解析并呈现出来。所以，我们要做的爬虫其实就和浏览器类似，将网页源代码获取下来之后将内容解析出来就好了，只不过我们用的不是浏览器，而是 Python。</p>
                  <p>刚才说，最关键的部分就是构造一个请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样用 Python 实现呢？</p>
                  <p>Python 提供了许多库来帮助我们实现这个操作，如 urllib、requests 等。我们可以用这些库来实现 HTTP 请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的 <code>body</code> 部分即可，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。</p>
                  <h3 id="2-提取信息"><a href="#2-提取信息" class="headerlink" title="(2) 提取信息"></a>(2) 提取信息</h3>
                  <p>获取网页的源代码后，接下来就是分析网页的源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。</p>
                  <p>另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 Beautiful Soup、pyquery、lxml 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。</p>
                  <p>提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理、清晰，以便我们后续处理和分析数据。</p>
                  <h3 id="3-保存数据"><a href="#3-保存数据" class="headerlink" title="(3) 保存数据"></a>(3) 保存数据</h3>
                  <p>提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等，还可保存至远程服务器，如借助 SFTP 进行操作等。</p>
                  <h3 id="4-自动化程序"><a href="#4-自动化程序" class="headerlink" title="(4) 自动化程序"></a>(4) 自动化程序</h3>
                  <p>说到自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。</p>
                  <h2 id="3-能爬怎样的数据？"><a href="#3-能爬怎样的数据？" class="headerlink" title="3. 能爬怎样的数据？"></a>3. 能爬怎样的数据？</h2>
                  <p>在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML 代码，而最常抓取的便是 HTML 源代码。</p>
                  <p>另外，可能有些网页返回的不是 HTML 代码，而是一个 JSON 字符串（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。</p>
                  <p>此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。</p>
                  <p>另外，还可以看到各种扩展名的文件，如 CSS、JavaScript 和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。</p>
                  <p>上述内容其实都对应各自的 URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取。</p>
                  <h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2>
                  <p>本节结束，我们已经对爬虫有了基本的了解，接下来让我们一起接着迈入爬虫学习的世界吧！</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-12 08:38:31" itemprop="dateCreated datePublished" datetime="2022-02-12T08:38:31+08:00">2022-02-12</time>
                </span>
                <span id="/202211.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - 爬虫是什么？" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>2.5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202213.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202213.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - Web网页基础</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>用浏览器访问网站时，页面各不相同，你有没有想过它为何会呈现这个样子呢？本节中，我们就来了解一下网页的组成、结构和节点等内容。</p>
                  <h2 id="1-网页的组成"><a href="#1-网页的组成" class="headerlink" title="1. 网页的组成"></a>1. 网页的组成</h2>
                  <p>网页可以分为三大部分 —— HTML、CSS 和 JavaScript。如果把网页比作一个人的话，HTML 相当于骨架，JavaScript 相当于肌肉，CSS 相当于皮肤，三者结合起来才能形成一个完善的网页。下面我们分别来介绍一下这三部分的功能。</p>
                  <h3 id="（1）HTML"><a href="#（1）HTML" class="headerlink" title="（1）HTML"></a>（1）HTML</h3>
                  <p>HTML，其英文叫做 HyperText Markup Language，中文翻译叫做超文本标记语言，但我们通常不会用中文翻译来称呼它，一般就叫 HTML。</p>
                  <p>HTML 是用来描述网页的一种语言，网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是 HTML。不同类型的元素通过不同类型的标签来表示，如图片用 <code>img</code> 标签表示，视频用 <code>video</code> 标签表示，段落用 <code>p</code> 标签表示，它们之间的布局又常通过布局标签 <code>div</code> 嵌套组合而成，各种标签通过不同的排列和嵌套才形成了网页的框架。</p>
                  <p>那 HTML 长什么样子呢？我们可以随意打开一个网站，比如淘宝 <a href="https://www.taobao.com，然后右键菜单点击“检查元素”或者按" target="_blank" rel="noopener">https://www.taobao.com，然后右键菜单点击“检查元素”或者按</a> F12 快捷键，即可打开浏览器开发者工具，切换到 Elements 面板，这时候就可以看到这里呈现的就是淘宝网对应的 HTML，它包含了一系列标签，浏览器解析这些标签后，便会在网页中渲染成一个个的节点，这便形成了我们平常看到的网页。比如这里可以看到一个输入框就对应一个 input 标签，可以用于输入文字。</p>
                  <p><img src="https://cdn.cuiqingcai.com/jq4ak.png" alt=""></p>
                  <p>不同的标签对应着不同的功能，这些标签定义的节点相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。</p>
                  <h3 id="（2）CSS"><a href="#（2）CSS" class="headerlink" title="（2）CSS"></a>（2）CSS</h3>
                  <p>HTML 定义了网页的结构，但是只有 HTML 页面的布局并不美观，可能只是简单的节点元素的排列。为了让网页看起来更好看一些，这里借助了 CSS。</p>
                  <p>CSS，全称叫作 Cascading Style Sheets，即层叠样式表。“层叠” 是指当在 HTML 中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式” 指网页中文字大小、颜色、元素间距、排列等格式。CSS 是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。</p>
                  <p>在上图中，Styles 面板呈现的就是一系列 CSS 样式，比如摘抄一段 CSS，内容如下：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-id">#head_wrapper</span><span class="selector-class">.s-ps-islite</span> <span class="selector-class">.s-p-top</span> &#123;</span><br><span class="line">  <span class="attribute">position</span>: absolute;</span><br><span class="line">  <span class="attribute">bottom</span>: <span class="number">40px</span>;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">181px</span>;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是一个 CSS 样式。大括号前面是一个 CSS 选择器。此选择器的意思是首先选中 <code>id</code> 为 <code>head_wrapper</code> 且 <code>class</code> 为 <code>s-ps-islite</code> 的节点，然后再选中其内部的 <code>class</code> 为 <code>s-p-top</code> 的节点。大括号内部写的就是一条条样式规则，例如 <code>position</code> 指定了这个节点的布局方式为绝对布局，<code>bottom</code> 指定节点的下边距为 40 像素，<code>width</code> 指定了宽度为 100%，表示占满父节点，height 则指定了节点的高度。也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上 CSS 选择器，这就代表这个样式对 CSS 选择器选中的节点生效，节点就会根据此样式来展示了。</p>
                  <p>在网页中，一般会统一定义整个网页的样式规则，并写入 CSS 文件中（其后缀为 css）。在 HTML 中，只需要用 <code>link</code> 标签即可引入写好的 CSS 文件，这样整个页面就会变得美观、优雅。</p>
                  <h3 id="（3）JavaScript"><a href="#（3）JavaScript" class="headerlink" title="（3）JavaScript"></a>（3）JavaScript</h3>
                  <p>JavaScript，简称 JS，是一种脚本语言。HTML 和 CSS 配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是 JavaScript 的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。</p>
                  <p>JavaScript 通常也是以单独的文件形式加载的，后缀为 js，在 HTML 中通过 <code>script</code> 标签即可引入，例如：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;script src&#x3D;&quot;jquery-2.1.0.js&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>综上所述，HTML 定义了网页的内容和结构，CSS 描述了网页的样式，JavaScript 定义了网页的行为。</p>
                  <h2 id="2-网页的结构"><a href="#2-网页的结构" class="headerlink" title="2. 网页的结构"></a>2. 网页的结构</h2>
                  <p>我们首先用例子来感受一下 HTML 的基本结构。新建一个文本文件，名称叫做 test.html，内容如下：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>Hello World<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"text"</span>&gt;</span>Hello, this is a paragraph.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是一个最简单的 HTML 实例。开头用 <code>DOCTYPE</code> 定义了文档类型，其次最外层是 <code>html</code> 标签，最后还有对应的结束标签来表示闭合，其内部是 <code>head</code> 标签和 <code>body</code> 标签，分别代表网页头和网页体，它们也需要结束标签。<code>head</code> 标签内定义了一些页面的配置和引用，如：</p>
                  <figure class="highlight html">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span> /&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>它指定了网页的编码为 UTF-8。</p>
                  <p><code>title</code> 标签则定义了网页的标题，会显示在网页的选项卡中，不会显示在正文中。<code>body</code> 标签内则是在网页正文中显示的内容。<code>div</code> 标签定义了网页中的区块，它的 <code>id</code> 是 <code>container</code>，这是一个非常常用的属性，且 <code>id</code> 的内容在网页中是唯一的，我们可以通过它来获取这个区块。然后在此区块内又有一个 <code>div</code> 标签，它的 <code>class</code> 为 <code>wrapper</code>，这也是一个非常常用的属性，经常与 CSS 配合使用来设定样式。然后此区块内部又有一个 <code>h2</code> 标签，这代表一个二级标题。另外，还有一个 <code>p</code> 标签，这代表一个段落。在这两者中直接写入相应的内容即可在网页中呈现出来，它们也有各自的 <code>class</code> 属性。</p>
                  <p>将代码保存后，双击该文件在浏览器中打开，可以看到如图所示的内容。</p>
                  <p><img src="https://cdn.cuiqingcai.com/wi3k2.png" alt="运行结果"></p>
                  <p>可以看到，选项卡上显示了 This is a Demo 字样，这是我们在 <code>head</code> 中的 <code>title</code> 里定义的文字。而网页正文是 <code>body</code> 标签内部定义的各个元素生成的，可以看到这里显示了二级标题和段落。</p>
                  <p>这个实例便是网页的一般结构。一个网页的标准形式是 <code>html</code> 标签内嵌套 <code>head</code> 和 <code>body</code> 标签，<code>head</code> 内定义网页的配置和引用，<code>body</code> 内定义网页的正文。</p>
                  <h2 id="3-节点树及节点间的关系"><a href="#3-节点树及节点间的关系" class="headerlink" title="3 节点树及节点间的关系"></a>3 节点树及节点间的关系</h2>
                  <p>在 HTML 中，所有标签定义的内容都是节点，它们构成了一个 HTML 节点树，也称之为 HTML DOM 树。</p>
                  <p>我们先看下什么是 DOM。DOM 是 W3C（万维网联盟）的标准，其英文全称 Document Object Model，即文档对象模型。它定义了访问 HTML 和 XML 文档的标准。根据 W3C 的 HTML DOM 标准，HTML 文档中的所有内容都是节点。</p>
                  <ul>
                    <li>整个网站文档是一个文档节点。</li>
                    <li>每个 html 标签对应一个根元素节点，即上例中的 html 标签，这属于一个跟元素节点。</li>
                    <li>节点内的文本是文本节点，比如 a 节点代表一个超链接，它内部的文本也被认为是一个文本节点。</li>
                    <li>每个节点的属性是属性节点，比如 a 节点有一个 href 属性，它就是一个属性节点。</li>
                    <li>注释是注释节点，在 HTML 中有特殊的语法会被解析为注释，但其也会对应一个节点。</li>
                  </ul>
                  <p>所以，HTML DOM 将 HTML 文档视作树结构，这种结构被称为节点树，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/5fe0q.jpg" alt="节点树"></p>
                  <p>通过 HTML DOM，树中的所有节点均可通过 JavaScript 访问，所有 HTML 节点元素均可被修改，也可以被创建或删除。</p>
                  <p>节点树中的节点彼此拥有层级关系。我们常用父（parent）、子（child）和兄弟（sibling）等术语描述这些关系。父节点拥有子节点，同级的子节点被称为兄弟节点。</p>
                  <p>在节点树中，顶端节点称为根（root）。除了根节点之外，每个节点都有父节点，同时可拥有任意数量的子节点或兄弟节点。图展示了节点树以及节点之间的关系。</p>
                  <p><img src="https://cdn.cuiqingcai.com/muz4m.jpg" alt="节点树及节点间的关系"></p>
                  <h2 id="4-选择器"><a href="#4-选择器" class="headerlink" title="4. 选择器"></a>4. 选择器</h2>
                  <p>我们知道网页由一个个节点组成，CSS 选择器会根据不同的节点设置不同的样式规则，那么怎样来定位节点呢？</p>
                  <p>在 CSS 中，我们使用 CSS 选择器来定位节点。例如，上例中 <code>div</code> 节点的 <code>id</code> 为 <code>container</code>，那么就可以表示为 <code>#container</code>，其中 <code>#</code> 开头代表选择 <code>id</code>，其后紧跟 <code>id</code> 的名称。另外，如果我们想选择 <code>class</code> 为 <code>wrapper</code> 的节点，便可以使用<code>.wrapper</code>，这里以点（.）开头代表选择 <code>class</code>，其后紧跟 <code>class</code> 的名称。另外，还有一种选择方式，那就是根据标签名筛选，例如想选择二级标题，直接用 <code>h2</code> 即可。这是最常用的 3 种表示，分别是根据 <code>id</code>、<code>class</code>、标签名筛选，请牢记它们的写法。</p>
                  <p>另外，CSS 选择器还支持嵌套选择，各个选择器之间加上空格分隔开便可以代表嵌套关系，如 <code>#container .wrapper p</code> 则代表先选择 <code>id</code> 为 <code>container</code> 的节点，然后选中其内部的 <code>class</code> 为 <code>wrapper</code> 的节点，然后再进一步选中其内部的 <code>p</code> 节点。另外，如果不加空格，则代表并列关系，如 <code>div#container .wrapper p.text</code> 代表先选择 <code>id</code> 为 <code>container</code> 的 <code>div</code> 节点，然后选中其内部的 <code>class</code> 为 <code>wrapper</code> 的节点，再进一步选中其内部的 <code>class</code> 为 <code>text</code> 的 <code>p</code> 节点。这就是 CSS 选择器，其筛选功能还是非常强大的。</p>
                  <p>我们可以在浏览器中测试 CSS 选择器的效果，依然还是打开浏览器的开发者工具，然后按快捷键 Ctrl + F（如果你用的是 Mac，则是 Command + F），这时候在左下角便会出现一个搜索框，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/x3taf.png" alt=""></p>
                  <p>这时候我们输入 <code>.title</code> 就是选中了 class 为 title 的节点，这时候该节点就会被选中并在网页中高亮显示，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/ww6nn.png" alt=""></p>
                  <p>输入 <code>div#container .wrapper p.text</code> 就逐层选中了 id 为 container 中 class 为 wrapper 节点中的 p 节点，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/3g8jg.png" alt=""></p>
                  <p>另外，CSS 选择器还有一些其他语法规则，具体如下表所示。</p>
                  <p>CSS 选择器的其他语法规则</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">选　择　器</th>
                          <th style="text-align:left">例　　子</th>
                          <th style="text-align:left">例子描述</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left"><code>.class</code></td>
                          <td style="text-align:left"><code>.intro</code></td>
                          <td style="text-align:left">选择 <code>class=&quot;intro&quot;</code> 的所有节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>#id</code></td>
                          <td style="text-align:left"><code>#firstname</code></td>
                          <td style="text-align:left">选择 <code>id=&quot;firstname&quot;</code> 的所有节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>*</code></td>
                          <td style="text-align:left"><code>*</code></td>
                          <td style="text-align:left">选择所有节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>element</code></td>
                          <td style="text-align:left"><code>p</code></td>
                          <td style="text-align:left">选择所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>element,element</code></td>
                          <td style="text-align:left"><code>div,p</code></td>
                          <td style="text-align:left">选择所有 <code>div</code> 节点和所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>element element</code></td>
                          <td style="text-align:left"><code>div p</code></td>
                          <td style="text-align:left">选择 <code>div</code> 节点内部的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>element&gt;element</code></td>
                          <td style="text-align:left"><code>div&gt;p</code></td>
                          <td style="text-align:left">选择父节点为 <code>div</code> 节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>element+element</code></td>
                          <td style="text-align:left"><code>div+p</code></td>
                          <td style="text-align:left">选择紧接在 <code>div</code> 节点之后的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[attribute]</code></td>
                          <td style="text-align:left"><code>[target]</code></td>
                          <td style="text-align:left">选择带有 <code>target</code> 属性的所有节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[attribute=value]</code></td>
                          <td style="text-align:left"><code>[target=blank]</code></td>
                          <td style="text-align:left">选择 <code>target=&quot;blank&quot;</code> 的所有节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[attribute~=value]</code></td>
                          <td style="text-align:left"><code>[title~=flower]</code></td>
                          <td style="text-align:left">选择 <code>title</code> 属性包含单词 <code>flower</code> 的所有节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:link</code></td>
                          <td style="text-align:left"><code>a:link</code></td>
                          <td style="text-align:left">选择所有未被访问的链接</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:visited</code></td>
                          <td style="text-align:left"><code>a:visited</code></td>
                          <td style="text-align:left">选择所有已被访问的链接</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:active</code></td>
                          <td style="text-align:left"><code>a:active</code></td>
                          <td style="text-align:left">选择活动链接</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:hover</code></td>
                          <td style="text-align:left"><code>a:hover</code></td>
                          <td style="text-align:left">选择鼠标指针位于其上的链接</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:focus</code></td>
                          <td style="text-align:left"><code>input:focus</code></td>
                          <td style="text-align:left">选择获得焦点的 <code>input</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:first-letter</code></td>
                          <td style="text-align:left"><code>p:first-letter</code></td>
                          <td style="text-align:left">选择每个 <code>p</code> 节点的首字母</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:first-line</code></td>
                          <td style="text-align:left"><code>p:first-line</code></td>
                          <td style="text-align:left">选择每个 <code>p</code> 节点的首行</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:first-child</code></td>
                          <td style="text-align:left"><code>p:first-child</code></td>
                          <td style="text-align:left">选择属于父节点的第一个子节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:before</code></td>
                          <td style="text-align:left"><code>p:before</code></td>
                          <td style="text-align:left">在每个 <code>p</code> 节点的内容之前插入内容</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:after</code></td>
                          <td style="text-align:left"><code>p:after</code></td>
                          <td style="text-align:left">在每个 <code>p</code> 节点的内容之后插入内容</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:lang(language)</code></td>
                          <td style="text-align:left"><code>p:lang</code></td>
                          <td style="text-align:left">选择带有以 <code>it</code> 开头的 <code>lang</code> 属性值的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>element1~element2</code></td>
                          <td style="text-align:left"><code>p~ul</code></td>
                          <td style="text-align:left">选择前面有 <code>p</code> 节点的所有 <code>ul</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[attribute^=value]</code></td>
                          <td style="text-align:left"><code>a[src^=&quot;https&quot;]</code></td>
                          <td style="text-align:left">选择其 <code>src</code> 属性值以 <code>https</code> 开头的所有 <code>a</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[attribute$=value]</code></td>
                          <td style="text-align:left"><code>a[src$=&quot;.pdf&quot;]</code></td>
                          <td style="text-align:left">选择其 <code>src</code> 属性以 .pdf 结尾的所有 <code>a</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>[attribute*=value]</code></td>
                          <td style="text-align:left"><code>a[src*=&quot;abc&quot;]</code></td>
                          <td style="text-align:left">选择其 <code>src</code> 属性中包含 <code>abc</code> 子串的所有 <code>a</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:first-of-type</code></td>
                          <td style="text-align:left"><code>p:first-of-type</code></td>
                          <td style="text-align:left">选择属于其父节点的首个 <code>p</code> 节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:last-of-type</code></td>
                          <td style="text-align:left"><code>p:last-of-type</code></td>
                          <td style="text-align:left">选择属于其父节点的最后一个 <code>p</code> 节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:only-of-type</code></td>
                          <td style="text-align:left"><code>p:only-of-type</code></td>
                          <td style="text-align:left">选择属于其父节点唯一的 <code>p</code> 节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:only-child</code></td>
                          <td style="text-align:left"><code>p:only-child</code></td>
                          <td style="text-align:left">选择属于其父节点的唯一子节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:nth-child(n)</code></td>
                          <td style="text-align:left"><code>p:nth-child</code></td>
                          <td style="text-align:left">选择属于其父节点的第二个子节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:nth-last-child(n)</code></td>
                          <td style="text-align:left"><code>p:nth-last-child</code></td>
                          <td style="text-align:left">同上，从最后一个子节点开始计数</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:nth-of-type(n)</code></td>
                          <td style="text-align:left"><code>p:nth-of-type</code></td>
                          <td style="text-align:left">选择属于其父节点第二个 <code>p</code> 节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:nth-last-of-type(n)</code></td>
                          <td style="text-align:left"><code>p:nth-last-of-type</code></td>
                          <td style="text-align:left">同上，但是从最后一个子节点开始计数</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:last-child</code></td>
                          <td style="text-align:left"><code>p:last-child</code></td>
                          <td style="text-align:left">选择属于其父节点最后一个子节点的所有 <code>p</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:root</code></td>
                          <td style="text-align:left"><code>:root</code></td>
                          <td style="text-align:left">选择文档的根节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:empty</code></td>
                          <td style="text-align:left"><code>p:empty</code></td>
                          <td style="text-align:left">选择没有子节点的所有 <code>p</code> 节点（包括文本节点）</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:target</code></td>
                          <td style="text-align:left"><code>#news:target</code></td>
                          <td style="text-align:left">选择当前活动的 <code>#news</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:enabled</code></td>
                          <td style="text-align:left"><code>input:enabled</code></td>
                          <td style="text-align:left">选择每个启用的 <code>input</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:disabled</code></td>
                          <td style="text-align:left"><code>input:disabled</code></td>
                          <td style="text-align:left">选择每个禁用的 <code>input</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:checked</code></td>
                          <td style="text-align:left"><code>input:checked</code></td>
                          <td style="text-align:left">选择每个被选中的 <code>input</code> 节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>:not(selector)</code></td>
                          <td style="text-align:left"><code>:not</code></td>
                          <td style="text-align:left">选择非 <code>p</code> 节点的所有节点</td>
                        </tr>
                        <tr>
                          <td style="text-align:left"><code>::selection</code></td>
                          <td style="text-align:left"><code>::selection</code></td>
                          <td style="text-align:left">选择被用户选取的节点部分</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>另外，还有一种比较常用的选择器 XPath，这种选择方式后面会详细介绍。</p>
                  <h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2>
                  <p>本节介绍了网页的结构和节点间的关系，了解了这些内容，我们才有更加清晰的思路去解析和提取网页内容。</p>
                  <p>本节参考来源：</p>
                  <ul>
                    <li>文档 - HTML - MDN Web Docs：<a href="https://developer.mozilla.org/en-US/docs/Web/HTML" target="_blank" rel="noopener">https://developer.mozilla.org/en-US/docs/Web/HTML</a></li>
                    <li>文档 - JavaScript - MDN Web Docs：<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript" target="_blank" rel="noopener">https://developer.mozilla.org/en-US/docs/Web/JavaScript</a></li>
                    <li>文档 - HTML DOM 节点 - W3School：<a href="http://www.w3school.com.cn/htmldom/dom_nodes.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/htmldom/dom_nodes.asp</a></li>
                    <li>文档 - HTML - 维基百科：<a href="https://en.wikipedia.org/wiki/HTML" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/HTML</a></li>
                    <li>文档 - CSS Selector - W3School：<a href="https://www.w3schools.com/cssref/css_selectors.asp" target="_blank" rel="noopener">https://www.w3schools.com/cssref/css_selectors.asp</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-11 02:38:31" itemprop="dateCreated datePublished" datetime="2022-02-11T02:38:31+08:00">2022-02-11</time>
                </span>
                <span id="/202213.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - Web网页基础" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>6.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>6 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202212.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/202212.html" class="post-title-link" itemprop="url">【2022 年】Python3 爬虫教程 - HTTP 基本原理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <blockquote>
                    <p>系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                  </blockquote>
                  <p>在正式学习网络爬虫之前，我们需要详细了解 HTTP 的基本原理，了解在浏览器中敲入 URL 到获取网页内容之间发生了什么。了解这些内容，有助于我们进一步了解爬虫的基本原理。</p>
                  <h1 id="1-1-HTTP-基本原理"><a href="#1-1-HTTP-基本原理" class="headerlink" title="1.1 HTTP 基本原理"></a>1.1 HTTP 基本原理</h1>
                  <p>在本节中，我们会详细了解 HTTP 的基本原理，了解在浏览器中敲入 URL 到获取网页内容之间发生了什么。了解这些内容，有助于我们进一步了解爬虫的基本原理。</p>
                  <h2 id="1-URI-和-URL"><a href="#1-URI-和-URL" class="headerlink" title="1. URI 和 URL"></a>1. URI 和 URL</h2>
                  <p>这里我们先了解一下 URI 和 URL。URI 的全称为 Uniform Resource Identifier，即统一资源标志符；而 URL 的全称为 Universal Resource Locator，即统一资源定位符。举例来说，<a href="https://github.com/favicon.ico" target="_blank" rel="noopener">https://github.com/favicon.ico</a> 是一个 URL，也是一个 URI。即有这样一个图标资源，我们用 URL/URI 来唯一指定了它的访问方式，这其中包括了访问协议 https、访问路径（即根目录）和资源名称 favicon.ico。通过这样一个链接，我们便可以从互联网上找到这个资源，这就是 URL/URI。</p>
                  <p>URL 是 URI 的子集，也就是说每个 URL 都是 URI，但不是每个 URI 都是 URL。那么，怎样的 URI 不是 URL 呢？URI 还包括一个子类，叫作 URN，它的全称为 Universal Resource Name，即统一资源名称。URN 只命名资源而不指定如何定位资源，比如 urn:isbn:0451450523 指定了一本书的 ISBN，可以唯一标识这本书，但是没有指定到哪里定位这本书，这就是 URN。URL、URN 和 URI 的关系可以用图 1-1 表示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/l7600.jpg" alt="URL、URN 和 URI 关系图"></p>
                  <p>但是在目前的互联网，URN 使用得非常少，几乎所有的 URI 都是 URL，所以对于一般的网页链接，我们既可以称之为 URL，也可以称之为 URI，我个人习惯称之为 URL。</p>
                  <p>但 URL 也不是随便写的，它也是需要遵循一定的格式规范的，基本的组成格式如下：</p>
                  <figure class="highlight markdown">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scheme://[<span class="string">username:password@</span>]hostname[<span class="string">:port</span>][<span class="symbol">/path</span>][<span class="string">;parameters</span>][<span class="symbol">?query</span>][<span class="string">#fragment</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中这里中括号包括的内容代表非必要部分，比如 <a href="https://www.baidu.com" target="_blank" rel="noopener">https://www.baidu.com</a> 这个 URL，这里就只包含了 scheme 和 host 两部分，其他的 port、path、parameters、query、fragment 都没有。</p>
                  <p>这里我们分别介绍下几部分代表的含义和作用：</p>
                  <ul>
                    <li>scheme：协议。比如常用的协议有 http、https、ftp 等等，另外 scheme 也被常称作 protocol，都代表协议的意思。</li>
                    <li>username、password：用户名和密码。在某些情况下 URL 需要提供用户名和密码才能访问，这时候可以把用户名密码放在 host 前面。比如 <a href="https://ssr3.scrape.center" target="_blank" rel="noopener">https://ssr3.scrape.center</a> 这个 URL 需要用户名密码才能访问，那么可以直接写为 <a href="https://admin:admin@ssr3.scrape.center" target="_blank" rel="noopener">https://admin:admin@ssr3.scrape.center</a> 则可以直接访问。</li>
                    <li>hostname：主机地址。可以是域名或 IP 地址，比如 <a href="https://www.baidu.com" target="_blank" rel="noopener">https://www.baidu.com</a> 这个 URL 中的 hostname 就是 www.baidu.com，这就是百度的二级域名。比如 <a href="https://8.8.8.8" target="_blank" rel="noopener">https://8.8.8.8</a> 这个 URL 中 hostname 就是 8.8.8.8，它是一个 IP 地址。</li>
                    <li>port：端口。这是服务器设定的服务端口，比如 <a href="https://8.8.8.8:12345" target="_blank" rel="noopener">https://8.8.8.8:12345</a> 这个 URL 中的端口就是 12345。但是有些 URL 中没有端口信息，这是使用了默认的端口，http 协议的默认端口是 80，https 协议的默认端口是 443。所以 <a href="https://www.baidu.com" target="_blank" rel="noopener">https://www.baidu.com</a> 其实相当于 <a href="https://www.baidu.com:443，而">https://www.baidu.com:443，而</a> <a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a> 其实相当于 <a href="http://www.baidu.com:80。">http://www.baidu.com:80。</a></li>
                    <li>path：路径。指的是网络资源在服务器中的指定地址，比如 <a href="https://github.com/favicon.ico" target="_blank" rel="noopener">https://github.com/favicon.ico</a> 这里 path 就是 favicon.ico，指的就是访问 GitHub 上的根目录下的 favicon.ico 这个资源。</li>
                    <li>parameters：参数。用来制定访问某个资源的时候的附加信息，比如 <a href="https://8.8.8.8:12345/hello;user" target="_blank" rel="noopener">https://8.8.8.8:12345/hello;user</a> 这里的 user 就是 parameters。但是 parameters 现在用得很少，所以目前很多人会把该参数后面的 query 部分称为参数，甚至把 parameters 和 query 混用。严格意义上来说，parameters 是分号 ; 后面的内容。</li>
                    <li>query：查询。用来查询某类资源，如果有多个查询，则用 &amp; 隔开。query 其实非常常见，比如 <a href="https://www.baidu.com/s?wd=nba&amp;ie=utf-8，这里的" target="_blank" rel="noopener">https://www.baidu.com/s?wd=nba&amp;ie=utf-8，这里的</a> query 部分就是 wd=nba&amp;ie=utf-8，这里指定了 wd 是 nba，ie 是 utf-8。由于 query 比刚才所说的 parameters 使用频率高太多，所以平时我们见到的参数、GET 请求参数、parameters、params 等称呼多数情况指代的也是 query。严格意义上来说，其实应该用 query 来表示。</li>
                    <li>fragment：片段。它是对资源描述的部分补充，可以理解为资源内部的书签。目前它有两个主要应用，一个是用作单页面路由，比如 现代前端框架 Vue、React 都可以借助它来做路由管理；另外一个应用是用作 HTML 锚点，用它可以控制一个页面打开时自动下滑滚动到某个特定的位置。</li>
                  </ul>
                  <p>以上我们就简单了解了 URL 的基本概念和构成，后文我们会结合多个实战案例练习来帮助加深其理解。</p>
                  <h2 id="2-HTTP-和-HTTPS"><a href="#2-HTTP-和-HTTPS" class="headerlink" title="2. HTTP 和 HTTPS"></a>2. HTTP 和 HTTPS</h2>
                  <p>刚才我们了解了 URL 的基本构成，其支持的协议有很多，比如 http、https、ftp、sftp、smb 等等。</p>
                  <p>在爬虫中，我们抓取的页面通常基于 http 或 https 协议，这里首先我们先来了解一下这两个协议的含义。</p>
                  <p>HTTP 的全称是 Hyper Text Transfer Protocol，中文名叫作超文本传输协议。HTTP 协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP 由万维网协会（World Wide Web Consortium）和 Internet 工作小组 IETF（Internet Engineering Task Force）共同合作制定的规范，目前广泛使用的是 HTTP 1.1 版本，当然 HTTP 2.0 现在不少网站也增加了支持。</p>
                  <p>其发展历史见下表：</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th>版本</th>
                          <th>产生时间</th>
                          <th>主要特点</th>
                          <th>发展现状</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td>HTTP/0.9</td>
                          <td>1991 年</td>
                          <td>不涉及数据包传输，规定客户端和服务器之间通信格式，只能 GET 请求</td>
                          <td>没有作为正式的标准</td>
                        </tr>
                        <tr>
                          <td>HTTP/1.0</td>
                          <td>1996 年</td>
                          <td>传输内容格式不限制，增加 PUT、PATCH、HEAD、 OPTIONS、DELETE 命令</td>
                          <td>正式作为标准</td>
                        </tr>
                        <tr>
                          <td>HTTP/1.1</td>
                          <td>1997 年</td>
                          <td>持久连接(长连接)、节约带宽、HOST 域、管道机制、分块传输编码</td>
                          <td>正式作为标准并广泛使用</td>
                        </tr>
                        <tr>
                          <td>HTTP/2.0</td>
                          <td>2015 年</td>
                          <td>多路复用、服务器推送、头信息压缩、二进制协议等</td>
                          <td>逐渐覆盖市场</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>HTTPS 的全称是 Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的 HTTP 通道，简单讲是 HTTP 的安全版，即在 HTTP 下加入 SSL 层，简称为 HTTPS。</p>
                  <p>HTTPS 的安全基础是 SSL，因此通过它传输的内容都是经过 SSL 加密的，它的主要作用分为以下两种。</p>
                  <ul>
                    <li>建立一个信息安全通道，保证数据传输的安全性。</li>
                    <li>确认网站的真实性。凡是使用了 https 的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过 CA 机构颁发的安全签章来查询。</li>
                  </ul>
                  <p>现在越来越多的网站和 App 都已经向 HTTPS 方向发展，举例如下。</p>
                  <ul>
                    <li>苹果公司强制所有 iOS App 在 2017 年 1 月 1 日前全部改为使用 HTTPS 加密，否则 App 就无法在应用商店上架。</li>
                    <li>谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址亮出风险提示，即在地址栏的显著位置提醒用户 “此网页不安全”。</li>
                    <li>腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。</li>
                  </ul>
                  <p>因此，HTTPS 已经是大势所趋。</p>
                  <blockquote>
                    <p>注：HTTP 和 HTTPS 协议都属于计算机网络中的应用层协议，其下层是基于 TCP 协议实现的，TCP 协议属于计算机网络中的传输层协议，包括建立连接时的三次握手和断开时的四次挥手等过程。但本书主要讲的是网络爬虫相关，主要爬取的是 HTTP/HTTPS 协议相关的内容，所以这里就不再展开深入讲解 TCP、IP 等相关知识了，感兴趣的读者可以搜索相关资料了解下，如《计算机网络》、《图解 HTTP》等书籍。</p>
                  </blockquote>
                  <h2 id="3-HTTP-请求过程"><a href="#3-HTTP-请求过程" class="headerlink" title="3. HTTP 请求过程"></a>3. HTTP 请求过程</h2>
                  <p>我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容。</p>
                  <p>实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器。</p>
                  <p>由于响应里包含页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，流程如图 1-3 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/aod7o.jpg" alt="模型图"></p>
                  <p>此处客户端即代表我们自己的 PC 或手机浏览器，服务器即要访问的网站所在的服务器。</p>
                  <p>为了更直观地说明这个过程，这里用 Chrome 浏览器开发者模式下的 Network 监听组件来做下演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。</p>
                  <p>打开 Chrome 浏览器，访问百度 <a href="http://www.baidu.com/" target="_blank" rel="noopener">http://www.baidu.com/</a>，这时候鼠标右键并选择“检查”菜单（或直接按快捷键 F12），即可打开浏览器的开发者工具，如下图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/rc3jq.png" alt="打开浏览器的开发者工具"></p>
                  <p>我们切换到 Network 面板，然后重新刷新网页，这时候就可以看到在 Network 面板下方出现了很多个条目，其中一个条目就代表一次发送请求和接收响应的过程，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/lwt4l.png" alt="请求和接收响应的过程"></p>
                  <p>我们先观察第一个网络请求，即 www.baidu.com，其中各列的含义如下。</p>
                  <ul>
                    <li>第一列 Name：请求的名称，一般会将 URL 的最后一部分内容当作名称。</li>
                    <li>第二列 Status：响应的状态码，这里显示为 200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。</li>
                    <li>第三列 Protocol：请求的协议类型，这里 http/1.1 代表是 HTTP 1.1 版本，h2 代表 HTTP 2.0 版本。</li>
                    <li>第四列 Type：请求的文档类型。这里为 document，代表我们这次请求的是一个 HTML 文档，内容就是一些 HTML 代码。</li>
                    <li>第五列 Initiator：请求源。用来标记请求是由哪个对象或进程发起的。</li>
                    <li>第六列 Size：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示 from cache。</li>
                    <li>第七列 Time：发起请求到获取响应所用的总时间。</li>
                    <li>第八列 Waterfall：网络请求的可视化瀑布流。</li>
                  </ul>
                  <p>我们点击这个条目，即可看到其更详细的信息，如图所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/0utz7.png" alt="更详细的信息"></p>
                  <p>首先是 General 部分，其中 Request URL 为请求的 URL，Request Method 为请求的方法，Status Code 为响应状态码，Remote Address 为远程服务器的地址和端口，Referrer Policy 为 Referrer 判别策略。</p>
                  <p>再继续往下可以看到，有 Response Headers 和 Request Headers，它们分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookie、Host 等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图 1-5 中看到的 Response Headers 就是响应的一部分，其中包含了服务器的类型、文档类型、日期等信息，浏览器接收到响应后，会解析响应内容，进而呈现网页内容。</p>
                  <p>下面我们分别来介绍一下请求和响应都包含哪些内容。</p>
                  <h2 id="4-请求（Request）"><a href="#4-请求（Request）" class="headerlink" title="4. 请求（Request）"></a>4. 请求（Request）</h2>
                  <p>请求，英文为 Request，由客户端向服务器发出，可以分为 4 部分内容：请求方法（Request Method）、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。</p>
                  <p>下面我们分别予以介绍。</p>
                  <h3 id="请求方法（Request-Method）"><a href="#请求方法（Request-Method）" class="headerlink" title="请求方法（Request Method）"></a>请求方法（Request Method）</h3>
                  <p>请求方法，英文为 Request Method，用于标识请求客户端请求服务端的方式，常见的请求方法有两种：GET 和 POST。</p>
                  <p>在浏览器中直接输入 URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里。例如，在百度中搜索 Python，这就是一个 GET 请求，链接为 <a href="https://www.baidu.com/s?wd=Python" target="_blank" rel="noopener">https://www.baidu.com/s?wd=Python</a>，其中 URL 中包含了请求的 query 信息，这里的参数 wd 表示要搜寻的关键字。POST 请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击 “登录” 按钮，这通常会发起一个 POST 请求，其数据通常以表单的形式传输，而不会体现在 URL 中。</p>
                  <p>GET 和 POST 请求方法有如下区别：</p>
                  <ul>
                    <li>GET 请求中的参数包含在 URL 里面，数据可以在 URL 中看到；而 POST 请求的 URL 不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。</li>
                    <li>GET 请求提交的数据最多只有 1024 字节，而 POST 方式没有限制。</li>
                  </ul>
                  <p>一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用 GET 方式请求的话，密码就会暴露在 URL 里面，造成密码泄露，所以这里最好以 POST 方式发送。上传文件时，由于文件内容比较大，也会选用 POST 方式。</p>
                  <p>我们平常遇到的绝大部分请求都是 GET 或 POST 请求。另外，还有一些请求方法，如 GET、HEAD、POST、PUT、DELETE、CONNECT、OPTIONS、TRACE 等，我们简单将其总结为下表。</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">方　　法</th>
                          <th style="text-align:left">描　　述</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left">GET</td>
                          <td style="text-align:left">请求页面，并返回页面内容</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">HEAD</td>
                          <td style="text-align:left">类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">POST</td>
                          <td style="text-align:left">大多用于提交表单或上传文件，数据包含在请求体中</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">PUT</td>
                          <td style="text-align:left">从客户端向服务器传送的数据取代指定文档中的内容</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">DELETE</td>
                          <td style="text-align:left">请求服务器删除指定的页面</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">CONNECT</td>
                          <td style="text-align:left">把服务器当作跳板，让服务器代替客户端访问其他网页</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">OPTIONS</td>
                          <td style="text-align:left">允许客户端查看服务器的性能</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">TRACE</td>
                          <td style="text-align:left">回显服务器收到的请求，主要用于测试或诊断</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>本表参考：<a href="http://www.runoob.com/http/http-methods.html" target="_blank" rel="noopener">http://www.runoob.com/http/http-methods.html</a>。</p>
                  <h3 id="请求的网址（Request-URL）"><a href="#请求的网址（Request-URL）" class="headerlink" title="请求的网址（Request URL）"></a>请求的网址（Request URL）</h3>
                  <p>请求的网址，英文为 Reqeust URL，它可以唯一确定我们想请求的资源。关于 URL 的构成和各个部分的功能我们在前文已经提及到了，这里就不再赘述。</p>
                  <h3 id="请求头（Request-Headers）"><a href="#请求头（Request-Headers）" class="headerlink" title="请求头（Request Headers）"></a>请求头（Request Headers）</h3>
                  <p>请求头，英文为 Request Headers，用来说明服务器要使用的附加信息，比较重要的信息有 Cookie、Referer、User-Agent 等。</p>
                  <p>下面简要说明一些常用的头信息：</p>
                  <ul>
                    <li>Accept：请求报头域，用于指定客户端可接受哪些类型的信息。</li>
                    <li>Accept-Language：指定客户端可接受的语言类型。</li>
                    <li>Accept-Encoding：指定客户端可接受的内容编码。</li>
                    <li>Host：用于指定请求资源的主机 IP 和端口号，其内容为请求 URL 的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。</li>
                    <li>Cookie：也常用复数形式 Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是 Cookie 的功劳。Cookie 里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上 Cookie 并将其发送给服务器，服务器通过 Cookie 识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。</li>
                    <li>Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。</li>
                    <li>User-Agent：简称 UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别为爬虫。</li>
                    <li>Content-Type：也叫互联网媒体类型（Internet Media Type）或者 MIME 类型，在 HTTP 协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html 代表 HTML 格式，image/gif 代表 GIF 图片，application/json 代表 JSON 类型，更多对应关系可以查看此对照表：<a href="http://tool.oschina.net/commons" target="_blank" rel="noopener">http://tool.oschina.net/commons</a>。</li>
                  </ul>
                  <p>因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。</p>
                  <h3 id="请求体（Request-Body）"><a href="#请求体（Request-Body）" class="headerlink" title="请求体（Request Body）"></a>请求体（Request Body）</h3>
                  <p>请求体，即 Request Body 一般承载的内容是 POST 请求中的表单数据，而对于 GET 请求，请求体则为空。</p>
                  <p>例如，这里我登录 GitHub 时捕获到的请求和响应如图 1-6 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/4tinn.jpg" alt="请求和响应"></p>
                  <p>登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意 Request Headers 中指定 Content-Type 为 application/x-www-form-urlencoded。只有设置 Content-Type 为 application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将 Content-Type 设置为 application/json 来提交 JSON 数据，或者设置为 multipart/form-data 来上传文件。</p>
                  <p>如下表是 Content-Type 和 POST 提交数据方式的关系</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">Content-Type</th>
                          <th style="text-align:left">提交数据的方式</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left">application/x-www-form-urlencoded</td>
                          <td style="text-align:left">表单数据</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">multipart/form-data</td>
                          <td style="text-align:left">表单文件上传</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">application/json</td>
                          <td style="text-align:left">序列化 JSON 数据</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">text/xml</td>
                          <td style="text-align:left">XML 数据</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <p>在爬虫中，如果要构造 POST 请求，需要使用正确的 Content-Type，并了解各种请求库的各个参数设置时使用的是哪种 Content-Type，不然可能会导致 POST 提交后无法正常响应。</p>
                  <h2 id="5-响应（Response）"><a href="#5-响应（Response）" class="headerlink" title="5. 响应（Response）"></a>5. 响应（Response）</h2>
                  <p>响应，即 Response，由服务器返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。</p>
                  <h3 id="响应状态码（Response-Status-Code）"><a href="#响应状态码（Response-Status-Code）" class="headerlink" title="响应状态码（Response Status Code）"></a>响应状态码（Response Status Code）</h3>
                  <p>响应状态码，即 Response Status Code，表示服务器的响应状态，如 200 代表服务器正常响应，404 代表页面未找到，500 代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为 200，则证明成功返回数据，再进行进一步的处理，否则直接忽略。下表列出了常见的错误代码及错误原因。</p>
                  <p>常见的错误代码及错误原因</p>
                  <div class="table-container">
                    <table>
                      <thead>
                        <tr>
                          <th style="text-align:left">状态码</th>
                          <th style="text-align:left">说　　明</th>
                          <th style="text-align:left">详　　情</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left">100</td>
                          <td style="text-align:left">继续</td>
                          <td style="text-align:left">请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">101</td>
                          <td style="text-align:left">切换协议</td>
                          <td style="text-align:left">请求者已要求服务器切换协议，服务器已确认并准备切换</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">200</td>
                          <td style="text-align:left">成功</td>
                          <td style="text-align:left">服务器已成功处理了请求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">201</td>
                          <td style="text-align:left">已创建</td>
                          <td style="text-align:left">请求成功并且服务器创建了新的资源</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">202</td>
                          <td style="text-align:left">已接受</td>
                          <td style="text-align:left">服务器已接受请求，但尚未处理</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">203</td>
                          <td style="text-align:left">非授权信息</td>
                          <td style="text-align:left">服务器已成功处理了请求，但返回的信息可能来自另一个源</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">204</td>
                          <td style="text-align:left">无内容</td>
                          <td style="text-align:left">服务器成功处理了请求，但没有返回任何内容</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">205</td>
                          <td style="text-align:left">重置内容</td>
                          <td style="text-align:left">服务器成功处理了请求，内容被重置</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">206</td>
                          <td style="text-align:left">部分内容</td>
                          <td style="text-align:left">服务器成功处理了部分请求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">300</td>
                          <td style="text-align:left">多种选择</td>
                          <td style="text-align:left">针对请求，服务器可执行多种操作</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">301</td>
                          <td style="text-align:left">永久移动</td>
                          <td style="text-align:left">请求的网页已永久移动到新位置，即永久重定向</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">302</td>
                          <td style="text-align:left">临时移动</td>
                          <td style="text-align:left">请求的网页暂时跳转到其他页面，即暂时重定向</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">303</td>
                          <td style="text-align:left">查看其他位置</td>
                          <td style="text-align:left">如果原来的请求是 POST，重定向目标文档应该通过 GET 提取</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">304</td>
                          <td style="text-align:left">未修改</td>
                          <td style="text-align:left">此次请求返回的网页未修改，继续使用上次的资源</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">305</td>
                          <td style="text-align:left">使用代理</td>
                          <td style="text-align:left">请求者应该使用代理访问该网页</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">307</td>
                          <td style="text-align:left">临时重定向</td>
                          <td style="text-align:left">请求的资源临时从其他位置响应</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">400</td>
                          <td style="text-align:left">错误请求</td>
                          <td style="text-align:left">服务器无法解析该请求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">401</td>
                          <td style="text-align:left">未授权</td>
                          <td style="text-align:left">请求没有进行身份验证或验证未通过</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">403</td>
                          <td style="text-align:left">禁止访问</td>
                          <td style="text-align:left">服务器拒绝此请求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">404</td>
                          <td style="text-align:left">未找到</td>
                          <td style="text-align:left">服务器找不到请求的网页</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">405</td>
                          <td style="text-align:left">方法禁用</td>
                          <td style="text-align:left">服务器禁用了请求中指定的方法</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">406</td>
                          <td style="text-align:left">不接受</td>
                          <td style="text-align:left">无法使用请求的内容响应请求的网页</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">407</td>
                          <td style="text-align:left">需要代理授权</td>
                          <td style="text-align:left">请求者需要使用代理授权</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">408</td>
                          <td style="text-align:left">请求超时</td>
                          <td style="text-align:left">服务器请求超时</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">409</td>
                          <td style="text-align:left">冲突</td>
                          <td style="text-align:left">服务器在完成请求时发生冲突</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">410</td>
                          <td style="text-align:left">已删除</td>
                          <td style="text-align:left">请求的资源已永久删除</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">411</td>
                          <td style="text-align:left">需要有效长度</td>
                          <td style="text-align:left">服务器不接受不含有效内容长度标头字段的请求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">412</td>
                          <td style="text-align:left">未满足前提条件</td>
                          <td style="text-align:left">服务器未满足请求者在请求中设置的其中一个前提条件</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">413</td>
                          <td style="text-align:left">请求实体过大</td>
                          <td style="text-align:left">请求实体过大，超出服务器的处理能力</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">414</td>
                          <td style="text-align:left">请求 URI 过长</td>
                          <td style="text-align:left">请求网址过长，服务器无法处理</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">415</td>
                          <td style="text-align:left">不支持类型</td>
                          <td style="text-align:left">请求格式不被请求页面支持</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">416</td>
                          <td style="text-align:left">请求范围不符</td>
                          <td style="text-align:left">页面无法提供请求的范围</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">417</td>
                          <td style="text-align:left">未满足期望值</td>
                          <td style="text-align:left">服务器未满足期望请求标头字段的要求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">500</td>
                          <td style="text-align:left">服务器内部错误</td>
                          <td style="text-align:left">服务器遇到错误，无法完成请求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">501</td>
                          <td style="text-align:left">未实现</td>
                          <td style="text-align:left">服务器不具备完成请求的功能</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">502</td>
                          <td style="text-align:left">错误网关</td>
                          <td style="text-align:left">服务器作为网关或代理，从上游服务器收到无效响应</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">503</td>
                          <td style="text-align:left">服务不可用</td>
                          <td style="text-align:left">服务器目前无法使用</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">504</td>
                          <td style="text-align:left">网关超时</td>
                          <td style="text-align:left">服务器作为网关或代理，但是没有及时从上游服务器收到请求</td>
                        </tr>
                        <tr>
                          <td style="text-align:left">505</td>
                          <td style="text-align:left">HTTP 版本不支持</td>
                          <td style="text-align:left">服务器不支持请求中所用的 HTTP 协议版本</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                  <h3 id="响应头（Response-Headers）"><a href="#响应头（Response-Headers）" class="headerlink" title="响应头（Response Headers）"></a>响应头（Response Headers）</h3>
                  <p>响应头，即 Response Headers，包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等。下面简要说明一些常用的头信息。</p>
                  <ul>
                    <li>Date：标识响应产生的时间。</li>
                    <li>Last-Modified：指定资源的最后修改时间。</li>
                    <li>Content-Encoding：指定响应内容的编码。</li>
                    <li>Server：包含服务器的信息，比如名称、版本号等。</li>
                    <li>Content-Type：文档类型，指定返回的数据类型是什么，如 text/html 代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回图片。</li>
                    <li>Set-Cookie：设置 Cookie。响应头中的 Set-Cookie 告诉浏览器需要将此内容放在 Cookie 中，下次请求携带 Cookie 请求。</li>
                    <li>Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。</li>
                  </ul>
                  <h3 id="响应体（Response-Body）"><a href="#响应体（Response-Body）" class="headerlink" title="响应体（Response Body）"></a>响应体（Response Body）</h3>
                  <p>响应体，即 Response Body，这可以说是最关键的部分了，响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的 HTML 代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图 1-7 所示。</p>
                  <p><img src="https://cdn.cuiqingcai.com/6sq5o.jpg" alt="响应体"></p>
                  <p>在浏览器开发者工具中点击 Preview，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。</p>
                  <p>在做爬虫时，我们主要通过响应体得到网页的源代码、JSON 数据等，然后从中做相应内容的提取。</p>
                  <p>本节中，我们了解了 HTTP 的基本原理，大概了解了访问网页时背后的请求和响应过程。本节涉及的知识点需要好好掌握，后面分析网页请求时会经常用到。</p>
                  <h2 id="6-HTTP-2-0"><a href="#6-HTTP-2-0" class="headerlink" title="6. HTTP/2.0"></a>6. HTTP/2.0</h2>
                  <p>前面我们也提到了 HTTP 协议从 2015 年起发布了 2.0 版本，相比 HTTP/1.1 来说，HTTP/2.0 变得更快、更简单、更稳定，HTTP/2.0 在传输层做了很多优化，HTTP/2.0 的主要目标是通过支持完整的请求与响应复用来减少延迟，并通过有效压缩 HTTP 请求头字段将协议开销降至最低，同时增加对请求优先级和服务器推送的支持，这些优化一笔勾销了 HTTP/1.1 为做传输优化想出的一系列“歪招”。</p>
                  <p>有读者这时候可能会问，为什么不叫 HTTP/1.2 而叫 HTTP/2.0 呢？因为 HTTP/2.0 在内部实现上新的二进制分帧层，这是没法与之前的 HTTP/1.x 的服务器和客户端实现向后兼容的，所以直接修改了主版本号为 2.0。</p>
                  <p>下面我们就来了解下 HTTP/2.0 相比 HTTP/1.1 来说做了哪些优化吧。</p>
                  <h3 id="二进制分帧层"><a href="#二进制分帧层" class="headerlink" title="二进制分帧层"></a>二进制分帧层</h3>
                  <p>HTTP/2.0 所有性能增强的核心就在于这个新的二进制分帧层。在 HTTP/1.x 中，不管是请求（Request）还是响应（Response），它们都是用文本格式传输的，其头部（Headers）、实体（Body）之间也是用文本换行符分隔开的。HTTP/2.0 对其做了优化，将文本格式修改为了二进制格式，使得解析起来更加高效。同时将请求和响应数据分割为更小的帧，并采用二进制编码。</p>
                  <p>所以这里就引入了几个新的概念：</p>
                  <ul>
                    <li>帧：只存在于 HTTP/2.0 中的概念，是数据通信的最小单位，比如一个请求被分为了请求头帧（Request Headers frame）和请求体/数据帧（Request Data frame）。</li>
                    <li>数据流：一个虚拟通道，可以承载双向的消息，每个流都有一个唯一的整数 ID 来标识。</li>
                    <li>消息：与逻辑请求或响应消息对应的完整的一系列帧。</li>
                  </ul>
                  <p>在 HTTP/2.0 中，同域名下的所有通信都可以在单个连接上完成，该连接可以承载任意数量的双向数据流，数据流是用于承载双向消息的，每条消息都是一条逻辑 HTTP 消息（例如请求或响应），它可以包含一个或多个帧。</p>
                  <p>简而言之，HTTP/2.0 将 HTTP 协议通信分解为二进制编码帧的交换，这些帧对应着特定数据流中的消息，所有这些都在一个 TCP 连接内复用，这是 HTTP/2.0 协议所有其他功能和性能优化的基础。</p>
                  <h3 id="多路复用"><a href="#多路复用" class="headerlink" title="多路复用"></a>多路复用</h3>
                  <p>在 HTTP/1.x 中，如果客户端要想发起多个并行请求以提升性能，则必须使用多个 TCP 连接，而且浏览器位了控制资源，还会对单个域名有 6-8 个 TCP 连接请求的限制。但在 HTTP/2.0 中，由于又了二进制分帧技术的加持，HTTP/2.0 不用再以来 TCP 连接去实现多路并行了，客户端和服务器可以将 HTTP 消息分解为互不依赖的帧，然后交错发送，最后再在另一端把它们重新组装起来，让我们可以:</p>
                  <ul>
                    <li>并行交错地发送多个请求，请求之间互不影响。</li>
                    <li>并行交错地发送多个响应，响应之间互不干扰。</li>
                    <li>使用一个连接并行发送多个请求和响应。</li>
                    <li>不必再为绕过 HTTP/1.x 限制而做很多工作。</li>
                    <li>消除不必要的延迟和提高现有网络容量的利用率，从而减少页面加载时间。</li>
                  </ul>
                  <p>这样以来，整个数据传输使性能就有了极大提升：</p>
                  <ul>
                    <li>同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应，消除了因多个 TCP 连接而带来的延时和内存消耗。</li>
                    <li>并行交错地发送多个请求和详情，而且之间互不影响。</li>
                    <li>在 HTTP/2.0 中，每个请求都可以带一个 31bit 的优先值，0 表示最高优先级， 数值越大优先级越低。有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。</li>
                  </ul>
                  <h3 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h3>
                  <p>流控制是一种阻止发送方向接收方发送大量数据的机制，以免超出后者的需求或处理能力。可以理解为，接收方已经太繁忙了，来不及处理收到的消息了，但是发送方还在一直大量发送消息，这样就会出现一些问题。</p>
                  <p>比如说，客户端可能请求了一个具有较高优先级的大型视频流，但是用户已经暂停视频，客户端现在希望暂停或限制从服务器的传输，以免提取和缓冲不必要的数据。 再比如，一个代理服务器可能具有较快的下游连接和较慢的上游连接，并且也希望调节下游连接传输数据的速度以匹配上游连接的速度来控制其资源利用率等等。</p>
                  <p>由于 HTTP 是基于 TCP 实现的，虽然 TCP 原生有流量控制机制，但是由于 HTTP/2.0 数据流在一个 TCP 连接内复用，TCP 流控制既不够精细，也无法提供必要的应用级 API 来调节各个数据流的传输。</p>
                  <p>为了解决这一问题，HTTP/2.0 提供了一组简单的构建块，这些构建块允许客户端和服务器实现其自己的数据流和连接级流控制:</p>
                  <ul>
                    <li>流控制具有方向性。 每个接收方都可以根据自身需要选择为每个数据流和整个连接设置任意的窗口大小。</li>
                    <li>流控制基于信用。 每个接收方都可以公布其初始连接和数据流流控制窗口（以字节为单位），每当发送方发出 <code>DATA</code> 帧时都会减小，在接收方发出 <code>WINDOW_UPDATE</code> 帧时增大。</li>
                    <li>流控制无法停用。 建立 HTTP/2.0 连接后，客户端将与服务器交换 <code>SETTINGS</code> 帧，这会在两个方向上设置流控制窗口。 流控制窗口的默认值设为 65535 字节，但是接收方可以设置一个较大的最大窗口大小（<code>2^31-1</code> 字节），并在接收到任意数据时通过发送 <code>WINDOW_UPDATE</code> 帧来维持这一大小。</li>
                    <li>流控制为逐跃点控制，而非端到端控制。 即，可信中介可以使用它来控制资源使用，以及基于自身条件和启发式算法实现资源分配机制。</li>
                  </ul>
                  <p>由此可见，HTTP/2.0 提供了简单的构建块实现了自定义策略来调节资源使用和分配，以及实现新传输能力，同时提升了网页应用的实际性能和感知性能。</p>
                  <h3 id="服务端推送"><a href="#服务端推送" class="headerlink" title="服务端推送"></a>服务端推送</h3>
                  <p>HTTP/2.0 新增的另一个强大的新功能是，服务器可以对一个客户端请求发送多个响应。 换句话说，除了对最初请求的响应外，服务器还可以向客户端推送额外资源，而无需客户端明确地请求。</p>
                  <p>如果某些资源客户端是一定会请求的，这时就可以采取服务端推送的技术，在客户端发起一次请求后，额外提前给客户端推送必要的资源，这样就可以相对减少一点延迟时间。例如，服务端可以主动把 JS 和 CSS 文件推送给客户端，而不需要客户端解析 HTML 时再发送这些请求。</p>
                  <p><img src="https://cdn.cuiqingcai.com/y0vea.png" alt="服务端推送"></p>
                  <p>服务端可以主动推送，当然客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送 RST_STREAM 帧来拒收。</p>
                  <p>另外主动推送也遵守同源策略，即服务器不能随便将第三方资源推送给客户端，而必须是经过双方确认才行，这样也能保证一定的安全性。</p>
                  <h3 id="HTTP-2-0-发展现状"><a href="#HTTP-2-0-发展现状" class="headerlink" title="HTTP/2.0 发展现状"></a>HTTP/2.0 发展现状</h3>
                  <p>HTTP/2.0 的普及是一件任重而道远的事情，一些主流的网站现在已经支持了 HTTP/2.0，主流浏览器现在都已经实现了 HTTP/2.0 的支持，但总的来看，目前大部分网站依然还是以 HTTP/1.1 为主。</p>
                  <p>另外一些编程语言的库还没有完全支持 HTTP/2.0，比如对于 Python 来说，hyper、httpx 等库已经支持了 HTTP/2.0，但广泛使用的 requests 库依然还是只支持 HTTP/1.1。</p>
                  <h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2>
                  <p>本节介绍了关于 HTTP 的一些基础知识，内容不少，需要好好掌握，这些知识对于后面我们编写和理解网络爬虫具有非常大的帮助。</p>
                  <p>由于本节的内容多数为概念介绍，内容参考了很多书籍、文档、博客，来源如下：</p>
                  <ul>
                    <li>书籍 - 《HTTP 权威指南》- 作者 David Gourley / Brian Totty</li>
                    <li>文档 - HTTP - 维基百科：<a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol</a></li>
                    <li>文档 - HTTP - 百度百科：<a href="https://baike.baidu.com/item/HTTP/243074" target="_blank" rel="noopener">https://baike.baidu.com/item/HTTP/243074</a></li>
                    <li>文档 - HTTP - MDN Web Docs：<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP" target="_blank" rel="noopener">https://developer.mozilla.org/en-US/docs/Web/HTTP</a></li>
                    <li>文档 - HTTP/2 简介 - Google 开发文档：<a href="https://developers.google.com/web/fundamentals/performance/http2" target="_blank" rel="noopener">https://developers.google.com/web/fundamentals/performance/http2</a></li>
                    <li>博客 - 一文读懂 HTTP/2 及 HTTP/3 特性：<a href="https://blog.fundebug.com/2019/03/07/understand-http2-and-http3/" target="_blank" rel="noopener">https://blog.fundebug.com/2019/03/07/understand-http2-and-http3/</a></li>
                    <li>博客 - 一文读懂 HTTP/2 特性：<a href="https://zhuanlan.zhihu.com/p/26559480" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26559480</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-02-11 00:38:31" itemprop="dateCreated datePublished" datetime="2022-02-11T00:38:31+08:00">2022-02-11</time>
                </span>
                <span id="/202212.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - HTTP 基本原理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>13k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>12 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36051.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Markdown <i class="label-arrow"></i>
                  </a>
                  <a href="/36051.html" class="post-title-link" itemprop="url">用 Markdown 做 PPT，就是这么简单！</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>相信绝大多数朋友做 PPT（幻灯片 / Slides / Deck 等各种称呼了）都是用的 PowerPoint 或者 KeyNote 吧？功能是比较强大，但你有没有遇到过这样的痛点：</p>
                  <ul>
                    <li>各种标题、段落的格式不统一，比如字体大小、行间距等等各个页面不太一样，然后得用格式刷来挨个刷一下。</li>
                    <li>想给 PPT 做版本控制，然后就保存了各种复制版本，比如“一版”、“二版”、“终版”、“最终版”、“最终不改版”、“最终稳定不改版”等等，想必大家都见过类似这样的场景吧。</li>
                    <li>想插入代码，但是插入之后发现格式全乱了或者高亮全没了，然后不得不截图插入进去。</li>
                    <li>想插入个公式，然后发现 PPT、Keynote 对 Latex 兼容不太好或者配置稍微麻烦，就只能自己重新敲一遍或者贴截图。</li>
                    <li>想插入一个酷炫的交互组件，比如嵌入一个微博的网页页面实时访问、插入一个可以交互的组件、插入一个音乐播放器组件，原生的 PPT 功能几乎都不支持，这全得依赖于 PowerPoint 或者 KeyNote 来支持才行。</li>
                  </ul>
                  <p>如果你遇到这些痛点，那请你一定要看下去。如果你没有遇到，那也请你看下去吧（拜托。</p>
                  <p>好，说回正题，我列举了那么多痛点，那这些痛点咋解决呢？</p>
                  <p>能！甚至解决方案更加轻量级，那就是用 Markdown 来做 PPT！</p>
                  <p>你试过用 Markdown 写 PPT 吗？没有吧，试试吧，试过之后你就发现上面的功能简直易如反掌。</p>
                  <p>具体怎么实现呢？</p>
                  <p>接下来，就有请今天的主角登场了！它就是 Slidev。</p>
                  <h2 id="什么是-Slidev？"><a href="#什么是-Slidev？" class="headerlink" title="什么是 Slidev？"></a>什么是 Slidev？</h2>
                  <p>简而言之，Slidev 就是可以让我们用 Markdown 写 PPT 的工具库，基于 Node.js、Vue.js 开发。</p>
                  <p>利用它我们可以简单地把 Markdown 转化成 PPT，而且它可以支持各种好看的主题、代码高亮、公式、流程图、自定义的网页交互组件，还可以方便地导出成 pdf 或者直接部署成一个网页使用。</p>
                  <p>官方主页：<a href="https://sli.dev/" target="_blank" rel="noopener">https://sli.dev/</a></p>
                  <p>GitHub：<a href="https://github.com/slidevjs/slidev" target="_blank" rel="noopener">https://github.com/slidevjs/slidev</a></p>
                  <h2 id="安装和启动"><a href="#安装和启动" class="headerlink" title="安装和启动"></a>安装和启动</h2>
                  <p>下面我们就来了解下它的基本使用啦。</p>
                  <p>首先我们需要先安装好 Node.js，推荐 14.x 及以上版本，安装方法见 <a href="https://setup.scrape.center/nodejs" target="_blank" rel="noopener">https://setup.scrape.center/nodejs</a>。</p>
                  <p>接着，我们就可以使用 npm 这个命令了。</p>
                  <p>然后我们可以初始化一个仓库，运行命令如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">npm init slidev@latest</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个命令就是初始化一个 Slidev 的仓库，运行之后它会让我们输入和选择一些选项，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/ct067.png" alt=""></p>
                  <p>比如上图就是先输入项目文件夹的名称，比如这里我取名叫做 slidevtest。</p>
                  <p>总之一些选项完成之后，Slidev 会在本地 3000 端口上启动，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/llmmz.png" alt=""></p>
                  <p>接着，我们就可以打开浏览器 <a href="http://localhost:3000" target="_blank" rel="noopener">http://localhost:3000</a> 来查看一个 HelloWorld 版本的 PPT 了，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/0qyem.png" alt=""></p>
                  <p>我们可以点击空格进行翻页，第二页展示了一张常规的 PPT 的样式，包括标题、正文、列表等，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/3jo6f.png" alt=""></p>
                  <p>那这一页的 Markdown 是什么样的呢？其实就是非常常规的 Markdown 文章的写法，内容如下：</p>
                  <figure class="highlight markdown">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section"># What is Slidev?</span></span><br><span class="line"></span><br><span class="line">Slidev is a slides maker and presenter designed for developers, consist of the following features</span><br><span class="line"></span><br><span class="line"><span class="bullet">- </span>📝 <span class="strong">**Text-based**</span> - focus on the content with Markdown, and then style them later</span><br><span class="line"><span class="bullet">- </span>🎨 <span class="strong">**Themable**</span> - theme can be shared and used with npm packages</span><br><span class="line"><span class="bullet">- </span>🧑‍💻 <span class="strong">**Developer Friendly**</span> - code highlighting, live coding with autocompletion</span><br><span class="line"><span class="bullet">- </span>🤹 <span class="strong">**Interactive**</span> - embedding Vue components to enhance your expressions</span><br><span class="line"><span class="bullet">- </span>🎥 <span class="strong">**Recording**</span> - built-in recording and camera view</span><br><span class="line"><span class="bullet">- </span>📤 <span class="strong">**Portable**</span> - export into PDF, PNGs, or even a hostable SPA</span><br><span class="line"><span class="bullet">- </span>🛠 <span class="strong">**Hackable**</span> - anything possible on a webpage</span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">Read more about [<span class="string">Why Slidev?</span>](<span class="link">https://sli.dev/guide/why</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>是不是？我们只需要用同样格式的 Markdown 语法就可以轻松将其转化为 PPT 了。</p>
                  <h2 id="快捷键操作"><a href="#快捷键操作" class="headerlink" title="快捷键操作"></a>快捷键操作</h2>
                  <p>再下一页介绍了各种快捷键的操作，这个就很常规了，比如点击空格、上下左右键来进行页面切换，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/ipa66.png" alt=""></p>
                  <p>更多快捷键的操作可以看这里的说明：<a href="https://sli.dev/guide/navigation.html" target="_blank" rel="noopener">https://sli.dev/guide/navigation.html</a>，一些简单的快捷键列举如下：</p>
                  <ul>
                    <li>f：切换全屏</li>
                    <li>right / space：下一动画或幻灯片</li>
                    <li>left：上一动画或幻灯片</li>
                    <li>up：上一张幻灯片</li>
                    <li>down：下一张幻灯片</li>
                    <li>o：切换<a href="https://cn.sli.dev/guide/navigation.html#slides-overview" target="_blank" rel="noopener">幻灯片总览</a></li>
                    <li>d：切换暗黑模式</li>
                    <li>g：显示“前往…”</li>
                  </ul>
                  <h2 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h2>
                  <p>接下来就是代码环节了，因为 Markdown 对代码编写非常友好，所以展示自然也不是问题了，比如代码高亮、代码对齐等都是常规操作，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/0kjgi.png" alt=""></p>
                  <p>那左边的代码定义就直接这么写就行了：</p>
                  <figure class="highlight markdown">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section"># Code</span></span><br><span class="line"></span><br><span class="line">Use code snippets and get the highlighting directly![^1]</span><br><span class="line"></span><br><span class="line"><span class="code">```</span>ts &#123;all|2|1-6|9|all&#125;</span><br><span class="line">interface User &#123;</span><br><span class="line">  id: number</span><br><span class="line">  firstName: string</span><br><span class="line">  lastName: string</span><br><span class="line">  role: string</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function updateUser(id: number, update: User) &#123;</span><br><span class="line">  const user = getUser(id)</span><br><span class="line">  const newUser = &#123;...user, ...update&#125;</span><br><span class="line">  saveUser(id, newUser)</span><br><span class="line">&#125;</span><br><span class="line"><span class="code">```</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于是 Markdown，所以我们可以指定是什么语言，比如 TypeScript、Python 等等。</p>
                  <h2 id="网页组件"><a href="#网页组件" class="headerlink" title="网页组件"></a>网页组件</h2>
                  <p>接下来就是非常酷炫的环节了，我们还可以自定义一些网页组件，然后展示出来。</p>
                  <p>比如我们看下面的一张图。左边就呈现了一个数字计数器，点击左侧数字就会减 1，点击右侧数字就会加 1；另外图的右侧还嵌入了一个组件，这里显示了一个推特的消息，通过一个卡片的形式呈现了出来，不仅仅可以看内容，甚至我们还可以点击下方的喜欢、回复、复制等按钮来进行一些交互。</p>
                  <p>这些功能在网页里面并不稀奇，但是如果能做到 PPT 里面，那感觉就挺酷的。</p>
                  <p><img src="https://cdn.cuiqingcai.com/ixf94.png" alt=""></p>
                  <p>那这一页怎么做到的呢？这个其实是引入了一些基于 Vue.js 的组件，本节对应的 Markdown 代码如下：</p>
                  <figure class="highlight markdown">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section"># Components</span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">grid</span>=<span class="string">"~ cols-2 gap-4"</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">You can use Vue components directly inside your slides.</span><br><span class="line"></span><br><span class="line">We have provided a few built-in components like <span class="code">`&lt;Tweet/&gt;`</span> and <span class="code">`&lt;Youtube/&gt;`</span> that you can use directly. And adding your custom components is also super easy.</span><br><span class="line"></span><br><span class="line"><span class="code">```html</span></span><br><span class="line"><span class="code">&lt;Counter :count="10" /&gt;</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- ./components/Counter.vue --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">Counter</span> <span class="attr">:count</span>=<span class="string">"10"</span> <span class="attr">m</span>=<span class="string">"t-4"</span> /&gt;</span></span></span><br><span class="line"></span><br><span class="line">Check out [<span class="string">the guides</span>](<span class="link">https://sli.dev/builtin/components.html</span>) for more.</span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="code">```html</span></span><br><span class="line"><span class="code">&lt;Tweet id="1390115482657726468" /&gt;</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">Tweet</span> <span class="attr">id</span>=<span class="string">"1390115482657726468"</span> <span class="attr">scale</span>=<span class="string">"0.65"</span> /&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们可以看到，这里引入了 Counter、Tweet 组件，而这个 Counter 就是 Vue.js 的组件，代码如下：</p>
                  <figure class="highlight markdown">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">setup</span> <span class="attr">lang</span>=<span class="string">"ts"</span>&gt;</span></span></span><br><span class="line">import &#123; ref &#125; from 'vue'</span><br><span class="line"></span><br><span class="line">const props = defineProps(&#123;</span><br><span class="line">  count: &#123;</span><br><span class="line"><span class="code">    default: 0,</span></span><br><span class="line">  &#125;,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">const counter = ref(props.count)</span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">template</span>&gt;</span></span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">flex</span>=<span class="string">"~"</span> <span class="attr">w</span>=<span class="string">"min"</span> <span class="attr">border</span>=<span class="string">"~ gray-400 opacity-50 rounded-md"</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;button</span></span><br><span class="line"><span class="code">      border="r gray-400 opacity-50"</span></span><br><span class="line"><span class="code">      p="2"</span></span><br><span class="line"><span class="code">      font="mono"</span></span><br><span class="line"><span class="code">      outline="!none"</span></span><br><span class="line"><span class="code">      hover:bg="gray-400 opacity-20"</span></span><br><span class="line"><span class="code">      @click="counter -= 1"</span></span><br><span class="line"><span class="code">    &gt;</span></span><br><span class="line"><span class="code">      -</span></span><br><span class="line"><span class="code">    &lt;/button&gt;</span></span><br><span class="line"><span class="code">    &lt;span m="auto" p="2"&gt;&#123;&#123; counter &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="code">    &lt;button</span></span><br><span class="line"><span class="code">      border="l gray-400 opacity-50"</span></span><br><span class="line"><span class="code">      p="2"</span></span><br><span class="line"><span class="code">      font="mono"</span></span><br><span class="line"><span class="code">      outline="!none"</span></span><br><span class="line"><span class="code">      hover:bg="gray-400 opacity-20"</span></span><br><span class="line"><span class="code">      @click="counter += 1"</span></span><br><span class="line"><span class="code">    &gt;</span></span><br><span class="line"><span class="code">      +</span></span><br><span class="line"><span class="code">    &lt;/button&gt;</span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">template</span>&gt;</span></span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是一个标准的基于 Vue.js 3.x 的组件，都是标准的 Vue.js 语法，所以如果我们要添加想要的组件，直接自己写就行了，什么都能实现，只要网页能支持的，统统都能写！</p>
                  <h2 id="主题定义"><a href="#主题定义" class="headerlink" title="主题定义"></a>主题定义</h2>
                  <p>当然，一些主题定制也是非常方便的，我们可以在 Markdown 文件直接更改一些配置就好了，比如就把 theme 换个名字，整个主题样式就变了，看如下的对比图：</p>
                  <p><img src="https://cdn.cuiqingcai.com/7p7ud.png" alt=""></p>
                  <p>上面就是一些内置主题，当然我们也可以去官方文档查看一些别人已经写好的主题，见：<a href="https://sli.dev/themes/gallery.html" target="_blank" rel="noopener">https://sli.dev/themes/gallery.html</a>。</p>
                  <p>另外我们自己写主题也是可以的，所有的主题样式都可以通过 CSS 等配置好，想要什么就可以有什么，见：<a href="https://sli.dev/themes/write-a-theme.html" target="_blank" rel="noopener">https://sli.dev/themes/write-a-theme.html。</a></p>
                  <h2 id="公式和图表"><a href="#公式和图表" class="headerlink" title="公式和图表"></a>公式和图表</h2>
                  <p>接下来就是一个非常强大实用的功能，公式和图表，支持 Latex、流程图，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/d1sju.png" alt=""></p>
                  <p><img src="https://cdn.cuiqingcai.com/vs2i9.png" alt=""></p>
                  <p>比如上面的 Latex 的源代码就是这样的：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Inline $\sqrt&#123;3x-1&#125;+(1+x)^2$</span><br><span class="line"></span><br><span class="line">Block</span><br><span class="line">$$</span><br><span class="line">\begin&#123;array&#125;&#123;c&#125;</span><br><span class="line"></span><br><span class="line">\nabla \times \vec&#123;\mathbf&#123;B&#125;&#125; -\, \frac1c\, \frac&#123;\partial\vec&#123;\mathbf&#123;E&#125;&#125;&#125;&#123;\partial t&#125; &amp;</span><br><span class="line">&#x3D; \frac&#123;4\pi&#125;&#123;c&#125;\vec&#123;\mathbf&#123;j&#125;&#125;    \nabla \cdot \vec&#123;\mathbf&#123;E&#125;&#125; &amp; &#x3D; 4 \pi \rho \\</span><br><span class="line"></span><br><span class="line">\nabla \times \vec&#123;\mathbf&#123;E&#125;&#125;\, +\, \frac1c\, \frac&#123;\partial\vec&#123;\mathbf&#123;B&#125;&#125;&#125;&#123;\partial t&#125; &amp; &#x3D; \vec&#123;\mathbf&#123;0&#125;&#125; \\</span><br><span class="line"></span><br><span class="line">\nabla \cdot \vec&#123;\mathbf&#123;B&#125;&#125; &amp; &#x3D; 0</span><br><span class="line"></span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">$$</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其语法也是和 Latex 一样的。</p>
                  <p>其背后是怎么实现的呢？其实是因为 Slidev 默认集成了 Katex 这个库，见：<a href="https://katex.org/" target="_blank" rel="noopener">https://katex.org/</a>，有了 Katex 的加持，所有公式的显示都不是事。</p>
                  <h2 id="页面分隔"><a href="#页面分隔" class="headerlink" title="页面分隔"></a>页面分隔</h2>
                  <p>有的朋友就好奇了，既然是用 Markdown 写 PPT，那么每一页之间是怎么分割的呢？</p>
                  <p>其实很简单，最常规的，用三条横线分割就好了，比如：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">---</span><br><span class="line">layout: cover</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># 第 1 页</span><br><span class="line"></span><br><span class="line">This is the cover page.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># 第 2 页</span><br><span class="line"></span><br><span class="line">The second page</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当然，除了使用三横线，我们还可以使用更丰富的定义模式，可以给每一页制定一些具体信息，就是使用两层三横线。</p>
                  <p>比如这样：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">---</span><br><span class="line">theme: seriph</span><br><span class="line">layout: cover</span><br><span class="line">background: &#39;https:&#x2F;&#x2F;source.unsplash.com&#x2F;1600x900&#x2F;?nature,water&#39;</span><br><span class="line">---</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>上面这样的配置可以替代三横线，是另一种可以用作页面分隔的写法，借助这种写法我们可以定义更多页面的具体信息。</p>
                  <h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2>
                  <p>当然我们肯定也想给 PPT 添加备注，这个也非常简单，通过注释的形式写到 Markdown 源文件就好了：</p>
                  <figure class="highlight plain">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">---</span><br><span class="line">layout: cover</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># 第 1 页</span><br><span class="line"></span><br><span class="line">This is the cover page.</span><br><span class="line"></span><br><span class="line">&lt;!-- 这是一条备注 --&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里可以看到其实就是用了注释的特定语法。</p>
                  <h2 id="演讲者头像"><a href="#演讲者头像" class="headerlink" title="演讲者头像"></a>演讲者头像</h2>
                  <p>当然还有很多酷炫的功能，比如说，我们在讲 PPT 的时候，可能想同时自己也出镜，Slidev 也可以支持。</p>
                  <p>因为开的是网页，而网页又有捕捉摄像头的功能，所以最终效果可以是这样子：</p>
                  <p><img src="https://cdn.cuiqingcai.com/x3x75.png" alt=""></p>
                  <p>是的没错！右下角就是演讲者的个人头像，它被嵌入到了 PPT 中！是不是非常酷！</p>
                  <h2 id="演讲录制"><a href="#演讲录制" class="headerlink" title="演讲录制"></a>演讲录制</h2>
                  <p>当然，Slidev 还支持演讲录制功能，因为它背后集成了 WebRTC 和 RecordRTC 的 API，一些录制配置如下所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/v1y0a.png" alt=""></p>
                  <p>所以，演讲过程的录制完全不是问题。</p>
                  <p>具体的操作可以查看：<a href="https://sli.dev/guide/recording.html" target="_blank" rel="noopener">https://sli.dev/guide/recording.html</a>。</p>
                  <h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2>
                  <p>当然用 Slidev 写的 PPT 还可以支持部署，因为这毕竟就是一个网页。</p>
                  <p>而且部署非常简单和轻量级，因为这就是一些纯静态的 HTML、JavaScript 文件，我们可以轻松把它部署到 GitHub Pages、Netlify 等站点上。</p>
                  <p><strong>试想这么一个场景：别人在演讲之前还在各种拷贝 PPT，而你打开了一个浏览器直接输入了一个网址，PPT 就出来了，众人惊叹，就问你装不装逼？</strong></p>
                  <p>具体的部署操作可以查看：<a href="https://sli.dev/guide/hosting.html" target="_blank" rel="noopener">https://sli.dev/guide/hosting.html</a></p>
                  <p>让我们看几个别人已经部署好的 PPT，直接网页打开就行了：</p>
                  <ul>
                    <li><a href="https://demo.sli.dev/composable-vue/1" target="_blank" rel="noopener">https://demo.sli.dev/composable-vue</a></li>
                    <li><a href="https://masukin.link/talks/simply-publish-your-package-to-npm/#/1" target="_blank" rel="noopener">https://masukin.link/talks/simply-publish-your-package-to-npm</a></li>
                  </ul>
                  <p>就是这么简单方便。</p>
                  <h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2>
                  <p>什么？你想实现版本控制，那再简单不过了。</p>
                  <p>Markdown 嘛，配合下专业版本管理工具 Git，版本控制再也不是难题。</p>
                  <h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>
                  <p>以上就是对 Slidev 的简单介绍，确实不得不说有些功能真的非常实用，而且我本身特别喜欢 Markdown 和网页开发，所以这个简直对我来说太方便了。</p>
                  <p>如果你感兴趣的话，不妨也来试试吧～</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-01-29 23:50:22" itemprop="dateCreated datePublished" datetime="2022-01-29T23:50:22+08:00">2022-01-29</time>
                </span>
                <span id="/36051.html" class="post-meta-item leancloud_visitors" data-flag-title="用 Markdown 做 PPT，就是这么简单！" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>6.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>6 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36050.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/36050.html" class="post-title-link" itemprop="url">两行代码，给 Python 脚本生成命令行</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>有时候我们会有这样的一个需求：</p>
                  <p>我们定义了一个 Python 的方法，方法接收一些参数，但是调用的时候想将这些参数用命令行暴露出来。</p>
                  <p>比如说这里有个爬取方法：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape</span><span class="params">(url, timeout=<span class="number">10</span>)</span>:</span></span><br><span class="line">    response = requests.get(url, timeout=timeout)</span><br><span class="line">    print(response.text)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了一个 scrape 方法，第一个参数接收 url，即爬取的网址，第二个参数接收 timeout，即指定超时时间。</p>
                  <p>调用的时候我们可能这么调用：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrape(<span class="string">'https:///www.baidu.com'</span>, <span class="number">10</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果我们想改参数换 url，那就得改代码对吧。</p>
                  <p>所以有时候我们就想把这些参数用命令行暴露出来，这时候我们可能就用上了 argparse 等等的库，挨个声明各个参数是干嘛的，非常繁琐，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Scrape Function'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'url'</span>, type=str,</span><br><span class="line">                    help=<span class="string">'an integer for the accumulator'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'timeout'</span>,  type=int,</span><br><span class="line">                    help=<span class="string">'sum the integers (default: find the max)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    scrape(args.url, args.timeout)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们才能顺利地使用命令行来调用这个脚本：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 main.py https://www.baidu.com <span class="number">10</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>是不是感觉非常麻烦？argparse 写起来又臭又长，想想就费劲。</p>
                  <h2 id="Fire"><a href="#Fire" class="headerlink" title="Fire"></a>Fire</h2>
                  <p>但接下来我们要介绍一个库，用它我们只需要两行代码就可以做到如上操作。</p>
                  <p>这个库的名字叫做Fire，它可以快速为某个 Python 方法或者类添加命令行的参数支持。</p>
                  <p>先看看安装方法，使用 pip3 安装即可：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 install fire</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就安装好了。</p>
                  <h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2>
                  <p>下面我们来看几个例子。</p>
                  <h3 id="方法支持"><a href="#方法支持" class="headerlink" title="方法支持"></a>方法支持</h3>
                  <p>第一个代码示例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name=<span class="string">"World"</span>)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> <span class="string">"Hello %s!"</span> % name</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  fire.Fire(hello)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 hello 方法，然后接收一个 name 参数，默认值是 World，接着输出了 Hello 加 name 这个字符串。</p>
                  <p>然后接着我们导入了 fire 这个库，调用它的 Fire 方法并传入 hello 这个方法声明，会发生什么事情呢？</p>
                  <p>我们把这段代码保存为 demo1.py，接着用 Python3 来运行一下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo1.py</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Hello World!</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>看起来并没有什么不同。</p>
                  <p>但我们这时候如果运行如下命令，就可以看到一些神奇的事情了：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo1.py --help</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">NAME</span><br><span class="line">    demo1.py</span><br><span class="line"></span><br><span class="line">SYNOPSIS</span><br><span class="line">    demo1.py &lt;flags&gt;</span><br><span class="line"></span><br><span class="line">FLAGS</span><br><span class="line">    --name=NAME</span><br><span class="line">        Default: <span class="string">'World'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里它将 name 这个参数转化成了命令行的一个可选参数，我们可以通过 <code>—-name</code> 来替换 name 参数。</p>
                  <p>我们来试下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo1.py --name <span class="number">123</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们传入了一个 name 参数是 123，这时候我们就发现运行结果就变成了如下内容：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Hello <span class="number">123</span>!</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>是不是非常方便？我们没有借助 argparse 就轻松完成了命令行参数的支持和替换。</p>
                  <p>那如果我们将 name 这个参数的默认值取消呢？代码改写如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> <span class="string">"Hello %s!"</span> % name</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  fire.Fire(hello)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候重新运行：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo1.py --help</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>就可以看到结果变成了如下内容：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">NAME</span><br><span class="line">    demo1.py</span><br><span class="line"></span><br><span class="line">SYNOPSIS</span><br><span class="line">    demo1.py NAME</span><br><span class="line"></span><br><span class="line">POSITIONAL ARGUMENTS</span><br><span class="line">    NAME</span><br><span class="line"></span><br><span class="line">NOTES</span><br><span class="line">    You can also use flags syntax <span class="keyword">for</span> POSITIONAL ARGUMENTS</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候我们发现 name 这个参数就变成了必传参数，我们必须在命令行里指定这个参数内容，调用就会变成如下命令：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo1.py <span class="number">123</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果还是一样的。</p>
                  <h3 id="类支持"><a href="#类支持" class="headerlink" title="类支持"></a>类支持</h3>
                  <p>当然 fire 这个库不仅仅支持给方法添加命令行的支持，还支持给一个类添加命令行的支持。</p>
                  <p>下面我们再看一个例子：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Calculator</span><span class="params">(object)</span>:</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">double</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * number</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire(Calculator)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们把这个代码保存为 demo2.py，然后运行：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo2.py</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">NAME</span><br><span class="line">    demo2.py</span><br><span class="line"></span><br><span class="line">SYNOPSIS</span><br><span class="line">    demo2.py COMMAND</span><br><span class="line"></span><br><span class="line">COMMANDS</span><br><span class="line">    COMMAND <span class="keyword">is</span> one of the following:</span><br><span class="line"></span><br><span class="line">     double</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里它将 Calculator 这个类中的方法识别出来了，COMMAND 之一就是 double，我们试着调用下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo2.py double</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ERROR: The function received no value <span class="keyword">for</span> the required argument: number</span><br><span class="line">Usage: demo2.py double NUMBER</span><br><span class="line"></span><br><span class="line">For detailed information on this command, run:</span><br><span class="line">  demo2.py double --help</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里就说了，这里必须要指定另外一个参数，叫做 NUMBER，同时这个参数还是必填参数，我们试着加下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 demo2.py double <span class="number">4</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">8</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候就可以达到正确结果了。</p>
                  <p>所以说，综合来看，fire 可以为一个类命令行，每个命令都对应一个方法的名称，同时在后面添加额外的可选或必选参数，加到命令行参数的后面。</p>
                  <h2 id="重新改写"><a href="#重新改写" class="headerlink" title="重新改写"></a>重新改写</h2>
                  <p>最后，让我们回过头来，给我们一开始定义的 scrape 方法添加命令行的参数支持：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape</span><span class="params">(url, timeout=<span class="number">10</span>)</span>:</span></span><br><span class="line">    response = requests.get(url, timeout=timeout)</span><br><span class="line">    print(response.text)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire(scrape)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就可以了！省去了冗长的 argparse 的代码，是不是非常方便？</p>
                  <p>调用就是如下形式：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">NAME</span><br><span class="line">    main.py</span><br><span class="line"></span><br><span class="line">SYNOPSIS</span><br><span class="line">    main.py URL &lt;flags&gt;</span><br><span class="line"></span><br><span class="line">POSITIONAL ARGUMENTS</span><br><span class="line">    URL</span><br><span class="line"></span><br><span class="line">FLAGS</span><br><span class="line">    --timeout=TIMEOUT</span><br><span class="line">        Default: <span class="number">10</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里说了，URL 是必传参数，timeout 是可选参数。</p>
                  <p>最后调用下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 main.py https://www.baidu.com</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就可以轻松将 url 通过命令行传递过去了。</p>
                  <p>当然 timeout 还是可选值，我们可以通过 <code>—-timeout</code> 来指定 timeout 参数：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 main.py https://www.baidu.com --timeout <span class="number">5</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样两个参数就都能顺利赋值了，最后效果就是爬取百度，5 秒超时。</p>
                  <p>怎么样？是不是很方便？大家快用起来吧！</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-01-05 18:20:22" itemprop="dateCreated datePublished" datetime="2022-01-05T18:20:22+08:00">2022-01-05</time>
                </span>
                <span id="/36050.html" class="post-meta-item leancloud_visitors" data-flag-title="两行代码，给 Python 脚本生成命令行" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.2k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36049.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 个人随笔 <i class="label-arrow"></i>
                  </a>
                  <a href="/36049.html" class="post-title-link" itemprop="url">做事要有“笃定”的态度</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>之前跟一个朋友聊天，思考到一点，分享一下。</p>
                  <p>我这个朋友在创业，大家知道，创业都是非常艰难的，几乎是九死一生。</p>
                  <p>他创业一年多了，业务现在也挺好的，一直很有拼劲。</p>
                  <p>我就问他是什么让他一直保持这样持续向前向上的状态的。</p>
                  <p>我们聊了很多，但我发现其中有一个很重要的点就是那种笃定的态度，核心就在于两个字，笃定。</p>
                  <p>这个笃定分为两个方面，一个是做事上的笃定，一个是信念上的笃定。</p>
                  <p>分别说说。</p>
                  <ul>
                    <li>关于做事上的笃定。就是能认定了一件事之后，不会轻言放弃或更改，不能今天干了这一点，第二天就换去干另外一个了，也不能看到一点点困难就立即退缩不干了。他说他历经了一些困难的时刻，但经过反复复盘和思考，不断做到了螺旋式上升，现在经验越来越多，做得也越来越顺。所以在做事的时候，内心要有一种笃定的态度，认准了目标不断进发，虽然说努力了不一定成功，但成功几率一定比不努力或直接放弃大的多多多。</li>
                    <li>关于信念上的笃定。就是能够在心中有一种信念，认定做某件事是有意义的有价值的，不会轻易被动摇。现在很多人做事都是，听到别人说做什么做什么挺赚钱，那就跟风去做了。或者做一个产品，他觉得会有市场需求有用户去用，完全迎合用户，但到头来发现是众口难调。真正信念上的笃定是一个人觉得做这件事是真正有意义的，有价值的，那就够了，不会被轻易动摇，不会一味迎合他人。只要他认定的正确的事，那就去做，这就是信念上的笃定。</li>
                  </ul>
                  <p>当然可能有人会反驳了，这么笃定，都不听取别人的意见吗？一味向前冲不是死脑筋吗？这里要解释一下。这里说的笃定是一种广义上的笃定，这种态度信念是要有的，是一种大目标，总体的态度是要这样，但不代表不知变通或者死脑筋。在做具体的事情的时候，肯定还是反思、复盘，去思考什么才是达成大目标的方案，达成大目标的一步步小目标是怎样的，不断提升和完善的过程中慢慢地去接近笃定的大目标，这才是正确的。</p>
                  <p>与君共勉。</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-01-03 13:20:22" itemprop="dateCreated datePublished" datetime="2022-01-03T13:20:22+08:00">2022-01-03</time>
                </span>
                <span id="/36049.html" class="post-meta-item leancloud_visitors" data-flag-title="做事要有“笃定”的态度" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>803</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36048.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/36048.html" class="post-title-link" itemprop="url">在浏览器里面运行命令行，真香！</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>上一篇我写过一篇《<a href="https://cuiqingcai.com/36047.html">万物皆可 API</a>》，这个项目就是把一些脚本的执行结果输出到了网页里面。</p>
                  <p>但是这个还是有很多改进空间，比如说 UI 能好看些，甚至能执行交互命令该多好，最后思来想去，它的究极形态不就是一个 Web 版的 Terminal （终端）吗？</p>
                  <p>然后本来我还想着对项目进行改造来着，但是想想，最终如果要改造成一个 Web 版的 Terminal，这个肯定已经有开源实现了。</p>
                  <p>于是我就开始搜，最后搜到几个还不错的。</p>
                  <h2 id="Web-Terminal"><a href="#Web-Terminal" class="headerlink" title="Web Terminal"></a>Web Terminal</h2>
                  <ul>
                    <li>ttyd：<a href="https://github.com/tsl0922/ttyd" target="_blank" rel="noopener">https://github.com/tsl0922/ttyd</a>，一款可以将命令行转到 Web 执行的工具，基于 C 编写的。</li>
                    <li>gotty：<a href="https://github.com/yudai/gotty" target="_blank" rel="noopener">https://github.com/yudai/gotty</a>，和 ttyd 一样，只不过是 Go 语言写的，但最新更新是在 2017 年了，估计失修了。</li>
                    <li>wetty：<a href="https://github.com/butlerx/wetty" target="_blank" rel="noopener">https://github.com/butlerx/wetty</a>，基于 Node.js 开发的，也可以将命令行转到 Web 执行，但是需要基于 SSH 登录，其实就是个 Web 版的 SSH 终端。</li>
                    <li>Secure Shell (Chrome App)：Google 浏览器插件，也可以提供网页版 SSH 终端。</li>
                    <li>tmate：<a href="https://tmate.io/" target="_blank" rel="noopener">https://tmate.io/</a>，从 tmux 修改而来，可以支持 Terminal 分享。</li>
                  </ul>
                  <p>经过一番试用，我个人首推的还是 ttyd，其他的几个要么是基于 SSH 的，要么不怎么好用或停止维护了。</p>
                  <p>下面我就来介绍下 ttyd 的简单用法。</p>
                  <h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2>
                  <p>安装其实非常简单，我用的是 Mac，所以用 HomeBrew 直接安装即可：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">brew install ttyd</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果你用的是 Windows、Linux，依然也可以支持，安装可以参考 <a href="https://github.com/tsl0922/ttyd#installation" target="_blank" rel="noopener">https://github.com/tsl0922/ttyd#installation</a> 章节。</p>
                  <h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2>
                  <p>ttyd 支持不少功能配置，完整命令如下：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ttyd is a tool <span class="keyword">for</span> sharing terminal over the web</span><br><span class="line"></span><br><span class="line">USAGE:</span><br><span class="line">    ttyd [<span class="type">options</span>] &lt;command&gt; [&lt;<span class="type">arguments...</span>&gt;]</span><br><span class="line"></span><br><span class="line">VERSION:</span><br><span class="line">    <span class="number">1.6</span>.<span class="number">3</span></span><br><span class="line"></span><br><span class="line">OPTIONS:</span><br><span class="line">    <span class="literal">-p</span>, -<span class="literal">-port</span>              Port to listen (default: <span class="number">7681</span>, use `0` <span class="keyword">for</span> random port)</span><br><span class="line">    <span class="literal">-i</span>, -<span class="literal">-interface</span>         Network interface to bind (eg: eth0), or UNIX domain socket path (eg: /var/run/ttyd.sock)</span><br><span class="line">    <span class="literal">-c</span>, -<span class="literal">-credential</span>        Credential <span class="keyword">for</span> Basic Authentication (format: username:password)</span><br><span class="line">    <span class="literal">-u</span>, -<span class="literal">-uid</span>               User id to run with</span><br><span class="line">    <span class="literal">-g</span>, -<span class="literal">-gid</span>               Group id to run with</span><br><span class="line">    <span class="literal">-s</span>, -<span class="literal">-signal</span>            Signal to send to the command when <span class="keyword">exit</span> it (default: <span class="number">1</span>, SIGHUP)</span><br><span class="line">    <span class="literal">-a</span>, -<span class="literal">-url</span><span class="literal">-arg</span>           Allow client to send command line arguments <span class="keyword">in</span> URL (eg: http://localhost:<span class="number">7681</span>?arg=foo&amp;arg=bar)</span><br><span class="line">    <span class="literal">-R</span>, -<span class="literal">-readonly</span>          <span class="keyword">Do</span> not allow clients to write to the TTY</span><br><span class="line">    <span class="literal">-t</span>, -<span class="literal">-client</span><span class="literal">-option</span>     Send option to client (format: key=value), repeat to add more options</span><br><span class="line">    <span class="literal">-T</span>, -<span class="literal">-terminal</span><span class="literal">-type</span>     Terminal type to report, default: xterm<span class="literal">-256color</span></span><br><span class="line">    <span class="literal">-O</span>, -<span class="literal">-check</span><span class="literal">-origin</span>      <span class="keyword">Do</span> not allow websocket connection from different origin</span><br><span class="line">    <span class="literal">-m</span>, -<span class="literal">-max</span><span class="literal">-clients</span>       Maximum clients to support (default: <span class="number">0</span>, no limit)</span><br><span class="line">    <span class="literal">-o</span>, -<span class="literal">-once</span>              Accept only one client and <span class="keyword">exit</span> on disconnection</span><br><span class="line">    <span class="literal">-B</span>, -<span class="literal">-browser</span>           Open terminal with the default system browser</span><br><span class="line">    <span class="literal">-I</span>, -<span class="literal">-index</span>             Custom index.html path</span><br><span class="line">    <span class="literal">-b</span>, -<span class="literal">-base</span><span class="literal">-path</span>         Expected base path <span class="keyword">for</span> requests coming from a reverse proxy (eg: /mounted/here)</span><br><span class="line">    <span class="literal">-P</span>, -<span class="literal">-ping</span><span class="literal">-interval</span>     Websocket ping interval(sec) (default: <span class="number">300</span>)</span><br><span class="line">    <span class="literal">-6</span>, -<span class="literal">-ipv6</span>              Enable IPv6 support</span><br><span class="line">    <span class="literal">-S</span>, -<span class="literal">-ssl</span>               Enable SSL</span><br><span class="line">    <span class="literal">-C</span>, -<span class="literal">-ssl</span><span class="literal">-cert</span>          SSL certificate file path</span><br><span class="line">    <span class="literal">-K</span>, -<span class="literal">-ssl</span><span class="literal">-key</span>           SSL key file path</span><br><span class="line">    <span class="literal">-A</span>, -<span class="literal">-ssl</span><span class="literal">-ca</span>            SSL CA file path <span class="keyword">for</span> client certificate verification</span><br><span class="line">    <span class="literal">-d</span>, -<span class="literal">-debug</span>             Set log level (default: <span class="number">7</span>)</span><br><span class="line">    <span class="literal">-v</span>, -<span class="literal">-version</span>           Print the version and <span class="keyword">exit</span></span><br><span class="line">    <span class="literal">-h</span>, -<span class="literal">-help</span>              Print this text and <span class="keyword">exit</span></span><br><span class="line"></span><br><span class="line">Visit https://github.com/tsl0922/ttyd to get more information and report bugs.</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到，这里可以使用 -p 来指定运行端口，使用 -c 指定登录密码等等。</p>
                  <h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3>
                  <p>我们来试下，最基本的命令如下：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ttyd bash</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就使用启动了一个 Web 版的 bash，运行结果如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/addfi.png" alt=""></p>
                  <p>这里显示是在 7681 上运行的，那我们就可以打开 <a href="http://localhost:7681/" target="_blank" rel="noopener">http://localhost:7681/</a>，就可以直接运行命令了：</p>
                  <p><img src="https://cdn.cuiqingcai.com/esx37.png" alt=""></p>
                  <p>非常丝滑。</p>
                  <p>看了下背后的传输协议是 WebSocket，所以稳定性还是有保障的：</p>
                  <p><img src="https://cdn.cuiqingcai.com/54o5p.png" alt=""></p>
                  <p>当然，我们也可以不用 bash，用自己喜欢的 Shell，比如 zsh，命令如下：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ttyd zsh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的话浏览器里面的 Shell 就是 zsh 啦：</p>
                  <p><img src="https://cdn.cuiqingcai.com/b0f9k.png" alt=""></p>
                  <h3 id="绑定端口"><a href="#绑定端口" class="headerlink" title="绑定端口"></a>绑定端口</h3>
                  <p>当然我们也可以更换端口，比如 8000，则可以使用如下命令：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ttyd <span class="literal">-p</span> <span class="number">8000</span> zsh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样 ttyd 就可以在 8000 端口运行 HTTP 服务，我们打开 <a href="http://localhost:8000/" target="_blank" rel="noopener">http://localhost:8000/</a> 就可以执行命令了。</p>
                  <h3 id="Basic-Auth"><a href="#Basic-Auth" class="headerlink" title="Basic Auth"></a>Basic Auth</h3>
                  <p>当然这么直接暴露出去似乎也不太安全，我们可以设置 Basic Auth，使用 -c 这个选项即可指定用户名密码，格式为 <code>username:password</code>，例如我们指定用户名和密码都是 admin，那命令就这么写：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ttyd <span class="literal">-p</span> <span class="number">8000</span> <span class="literal">-c</span> admin:admin zsh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样打开 <a href="http://localhost:8000/" target="_blank" rel="noopener">http://localhost:8000/</a> 之后就需要输入用户名密码才可以登录了：</p>
                  <p><img src="https://cdn.cuiqingcai.com/6mu2o.png" alt=""></p>
                  <h3 id="自动打开浏览器"><a href="#自动打开浏览器" class="headerlink" title="自动打开浏览器"></a>自动打开浏览器</h3>
                  <p>我们还可以使用 -B 命令让它自动打开浏览器：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ttyd <span class="literal">-p</span> <span class="number">8000</span> <span class="literal">-B</span> zsh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样运行之后，默认的浏览器就会自动打开 <a href="http://localhost:8000/" target="_blank" rel="noopener">http://localhost:8000/</a>，不用我们再去敲网址了，十分方便。</p>
                  <p>所以，上面这个命令甚至我们还可以做成一个 alias，比如：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">alias webcmd=<span class="string">"ttyd -p 8000 -B zsh"</span>;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样输入 <code>webcmd</code> 就可以轻松打开一个 Web 版命令行了。</p>
                  <h3 id="Docker-支持"><a href="#Docker-支持" class="headerlink" title="Docker 支持"></a>Docker 支持</h3>
                  <p>另外 ttyd 还提供了 Docker 镜像，如果你不想安装的话，可以直接启 Docker，比如这样的话就可以在 7681 上启动：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker run <span class="literal">-it</span> -<span class="literal">-rm</span> <span class="literal">-p</span> <span class="number">7681</span>:<span class="number">7681</span> tsl0922/ttyd</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>但这实际上是把容器内部的命令行暴露出来了，如果要暴露宿主机的命令行还需要 mount 下磁盘：</p>
                  <h3 id="SSH-终端"><a href="#SSH-终端" class="headerlink" title="SSH 终端"></a>SSH 终端</h3>
                  <p>ttyd 还支持 SSH 终端，命令如下：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ttyd login</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的话，打开浏览器之后就需要 SSH 登录，输入正确的 SSH 用户名和密码后才能使用。</p>
                  <h3 id="SSL-支持"><a href="#SSL-支持" class="headerlink" title="SSL 支持"></a>SSL 支持</h3>
                  <p>如果你想配置 SSL 支持，即支持 HTTPS 的话，可以自己生成证书并添加对应的参数来启动 ttyd，参考链接是：<a href="https://github.com/tsl0922/ttyd/wiki/SSL-Usage" target="_blank" rel="noopener">https://github.com/tsl0922/ttyd/wiki/SSL-Usage</a>。</p>
                  <h3 id="更多"><a href="#更多" class="headerlink" title="更多"></a>更多</h3>
                  <p>上面的用法基本能满足日常需要了，如果想要了解更多用法，可以参考其 Wiki，链接是：<a href="https://github.com/tsl0922/ttyd/wiki/Example-Usage" target="_blank" rel="noopener">https://github.com/tsl0922/ttyd/wiki/Example-Usage</a>。</p>
                  <h2 id="公网暴露"><a href="#公网暴露" class="headerlink" title="公网暴露"></a>公网暴露</h2>
                  <p>当然，我们如果想把它公网暴露出来，还可以配合 Ngrok，比如 ttyd 运行在 8000 端口上，我可以使用 Ngrok 将其暴露出来：</p>
                  <figure class="highlight powershell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">ngrok http <span class="number">8000</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/pi1ri.png" alt=""></p>
                  <p>这样我就可以通过指定的 URL 访问这个终端了，比如这里我就可以使用 <a href="https://11b4-2404-f801-8050-3-bf-00-55.ngrok.io/" target="_blank" rel="noopener">https://11b4-2404-f801-8050-3-bf-00-55.ngrok.io/</a> 来访问我的终端了：</p>
                  <p><img src="https://cdn.cuiqingcai.com/sc0lx.png" alt=""></p>
                  <p>非常 Nice！</p>
                  <h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>
                  <p>好了，以上就是 ttyd 的基本使用了，有了它，我们就可以轻松将某台机器上的终端转到 Web 上来执行了，还是非常方便有用的。</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-01-03 01:50:22" itemprop="dateCreated datePublished" datetime="2022-01-03T01:50:22+08:00">2022-01-03</time>
                </span>
                <span id="/36048.html" class="post-meta-item leancloud_visitors" data-flag-title="在浏览器里面运行命令行，真香！" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36047.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/36047.html" class="post-title-link" itemprop="url">万物皆可 API</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>今天看到一个开源项目，叫做 Command2API，感觉挺有意思的，分享给大家。</p>
                  <h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2>
                  <p>关于这个项目为什么诞生，原 Repo 有这么一段：</p>
                  <blockquote>
                    <p>以近期 Log4j 的 RCE 举例，在内网的安全测试中，由于网络环境限制导致没有 DNSLog 平台可用，这时候做 Log4j 的漏洞验证就考虑直接查看 LDAP 服务是否有连接进来，但是现成的 JNDI 注入工具开启服务并没有 API 可以直接拉取对应服务的结果，这就导致需要人工去查看，很费时间，再加上已经写好 BurpSuite 被动插件进行扫描了，为了节省时间就简单写了这个脚本用于获取 JNDI 工具的执行结果并通过 API 的形式返回，便于插件拉取结果进行漏洞验证。</p>
                  </blockquote>
                  <p>反正大意就是说，有些命令的执行结果如果能够通过 HTTP 的 API 暴露出来，我们就能更方便地获取到命令的执行结果，在某些场景下会非常方便。</p>
                  <p>所以，这里作者写了这个项目。</p>
                  <h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2>
                  <p>这个原理其实非常简单，就是用一个 Python 线程开启 Web 服务，一个线程执行命令，通过全局变量与 Web 服务共享执行命令的结果。</p>
                  <h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2>
                  <p>这里我们来运行下看看效果吧。</p>
                  <p>首先需要下载下项目：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">git clone https://github.com/gh0stkey/Command2API.git</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后接着指定想运行的命令和 API 运行的端口就好了，样例如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python Command2Api.py <span class="string">"执行的命令"</span> Web运行的端口</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <blockquote>
                    <p>注意，这里的 python 使用的 Python2，而不是 Python3，因为原项目引用了一个包叫 BaseHTTPServer，Python3 是没有的。</p>
                  </blockquote>
                  <p>这里我们执行一个 ping 命令来试试：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python Command2Api.py <span class="string">"ping www.baidu.com"</span> <span class="number">8888</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/1sspx.png" alt=""></p>
                  <p>可以看到，这里首先输出了一个运行的地址：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">URL: http://HOST:<span class="number">8888</span>/c1IvlLF9</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这时候我们打开 <a href="http://localhost:8888/c1IvlLF9" target="_blank" rel="noopener">http://localhost:8888/c1IvlLF9</a> 看下。</p>
                  <p><img src="https://cdn.cuiqingcai.com/kcl2f.png" alt=""></p>
                  <p>可以看到控制台结果就呈现在网页里面了。</p>
                  <blockquote>
                    <p>但是这个页面没法自动刷新，需要点击刷新来获取最新的结果。</p>
                  </blockquote>
                  <p>介绍完了。</p>
                  <p>所以，这个项目在某些情况下还是挺有用的。</p>
                  <p>比如说：</p>
                  <ul>
                    <li>内网安全测试中，可以用于获取 JNDI 工具的执行结果并通过 API 的形式返回，可以更方便地观测执行结果。</li>
                    <li>我们想监控或实时获取某个命令行程序的输出结果，比如 Scrapy 爬虫、比如 Web Server 等等，可以将其暴露出来。</li>
                    <li>我们想快速分享某个程序的执行结果，则可以通过这个命令配合 Ngrok 生成一个网站分享出去。</li>
                  </ul>
                  <p>等等。</p>
                  <h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2>
                  <p>我们再来看看源码吧，其实非常简单，一共就这些代码：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> BaseHTTPServer</span><br><span class="line"><span class="keyword">import</span> SimpleHTTPServer</span><br><span class="line"><span class="keyword">import</span> cgi</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">l = []</span><br><span class="line"></span><br><span class="line">uri = <span class="string">'/'</span> + <span class="string">''</span>.join(random.sample(string.ascii_letters+string.digits,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">thread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, threadname, command)</span>:</span></span><br><span class="line">    threading.Thread.__init__(self, name=<span class="string">'Thread_'</span> + threadname)</span><br><span class="line">    self.threadname = int(threadname)</span><br><span class="line">    self.command = command</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> l</span><br><span class="line">    ret = subprocess.Popen(</span><br><span class="line">      self.command,</span><br><span class="line">      shell=<span class="literal">True</span>,</span><br><span class="line">      stdin=subprocess.PIPE,</span><br><span class="line">      stdout=subprocess.PIPE,</span><br><span class="line">      stderr=subprocess.PIPE</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> iter(ret.stdout.readline, <span class="string">b""</span>):</span><br><span class="line">      res = i.decode().strip()</span><br><span class="line">      print(res)</span><br><span class="line">      l.append(res)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ServerHandler</span><span class="params">(SimpleHTTPServer.SimpleHTTPRequestHandler)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">do_GET</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> l</span><br><span class="line">    <span class="keyword">if</span> self.path == uri:</span><br><span class="line">      self.send_response(<span class="number">200</span>)</span><br><span class="line">      self.send_header(<span class="string">'Content-Type'</span>, <span class="string">'text/plain'</span>)</span><br><span class="line">      self.end_headers()</span><br><span class="line">      self.wfile.write(l)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  <span class="comment"># New Thread: Get Command Result</span></span><br><span class="line">  t1 = thread(<span class="string">'1'</span>, sys.argv[<span class="number">1</span>])</span><br><span class="line">  t1.start()</span><br><span class="line">  <span class="comment"># Webserver</span></span><br><span class="line">  port = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">  print(<span class="string">"URL: http://HOST:&#123;0&#125;&#123;1&#125;"</span>.format(port, uri))</span><br><span class="line">  Handler = ServerHandler</span><br><span class="line">  httpd = BaseHTTPServer.HTTPServer((<span class="string">'0.0.0.0'</span>, port), Handler)</span><br><span class="line">  httpd.serve_forever()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到这个命令就是 Popen 执行的，然后通过 PIPE 将结果捕获出来赋值为变量，然后同时另外一个线程启动服务器，将这个结果写入到 Response 里面。</p>
                  <p>就是这么简单的代码，实现了如此便捷的功能。</p>
                  <h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2>
                  <p>不过我看这个项目还是有很多优化空间的，简单总结下：</p>
                  <ul>
                    <li>现在支持的是 Python2 而不是 Python3。</li>
                    <li>网页结果不能自动刷新。</li>
                    <li>网页结果是一个列表，和控制台的结果格式不太统一。</li>
                    <li>不能通过 pip 来安全这个工具包。</li>
                    <li>输出结果的 HOST 可以优化一下，直接复制出来不好访问。</li>
                    <li>可以配合 Ngrok 将结果进行公开暴露。</li>
                    <li>如果能通过网页来对命令进行交互控制就更好了。</li>
                  </ul>
                  <p>我看看如果有时间的话，我可以试着将这个项目改写下并实现如上的一些优化功能哈，到时候写完了发出来。</p>
                  <p>谢谢阅读～</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2022-01-02 19:50:22" itemprop="dateCreated datePublished" datetime="2022-01-02T19:50:22+08:00">2022-01-02</time>
                </span>
                <span id="/36047.html" class="post-meta-item leancloud_visitors" data-flag-title="万物皆可 API" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>2.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36044.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 深度学习 <i class="label-arrow"></i>
                  </a>
                  <a href="/36044.html" class="post-title-link" itemprop="url">深度学习识别滑动验证码</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>本节我们就来了解下使用深度学习识别滑动验证码的方法。</p>
                  <h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2>
                  <p>我们这次主要侧重于完成利用深度学习模型来识别验证码缺口的过程，所以不会侧重于讲解深度学习模型的算法，另外由于整个模型实现较为复杂，本节也不会从零开始编写代码，而是倾向于把代码提前下载下来进行实操练习。</p>
                  <p>所以在最后，请提前代码下载下来，仓库地址为：<a href="https://github.com/Python3WebSpider/DeepLearningSlideCaptcha2，利用" target="_blank" rel="noopener">https://github.com/Python3WebSpider/DeepLearningSlideCaptcha2，利用</a> Git 把它克隆下来：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/Python3WebSpider/DeepLearningSlideCaptcha2.git</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行完毕之后，本地就会出现一个 DeepLearningImageCaptcha2 的文件夹，就证明克隆成功了。</p>
                  <p>克隆完毕之后，请切换到 DeepLearningImageCaptcha2 文件夹，安装必要的依赖库：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> -r requirements.txt</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行完毕之后，本项目运行所需要的依赖库就全部安装好了。</p>
                  <p>以上准备工作都完成之后，那就让我们就开始本节正式的学习吧。</p>
                  <h2 id="2-目标检测"><a href="#2-目标检测" class="headerlink" title="2. 目标检测"></a>2. 目标检测</h2>
                  <p>识别滑动验证码缺口的这个问题，其实可以归结为目标检测问题。那什么叫目标检测呢？在这里简单作下介绍。</p>
                  <p>目标检测，顾名思义，就是把我们想找的东西找出来。比如给一张「狗」的图片，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/4fre2.png" alt="image-20191107024841075"></p>
                  <p>我们想知道这只狗在哪，它的舌头在哪，找到了就把它们框选出来，这就是目标检测。</p>
                  <p>经过目标检测算法处理之后，我们期望得到的图片是这样的：</p>
                  <p><img src="https://cdn.cuiqingcai.com/i867f.png" alt="image-20191107025008947"></p>
                  <p>可以看到这只狗和它的舌头就被框选出来了，这就完成了一个不错的目标检测。</p>
                  <p>现在比较流行的目标检测算法有 R-CNN、Fast R-CNN、Faster R-CNN、SSD、YOLO 等，感兴趣可以了解一下，当然不太了解对本节要完成的目标也没有什么影响。</p>
                  <p>当前做目标检测的算法主要有两种方法，有一阶段式和两阶段式，英文叫做 One stage 和 Two stage，简述如下：</p>
                  <ul>
                    <li>Two Stage：算法首先生成一系列目标所在位置的候选框，然后再对这些框选出来的结果进行样本分类，即先找出来在哪，然后再分出来是啥，俗话说叫「看两眼」，这种算法有 R-CNN、Fast R-CNN、Faster R-CNN 等，这些算法架构相对复杂，但准确率上有优势。</li>
                    <li>One Stage：不需要产生候选框，直接将目标定位和分类的问题转化为回归问题，俗话说叫「看一眼」，这种算法有 YOLO、SSD，这些算法虽然准确率上不及 Two stage，但架构相对简单，检测速度更快。</li>
                  </ul>
                  <p>所以这次我们选用 One Stage 的有代表性的目标检测算法 YOLO 来实现滑动验证码缺口的识别。</p>
                  <p>YOLO，英文全称叫做 You Only Look Once，取了它们的首字母就构成了算法名，</p>
                  <p>目前 YOLO 算法最新的版本是 V5 版本，应用比较广泛的是 V3 版本，这里算法的具体流程我们就不过多介绍了，感兴趣的可以搜一下相关资料了解下，另外也可以了解下 YOLO V1-V3 版本的不同和改进之处，这里列几个参考链接：</p>
                  <ul>
                    <li>YOLO V3 论文：<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></li>
                    <li>YOLO V3 介绍：<a href="https://zhuanlan.zhihu.com/p/34997279" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34997279</a></li>
                    <li>YOLO V1-V3 对比介绍：<a href="https://www.cnblogs.com/makefile/p/yolov3.html" target="_blank" rel="noopener">https://www.cnblogs.com/makefile/p/yolov3.html</a></li>
                  </ul>
                  <h2 id="3-数据准备"><a href="#3-数据准备" class="headerlink" title="3. 数据准备"></a>3. 数据准备</h2>
                  <p>像上一节介绍的一样，要训练深度学习模型也需要准备训练数据，数据也是分为两部分，一部分是验证码图像，另一部分是数据标注，即缺口的位置。但和上一节不一样的是，这次标注不再是单纯的验证码文本了，因为这次我们需要表示的是缺口的位置，缺口对应的是一个矩形框，要表示一个矩形框，至少需要四个数据，如左上角点的横纵坐标 x、y，矩形的宽高 w、h，所以标注数据就变成了四个数字。</p>
                  <p>所以，接下来我们就需要准备一些验证码图片和对应的四位数字的标注了，比如下图的滑动验证码：</p>
                  <p><img src="https://cdn.cuiqingcai.com/9s7ms.png" alt=""></p>
                  <p>好，那接下来我们就完成这两步吧，第一步就是收集验证码图片，第二步就是标注缺口的位置并转为我们想要的四位数字。</p>
                  <p>在这里我们的示例网站是 <a href="https://captcha1.scrape.center/，打开之后点击登录按钮便会弹出一个滑动验证码，如图所示：" target="_blank" rel="noopener">https://captcha1.scrape.center/，打开之后点击登录按钮便会弹出一个滑动验证码，如图所示：</a></p>
                  <p><img src="https://cdn.cuiqingcai.com/b0tqg.png" alt="image-20210504182925384"></p>
                  <p>我们需要做的就是单独将滑动验证码的图像保存下来，也就是这个区域：</p>
                  <p><img src="https://cdn.cuiqingcai.com/3ihk2.png" alt="image-20210504183039997"></p>
                  <p>怎么做呢？靠手工截图肯定不太可靠，费时费力，而且不好准确定位边界，会导致存下来的图片有大有小。为了解决这个问题，我们可以简单写一个脚本来实现下自动化裁切和保存，就是仓库中的 collect.py 文件，代码如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> WebDriverException</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"></span><br><span class="line">COUNT = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, COUNT + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        browser = webdriver.Chrome()</span><br><span class="line">        wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">        browser.get(<span class="string">'https://captcha1.scrape.center/'</span>)</span><br><span class="line">        button = wait.until(EC.element_to_be_clickable(</span><br><span class="line">            (By.CSS_SELECTOR, <span class="string">'.el-button'</span>)))</span><br><span class="line">        button.click()</span><br><span class="line">        captcha = wait.until(</span><br><span class="line">            EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'.geetest_slicebg.geetest_absolute'</span>)))</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        captcha.screenshot(<span class="string">f'data/captcha/images/captcha_<span class="subst">&#123;i&#125;</span>.png'</span>)</span><br><span class="line">    <span class="keyword">except</span> WebDriverException <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">f'webdriver error occurred <span class="subst">&#123;e.msg&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        browser.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们先定义了一个循环，循环次数为 COUNT 次，每次循环都使用 Selenium 启动一个浏览器，然后打开目标网站，模拟点击登录按钮触发验证码弹出，然后截取验证码对应的节点，再用 screenshot 方法将其保存下来。</p>
                  <p>我们将其运行：</p>
                  <figure class="highlight vim">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">python3</span> collect.<span class="keyword">py</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行完了之后我们就可以在 <code>data/captcha/images/</code> 目录获得很多验证码图片了，样例如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/plwov.png" alt="image-20210504194022826"></p>
                  <p>获得验证码图片之后，我们就需要进行数据标注了，这里推荐的工具是 labelImg，GitHub 地址为 <a href="https://github.com/tzutalin/labelImg，使用" target="_blank" rel="noopener">https://github.com/tzutalin/labelImg，使用</a> pip3 安装即可：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> labelImg</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>安装完成之后可以直接命令行运行：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">labelImg</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就成功启动了 labelImg：</p>
                  <p><img src="https://cdn.cuiqingcai.com/sagzl.png" alt="image-20210504194644729"></p>
                  <p>点击 <code>Open Dir</code> 打开 <code>data/captcha/images/</code> 目录，然后点击左下角的 <code>Create RectBox</code> 创建一个标注框，我们可以将缺口所在的矩形框框选出来，框选完毕之后 labelImg 就会提示保存一个名称，我们将其命名为 target，然后点击 OK，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/1eqpy.png" alt="image-20210504194608969"></p>
                  <p>这时候我们可以发现其保存了一个 xml 文件，内容如下：</p>
                  <figure class="highlight xml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="tag">&lt;<span class="name">annotation</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">folder</span>&gt;</span>images<span class="tag">&lt;/<span class="name">folder</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">filename</span>&gt;</span>captcha_0.png<span class="tag">&lt;/<span class="name">filename</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">path</span>&gt;</span>data/captcha/images/captcha_0.png<span class="tag">&lt;/<span class="name">path</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">source</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">database</span>&gt;</span>Unknown<span class="tag">&lt;/<span class="name">database</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">size</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">width</span>&gt;</span>520<span class="tag">&lt;/<span class="name">width</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">height</span>&gt;</span>320<span class="tag">&lt;/<span class="name">height</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">depth</span>&gt;</span>3<span class="tag">&lt;/<span class="name">depth</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">size</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">segmented</span>&gt;</span>0<span class="tag">&lt;/<span class="name">segmented</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">object</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>target<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">pose</span>&gt;</span>Unspecified<span class="tag">&lt;/<span class="name">pose</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">truncated</span>&gt;</span>0<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">difficult</span>&gt;</span>0<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">xmin</span>&gt;</span>321<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">ymin</span>&gt;</span>87<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">xmax</span>&gt;</span>407<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">ymax</span>&gt;</span>167<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">object</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中可以看到 size 节点里有三个节点，分别是 width、height、depth，分别代表原验证码图片的宽度、高度、通道数。另外 object 节点下的 bndbox 节点就包含了标注缺口的位置，通过观察对比可以知道 xmin、ymin 指的就是左上角的坐标，xmax、ymax 指的就是右下角的坐标。</p>
                  <p>我们可以用下面的方法简单进行下数据处理：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">import</span> xmltodict</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_xml</span><span class="params">(file)</span>:</span></span><br><span class="line">    xml_str = open(file, encoding=<span class="string">'utf-8'</span>).read()</span><br><span class="line">    data = xmltodict.parse(xml_str)</span><br><span class="line">    data = json.loads(json.dumps(data))</span><br><span class="line">    annoatation = data.get(<span class="string">'annotation'</span>)</span><br><span class="line">    width = int(annoatation.get(<span class="string">'size'</span>).get(<span class="string">'width'</span>))</span><br><span class="line">    height = int(annoatation.get(<span class="string">'size'</span>).get(<span class="string">'height'</span>))</span><br><span class="line">    bndbox = annoatation.get(<span class="string">'object'</span>).get(<span class="string">'bndbox'</span>)</span><br><span class="line">    box_xmin = int(bndbox.get(<span class="string">'xmin'</span>))</span><br><span class="line">    box_xmax = int(bndbox.get(<span class="string">'xmax'</span>))</span><br><span class="line">    box_ymin = int(bndbox.get(<span class="string">'ymin'</span>))</span><br><span class="line">    box_ymax = int(bndbox.get(<span class="string">'ymax'</span>))</span><br><span class="line">    box_width = (box_xmax - box_xmin) / width</span><br><span class="line">    box_height = (box_ymax - box_ymin) / height</span><br><span class="line">    <span class="keyword">return</span> box_xmin / width, box_ymin / height, box_width / width, box_height / height</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 parse_xml 方法，这个方法首先读取了 xml 文件，然后使用 xmltodict 库就可以将 XML 字符串转为 JSON，然后依次读取出验证码的宽高信息，缺口的位置信息，最后返回了想要的数据格式—— 缺口左上角的坐标和宽高相对值，以元组的形式返回。</p>
                  <p>都标注完成之后，对每个 xml 文件调用此方法便可以生成想要的标注结果了。</p>
                  <p>在这里，我已经将对应的标注结果都处理好了，可以直接使用，路径为 data/captcha/labels，如图所示：</p>
                  <p><img src="https://cdn.cuiqingcai.com/j358p.png" alt="image-20210504200730482"></p>
                  <p>每个 txt 文件对应一张验证码图的标注结果，内容类似如下：</p>
                  <figure class="highlight basic">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">0 </span><span class="number">0.6153846153846154</span> <span class="number">0.275</span> <span class="number">0.16596774</span> <span class="number">0.24170968</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一位 0 代表标注目标的索引，由于我们只需要检测一个缺口，所以索引就是 0；第 2、3 位代表缺口的左上角的位置，比如 0.615 则代表缺口左上角的横坐标在相对验证码的 61.5% 处，乘以验证码的宽度 520，结果大约就是 320，即左上角偏移值是 320 像素；第 4、5 代表缺口的宽高相对验证码图片的占比，比如第 5 位 0.24 乘以验证码的高度 320，结果大约是 77，即缺口的高度大约为 77 像素。</p>
                  <p>好了，到此为止数据准备阶段就完成了。</p>
                  <h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2>
                  <p>为了更好的训练效果，我们还需要下载一些预训练模型。预训练的意思就是已经有一个提前训练过的基础模型了，我们可以直接使用提前训练好的模型里面的权重文件，我们就不用从零开始训练了，只需要基于之前的模型进行微调就好了，这样既可以节省训练时间，又可以有比较好的效果。</p>
                  <p>YOLOV3 的训练要加载预训练模型才能有不错的训练效果，预训练模型下载命令如下：</p>
                  <figure class="highlight arduino">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">bash <span class="built_in">prepare</span>.sh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <blockquote>
                    <p>注意：在 Windows 下请使用 Bash 命令行工具如 Git Bash 来运行此命令。</p>
                  </blockquote>
                  <p>执行这个脚本，就能下载 YOLO V3 模型的一些权重文件，包括 yolov3 和 weights 还有 darknet 的 weights，在训练之前我们需要用这些权重文件初始化 YOLO V3 模型。</p>
                  <p>接下来就可以开始训练了，执行如下脚本：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">bash </span>train.sh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <blockquote>
                    <p>注意：在 Windows 下请同样使用 Bash 命令行工具如 Git Bash 来运行此命令。</p>
                  </blockquote>
                  <p>同样推荐使用 GPU 进行训练，训练过程中我们可以使用 TensorBoard 来看看 loss 和 mAP 的变化，运行 TensorBoard：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">tensorboard --logdir=<span class="string">'logs'</span> --port=<span class="number">6006</span> --host <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <blockquote>
                    <p>注意：请确保已经正确安装了本项目的所有依赖库，其中就包括 TensorBoard，安装成功之后便可以使用 tensorboard 命令。</p>
                  </blockquote>
                  <p>运行此命令后可以在 <a href="http://localhost:6006" target="_blank" rel="noopener">http://localhost:6006</a> 观察到训练过程中的 loss 变化。</p>
                  <p>loss_1 变化类似如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/qrowq.png" alt="loss 变化"></p>
                  <p>val_mAP 变化类似如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/pylwj.png" alt="mAP 变化"></p>
                  <p>可以看到 loss 从最初的非常高下降到了很低，准确率也逐渐接近 100%。</p>
                  <p>这是训练过程中的命令行的一些输出结果：</p>
                  <figure class="highlight gherkin">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">---- [Epoch 99/100, Batch 27/29] ----</span><br><span class="line">+------------+--------------+--------------+--------------+</span><br><span class="line">|<span class="string"> Metrics    </span>|<span class="string"> YOLO Layer 0 </span>|<span class="string"> YOLO Layer 1 </span>|<span class="string"> YOLO Layer 2 </span>|</span><br><span class="line">+------------+--------------+--------------+--------------+</span><br><span class="line">|<span class="string"> grid_size  </span>|<span class="string"> 14           </span>|<span class="string"> 28           </span>|<span class="string"> 56           </span>|</span><br><span class="line">|<span class="string"> loss       </span>|<span class="string"> 0.028268     </span>|<span class="string"> 0.046053     </span>|<span class="string"> 0.043745     </span>|</span><br><span class="line">|<span class="string"> x          </span>|<span class="string"> 0.002108     </span>|<span class="string"> 0.005267     </span>|<span class="string"> 0.008111     </span>|</span><br><span class="line">|<span class="string"> y          </span>|<span class="string"> 0.004561     </span>|<span class="string"> 0.002016     </span>|<span class="string"> 0.009047     </span>|</span><br><span class="line">|<span class="string"> w          </span>|<span class="string"> 0.001284     </span>|<span class="string"> 0.004618     </span>|<span class="string"> 0.000207     </span>|</span><br><span class="line">|<span class="string"> h          </span>|<span class="string"> 0.000594     </span>|<span class="string"> 0.000528     </span>|<span class="string"> 0.000946     </span>|</span><br><span class="line">|<span class="string"> conf       </span>|<span class="string"> 0.019700     </span>|<span class="string"> 0.033624     </span>|<span class="string"> 0.025432     </span>|</span><br><span class="line">|<span class="string"> cls        </span>|<span class="string"> 0.000022     </span>|<span class="string"> 0.000001     </span>|<span class="string"> 0.000002     </span>|</span><br><span class="line">|<span class="string"> cls_acc    </span>|<span class="string"> 100.00%      </span>|<span class="string"> 100.00%      </span>|<span class="string"> 100.00%      </span>|</span><br><span class="line">|<span class="string"> recall50   </span>|<span class="string"> 1.000000     </span>|<span class="string"> 1.000000     </span>|<span class="string"> 1.000000     </span>|</span><br><span class="line">|<span class="string"> recall75   </span>|<span class="string"> 1.000000     </span>|<span class="string"> 1.000000     </span>|<span class="string"> 1.000000     </span>|</span><br><span class="line">|<span class="string"> precision  </span>|<span class="string"> 1.000000     </span>|<span class="string"> 0.800000     </span>|<span class="string"> 0.666667     </span>|</span><br><span class="line">|<span class="string"> conf_obj   </span>|<span class="string"> 0.994271     </span>|<span class="string"> 0.999249     </span>|<span class="string"> 0.997762     </span>|</span><br><span class="line">|<span class="string"> conf_noobj </span>|<span class="string"> 0.000126     </span>|<span class="string"> 0.000158     </span>|<span class="string"> 0.000140     </span>|</span><br><span class="line">+------------+--------------+--------------+--------------+</span><br><span class="line">Total loss 0.11806630343198776</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里显示了训练过程中各个指标的变化情况，如 loss、recall、precision、confidence 等，分别代表训练过程的损失（越小越好）、召回率（能识别出的结果占应该识别出结果的比例，越高越好）、精确率（识别出的结果中正确的比率，越高越好）、置信度（模型有把握识别对的概率，越高越好），可以作为参考。</p>
                  <h2 id="5-测试"><a href="#5-测试" class="headerlink" title="5. 测试"></a>5. 测试</h2>
                  <p>训练完毕之后会在 checkpoints 文件夹生成 pth 文件，这就是一些模型文件，和上一节的 best_model.pkl 是一样的原理，只不过表示形式略有不同，我们可直接使用这些模型来预测生成标注结果。</p>
                  <p>要运行测试，我们可以先在测试文件夹 <code>data/captcha/test</code> 放入一些验证码图片：</p>
                  <p>样例验证码如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/xwqzr.png" alt="captcha_435"></p>
                  <p>要运行测试，执行如下脚本：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">bash </span>detect.sh</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>该脚本会读取测试文件夹所有图片，并将处理后的结果输出到 <code>data/captcha/result</code> 文件夹，控制台输出了一些验证码的识别结果。</p>
                  <p>同时在 <code>data/captcha/result</code> 生成了标注的结果，样例如下：</p>
                  <p><img src="https://cdn.cuiqingcai.com/8ax68.png" alt=""></p>
                  <p>可以看到，缺口就被准确识别出来了。</p>
                  <p>实际上，detect.sh 是执行了 detect.py 文件，在代码中有一个关键的输出结果如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">bbox = patches.Rectangle((x1 + box_w / <span class="number">2</span>, y1 + box_h / <span class="number">2</span>), box_w, box_h, linewidth=<span class="number">2</span>, edgecolor=color, facecolor=<span class="string">"none"</span>)</span><br><span class="line">print(<span class="string">'bbox'</span>, (x1, y1, box_w, box_h), <span class="string">'offset'</span>, x1)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 bbox 指的就是最终缺口的轮廓位置，同时 x1 就是指的轮廓最左侧距离整个验证码最左侧的横向偏移量，即 offset。通过这两个信息，我们就能得到缺口的关键位置了。</p>
                  <p>有了目标滑块位置之后，我们便可以进行一些模拟滑动操作从而实现通过验证码的检测了。</p>
                  <h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2>
                  <p>本节主要介绍了训练深度学习模型来识别滑动验证码缺口的整体流程，最终我们成功实现了模型训练过程，并得到了一个深度学习模型文件。</p>
                  <p>利用这个模型，我们可以输入一张滑动验证码，模型便会预测出其中的缺口的位置，包括偏移量、宽度等，最后可以通过缺口的信息绘制出对应的位置。</p>
                  <p>和上一节一样，本节介绍的内容也可以进一步优化：</p>
                  <ul>
                    <li>当前模型的预测过程是通过命令行执行的，但在实际使用的时候可能并不太方便，可以考虑将预测过程对接 API 服务器暴露出来，比如对接 Flask、Django、FastAPI 等把预测过程实现为一个支持 POST 请求的接口，接口可以接收一张验证码图片，返回验证码的文本信息，这样会使得模型更加方便易用。</li>
                  </ul>
                  <p>本节代码：<a href="https://github.com/Python3WebSpider/DeepLearningSlideCaptcha2。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/DeepLearningSlideCaptcha2。</a></p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2021-12-31 02:39:41" itemprop="dateCreated datePublished" datetime="2021-12-31T02:39:41+08:00">2021-12-31</time>
                </span>
                <span id="/36044.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习识别滑动验证码" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>7.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>7 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/36046.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 个人随笔 <i class="label-arrow"></i>
                  </a>
                  <a href="/36046.html" class="post-title-link" itemprop="url">想想做事的终局</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>之前跟一位朋友聊天，他说了有一点让我触动很大，这里专门来写写。</p>
                  <p>就我个人来讲，我在我的这个年纪一直在尝试各种各样的事情，除了工作之外，我还尝试了多种副业，比如写公众号、写书、录课、做视频、写各种开源项目等等，其实整体说来，还是比较“杂”的。</p>
                  <p>他给了我一个建议，叫做“想想做事的终局”，我感觉挺有道理的。</p>
                  <p>下面是我的一些思考。</p>
                  <p>其实年轻多尝试没有什么坏事，比如在三十岁之前，我们可能并不清楚我们真正擅长的或喜欢的到底是什么，所以就得多尝试不同的方向，探索一些未知的领域。找到真正喜欢或者适合的事情之后，后面的几十年可以潜心放在这上面，深入去发展它。</p>
                  <p>当然很多时候，我们可能并不知道我们在做的这件事以后究竟会是怎样的，这里面有两个方面：</p>
                  <ul>
                    <li>验证这件事究竟合不合适、能不能做</li>
                    <li>思考这件事以后到底能做到什么地步</li>
                  </ul>
                  <p>这里针对这两点分别说下。</p>
                  <p>首先就是验证这件事究竟合不合适、能不能做。我们比较好的办法就是进行快速试错，比如一个项目，我们做一些 POC 来验证可行性，或者前期调研的时候通过调查问卷了解行情，这些都是十分重要的。另外我们也不能想当然，比如做一件事情之前，凭空一想就把自己否定了，那就会少掉很多机会。该行动还是要行动的，但前期也要尽量控制一些成本。</p>
                  <p>另外一个重要的就是想想这件事到底能做到什么地步，也就是终局在哪里。比如说，一件事，它本身的可扩展性、可复用性就很小，这个就得好好考虑下要不要投入大的成本一直做下去。如果一件事投入很多的精力，以后可做的事情非常多，或者能做的很大很大，甚至无限大，这种事情就值得多去投入些。比如说，我做一个开源项目，投入大量精力，最后的用户可能就是很有限的，而且场景也很有限，那这个以后就做不大。如果我写作、课程，如果我能把我学习和进步的都输出出来，不断积累，那以后的扩展的面就会有很多很多，这种相对来说就能做大。这就是一些事的终局。所以，平时我们也需要考虑下这件事的终局在哪里。</p>
                  <p>嗯，总之，找到适合做、喜欢做的事，同时这件事的终局也够大，那就是非常难得的，持续努力，结果会回报我们的。</p>
                  <p>更多精彩内容，请关注我的公众号「进击的 Coder」和「崔庆才丨静觅」。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2021-12-29 10:50:22" itemprop="dateCreated datePublished" datetime="2021-12-29T10:50:22+08:00">2021-12-29</time>
                </span>
                <span id="/36046.html" class="post-meta-item leancloud_visitors" data-flag-title="想想做事的终局" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>879</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/33045.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 安装配置 <i class="label-arrow"></i>
                  </a>
                  <a href="/33045.html" class="post-title-link" itemprop="url">Pika 的安装</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>Pika 是 Python 中用于连接和操作 RabbitMQ 的库，本节我们了解下 Pika 的安装方式。</p>
                  <h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2>
                  <ul>
                    <li>官方文档：<a href="https://pika.readthedocs.io" target="_blank" rel="noopener">https://pika.readthedocs.io</a></li>
                    <li>GitHub：<a href="https://github.com/pika/pika" target="_blank" rel="noopener">https://github.com/pika/pika</a></li>
                  </ul>
                  <h2 id="先置条件"><a href="#先置条件" class="headerlink" title="先置条件"></a>先置条件</h2>
                  <p>安装之前请先安装下 RabbitMQ，安装参考说明：<a href="https://setup.scrape.center/rabbitmq" target="_blank" rel="noopener">https://setup.scrape.center/rabbitmq</a>。</p>
                  <h2 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h2>
                  <h3 id="pip-安装"><a href="#pip-安装" class="headerlink" title="pip 安装"></a>pip 安装</h3>
                  <p>推荐使用 pip3 安装，命令如下：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> pika</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>命令执行完毕之后即可完成安装。</p>
                  <h2 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h2>
                  <p>安装完成之后，可以在 Python 命令行下测试。</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">$ python3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pika</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果没有错误报出，则证明库已经安装好了。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2021-12-03 07:39:41" itemprop="dateCreated datePublished" datetime="2021-12-03T07:39:41+08:00">2021-12-03</time>
                </span>
                <span id="/33045.html" class="post-meta-item leancloud_visitors" data-flag-title="Pika 的安装" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>319</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/33044.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 安装配置 <i class="label-arrow"></i>
                  </a>
                  <a href="/33044.html" class="post-title-link" itemprop="url">RabbitMQ 的安装</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>RabbmitMQ 是一个非常强大易用的消息队列，支持 Windows、Mac、Linux 等各个平台。</p>
                  <p>每个平台都有多种安装方式，这里进行简单归纳总结，具体的安装方式请移步官方文档说明。</p>
                  <h2 id="Windows-上的安装"><a href="#Windows-上的安装" class="headerlink" title="Windows 上的安装"></a>Windows 上的安装</h2>
                  <p>Windows 主要有两种安装方式，一种是通过 Chocolatey 工具安装，另一种是通过官方安装包安装。</p>
                  <p>详细说明请移步：<a href="https://rabbitmq.com/install-windows.html" target="_blank" rel="noopener">https://rabbitmq.com/install-windows.html</a> 查看。</p>
                  <h3 id="Chocolatey"><a href="#Chocolatey" class="headerlink" title="Chocolatey"></a>Chocolatey</h3>
                  <p>Chocolatey 是一个 Windows 上管理软件的工具，RabbitMQ 发布了很多 Chocolatey 安装包，具体说明可以参考：<a href="https://rabbitmq.com/install-windows.html#chocolatey" target="_blank" rel="noopener">https://rabbitmq.com/install-windows.html#chocolatey</a>。</p>
                  <h3 id="官方安装包"><a href="#官方安装包" class="headerlink" title="官方安装包"></a>官方安装包</h3>
                  <p>就是下载一个二进制安装文件，具体的最新下载地址请移步 <a href="https://rabbitmq.com/install-windows.html#installer" target="_blank" rel="noopener">https://rabbitmq.com/install-windows.html#installer</a> 查看。</p>
                  <h2 id="Linux-上的安装"><a href="#Linux-上的安装" class="headerlink" title="Linux 上的安装"></a>Linux 上的安装</h2>
                  <p>Linux 上的安装方式又分了多种了，根据发行平台的不同有如下安装说明。</p>
                  <h3 id="Debian-和-Ubuntu"><a href="#Debian-和-Ubuntu" class="headerlink" title="Debian 和 Ubuntu"></a>Debian 和 Ubuntu</h3>
                  <p>请移步：<a href="https://rabbitmq.com/install-debian.html" target="_blank" rel="noopener">https://rabbitmq.com/install-debian.html</a> 查看。</p>
                  <h3 id="RedHat-Enterprise-Linux-CentOS-Fedora-openSUSE"><a href="#RedHat-Enterprise-Linux-CentOS-Fedora-openSUSE" class="headerlink" title="RedHat Enterprise Linux, CentOS, Fedora, openSUSE"></a>RedHat Enterprise Linux, CentOS, Fedora, openSUSE</h3>
                  <p>请移步：<a href="https://rabbitmq.com/install-rpm.html" target="_blank" rel="noopener">https://rabbitmq.com/install-rpm.html</a> 查看。</p>
                  <h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2>
                  <p>Mac 就是主要用 Homebrew 来安装了，具体安装说明请移步：<a href="https://rabbitmq.com/install-homebrew.html" target="_blank" rel="noopener">https://rabbitmq.com/install-homebrew.html</a>。</p>
                  <h2 id="更多"><a href="#更多" class="headerlink" title="更多"></a>更多</h2>
                  <p>更多平台的安装方式请移步：<a href="https://rabbitmq.com/download.html" target="_blank" rel="noopener">https://rabbitmq.com/download.html</a> 查看。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2021-12-03 07:39:41" itemprop="dateCreated datePublished" datetime="2021-12-03T07:39:41+08:00">2021-12-03</time>
                </span>
                <span id="/33044.html" class="post-meta-item leancloud_visitors" data-flag-title="RabbitMQ 的安装" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>738</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <script>
              document.querySelectorAll('.random').forEach(item => item.src = "https://picsum.photos/id/" + Math.floor(Math.random() * Math.floor(300)) + "/200/133")

            </script>
            <nav class="pagination">
              <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
            </nav>
          </div>
          <script>
            window.addEventListener('tabs:register', () =>
            {
              let
              {
                activeClass
              } = CONFIG.comments;
              if (CONFIG.comments.storage)
              {
                activeClass = localStorage.getItem('comments_active') || activeClass;
              }
              if (activeClass)
              {
                let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
                if (activeTab)
                {
                  activeTab.click();
                }
              }
            });
            if (CONFIG.comments.storage)
            {
              window.addEventListener('tabs:click', event =>
              {
                if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
                let commentClass = event.target.classList[1];
                localStorage.setItem('comments_active', commentClass);
              });
            }

          </script>
        </div>
        <div class="toggle sidebar-toggle">
          <span class="toggle-line toggle-line-first"></span>
          <span class="toggle-line toggle-line-middle"></span>
          <span class="toggle-line toggle-line-last"></span>
        </div>
        <aside class="sidebar">
          <div class="sidebar-inner">
            <ul class="sidebar-nav motion-element">
              <li class="sidebar-nav-toc"> 文章目录 </li>
              <li class="sidebar-nav-overview"> 站点概览 </li>
            </ul>
            <!--noindex-->
            <div class="post-toc-wrap sidebar-panel">
            </div>
            <!--/noindex-->
            <div class="site-overview-wrap sidebar-panel">
              <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
                <img class="site-author-image" itemprop="image" alt="崔庆才" src="/images/avatar.png">
                <p class="site-author-name" itemprop="name">崔庆才</p>
                <div class="site-description" itemprop="description">静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。</div>
              </div>
              <div class="site-state-wrap motion-element">
                <nav class="site-state">
                  <div class="site-state-item site-state-posts">
                    <a href="/archives/">
                      <span class="site-state-item-count">685</span>
                      <span class="site-state-item-name">日志</span>
                    </a>
                  </div>
                  <div class="site-state-item site-state-categories">
                    <a href="/categories/">
                      <span class="site-state-item-count">32</span>
                      <span class="site-state-item-name">分类</span></a>
                  </div>
                  <div class="site-state-item site-state-tags">
                    <a href="/tags/">
                      <span class="site-state-item-count">246</span>
                      <span class="site-state-item-name">标签</span></a>
                  </div>
                </nav>
              </div>
              <div class="links-of-author motion-element">
                <span class="links-of-author-item">
                  <a href="https://github.com/Germey" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Germey" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
                </span>
                <span class="links-of-author-item">
                  <a href="mailto:cqc@cuiqingcai.com.com" title="邮件 → mailto:cqc@cuiqingcai.com.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮件</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://weibo.com/cuiqingcai" title="微博 → https:&#x2F;&#x2F;weibo.com&#x2F;cuiqingcai" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>微博</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/Germey" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;Germey" rel="noopener" target="_blank"><i class="fa fa-magic fa-fw"></i>知乎</a>
                </span>
              </div>
            </div>
            <div style=" width: 100%;" class="sidebar-panel sidebar-panel-image sidebar-panel-active">
              <a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">
                <img src="https://cdn.cuiqingcai.com/ei5og.jpg" style=" width: 100%;">
              </a>
            </div>
            <div class="sidebar-panel sidebar-panel-categories sidebar-panel-active">
              <h4 class="name"> 分类 </h4>
              <div class="content">
                <ul class="category-list">
                  <li class="category-list-item"><a class="category-list-link" href="/categories/API/">API</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/C-C/">C/C++</a><span class="category-list-count">23</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/HTML/">HTML</a><span class="category-list-count">14</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/JavaScript/">JavaScript</a><span class="category-list-count">26</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">14</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Luma/">Luma</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Markdown/">Markdown</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Net/">Net</a><span class="category-list-count">4</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Nexior/">Nexior</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Other/">Other</a><span class="category-list-count">40</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/PHP/">PHP</a><span class="category-list-count">27</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">303</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/TypeScript/">TypeScript</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E5%B1%95%E7%A4%BA/">个人展示</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E6%97%A5%E8%AE%B0/">个人日记</a><span class="category-list-count">9</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E8%AE%B0%E5%BD%95/">个人记录</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/">个人随笔</a><span class="category-list-count">21</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">安装配置</a><span class="category-list-count">59</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/">技术杂谈</a><span class="category-list-count">96</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">4</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB%E7%AC%94%E8%AE%B0/">生活笔记</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A6%8F%E5%88%A9%E4%B8%93%E5%8C%BA/">福利专区</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E8%81%8C%E4%BD%8D%E6%8E%A8%E8%8D%90/">职位推荐</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E8%89%BA%E6%9C%AF%E4%BA%8C%E7%BB%B4%E7%A0%81/">艺术二维码</a><span class="category-list-count">1</span></li>
                </ul>
              </div>
            </div>
            <div class="sidebar-panel sidebar-panel-friends sidebar-panel-active">
              <h4 class="name"> 友情链接 </h4>
              <ul class="friends">
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/j2dub.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.findhao.net/" target="_blank" rel="noopener">FindHao</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/6apxu.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.51dev.com/" target="_blank" rel="noopener">IT技术社区</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/bqlbs.png">
                  </span>
                  <span class="link">
                    <a href="http://www.urselect.com/" target="_blank" rel="noopener">优社电商</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/8s88c.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yuanrenxue.com/" target="_blank" rel="noopener">猿人学</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/2wgg5.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yunlifang.cn/" target="_blank" rel="noopener">云立方</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="http://qianxunclub.com/favicon.png">
                  </span>
                  <span class="link">
                    <a href="http://qianxunclub.com/" target="_blank" rel="noopener">千寻啊千寻</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/0044u.jpg">
                  </span>
                  <span class="link">
                    <a href="http://kodcloud.com/" target="_blank" rel="noopener">可道云</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/ygnpn.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.kunkundashen.cn/" target="_blank" rel="noopener">坤坤大神</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/x714o.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.hubwiz.com/" target="_blank" rel="noopener">汇智网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/44hxf.png">
                  </span>
                  <span class="link">
                    <a href="http://redstonewill.com/" target="_blank" rel="noopener">红色石头</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/wkaus.jpg">
                  </span>
                  <span class="link">
                    <a href="https://zhaoshuai.me/" target="_blank" rel="noopener">碎念</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/pgo0r.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.chenwenguan.com/" target="_blank" rel="noopener">陈文管的博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/kk82a.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.lxlinux.net/" target="_blank" rel="noopener">良许Linux教程网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/lj0t2.jpg">
                  </span>
                  <span class="link">
                    <a href="https://tanqingbo.cn/" target="_blank" rel="noopener">IT码农</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/i8cdr.png">
                  </span>
                  <span class="link">
                    <a href="https://junyiseo.com/" target="_blank" rel="noopener">均益个人博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/chwv2.png">
                  </span>
                  <span class="link">
                    <a href="https://brucedone.com/" target="_blank" rel="noopener">大鱼的鱼塘</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://www.91vps.com/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="http://www.91vps.com/" target="_blank" rel="noopener">91VPS</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://webpage.qidian.qq.com/qidian/chatv3-gray/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="https://www.qg.net/" target="_blank" rel="noopener">青果网络</a>
                  </span>
                </li>
              </ul>
            </div>
            <div class="sidebar-panel sidebar-panel-tags sidebar-panel-active">
              <h4 class="name"> 标签云 </h4>
              <div class="content">
                <a href="/tags/2022/" style="font-size: 20px;">2022</a> <a href="/tags/2048/" style="font-size: 10px;">2048</a> <a href="/tags/ADSL/" style="font-size: 10px;">ADSL</a> <a href="/tags/API/" style="font-size: 16px;">API</a> <a href="/tags/Ajax/" style="font-size: 12px;">Ajax</a> <a href="/tags/Bootstrap/" style="font-size: 11px;">Bootstrap</a> <a href="/tags/Bug/" style="font-size: 10px;">Bug</a> <a href="/tags/CDN/" style="font-size: 10px;">CDN</a> <a href="/tags/CQC/" style="font-size: 10px;">CQC</a> <a href="/tags/CSS/" style="font-size: 10px;">CSS</a> <a href="/tags/CSS-%E5%8F%8D%E7%88%AC%E8%99%AB/" style="font-size: 10px;">CSS 反爬虫</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/ChatGPT/" style="font-size: 10px;">ChatGPT</a> <a href="/tags/Cookie/" style="font-size: 10px;">Cookie</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Eclipse/" style="font-size: 11px;">Eclipse</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/FTP/" style="font-size: 10px;">FTP</a> <a href="/tags/Flux/" style="font-size: 10px;">Flux</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/GitHub/" style="font-size: 13px;">GitHub</a> <a href="/tags/HTML5/" style="font-size: 10px;">HTML5</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/Hailuo/" style="font-size: 10px;">Hailuo</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Hook/" style="font-size: 10px;">Hook</a> <a href="/tags/IP/" style="font-size: 10px;">IP</a> <a href="/tags/IT/" style="font-size: 10px;">IT</a> <a href="/tags/JSON/" style="font-size: 10px;">JSON</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/JavaScript/" style="font-size: 14px;">JavaScript</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/LOGO/" style="font-size: 10px;">LOGO</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Luma/" style="font-size: 10px;">Luma</a> <a href="/tags/MIUI/" style="font-size: 10px;">MIUI</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/Midjourney/" style="font-size: 11px;">Midjourney</a> <a href="/tags/MongoDB/" style="font-size: 11px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Mysql/" style="font-size: 10px;">Mysql</a> <a href="/tags/NBA/" style="font-size: 10px;">NBA</a> <a href="/tags/Nexior/" style="font-size: 10px;">Nexior</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/OpenCV/" style="font-size: 10px;">OpenCV</a> <a href="/tags/PHP/" style="font-size: 11px;">PHP</a> <a href="/tags/PPT/" style="font-size: 10px;">PPT</a> <a href="/tags/PS/" style="font-size: 10px;">PS</a> <a href="/tags/Pathlib/" style="font-size: 10px;">Pathlib</a> <a href="/tags/PhantomJS/" style="font-size: 10px;">PhantomJS</a> <a href="/tags/Playwright/" style="font-size: 10px;">Playwright</a> <a href="/tags/Python/" style="font-size: 17px;">Python</a> <a href="/tags/Python-%E7%88%AC%E8%99%AB/" style="font-size: 18px;">Python 爬虫</a> <a href="/tags/Python3/" style="font-size: 11px;">Python3</a> <a href="/tags/Python3%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B/" style="font-size: 12px;">Python3爬虫教程</a> <a href="/tags/Pythonic/" style="font-size: 10px;">Pythonic</a> <a href="/tags/Python%E7%88%AC%E8%99%AB/" style="font-size: 19px;">Python爬虫</a> <a href="/tags/Python%E7%88%AC%E8%99%AB%E4%B9%A6/" style="font-size: 12px;">Python爬虫书</a> <a href="/tags/Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B/" style="font-size: 15px;">Python爬虫教程</a> <a href="/tags/QQ/" style="font-size: 10px;">QQ</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/ReCAPTCHA/" style="font-size: 10px;">ReCAPTCHA</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Riffusion/" style="font-size: 10px;">Riffusion</a> <a href="/tags/SAE/" style="font-size: 10px;">SAE</a> <a href="/tags/SSH/" style="font-size: 10px;">SSH</a> <a href="/tags/SVG/" style="font-size: 10px;">SVG</a> <a href="/tags/Scrapy-redis/" style="font-size: 10px;">Scrapy-redis</a> <a href="/tags/Scrapy%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 10px;">Scrapy分布式</a> <a href="/tags/Selenium/" style="font-size: 11px;">Selenium</a> <a href="/tags/Session/" style="font-size: 10px;">Session</a> <a href="/tags/Shell/" style="font-size: 10px;">Shell</a> <a href="/tags/Suno/" style="font-size: 10px;">Suno</a> <a href="/tags/TKE/" style="font-size: 10px;">TKE</a> <a href="/tags/TXT/" style="font-size: 10px;">TXT</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/Ubuntu/" style="font-size: 11px;">Ubuntu</a> <a href="/tags/VS-Code/" style="font-size: 10px;">VS Code</a> <a href="/tags/Veo/" style="font-size: 10px;">Veo</a> <a href="/tags/Vercel/" style="font-size: 10px;">Vercel</a> <a href="/tags/Vs-Code/" style="font-size: 10px;">Vs Code</a> <a href="/tags/Vue/" style="font-size: 11px;">Vue</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a> <a href="/tags/Webpack/" style="font-size: 10px;">Webpack</a> <a href="/tags/Web%E7%BD%91%E9%A1%B5/" style="font-size: 10px;">Web网页</a> <a href="/tags/Windows/" style="font-size: 10px;">Windows</a> <a href="/tags/Winpcap/" style="font-size: 10px;">Winpcap</a> <a href="/tags/WordPress/" style="font-size: 13px;">WordPress</a> <a href="/tags/XPath/" style="font-size: 12px;">XPath</a> <a href="/tags/Youtube/" style="font-size: 11px;">Youtube</a> <a href="/tags/acedata/" style="font-size: 12px;">acedata</a> <a href="/tags/aiohttp/" style="font-size: 10px;">aiohttp</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/ansible/" style="font-size: 10px;">ansible</a> <a href="/tags/api/" style="font-size: 13px;">api</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/cocos2d-x/" style="font-size: 10px;">cocos2d-x</a> <a href="/tags/dummy-change/" style="font-size: 10px;">dummy change</a> <a href="/tags/e6/" style="font-size: 10px;">e6</a> <a href="/tags/fitvids/" style="font-size: 10px;">fitvids</a>
              </div>
              <script>
                const tagsColors = ['#00a67c', '#5cb85c', '#d9534f', '#567e95', '#b37333', '#f4843d', '#15a287']
                const tagsElements = document.querySelectorAll('.sidebar-panel-tags .content a')
                tagsElements.forEach((item) =>
                {
                  item.style.backgroundColor = tagsColors[Math.floor(Math.random() * tagsColors.length)]
                })

              </script>
            </div>
          </div>
        </aside>
        <div id="sidebar-dimmer"></div>
      </div>
    </main>
    <footer class="footer">
      <div class="footer-inner">
        <div class="copyright">
          <span class="author" itemprop="copyrightHolder">崔庆才丨静觅</span> &copy; <span itemprop="copyrightYear">2025</span>
          <span class="with-love">
            <i class="fa fa-heart"></i>
          </span>
          <a href="https://cuiqingcai.com/sitemap.xml" style="display:none" title="爬虫教程" target="_blank"><strong>爬虫教程</strong></a>
          <a href="https://cuiqingcai.com/sitemap.html" style="display:none" title="爬虫教程" target="_blank"><strong>爬虫教程</strong></a>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-chart-area"></i>
          </span>
          <span title="站点总字数">3.3m</span>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-coffee"></i>
          </span>
          <span title="站点阅读时长">49:35</span>
        </div>
        <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动 </div>
        <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备18015597号-1 </a>
        </div>
        <script>
          (function ()
          {
            function leancloudSelector(url)
            {
              url = encodeURI(url);
              return document.getElementById(url).querySelector('.leancloud-visitors-count');
            }

            function addCount(Counter)
            {
              var visitors = document.querySelector('.leancloud_visitors');
              var url = decodeURI(visitors.id);
              var title = visitors.dataset.flagTitle;
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                if (results.length > 0)
                {
                  var counter = results[0];
                  leancloudSelector(url).innerText = counter.time + 1;
                  Counter('put', '/classes/Counter/' + counter.objectId,
                  {
                    time:
                    {
                      '__op': 'Increment',
                      'amount': 1
                    }
                  }).catch(error =>
                  {
                    console.error('Failed to save visitor count', error);
                  });
                }
                else
                {
                  Counter('post', '/classes/Counter',
                  {
                    title,
                    url,
                    time: 1
                  }).then(response => response.json()).then(() =>
                  {
                    leancloudSelector(url).innerText = 1;
                  }).catch(error =>
                  {
                    console.error('Failed to create', error);
                  });
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }

            function showTime(Counter)
            {
              var visitors = document.querySelectorAll('.leancloud_visitors');
              var entries = [...visitors].map(element =>
              {
                return decodeURI(element.id);
              });
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url:
                {
                  '$in': entries
                }
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                for (let url of entries)
                {
                  let target = results.find(item => item.url === url);
                  leancloudSelector(url).innerText = target ? target.time : 0;
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }
            let
            {
              app_id,
              app_key,
              server_url
            } = {
              "enable": true,
              "app_id": "6X5dRQ0pnPWJgYy8SXOg0uID-gzGzoHsz",
              "app_key": "ziLDVEy73ne5HtFTiGstzHMS",
              "server_url": "https://6x5drq0p.lc-cn-n1-shared.com",
              "security": false
            };

            function fetchData(api_server)
            {
              var Counter = (method, url, data) =>
              {
                return fetch(`${api_server}/1.1${url}`,
                {
                  method,
                  headers:
                  {
                    'X-LC-Id': app_id,
                    'X-LC-Key': app_key,
                    'Content-Type': 'application/json',
                  },
                  body: JSON.stringify(data)
                });
              };
              if (CONFIG.page.isPost)
              {
                if (CONFIG.hostname !== location.hostname) return;
                addCount(Counter);
              }
              else if (document.querySelectorAll('.post-title-link').length >= 1)
              {
                showTime(Counter);
              }
            }
            let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;
            if (api_server)
            {
              fetchData(api_server);
            }
            else
            {
              fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id).then(response => response.json()).then((
              {
                api_server
              }) =>
              {
                fetchData('https://' + api_server);
              });
            }
          })();

        </script>
      </div>
      <div class="footer-stat">
        <span id="cnzz_stat_icon_1279355174"></span>
        <script type="text/javascript">
          document.write(unescape("%3Cspan id='cnzz_stat_icon_1279355174'%3E%3C/span%3E%3Cscript src='https://v1.cnzz.com/z_stat.php%3Fid%3D1279355174%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));

        </script>
      </div>
    </footer>
  </div>
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/js/utils.js"></script>
  <script src="/.js"></script>
  <script src="/js/schemes/pisces.js"></script>
  <script src="/.js"></script>
  <script src="/js/next-boot.js"></script>
  <script src="/.js"></script>
  <script>
    (function ()
    {
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x = document.getElementsByTagName("link");
      //Find the last canonical URL
      if (x.length > 0)
      {
        for (i = 0; i < x.length; i++)
        {
          if (x[i].rel.toLowerCase() == 'canonical' && x[i].href)
          {
            canonicalURL = x[i].href;
          }
        }
      }
      //Get protocol
      if (!canonicalURL)
      {
        curProtocol = window.location.protocol.split(':')[0];
      }
      else
      {
        curProtocol = canonicalURL.split(':')[0];
      }
      //Get current URL if the canonical URL does not exist
      if (!canonicalURL) canonicalURL = window.location.href;
      //Assign script content. Replace current URL with the canonical URL
      ! function ()
      {
        var e = /([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,
          r = canonicalURL,
          t = document.referrer;
        if (!e.test(r))
        {
          var n = (String(curProtocol).toLowerCase() === 'https') ? "https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif" : "//api.share.baidu.com/s.gif";
          t ? (n += "?r=" + encodeURIComponent(document.referrer), r && (n += "&l=" + r)) : r && (n += "?l=" + r);
          var i = new Image;
          i.src = n
        }
      }(window);
    })();

  </script>
  <script src="/js/local-search.js"></script>
  <script src="/.js"></script>
</body>

</html>
