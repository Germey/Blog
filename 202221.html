<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
  <meta name="theme-color" content="#222">
  <meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>
  <script id="hexo-configurations">
    var NexT = window.NexT ||
    {};
    var CONFIG = {
      "hostname": "cuiqingcai.com",
      "root": "/",
      "scheme": "Pisces",
      "version": "7.8.0",
      "exturl": false,
      "sidebar":
      {
        "position": "right",
        "width": 360,
        "display": "post",
        "padding": 18,
        "offset": 12,
        "onmobile": false,
        "widgets": [
          {
            "type": "image",
            "name": "阿布云",
            "enable": false,
            "url": "https://www.abuyun.com/http-proxy/introduce.html",
            "src": "https://cdn.cuiqingcai.com/88au8.jpg",
            "width": "100%"
      },
          {
            "type": "image",
            "name": "爬虫书",
            "url": "https://item.jd.com/13527222.html",
            "src": "https://cdn.cuiqingcai.com/ei5og.jpg",
            "width": "100%",
            "enable": true
      },
          {
            "type": "categories",
            "name": "分类",
            "enable": true
      },
          {
            "type": "image",
            "name": "IPIDEA",
            "url": "http://www.ipidea.net/?utm-source=cqc&utm-keyword=?cqc",
            "src": "https://cdn.cuiqingcai.com/0ywun.png",
            "width": "100%",
            "enable": false
      },
          {
            "type": "image",
            "name": "Storm Proxies",
            "src": "https://cdn.cuiqingcai.com/a2zad8.png",
            "url": "https://www.stormproxies.cn/?keyword=jingmi",
            "width": "100%",
            "enable": false
      },
          {
            "type": "friends",
            "name": "友情链接",
            "enable": true
      },
          {
            "type": "hot",
            "name": "猜你喜欢",
            "enable": true
      },
          {
            "type": "tags",
            "name": "标签云",
            "enable": true
      }]
      },
      "copycode":
      {
        "enable": true,
        "show_result": true,
        "style": "mac"
      },
      "back2top":
      {
        "enable": true,
        "sidebar": false,
        "scrollpercent": true
      },
      "bookmark":
      {
        "enable": false,
        "color": "#222",
        "save": "auto"
      },
      "fancybox": false,
      "mediumzoom": false,
      "lazyload": false,
      "pangu": true,
      "comments":
      {
        "style": "tabs",
        "active": "gitalk",
        "storage": true,
        "lazyload": false,
        "nav": null,
        "activeClass": "gitalk"
      },
      "algolia":
      {
        "hits":
        {
          "per_page": 10
        },
        "labels":
        {
          "input_placeholder": "Search for Posts",
          "hits_empty": "We didn't find any results for the search: ${query}",
          "hits_stats": "${hits} results found in ${time} ms"
        }
      },
      "localsearch":
      {
        "enable": true,
        "trigger": "auto",
        "top_n_per_article": 10,
        "unescape": false,
        "preload": false
      },
      "motion":
      {
        "enable": false,
        "async": false,
        "transition":
        {
          "post_block": "bounceDownIn",
          "post_header": "slideDownIn",
          "post_body": "slideDownIn",
          "coll_header": "slideLeftIn",
          "sidebar": "slideUpIn"
        }
      },
      "path": "search.xml"
    };

  </script>
  <meta name="keywords" content="爬虫,Python爬虫,爬虫教程,网络爬虫,2022,urllib">
  <meta name="robots" content="index,follow">
  <meta name="GOOGLEBOT" content="index,follow">
  <meta name="author" content="静觅丨崔庆才的个人站点">
  <meta name="description" content="爬虫系列文章总目录：【2022 年】Python3 爬虫学习教程，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有">
  <meta property="og:type" content="article">
  <meta property="og:title" content="【2022 年】Python3 爬虫教程 - urllib 爬虫初体验">
  <meta property="og:url" content="https://cuiqingcai.com/202221.html">
  <meta property="og:site_name" content="静觅">
  <meta property="og:description" content="爬虫系列文章总目录：【2022 年】Python3 爬虫学习教程，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:image" content="https://cdn.cuiqingcai.com/yqq2r.png">
  <meta property="og:image" content="https://cdn.cuiqingcai.com/fries.png">
  <meta property="article:published_time" content="2022-02-12T23:44:31.000Z">
  <meta property="article:modified_time" content="2025-08-11T15:24:05.301Z">
  <meta property="article:author" content="崔庆才">
  <meta property="article:tag" content="爬虫">
  <meta property="article:tag" content="Python爬虫">
  <meta property="article:tag" content="爬虫教程">
  <meta property="article:tag" content="网络爬虫">
  <meta property="article:tag" content="2022">
  <meta property="article:tag" content="urllib">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:image" content="https://cdn.cuiqingcai.com/yqq2r.png">
  <link rel="canonical" href="https://cuiqingcai.com/202221.html">
  <script id="page-configurations">
    // https://hexo.io/docs/variables.html
    CONFIG.page = {
      sidebar: "",
      isHome: false,
      isPost: true,
      lang: 'zh-CN'
    };

  </script>
  <title>【2022 年】Python3 爬虫教程 - urllib 爬虫初体验 | 静觅</title>
  <meta name="google-site-verification" content="p_bIcnvirkFzG2dYKuNDivKD8-STet5W7D-01woA2fc" />
  <meta name="sogou_site_verification" content="kBOV53NQqT" />
  <noscript>
    <style>
      .use-motion .brand,
      .use-motion .menu-item,
      .sidebar-inner,
      .use-motion .post-block,
      .use-motion .pagination,
      .use-motion .comments,
      .use-motion .post-header,
      .use-motion .post-body,
      .use-motion .collection-header
      {
        opacity: initial;
      }

      .use-motion .site-title,
      .use-motion .site-subtitle
      {
        opacity: initial;
        top: initial;
      }

      .use-motion .logo-line-before i
      {
        left: initial;
      }

      .use-motion .logo-line-after i
      {
        right: initial;
      }

    </style>
  </noscript>
  <link rel="alternate" href="/atom.xml" title="静觅" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner">
        <div class="site-brand-container">
          <div class="site-nav-toggle">
            <div class="toggle" aria-label="切换导航栏">
              <span class="toggle-line toggle-line-first"></span>
              <span class="toggle-line toggle-line-middle"></span>
              <span class="toggle-line toggle-line-last"></span>
            </div>
          </div>
          <div class="site-meta">
            <a href="/" class="brand" rel="start">
              <span class="logo-line-before"><i></i></span>
              <h1 class="site-title">静觅 <span class="site-subtitle"> 崔庆才的个人站点 - Python爬虫教程 </span>
              </h1>
              <span class="logo-line-after"><i></i></span>
            </a>
          </div>
          <div class="site-nav-right">
            <div class="toggle popup-trigger">
              <i class="fa fa-search fa-fw fa-lg"></i>
            </div>
          </div>
        </div>
        <nav class="site-nav">
          <ul id="menu" class="main-menu menu">
            <li class="menu-item menu-item-home">
              <a href="/" rel="section">首页</a>
            </li>
            <li class="menu-item menu-item-archives">
              <a href="/archives/" rel="section">文章列表</a>
            </li>
            <li class="menu-item menu-item-tags">
              <a href="/tags/" rel="section">文章标签</a>
            </li>
            <li class="menu-item menu-item-categories">
              <a href="/categories/" rel="section">文章分类</a>
            </li>
            <li class="menu-item menu-item-about">
              <a href="/about/" rel="section">关于博主</a>
            </li>
            <li class="menu-item menu-item-message">
              <a href="/message/" rel="section">给我留言</a>
            </li>
            <li class="menu-item menu-item-search">
              <a role="button" class="popup-trigger">搜索 </a>
            </li>
          </ul>
        </nav>
        <div class="search-pop-overlay">
          <div class="popup search-popup">
            <div class="search-header">
              <span class="search-icon">
                <i class="fa fa-search"></i>
              </span>
              <div class="search-input-container">
                <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
              </div>
              <span class="popup-btn-close">
                <i class="fa fa-times-circle"></i>
              </span>
            </div>
            <div id="search-result">
              <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>
    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
      <span>0%</span>
    </div>
    <div class="reading-progress-bar"></div>
    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div class="topbanner">
            <a href="https://item.jd.com/13527222.html" target="_blank">
              <img src="https://cdn.cuiqingcai.com/prwgs.png">
            </a>
          </div>
          <div class="content post posts-expand">
            <article itemscope itemtype="http://schema.org/Article" class="post-block single" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/202221.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h1 class="post-title" itemprop="name headline"> 【2022 年】Python3 爬虫教程 - urllib 爬虫初体验 </h1>
                <div class="post-meta">
                  <span class="post-meta-item">
                    <span class="post-meta-item-icon">
                      <i class="far fa-user"></i>
                    </span>
                    <span class="post-meta-item-text">作者</span>
                    <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                  </span>
                  <span class="post-meta-item">
                    <span class="post-meta-item-icon">
                      <i class="far fa-calendar"></i>
                    </span>
                    <span class="post-meta-item-text">发表于</span>
                    <time title="创建时间：2022-02-13 07:44:31" itemprop="dateCreated datePublished" datetime="2022-02-13T07:44:31+08:00">2022-02-13</time>
                  </span>
                  <span class="post-meta-item">
                    <span class="post-meta-item-icon">
                      <i class="far fa-folder"></i>
                    </span>
                    <span class="post-meta-item-text">分类于</span>
                    <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                      <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                    </span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                      <a href="/categories/Python/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                    </span>
                  </span>
                  <span id="/202221.html" class="post-meta-item leancloud_visitors" data-flag-title="【2022 年】Python3 爬虫教程 - urllib 爬虫初体验" title="阅读次数">
                    <span class="post-meta-item-icon">
                      <i class="fa fa-eye"></i>
                    </span>
                    <span class="post-meta-item-text">阅读次数：</span>
                    <span class="leancloud-visitors-count"></span>
                  </span>
                  <span class="post-meta-item" title="本文字数">
                    <span class="post-meta-item-icon">
                      <i class="far fa-file-word"></i>
                    </span>
                    <span class="post-meta-item-text">本文字数：</span>
                    <span>27k</span>
                  </span>
                  <span class="post-meta-item" title="阅读时长">
                    <span class="post-meta-item-icon">
                      <i class="far fa-clock"></i>
                    </span>
                    <span class="post-meta-item-text">阅读时长 &asymp;</span>
                    <span>24 分钟</span>
                  </span>
                </div>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="advertisements">
                </div>
                <blockquote>
                  <p>爬虫系列文章总目录：<a href="https://cuiqingcai.com/17777.html">【2022 年】Python3 爬虫学习教程</a>，本教程内容多数来自于《Python3 网络爬虫开发实战（第二版）》一书，目前截止 2022 年，可以将爬虫基本技术进行系统讲解，同时将最新前沿爬虫技术如异步、JavaScript 逆向、AST、安卓逆向、Hook、智能解析、群控技术、WebAssembly、大规模分布式、Docker、Kubernetes 等，市面上目前就仅有<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">《Python3 网络爬虫开发实战（第二版）》</a>一书了，<a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">点击了解详情</a>。</p>
                </blockquote>
                <p>首先我们介绍一个 Python 库，叫做 urllib，利用它我们可以实现 HTTP 请求的发送，而不用去关心 HTTP 协议本身甚至更低层的实现。我们只需要指定请求的 URL、请求头、请求体等信息即可实现 HTTP 请求的发送，同时 urllib 还可以把服务器返回的响应转化为 Python 对象，通过该对象我们便可以方便地获取响应的相关信息了，如响应状态码、响应头、响应体等等。</p>
                <blockquote>
                  <p>注意：在 Python 2 中，有 urllib 和 urllib2 两个库来实现请求的发送。而在 Python 3 中，已经不存在 urllib2 这个库了，统一为 urllib，其官方文档链接为：<a href="https://docs.python.org/3/library/urllib.html" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.html</a>。</p>
                </blockquote>
                <p>首先，我们来了解一下 urllib 库的使用方法，它是 Python 内置的 HTTP 请求库，也就是说不需要额外安装即可使用。它包含如下 4 个模块。</p>
                <ul>
                  <li><strong>request</strong>：它是最基本的 HTTP 请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只需要给库方法传入 URL 以及额外的参数，就可以模拟实现这个过程了。</li>
                  <li><strong>error</strong>：异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。</li>
                  <li><strong>parse</strong>：一个工具模块，提供了许多 URL 处理方法，比如拆分、解析和合并等。</li>
                  <li><strong>robotparser</strong>：主要用来识别网站的 robots.txt 文件，然后判断哪些网站可以爬，哪些网站不可以爬，它其实用得比较少。</li>
                </ul>
                <h2 id="1-发送请求"><a href="#1-发送请求" class="headerlink" title="1. 发送请求"></a>1. 发送请求</h2>
                <p>使用 urllib 的 request 模块，我们可以方便地实现请求的发送并得到响应。我们先来看下它的具体用法。</p>
                <h3 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a><code>urlopen</code></h3>
                <p>urllib.request 模块提供了最基本的构造 HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理授权验证（Authentication）、重定向（Redirection)、浏览器 Cookie 以及其他内容。</p>
                <p>下面我们来看一下它的强大之处。这里以 Python 官网为例，我们来把这个网页抓下来：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如图所示。</p>
                <p><img src="https://cdn.cuiqingcai.com/yqq2r.png" alt="image-20200315212839610"></p>
                <p>图 运行结果</p>
                <p>这里我们只用了两行代码，便完成了 Python 官网的抓取，输出了网页的源代码。得到源代码之后呢？我们想要的链接、图片地址、文本信息不就都可以提取出来了吗？</p>
                <p>接下来，看看它返回的到底是什么。利用 <code>type</code> 方法输出响应的类型：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(type(response))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>输出结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">http</span>.<span class="title">client</span>.<span class="title">HTTPResponse</span>'&gt;</span></span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，它是一个 <code>HTTPResposne</code> 类型的对象，主要包含 <code>read</code>、<code>readinto</code>、<code>getheader</code>、<code>getheaders</code>、<code>fileno</code> 等方法，以及 <code>msg</code>、<code>version</code>、<code>status</code>、<code>reason</code>、<code>debuglevel</code>、<code>closed</code> 等属性。</p>
                <p>得到这个对象之后，我们把它赋值为 <code>response</code> 变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。</p>
                <p>例如，调用 <code>read</code> 方法可以得到返回的网页内容，调用 <code>status</code> 属性可以得到返回结果的状态码，如 200 代表请求成功，404 代表网页未找到等。</p>
                <p>下面再通过一个实例来看看：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(response.status)</span><br><span class="line">print(response.getheaders())</span><br><span class="line">print(response.getheader(<span class="string">'Server'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="number">200</span></span><br><span class="line">[(<span class="string">'Server'</span>, <span class="string">'nginx'</span>), (<span class="string">'Content-Type'</span>, <span class="string">'text/html; charset=utf-8'</span>), (<span class="string">'X-Frame-Options'</span>, <span class="string">'DENY'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 vegur'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 varnish'</span>), (<span class="string">'Content-Length'</span>, <span class="string">'48775'</span>), (<span class="string">'Accept-Ranges'</span>, <span class="string">'bytes'</span>), (<span class="string">'Date'</span>, <span class="string">'Sun, 15 Mar 2020 13:29:01 GMT'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 varnish'</span>), (<span class="string">'Age'</span>, <span class="string">'708'</span>), (<span class="string">'Connection'</span>, <span class="string">'close'</span>), (<span class="string">'X-Served-By'</span>, <span class="string">'cache-bwi5120-BWI, cache-tyo19943-TYO'</span>), (<span class="string">'X-Cache'</span>, <span class="string">'HIT, HIT'</span>), (<span class="string">'X-Cache-Hits'</span>, <span class="string">'2, 518'</span>), (<span class="string">'X-Timer'</span>, <span class="string">'S1584278942.717942,VS0,VE0'</span>), (<span class="string">'Vary'</span>, <span class="string">'Cookie'</span>), (<span class="string">'Strict-Transport-Security'</span>, <span class="string">'max-age=63072000; includeSubDomains'</span>)]</span><br><span class="line">nginx</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可见，前两个输出分别输出了响应的状态码和响应的头信息，最后一个输出通过调用 <code>getheader</code> 方法并传递一个参数 <code>Server</code> 获取了响应头中的 <code>Server</code> 值，结果是 <code>nginx</code>，意思是服务器是用 Nginx 搭建的。</p>
                <p>利用最基本的 <code>urlopen</code> 方法，可以完成最基本的简单网页的 GET 请求抓取。</p>
                <p>如果想给链接传递一些参数，该怎么实现呢？首先看一下 <code>urlopen</code> 方法的 API：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, [timeout,]*, cafile=<span class="literal">None</span>, capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，除了第一个参数可以传递 URL 之外，我们还可以传递其他内容，比如 <code>data</code>（附加数据）、<code>timeout</code>（超时时间）等。</p>
                <p>下面我们详细说明这几个参数的用法。</p>
                <h4 id="data-参数"><a href="#data-参数" class="headerlink" title="data 参数"></a><code>data</code> 参数</h4>
                <p><code>data</code> 参数是可选的。如果要添加该参数，需要使用 <code>bytes</code> 方法将参数转化为字节流编码格式的内容，即 <code>bytes</code> 类型。另外，如果传递了这个参数，则它的请求方式就不再是 GET 方式，而是 POST 方式。</p>
                <p>下面用实例来看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'name'</span>: <span class="string">'germey'</span>&#125;), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们传递了一个参数 <code>word</code>，值是 <code>hello</code>。它需要被转码成 <code>bytes</code>（字节流）类型。其中转字节流采用了 <code>bytes</code> 方法，该方法的第一个参数需要是 <code>str</code>（字符串）类型，需要用 <code>urllib.parse</code> 模块里的 <code>urlencode</code> 方法来将参数字典转化为字符串；第二个参数指定编码格式，这里指定为 <code>utf-8</code>。</p>
                <p>这里请求的站点是 httpbin.org，它可以提供 HTTP 请求测试。本次我们请求的 URL 为 <a href="https://httpbin.org/post" target="_blank" rel="noopener">https://httpbin.org/post</a>，这个链接可以用来测试 POST 请求，它可以输出 Request 的一些信息，其中就包含我们传递的 <code>data</code> 参数。</p>
                <p>运行结果如下：</p>
                <figure class="highlight javascript">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"form"</span>: &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"identity"</span>,</span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"11"</span>,</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Python-urllib/3.7"</span>,</span><br><span class="line">    <span class="string">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5ed27e43-9eee361fec88b7d3ce9be9db"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"json"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"17.220.233.154"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>我们传递的参数出现在了 <code>form</code> 字段中，这表明是模拟了表单提交的方式，以 POST 方式传输数据。</p>
                <h4 id="timeout-参数"><a href="#timeout-参数" class="headerlink" title="timeout 参数"></a><code>timeout</code> 参数</h4>
                <p><code>timeout</code> 参数用于设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。它支持 HTTP、HTTPS、FTP 请求。</p>
                <p>下面用实例来看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line">print(response.read())</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果可能如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">During handling of the above exception, another exception occurred:</span><br><span class="line">Traceback (most recent call last): File <span class="string">"/var/py/python/urllibtest.py"</span>, line <span class="number">4</span>, <span class="keyword">in</span> &lt;module&gt; response =</span><br><span class="line">urllib.request.urlopen(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line">...</span><br><span class="line">urllib.error.URLError: &lt;urlopen error _ssl.c:<span class="number">1059</span>: The handshake operation timed out&gt;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们设置的超时时间是 1 秒。程序运行 1 秒过后，服务器依然没有响应，于是抛出了 <code>URLError</code> 异常。该异常属于 <code>urllib.error</code> 模块，错误原因是超时。</p>
                <p>因此，可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用 <code>try…except</code> 语句来实现，相关代码如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'https://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们请求了 <a href="https://httpbin.org/get" target="_blank" rel="noopener">https://httpbin.org/get</a> 这个测试链接，设置的超时时间是 0.1 秒，然后捕获了 <code>URLError</code> 这个异常，然后判断异常类型是 <code>socket.timeout</code>，意思就是超时异常。因此，得出它确实是因为超时而报错，打印输出了 <code>TIME OUT</code>。</p>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">TIME OUT</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>按照常理来说，0.1 秒内基本不可能得到服务器响应，因此输出了 <code>TIME OUT</code> 的提示。</p>
                <p>通过设置 <code>timeout</code> 这个参数来实现超时处理，有时还是很有用的。</p>
                <h4 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h4>
                <p>除了 <code>data</code> 参数和 <code>timeout</code> 参数外，还有 <code>context</code> 参数，它必须是 <code>ssl.SSLContext</code> 类型，用来指定 SSL 设置。</p>
                <p>此外，<code>cafile</code> 和 <code>capath</code> 这两个参数分别指定 CA 证书和它的路径，这个在请求 HTTPS 链接时会有用。</p>
                <p><code>cadefault</code> 参数现在已经弃用了，其默认值为 <code>False</code>。</p>
                <p>前面讲解了 <code>urlopen</code> 方法的用法，通过这个最基本的方法，我们可以完成简单的请求和网页抓取。若需更加详细的信息，可以参见官方文档：<a href="https://docs.python.org/3/library/urllib.request.html" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html</a>。</p>
                <h3 id="Request"><a href="#Request" class="headerlink" title="Request"></a><code>Request</code></h3>
                <p>我们知道利用 <code>urlopen</code> 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求。如果请求中需要加入 <code>Headers</code> 等信息，就可以利用更强大的 <code>Request</code> 类来构建。</p>
                <p>首先，我们用实例来感受一下 <code>Request</code> 类的用法：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(<span class="string">'https://python.org'</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，我们依然用 <code>urlopen</code> 方法来发送这个请求，只不过这次该方法的参数不再是 URL，而是一个 <code>Request</code> 类型的对象。通过构造这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可更加丰富和灵活地配置参数。</p>
                <p>下面我们看一下 <code>Request</code> 可以通过怎样的参数来构造，它的构造方法如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None)</span></span></span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>其中，第一个参数 <code>url</code> 用于请求 URL，这是必传参数，其他都是可选参数。</p>
                <p>第二个参数 <code>data</code> 如果要传，必须传 <code>bytes</code>（字节流）类型的。如果它是字典，可以先用 <code>urllib.parse</code> 模块里的 <code>urlencode()</code> 编码。</p>
                <p>第三个参数 <code>headers</code> 是一个字典，它就是请求头。我们在构造请求时，既可以通过 <code>headers</code> 参数直接构造，也可以通过调用请求实例的 <code>add_header()</code> 方法添加。</p>
                <p>添加请求头最常用的方法就是通过修改 <code>User-Agent</code> 来伪装浏览器。默认的 <code>User-Agent</code> 是 <code>Python-urllib</code>，我们可以通过修改它来伪装浏览器。比如要伪装火狐浏览器，你可以把它设置为：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">Mozilla&#x2F;5.0 (X11; U; Linux i686) Gecko&#x2F;20071127 Firefox&#x2F;2.0.0.11</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>第四个参数 <code>origin_req_host</code> 指的是请求方的 host 名称或者 IP 地址。</p>
                <p>第五个参数 <code>unverifiable</code> 表示这个请求是否是无法验证的，默认是 <code>False</code>，意思就是说用户没有足够权限来选择接收这个请求的结果。例如，我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 <code>unverifiable</code> 的值就是 <code>True</code>。</p>
                <p>第六个参数 <code>method</code> 是一个字符串，用来指示请求使用的方法，比如 GET、POST 和 PUT 等。</p>
                <p>下面我们传入多个参数来构建请求：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们通过 4 个参数构造了一个请求，其中 <code>url</code> 即请求 URL，<code>headers</code> 中指定了 <code>User-Agent</code> 和 <code>Host</code>，参数 <code>data</code> 用 <code>urlencode</code> 和 <code>bytes</code> 方法转成字节流。另外，指定了请求方式为 POST。</p>
                <p>运行结果如下：</p>
                <figure class="highlight javascript">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"form"</span>: &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"identity"</span>,</span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"11"</span>,</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)"</span>,</span><br><span class="line">    <span class="string">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-5ed27f77-884f503a2aa6760df7679f05"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"json"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"17.220.233.154"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>观察结果可以发现，我们成功设置了 <code>data</code>、<code>headers</code> 和 <code>method</code>。</p>
                <p>另外，<code>headers</code> 也可以用 <code>add_header</code> 方法来添加：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">req = request.Request(url=url, data=data, method=<span class="string">'POST'</span>)</span><br><span class="line">req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>如此一来，我们就可以更加方便地构造请求，实现请求的发送啦。</p>
                <h3 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h3>
                <p>在上面的过程中，我们虽然可以构造请求，但是对于一些更高级的操作（比如 Cookies 处理、代理设置等），该怎么办呢？</p>
                <p>接下来，就需要更强大的工具 Handler 登场了。简而言之，我们可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookie 的，有处理代理设置的。利用它们，我们几乎可以做到 HTTP 请求中所有的事情。</p>
                <p>首先，介绍一下 <code>urllib.request</code> 模块里的 <code>BaseHandler</code> 类，它是所有其他 Handler 的父类，它提供了最基本的方法，例如 <code>default_open</code>、<code>protocol_request</code> 等。</p>
                <p>接下来，就有各种 Handler 子类继承这个 <code>BaseHandler</code> 类，举例如下。</p>
                <ul>
                  <li><code>HTTPDefaultErrorHandler</code> 用于处理 HTTP 响应错误，错误都会抛出 <code>HTTPError</code> 类型的异常。</li>
                  <li><code>HTTPRedirectHandler</code> 用于处理重定向。</li>
                  <li><code>HTTPCookieProcessor</code> 用于处理 Cookies。</li>
                  <li><code>ProxyHandler</code> 用于设置代理，默认代理为空。</li>
                  <li><code>HTTPPasswordMgr</code> 用于管理密码，它维护了用户名和密码的表。</li>
                  <li><code>HTTPBasicAuthHandler</code> 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。</li>
                </ul>
                <p>另外，还有其他的 Handler 类，这里就不一一列举了，详情可以参考官方文档： <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a>。</p>
                <p>关于怎么使用它们，现在先不用着急，后面会有实例演示。</p>
                <p>另一个比较重要的类就是 <code>OpenerDirector</code>，我们可以称为 Opener。我们之前用过 <code>urlopen</code> 这个方法，实际上它就是 urllib 为我们提供的一个 Opener。</p>
                <p>那么，为什么要引入 Opener 呢？因为需要实现更高级的功能。之前使用的 <code>Request</code> 和 <code>urlopen</code> 相当于类库为你封装好了极其常用的请求方法，利用它们可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以需要深入一层进行配置，使用更底层的实例来完成操作，所以这里就用到了 Opener。</p>
                <p>Opener 可以使用 <code>open</code> 方法，返回的类型和 <code>urlopen</code> 如出一辙。那么，它和 Handler 有什么关系呢？简而言之，就是利用 Handler 来构建 Opener。</p>
                <p>下面用几个实例来看看它们的用法。</p>
                <h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4>
                <p>在访问某些设置了身份认证的网站时，例如 <a href="https://ssr3.scrape.center/，我们可能会遇到这样的认证窗口，如图" target="_blank" rel="noopener">https://ssr3.scrape.center/，我们可能会遇到这样的认证窗口，如图</a> 2- 所示：</p>
                <p><img src="https://cdn.cuiqingcai.com/fries.png" alt="image-20210704202140395"></p>
                <p>图 2- 认证窗口</p>
                <p>如果遇到了这种情况，那么这个网站就是启用了基本身份认证，英文叫作 HTTP Basic Access Authentication，它是一种用来允许网页浏览器或其他客户端程序在请求时提供用户名和口令形式的身份凭证的一种登录验证方式。</p>
                <p>那么，如果要请求这样的页面，该怎么办呢？借助 <code>HTTPBasicAuthHandler</code> 就可以完成，相关代码如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'admin'</span></span><br><span class="line">password = <span class="string">'admin'</span></span><br><span class="line">url = <span class="string">'https://ssr3.scrape.center/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里首先实例化 <code>HTTPBasicAuthHandler</code> 对象，其参数是 <code>HTTPPasswordMgrWithDefaultRealm</code> 对象，它利用 <code>add_password</code> 方法添加进去用户名和密码，这样就建立了一个处理验证的 Handler。</p>
                <p>接下来，利用这个 Handler 并使用 <code>build_opener</code> 方法构建一个 Opener，这个 Opener 在发送请求时就相当于已经验证成功了。</p>
                <p>接下来，利用 Opener 的 <code>open</code> 方法打开链接，就可以完成验证了。这里获取到的结果就是验证后的页面源码内容。</p>
                <h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4>
                <p>在做爬虫的时候，免不了要使用代理，如果要添加代理，可以这样做：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:8080'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:8080'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们在本地需要先事先搭建一个 HTTP 代理，运行在 8080 端口上。</p>
                <p>这里使用了 <code>ProxyHandler</code>，其参数是一个字典，键名是协议类型（比如 HTTP 或者 HTTPS 等），键值是代理链接，可以添加多个代理。</p>
                <p>然后，利用这个 Handler 及 <code>build_opener</code> 方法构造一个 Opener，之后发送请求即可。</p>
                <h4 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h4>
                <p>Cookie 的处理就需要相关的 Handler 了。</p>
                <p>我们先用实例来看看怎样将网站的 Cookie 获取下来，相关代码如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name + <span class="string">"="</span> + item.value)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>首先，我们必须声明一个 <code>CookieJar</code> 对象。接下来，就需要利用 <code>HTTPCookieProcessor</code> 来构建一个 Handler，最后利用 <code>build_opener</code> 方法构建出 Opener，执行 <code>open</code> 函数即可。</p>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">BAIDUID&#x3D;A09E6C4E38753531B9FB4C60CE9FDFCB:FG&#x3D;1</span><br><span class="line">BIDUPSID&#x3D;A09E6C4E387535312F8AA46280C6C502</span><br><span class="line">H_PS_PSSID&#x3D;31358_1452_31325_21088_31110_31253_31605_31271_31463_30823</span><br><span class="line">PSTM&#x3D;1590854698</span><br><span class="line">BDSVRTM&#x3D;10</span><br><span class="line">BD_HOME&#x3D;1</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，这里输出了每个 Cookie 条目的名称和值。</p>
                <p>不过既然能输出，那可不可以输出成文件格式呢？我们知道 Cookie 实际上也是以文本形式保存的。</p>
                <p>答案当然是肯定的，这里通过下面的实例来看看：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.request, http.cookiejar</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'cookie.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这时 <code>CookieJar</code> 就需要换成 <code>MozillaCookieJar</code>，它在生成文件时会用到，是 <code>CookieJar</code> 的子类，可以用来处理 Cookie 和文件相关的事件，比如读取和保存 Cookie，可以将 Cookie 保存成 Mozilla 型浏览器的 Cookie 格式。</p>
                <p>运行之后，可以发现生成了一个 cookie.txt 文件，其内容如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"># Netscape HTTP Cookie File</span><br><span class="line"># http:&#x2F;&#x2F;curl.haxx.se&#x2F;rfc&#x2F;cookie_spec.html</span><br><span class="line"># This is a generated file!  Do not edit.</span><br><span class="line"></span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE	1622390755	BAIDUID	0B4A68D74B0C0E53E5B82AFD9BF9178F:FG&#x3D;1</span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE	3738338402	BIDUPSID	0B4A68D74B0C0E53471FA6329280FA58</span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE		H_PS_PSSID	31262_1438_31325_21127_31110_31596_31673_31464_30823_26350</span><br><span class="line">.baidu.com	TRUE	&#x2F;	FALSE	3738338402	PSTM	1590854754</span><br><span class="line">www.baidu.com	FALSE	&#x2F;	FALSE		BDSVRTM	0</span><br><span class="line">www.baidu.com	FALSE	&#x2F;	FALSE		BD_HOME	1</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>另外，<code>LWPCookieJar</code> 同样可以读取和保存 Cookie，但是保存的格式和 <code>MozillaCookieJar</code> 不一样，它会保存成 libwww-perl（LWP）格式的 Cookie 文件。</p>
                <p>要保存成 LWP 格式的 Cookie 文件，可以在声明时就改为：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>此时生成的内容如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">#LWP-Cookies-2.0</span><br><span class="line">Set-Cookie3: BAIDUID&#x3D;&quot;1F30EEDA35C7A94320275F991CA5B3A5:FG&#x3D;1&quot;; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; expires&#x3D;&quot;2021-05-30 16:06:39Z&quot;; comment&#x3D;bd; version&#x3D;0</span><br><span class="line">Set-Cookie3: BIDUPSID&#x3D;1F30EEDA35C7A9433C97CF6245CBC383; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; expires&#x3D;&quot;2088-06-17 19:20:46Z&quot;; version&#x3D;0</span><br><span class="line">Set-Cookie3: H_PS_PSSID&#x3D;31626_1440_21124_31069_31254_31594_30841_31673_31464_31715_30823; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; discard; version&#x3D;0</span><br><span class="line">Set-Cookie3: PSTM&#x3D;1590854799; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;.baidu.com&quot;; path_spec; domain_dot; expires&#x3D;&quot;2088-06-17 19:20:46Z&quot;; version&#x3D;0</span><br><span class="line">Set-Cookie3: BDSVRTM&#x3D;11; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;www.baidu.com&quot;; path_spec; discard; version&#x3D;0</span><br><span class="line">Set-Cookie3: BD_HOME&#x3D;1; path&#x3D;&quot;&#x2F;&quot;; domain&#x3D;&quot;www.baidu.com&quot;; path_spec; discard; version&#x3D;0</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>由此看来，生成的格式还是有比较大差异的。</p>
                <p>那么，生成了 Cookie 文件后，怎样从文件中读取并利用呢？</p>
                <p>下面我们以 <code>LWPCookieJar</code> 格式为例来看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> urllib.request, http.cookiejar</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(<span class="string">'cookie.txt'</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，这里调用 <code>load</code> 方法来读取本地的 Cookie 文件，获取到了 Cookie 的内容。不过前提是我们首先生成了 <code>LWPCookieJar</code> 格式的 Cookie，并保存成文件，然后读取 Cookie 之后使用同样的方法构建 Handler 和 Opener 即可完成操作。</p>
                <p>运行结果正常的话，会输出百度网页的源代码。</p>
                <p>通过上面的方法，我们可以实现绝大多数请求功能的设置了。</p>
                <p>这便是 urllib 库中 request 模块的基本用法，如果想实现更多的功能，可以参考官方文档的说明：<a href="https://docs.python.org/3/library/urllib.request.html#basehandler-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#basehandler-objects</a>。</p>
                <h2 id="2-处理异常"><a href="#2-处理异常" class="headerlink" title="2. 处理异常"></a>2. 处理异常</h2>
                <p>在前一节中，我们了解了请求的发送过程，但是在网络不好的情况下，如果出现了异常，该怎么办呢？这时如果不处理这些异常，程序很可能因报错而终止运行，所以异常处理还是十分有必要的。</p>
                <p>urllib 的 error 模块定义了由 request 模块产生的异常。如果出现了问题，request 模块便会抛出 error 模块中定义的异常。</p>
                <h3 id="URLError"><a href="#URLError" class="headerlink" title="URLError"></a><code>URLError</code></h3>
                <p><code>URLError</code> 类来自 urllib 库的 error 模块，它继承自 <code>OSError</code> 类，是 error 异常模块的基类，由 request 模块产生的异常都可以通过捕获这个类来处理。</p>
                <p>它具有一个属性 <code>reason</code>，即返回错误的原因。</p>
                <p>下面用一个实例来看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'https://cuiqingcai.com/404'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了 <code>URLError</code> 这个异常，运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">Not Found</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>程序没有直接报错，而是输出了如上内容，这样就可以避免程序异常终止，同时异常得到了有效处理。</p>
                <h3 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a><code>HTTPError</code></h3>
                <p>它是 <code>URLError</code> 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等。它有如下 3 个属性。</p>
                <ul>
                  <li><code>code</code>：返回 HTTP 状态码，比如 404 表示网页不存在，500 表示服务器内部错误等。</li>
                  <li><code>reason</code>：同父类一样，用于返回错误的原因。</li>
                  <li><code>headers</code>：返回请求头。</li>
                </ul>
                <p>下面我们用几个实例来看看：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'https://cuiqingcai.com/404'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">Not Found</span><br><span class="line">404</span><br><span class="line">Server: nginx&#x2F;1.10.3 (Ubuntu)</span><br><span class="line">Date: Sat, 30 May 2020 16:08:42 GMT</span><br><span class="line">Content-Type: text&#x2F;html; charset&#x3D;UTF-8</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Connection: close</span><br><span class="line">Set-Cookie: PHPSESSID&#x3D;kp1a1b0o3a0pcf688kt73gc780; path&#x3D;&#x2F;</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Vary: Cookie</span><br><span class="line">Expires: Wed, 11 Jan 1984 05:00:00 GMT</span><br><span class="line">Cache-Control: no-cache, must-revalidate, max-age&#x3D;0</span><br><span class="line">Link: &lt;https:&#x2F;&#x2F;cuiqingcai.com&#x2F;wp-json&#x2F;&gt;; rel&#x3D;&quot;https:&#x2F;&#x2F;api.w.org&#x2F;&quot;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>依然是同样的网址，这里捕获了 <code>HTTPError</code> 异常，输出了 <code>reason</code>、<code>code</code> 和 <code>headers</code> 属性。</p>
                <p>因为 <code>URLError</code> 是 <code>HTTPError</code> 的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码的更好写法如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'https://cuiqingcai.com/404'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这样就可以做到先捕获 <code>HTTPError</code>，获取它的错误原因、状态码、<code>headers</code> 等信息。如果不是 <code>HTTPError</code> 异常，就会捕获 <code>URLError</code> 异常，输出错误原因。最后，用 <code>else</code> 来处理正常的逻辑。这是一个较好的异常处理写法。</p>
                <p>有时候，<code>reason</code> 属性返回的不一定是字符串，也可能是一个对象。再看下面的实例：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'https://www.baidu.com'</span>, timeout=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(type(e.reason))</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们直接设置超时时间来强制抛出 <code>timeout</code> 异常。</p>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span>'<span class="title">socket</span>.<span class="title">timeout</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">TIME</span> <span class="title">OUT</span></span></span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，<code>reason</code> 属性的结果是 <code>socket.timeout</code> 类。所以，这里我们可以用 <code>isinstance</code> 方法来判断它的类型，作出更详细的异常判断。</p>
                <p>本节中，我们讲述了 error 模块的相关用法，通过合理地捕获异常可以做出更准确的异常判断，使程序更加稳健。</p>
                <h2 id="3-解析链接"><a href="#3-解析链接" class="headerlink" title="3. 解析链接"></a>3. 解析链接</h2>
                <p>前面说过，urllib 库里还提供了 parse 模块，它定义了处理 URL 的标准接口，例如实现 URL 各部分的抽取、合并以及链接转换。它支持如下协议的 URL 处理：<code>file</code>、<code>ftp</code>、<code>gopher</code>、<code>hdl</code>、<code>http</code>、<code>https</code>、<code>imap</code>、<code>mailto</code>、<code>mms</code>、<code>news</code>、<code>nntp</code>、<code>prospero</code>、<code>rsync</code>、<code>rtsp</code>、<code>rtspu</code>、<code>sftp</code>、<code>sip</code>、<code>sips</code>、<code>snews</code>、<code>svn</code>、<code>svn+ssh</code>、<code>telnet</code> 和 <code>wais</code>。本节中，我们介绍一下该模块中常用的方法来看一下它的便捷之处。</p>
                <h3 id="urlparse"><a href="#urlparse" class="headerlink" title="urlparse"></a><code>urlparse</code></h3>
                <p>该方法可以实现 URL 的识别和分段，这里先用一个实例来看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们利用 <code>urlparse</code> 方法进行了一个 URL 的解析。首先，输出了解析结果的类型，然后将结果也输出出来。</p>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">urllib</span>.<span class="title">parse</span>.<span class="title">ParseResult</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">ParseResult</span><span class="params">(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span></span></span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，返回结果是一个 <code>ParseResult</code> 类型的对象，它包含 6 个部分，分别是 <code>scheme</code>、<code>netloc</code>、<code>path</code>、<code>params</code>、<code>query</code> 和 <code>fragment</code>。</p>
                <p>观察一下该实例的 URL：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;index.html;user?id&#x3D;5#comment</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，<code>urlparse</code> 方法将其拆分成了 6 个部分。大体观察可以发现，解析时有特定的分隔符。比如，<code>://</code> 前面的就是 <code>scheme</code>，代表协议；第一个 <code>/</code> 符号前面便是 <code>netloc</code>，即域名，后面是 <code>path</code>，即访问路径；分号<code>;</code>后面是 <code>params</code>，代表参数；问号 <code>?</code> 后面是查询条件 <code>query</code>，一般用作 GET 类型的 URL；井号 <code>#</code> 后面是锚点，用于直接定位页面内部的下拉位置。</p>
                <p>所以，可以得出一个标准的链接格式，具体如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">scheme:&#x2F;&#x2F;netloc&#x2F;path;params?query#fragment</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>一个标准的 URL 都会符合这个规则，利用 <code>urlparse</code> 方法可以将它拆分开来。</p>
                <p>除了这种最基本的解析方式外，<code>urlparse</code> 方法还有其他配置吗？接下来，看一下它的 API 用法：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，它有 3 个参数。</p>
                <ul>
                  <li><code>urlstring</code>：这是必填项，即待解析的 URL。</li>
                  <li><code>scheme</code>：它是默认的协议（比如 <code>http</code> 或 <code>https</code> 等）。假如这个链接没有带协议信息，会将这个作为默认的协议。我们用实例来看一下：</li>
                </ul>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br><span class="line">print(result)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">ParseResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">''</span>, path=<span class="string">'www.baidu.com/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，我们提供的 URL 没有包含最前面的 <code>scheme</code> 信息，但是通过默认的 <code>scheme</code> 参数，返回的结果是 <code>https</code>。</p>
                <p>假设我们带上了 <code>scheme</code>：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>则结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">ParseResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可见，<code>scheme</code> 参数只有在 URL 中不包含 <code>scheme</code> 信息时才生效。如果 URL 中有 <code>scheme</code> 信息，就会返回解析出的 <code>scheme</code>。</p>
                <ul>
                  <li><code>allow_fragments</code>：即是否忽略 <code>fragment</code>。如果它被设置为 <code>False</code>，<code>fragment</code> 部分就会被忽略，它会被解析为 <code>path</code>、<code>parameters</code> 或者 <code>query</code> 的一部分，而 <code>fragment</code> 部分为空。</li>
                </ul>
                <p>下面我们用实例来看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">ParseResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5#comment'</span>, fragment=<span class="string">''</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>假设 URL 中不包含 <code>params</code> 和 <code>query</code>，我们再通过实例看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">ParseResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html#comment'</span>, params=<span class="string">''</span>, query=<span class="string">''</span>, fragment=<span class="string">''</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，当 URL 中不包含 <code>params</code> 和 <code>query</code> 时，<code>fragment</code> 便会被解析为 <code>path</code> 的一部分。</p>
                <p>返回结果 <code>ParseResult</code> 实际上是一个元组，我们既可以用索引顺序来获取，也可以用属性名获取。示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'https://www.baidu.com/index.html#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result.scheme, result[<span class="number">0</span>], result.netloc, result[<span class="number">1</span>], sep=<span class="string">'\n'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们分别用索引和属性名获取了 <code>scheme</code> 和 <code>netloc</code>，其运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https</span><br><span class="line">https</span><br><span class="line">www.baidu.com</span><br><span class="line">www.baidu.com</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，二者的结果是一致的，两种方法都可以成功获取。</p>
                <h3 id="urlunparse"><a href="#urlunparse" class="headerlink" title="urlunparse"></a><code>urlunparse</code></h3>
                <p>有了 <code>urlparse</code> 方法，相应地就有了它的对立方法 <code>urlunparse</code>。它接收的参数是一个可迭代对象，但是它的长度必须是 6，否则会抛出参数数量不足或者过多的问题。先用一个实例看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line">data = [<span class="string">'https'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里参数 <code>data</code> 用了列表类型。当然，你也可以用其他类型，比如元组或者特定的数据结构。</p>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;index.html;user?a&#x3D;6#comment</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这样我们就成功实现了 URL 的构造。</p>
                <h3 id="urlsplit"><a href="#urlsplit" class="headerlink" title="urlsplit"></a><code>urlsplit</code></h3>
                <p>这个方法和 <code>urlparse</code> 方法非常相似，只不过它不再单独解析 <code>params</code> 这一部分，只返回 5 个结果。上面例子中的 <code>params</code> 会合并到 <code>path</code> 中。示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">SplitResult(scheme=<span class="string">'https'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html;user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，返回结果是 <code>SplitResult</code>，它其实也是一个元组类型，既可以用属性获取值，也可以用索引来获取。示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'https://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result.scheme, result[<span class="number">0</span>])</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https https</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <h3 id="urlunsplit"><a href="#urlunsplit" class="headerlink" title="urlunsplit"></a><code>urlunsplit</code></h3>
                <p>与 <code>urlunparse</code> 方法类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是长度必须为 5。示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunsplit</span><br><span class="line"></span><br><span class="line">data = [<span class="string">'https'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunsplit(data))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;index.html?a&#x3D;6#comment</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <h3 id="urljoin"><a href="#urljoin" class="headerlink" title="urljoin"></a><code>urljoin</code></h3>
                <p>有了 <code>urlunparse</code> 和 <code>urlunsplit</code> 方法，我们可以完成链接的合并，不过前提是必须要有特定长度的对象，链接的每一部分都要清晰分开。</p>
                <p>此外，生成链接还有另一个方法，那就是 <code>urljoin</code> 方法。我们可以提供一个 <code>base_url</code>（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析 <code>base_url</code> 的 <code>scheme</code>、<code>netloc</code> 和 <code>path</code> 这 3 个内容并对新链接缺失的部分进行补充，最后返回结果。</p>
                <p>下面通过几个实例看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com'</span>, <span class="string">'FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html?question=2'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com?wd=abc'</span>, <span class="string">'https://cuiqingcai.com/index.php'</span>))</span><br><span class="line">print(urljoin(<span class="string">'https://www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com#comment'</span>, <span class="string">'?category=2'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;FAQ.html</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;FAQ.html</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;FAQ.html</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;FAQ.html?question&#x3D;2</span><br><span class="line">https:&#x2F;&#x2F;cuiqingcai.com&#x2F;index.php</span><br><span class="line">https:&#x2F;&#x2F;www.baidu.com?category&#x3D;2#comment</span><br><span class="line">www.baidu.com?category&#x3D;2#comment</span><br><span class="line">www.baidu.com?category&#x3D;2</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以发现，<code>base_url</code> 提供了三项内容 <code>scheme</code>、<code>netloc</code> 和 <code>path</code>。如果这 3 项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而 <code>base_url</code> 中的 <code>params</code>、<code>query</code> 和 <code>fragment</code> 是不起作用的。</p>
                <p>通过 <code>urljoin</code> 方法，我们可以轻松实现链接的解析、拼合与生成。</p>
                <h3 id="urlencode"><a href="#urlencode" class="headerlink" title="urlencode"></a><code>urlencode</code></h3>
                <p>这里我们再介绍一个常用的方法 —— <code>urlencode</code>，它在构造 GET 请求参数的时候非常有用，示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'https://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里首先声明一个字典来将参数表示出来，然后调用 <code>urlencode</code> 方法将其序列化为 GET 请求参数。</p>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com?name&#x3D;germey&amp;age&#x3D;25</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，参数成功地由字典类型转化为 GET 请求参数了。</p>
                <p>这个方法非常常用。有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为 URL 的参数时，只需要调用该方法即可。</p>
                <h3 id="parse-qs"><a href="#parse-qs" class="headerlink" title="parse_qs"></a><code>parse_qs</code></h3>
                <p>有了序列化，必然就有反序列化。如果我们有一串 GET 请求参数，利用 <code>parse_qs</code> 方法，就可以将它转回字典，示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=25'</span></span><br><span class="line">print(parse_qs(query))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">&#123;<span class="string">'name'</span>: [<span class="string">'germey'</span>], <span class="string">'age'</span>: [<span class="string">'25'</span>]&#125;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，这样就成功转回为字典类型了。</p>
                <h3 id="parse-qsl"><a href="#parse-qsl" class="headerlink" title="parse_qsl"></a><code>parse_qsl</code></h3>
                <p>另外，还有一个 <code>parse_qsl</code> 方法，它用于将参数转化为元组组成的列表，示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=25'</span></span><br><span class="line">print(parse_qsl(query))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">[(<span class="string">'name'</span>, <span class="string">'germey'</span>), (<span class="string">'age'</span>, <span class="string">'25'</span>)]</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，运行结果是一个列表，而列表中的每一个元素都是一个元组，元组的第一个内容是参数名，第二个内容是参数值。</p>
                <h3 id="quote"><a href="#quote" class="headerlink" title="quote"></a><code>quote</code></h3>
                <p>该方法可以将内容转化为 URL 编码的格式。URL 中带有中文参数时，有时可能会导致乱码的问题，此时可以用这个方法可以将中文字符转化为 URL 编码，示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里我们声明了一个中文的搜索文字，然后用 <code>quote</code> 方法对其进行 URL 编码，最后得到的结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;%E5%A3%81%E7%BA%B8</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <h3 id="unquote"><a href="#unquote" class="headerlink" title="unquote"></a><code>unquote</code></h3>
                <p>有了 <code>quote</code> 方法，当然还有 <code>unquote</code> 方法，它可以进行 URL 解码，示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'</span></span><br><span class="line">print(unquote(url))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这是上面得到的 URL 编码后的结果，这里利用 <code>unquote</code> 方法还原，结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;壁纸</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>可以看到，利用 <code>unquote</code> 方法可以方便地实现解码。</p>
                <p>本节中，我们介绍了 parse 模块的一些常用 URL 处理方法。有了这些方法，我们可以方便地实现 URL 的解析和构造，建议熟练掌握。</p>
                <h2 id="4-分析-Robots-协议"><a href="#4-分析-Robots-协议" class="headerlink" title="4. 分析 Robots 协议"></a>4. 分析 Robots 协议</h2>
                <p>利用 urllib 的 robotparser 模块，我们可以实现网站 Robots 协议的分析。本节中，我们来简单了解一下该模块的用法。</p>
                <h3 id="1-Robots-协议"><a href="#1-Robots-协议" class="headerlink" title="1. Robots 协议"></a>1. Robots 协议</h3>
                <p>Robots 协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作 robots.txt 的文本文件，一般放在网站的根目录下。</p>
                <p>当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在 robots.txt 文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。</p>
                <p>下面我们看一个 robots.txt 的样例：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br><span class="line">Allow: &#x2F;public&#x2F;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这实现了对所有搜索爬虫只允许爬取 public 目录的功能，将上述内容保存成 robots.txt 文件，放在网站的根目录下，和网站的入口文件（比如 index.php、index.html 和 index.jsp 等）放在一起。</p>
                <p>上面的 <code>User-agent</code> 描述了搜索爬虫的名称，这里将其设置为 <code>*</code> 则代表该协议对任何爬取爬虫有效。比如，我们可以设置：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">User-agent: Baiduspider</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这就代表我们设置的规则对百度爬虫是有效的。如果有多条 <code>User-agent</code> 记录，就有多个爬虫会受到爬取限制，但至少需要指定一条。</p>
                <p><code>Disallow</code> 指定了不允许抓取的目录，比如上例子中设置为 <code>/</code> 则代表不允许抓取所有页面。</p>
                <p><code>Allow</code> 一般和 <code>Disallow</code> 一起使用，一般不会单独使用，用来排除某些限制。上例中我们设置为 <code>/public/</code>，则表示所有页面不允许抓取，但可以抓取 public 目录。</p>
                <p>下面我们再来看几个例子。禁止所有爬虫访问任何目录的代码如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>允许所有爬虫访问任何目录的代码如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">User-agent: *</span><br><span class="line">Disallow:</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>另外，直接把 robots.txt 文件留空也是可以的。</p>
                <p>禁止所有爬虫访问网站某些目录的代码如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;private&#x2F;</span><br><span class="line">Disallow: &#x2F;tmp&#x2F;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>只允许某一个爬虫访问的代码如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">User-agent: WebCrawler</span><br><span class="line">Disallow:</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这些是 robots.txt 的一些常见写法。</p>
                <h3 id="爬虫名称"><a href="#爬虫名称" class="headerlink" title="爬虫名称"></a>爬虫名称</h3>
                <p>大家可能会疑惑，爬虫名是从哪儿来的？为什么就叫这个名？其实它是有固定名字的了，比如百度的就叫作 BaiduSpider。表 2- 列出了一些常见搜索爬虫的名称及对应的网站。</p>
                <p>表 一些常见搜索爬虫的名称及其对应的网站</p>
                <div class="table-container">
                  <table>
                    <thead>
                      <tr>
                        <th style="text-align:left">爬虫名称</th>
                        <th style="text-align:left">名称</th>
                        <th style="text-align:left">网站</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td style="text-align:left">BaiduSpider</td>
                        <td style="text-align:left">百度</td>
                        <td style="text-align:left">www.baidu.com</td>
                      </tr>
                      <tr>
                        <td style="text-align:left">Googlebot</td>
                        <td style="text-align:left">谷歌</td>
                        <td style="text-align:left">www.google.com</td>
                      </tr>
                      <tr>
                        <td style="text-align:left">360Spider</td>
                        <td style="text-align:left">360 搜索</td>
                        <td style="text-align:left">www.so.com</td>
                      </tr>
                      <tr>
                        <td style="text-align:left">YodaoBot</td>
                        <td style="text-align:left">有道</td>
                        <td style="text-align:left">www.youdao.com</td>
                      </tr>
                      <tr>
                        <td style="text-align:left">ia_archiver</td>
                        <td style="text-align:left">Alexa</td>
                        <td style="text-align:left">www.alexa.cn</td>
                      </tr>
                      <tr>
                        <td style="text-align:left">Scooter</td>
                        <td style="text-align:left">altavista</td>
                        <td style="text-align:left">www.altavista.com</td>
                      </tr>
                      <tr>
                        <td style="text-align:left">Bingbot</td>
                        <td style="text-align:left">必应</td>
                        <td style="text-align:left">www.bing.com</td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <h3 id="robotparser"><a href="#robotparser" class="headerlink" title="robotparser"></a>robotparser</h3>
                <p>了解 Robots 协议之后，我们就可以使用 robotparser 模块来解析 robots.txt 了。该模块提供了一个类 <code>RobotFileParser</code>，它可以根据某网站的 robots.txt 文件来判断一个爬虫是否有权限来爬取这个网页。</p>
                <p>该类用起来非常简单，只需要在构造方法里传入 robots.txt 的链接即可。首先看一下它的声明：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">urllib.robotparser.RobotFileParser(url=<span class="string">''</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>当然，也可以在声明时不传入，默认为空，最后再使用 <code>set_url</code> 方法设置一下即可。</p>
                <p>下面列出了这个类常用的几个方法。</p>
                <ul>
                  <li><code>set_url</code>：用来设置 robots.txt 文件的链接。如果在创建 <code>RobotFileParser</code> 对象时传入了链接，那么就不需要再使用这个方法设置了。</li>
                  <li><code>read</code>：读取 robots.txt 文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 <code>False</code>，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。</li>
                  <li><code>parse</code>：用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。</li>
                  <li><code>can_fetch</code>：该方法用两个参数，第一个是 <code>User-Agent</code>，第二个是要抓取的 URL。返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 <code>True</code> 或 <code>False</code>。</li>
                  <li><code>mtime</code>：返回的是上次抓取和分析 robots.txt 的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。</li>
                  <li><code>modified</code>：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。</li>
                </ul>
                <p>下面我们用实例来看一下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'https://www.baidu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Googlebot'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里以百度为例，首先创建 <code>RobotFileParser</code> 对象，然后通过 <code>set_url</code> 方法设置了 robots.txt 的链接。当然，不用这个方法的话，可以在声明时直接用如下方法设置：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">rp = RobotFileParser(<span class="string">'https://www.baidu.com/robots.txt'</span>)</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>接着利用 <code>can_fetch</code> 方法判断网页是否可以被抓取。</p>
                <p>运行结果如下：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>这里同样可以使用 <code>parse</code> 方法执行读取和分析，示例如下：<br>可以看到这里我们利用 Baiduspider 可以抓取百度等首页以及 homepage 页面，但是 Googlebot 就不能抓取 homepage 页面。</p>
                <p>打开百度的 robots.txt 文件看下，可以看到如下的信息：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">User-agent: Baiduspider</span><br><span class="line">Disallow: &#x2F;baidu</span><br><span class="line">Disallow: &#x2F;s?</span><br><span class="line">Disallow: &#x2F;ulink?</span><br><span class="line">Disallow: &#x2F;link?</span><br><span class="line">Disallow: &#x2F;home&#x2F;news&#x2F;data&#x2F;</span><br><span class="line">Disallow: &#x2F;bh</span><br><span class="line"></span><br><span class="line">User-agent: Googlebot</span><br><span class="line">Disallow: &#x2F;baidu</span><br><span class="line">Disallow: &#x2F;s?</span><br><span class="line">Disallow: &#x2F;shifen&#x2F;</span><br><span class="line">Disallow: &#x2F;homepage&#x2F;</span><br><span class="line">Disallow: &#x2F;cpro</span><br><span class="line">Disallow: &#x2F;ulink?</span><br><span class="line">Disallow: &#x2F;link?</span><br><span class="line">Disallow: &#x2F;home&#x2F;news&#x2F;data&#x2F;</span><br><span class="line">Disallow: &#x2F;bh</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>由此我们可以看到，Baiduspider 没有限制 homepage 页面的抓取，而 Googlebot 则限制了 homepage 页面的抓取。</p>
                <p>这里同样可以使用 parse 方法执行读取和分析，示例如下：</p>
                <figure class="highlight python">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.parse(urlopen(<span class="string">'https://www.baidu.com/robots.txt'</span>).read().decode(<span class="string">'utf-8'</span>).split(<span class="string">'\n'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Baiduspider'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'Googlebot'</span>, <span class="string">'https://www.baidu.com/homepage/'</span>))</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>运行结果一样：</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>本节介绍了 robotparser 模块的基本用法和实例，利用它，我们可以方便地判断哪些页面可以抓取，哪些页面不可以抓取。</p>
                <h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2>
                <p>本节内容比较多，我们介绍了 urllib 的 request、error、parse、robotparser 模块的基本用法。这些是一些基础模块，其中有一些模块的实用性还是很强的，比如我们可以利用 parse 模块来进行 URL 的各种处理。</p>
                <p>本节代码：<a href="https://github.com/Python3WebSpider/UrllibTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/UrllibTest</a>。</p>
              </div>
              <div class="popular-posts-header">相关文章</div>
              <ul class="popular-posts">
                <li class="popular-posts-item">
                  <div class="popular-posts-title"><a href="/5052.html" rel="bookmark">Python3网络爬虫开发实战教程</a></div>
                </li>
                <li class="popular-posts-item">
                  <div class="popular-posts-title"><a href="/202212.html" rel="bookmark">【2022 年】Python3 爬虫教程 - HTTP 基本原理</a></div>
                </li>
                <li class="popular-posts-item">
                  <div class="popular-posts-title"><a href="/202213.html" rel="bookmark">【2022 年】Python3 爬虫教程 - Web网页基础</a></div>
                </li>
                <li class="popular-posts-item">
                  <div class="popular-posts-title"><a href="/202215.html" rel="bookmark">【2022 年】Python3 爬虫教程 - 1.5 代理的基本原理</a></div>
                </li>
                <li class="popular-posts-item">
                  <div class="popular-posts-title"><a href="/202246.html" rel="bookmark">【2022 年】Python3 爬虫教程 - 便于高效检索的 Elasticsearch 存储</a></div>
                </li>
              </ul>
              <div class="reward-container">
                <div></div>
                <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏 </button>
                <div id="qr" style="display: none;">
                  <div style="display: inline-block;">
                    <img src="/images/wechatpay.jpg" alt="崔庆才 微信支付">
                    <p>微信支付</p>
                  </div>
                  <div style="display: inline-block;">
                    <img src="/images/alipay.jpg" alt="崔庆才 支付宝">
                    <p>支付宝</p>
                  </div>
                </div>
              </div>
              <footer class="post-footer">
                <div class="post-tags">
                  <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a>
                  <a href="/tags/Python%E7%88%AC%E8%99%AB/" rel="tag"><i class="fa fa-tag"></i> Python爬虫</a>
                  <a href="/tags/%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B/" rel="tag"><i class="fa fa-tag"></i> 爬虫教程</a>
                  <a href="/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" rel="tag"><i class="fa fa-tag"></i> 网络爬虫</a>
                  <a href="/tags/2022/" rel="tag"><i class="fa fa-tag"></i> 2022</a>
                  <a href="/tags/urllib/" rel="tag"><i class="fa fa-tag"></i> urllib</a>
                </div>
                <div class="post-nav">
                  <div class="post-nav-item">
                    <a href="/202223.html" rel="prev" title="【2022 年】Python3 爬虫教程 - 强大灵活的正则表达式">
                      <i class="fa fa-chevron-left"></i> 【2022 年】Python3 爬虫教程 - 强大灵活的正则表达式 </a>
                  </div>
                  <div class="post-nav-item">
                    <a href="/202222.html" rel="next" title="【2022 年】Python3 爬虫教程 - 方便好用的 requests"> 【2022 年】Python3 爬虫教程 - 方便好用的 requests <i class="fa fa-chevron-right"></i>
                    </a>
                  </div>
                </div>
              </footer>
            </article>
          </div>
          <div class="comments" id="gitalk-container"></div>
          <script>
            window.addEventListener('tabs:register', () =>
            {
              let
              {
                activeClass
              } = CONFIG.comments;
              if (CONFIG.comments.storage)
              {
                activeClass = localStorage.getItem('comments_active') || activeClass;
              }
              if (activeClass)
              {
                let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
                if (activeTab)
                {
                  activeTab.click();
                }
              }
            });
            if (CONFIG.comments.storage)
            {
              window.addEventListener('tabs:click', event =>
              {
                if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
                let commentClass = event.target.classList[1];
                localStorage.setItem('comments_active', commentClass);
              });
            }

          </script>
        </div>
        <div class="toggle sidebar-toggle">
          <span class="toggle-line toggle-line-first"></span>
          <span class="toggle-line toggle-line-middle"></span>
          <span class="toggle-line toggle-line-last"></span>
        </div>
        <aside class="sidebar">
          <div class="sidebar-inner">
            <ul class="sidebar-nav motion-element">
              <li class="sidebar-nav-toc"> 文章目录 </li>
              <li class="sidebar-nav-overview"> 站点概览 </li>
            </ul>
            <!--noindex-->
            <div class="post-toc-wrap sidebar-panel">
              <div class="post-toc motion-element">
                <ol class="nav">
                  <li class="nav-item nav-level-2"><a class="nav-link" href="#1-发送请求"><span class="nav-number">1.</span> <span class="nav-text">1. 发送请求</span></a>
                    <ol class="nav-child">
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#urlopen"><span class="nav-number">1.1.</span> <span class="nav-text">urlopen</span></a>
                        <ol class="nav-child">
                          <li class="nav-item nav-level-4"><a class="nav-link" href="#data-参数"><span class="nav-number">1.1.1.</span> <span class="nav-text">data 参数</span></a></li>
                          <li class="nav-item nav-level-4"><a class="nav-link" href="#timeout-参数"><span class="nav-number">1.1.2.</span> <span class="nav-text">timeout 参数</span></a></li>
                          <li class="nav-item nav-level-4"><a class="nav-link" href="#其他参数"><span class="nav-number">1.1.3.</span> <span class="nav-text">其他参数</span></a></li>
                        </ol>
                      </li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#Request"><span class="nav-number">1.2.</span> <span class="nav-text">Request</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#高级用法"><span class="nav-number">1.3.</span> <span class="nav-text">高级用法</span></a>
                        <ol class="nav-child">
                          <li class="nav-item nav-level-4"><a class="nav-link" href="#验证"><span class="nav-number">1.3.1.</span> <span class="nav-text">验证</span></a></li>
                          <li class="nav-item nav-level-4"><a class="nav-link" href="#代理"><span class="nav-number">1.3.2.</span> <span class="nav-text">代理</span></a></li>
                          <li class="nav-item nav-level-4"><a class="nav-link" href="#Cookie"><span class="nav-number">1.3.3.</span> <span class="nav-text">Cookie</span></a></li>
                        </ol>
                      </li>
                    </ol>
                  </li>
                  <li class="nav-item nav-level-2"><a class="nav-link" href="#2-处理异常"><span class="nav-number">2.</span> <span class="nav-text">2. 处理异常</span></a>
                    <ol class="nav-child">
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#URLError"><span class="nav-number">2.1.</span> <span class="nav-text">URLError</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#HTTPError"><span class="nav-number">2.2.</span> <span class="nav-text">HTTPError</span></a></li>
                    </ol>
                  </li>
                  <li class="nav-item nav-level-2"><a class="nav-link" href="#3-解析链接"><span class="nav-number">3.</span> <span class="nav-text">3. 解析链接</span></a>
                    <ol class="nav-child">
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#urlparse"><span class="nav-number">3.1.</span> <span class="nav-text">urlparse</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#urlunparse"><span class="nav-number">3.2.</span> <span class="nav-text">urlunparse</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#urlsplit"><span class="nav-number">3.3.</span> <span class="nav-text">urlsplit</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#urlunsplit"><span class="nav-number">3.4.</span> <span class="nav-text">urlunsplit</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#urljoin"><span class="nav-number">3.5.</span> <span class="nav-text">urljoin</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#urlencode"><span class="nav-number">3.6.</span> <span class="nav-text">urlencode</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#parse-qs"><span class="nav-number">3.7.</span> <span class="nav-text">parse_qs</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#parse-qsl"><span class="nav-number">3.8.</span> <span class="nav-text">parse_qsl</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#quote"><span class="nav-number">3.9.</span> <span class="nav-text">quote</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#unquote"><span class="nav-number">3.10.</span> <span class="nav-text">unquote</span></a></li>
                    </ol>
                  </li>
                  <li class="nav-item nav-level-2"><a class="nav-link" href="#4-分析-Robots-协议"><span class="nav-number">4.</span> <span class="nav-text">4. 分析 Robots 协议</span></a>
                    <ol class="nav-child">
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#1-Robots-协议"><span class="nav-number">4.1.</span> <span class="nav-text">1. Robots 协议</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫名称"><span class="nav-number">4.2.</span> <span class="nav-text">爬虫名称</span></a></li>
                      <li class="nav-item nav-level-3"><a class="nav-link" href="#robotparser"><span class="nav-number">4.3.</span> <span class="nav-text">robotparser</span></a></li>
                    </ol>
                  </li>
                  <li class="nav-item nav-level-2"><a class="nav-link" href="#5-总结"><span class="nav-number">5.</span> <span class="nav-text">5. 总结</span></a></li>
                </ol>
              </div>
            </div>
            <!--/noindex-->
            <div class="site-overview-wrap sidebar-panel">
              <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
                <img class="site-author-image" itemprop="image" alt="崔庆才" src="/images/avatar.png">
                <p class="site-author-name" itemprop="name">崔庆才</p>
                <div class="site-description" itemprop="description">静觅丨崔庆才的个人站点专业为您提供爬虫教程,爬虫,Python,Python爬虫,Python爬虫教程,爬虫书的相关信息，想要了解更多详情，请联系我们。</div>
              </div>
              <div class="site-state-wrap motion-element">
                <nav class="site-state">
                  <div class="site-state-item site-state-posts">
                    <a href="/archives/">
                      <span class="site-state-item-count">685</span>
                      <span class="site-state-item-name">日志</span>
                    </a>
                  </div>
                  <div class="site-state-item site-state-categories">
                    <a href="/categories/">
                      <span class="site-state-item-count">32</span>
                      <span class="site-state-item-name">分类</span></a>
                  </div>
                  <div class="site-state-item site-state-tags">
                    <a href="/tags/">
                      <span class="site-state-item-count">246</span>
                      <span class="site-state-item-name">标签</span></a>
                  </div>
                </nav>
              </div>
              <div class="links-of-author motion-element">
                <span class="links-of-author-item">
                  <a href="https://github.com/Germey" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Germey" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
                </span>
                <span class="links-of-author-item">
                  <a href="mailto:cqc@cuiqingcai.com.com" title="邮件 → mailto:cqc@cuiqingcai.com.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮件</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://weibo.com/cuiqingcai" title="微博 → https:&#x2F;&#x2F;weibo.com&#x2F;cuiqingcai" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>微博</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/Germey" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;Germey" rel="noopener" target="_blank"><i class="fa fa-magic fa-fw"></i>知乎</a>
                </span>
              </div>
            </div>
            <div style=" width: 100%;" class="sidebar-panel sidebar-panel-image sidebar-panel-active">
              <a href="https://item.jd.com/13527222.html" target="_blank" rel="noopener">
                <img src="https://cdn.cuiqingcai.com/ei5og.jpg" style=" width: 100%;">
              </a>
            </div>
            <div class="sidebar-panel sidebar-panel-categories sidebar-panel-active">
              <h4 class="name"> 分类 </h4>
              <div class="content">
                <ul class="category-list">
                  <li class="category-list-item"><a class="category-list-link" href="/categories/API/">API</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/C-C/">C/C++</a><span class="category-list-count">23</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/HTML/">HTML</a><span class="category-list-count">14</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/JavaScript/">JavaScript</a><span class="category-list-count">26</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">14</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Luma/">Luma</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Markdown/">Markdown</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Net/">Net</a><span class="category-list-count">4</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Nexior/">Nexior</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Other/">Other</a><span class="category-list-count">40</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/PHP/">PHP</a><span class="category-list-count">27</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">303</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/TypeScript/">TypeScript</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E5%B1%95%E7%A4%BA/">个人展示</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E6%97%A5%E8%AE%B0/">个人日记</a><span class="category-list-count">9</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E8%AE%B0%E5%BD%95/">个人记录</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/">个人随笔</a><span class="category-list-count">21</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">安装配置</a><span class="category-list-count">59</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/">技术杂谈</a><span class="category-list-count">96</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">4</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB%E7%AC%94%E8%AE%B0/">生活笔记</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A6%8F%E5%88%A9%E4%B8%93%E5%8C%BA/">福利专区</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E8%81%8C%E4%BD%8D%E6%8E%A8%E8%8D%90/">职位推荐</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E8%89%BA%E6%9C%AF%E4%BA%8C%E7%BB%B4%E7%A0%81/">艺术二维码</a><span class="category-list-count">1</span></li>
                </ul>
              </div>
            </div>
            <div class="sidebar-panel sidebar-panel-friends sidebar-panel-active">
              <h4 class="name"> 友情链接 </h4>
              <ul class="friends">
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/j2dub.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.findhao.net/" target="_blank" rel="noopener">FindHao</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/6apxu.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.51dev.com/" target="_blank" rel="noopener">IT技术社区</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/bqlbs.png">
                  </span>
                  <span class="link">
                    <a href="http://www.urselect.com/" target="_blank" rel="noopener">优社电商</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/8s88c.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yuanrenxue.com/" target="_blank" rel="noopener">猿人学</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/2wgg5.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yunlifang.cn/" target="_blank" rel="noopener">云立方</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="http://qianxunclub.com/favicon.png">
                  </span>
                  <span class="link">
                    <a href="http://qianxunclub.com/" target="_blank" rel="noopener">千寻啊千寻</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/0044u.jpg">
                  </span>
                  <span class="link">
                    <a href="http://kodcloud.com/" target="_blank" rel="noopener">可道云</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/ygnpn.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.kunkundashen.cn/" target="_blank" rel="noopener">坤坤大神</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/x714o.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.hubwiz.com/" target="_blank" rel="noopener">汇智网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/44hxf.png">
                  </span>
                  <span class="link">
                    <a href="http://redstonewill.com/" target="_blank" rel="noopener">红色石头</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/wkaus.jpg">
                  </span>
                  <span class="link">
                    <a href="https://zhaoshuai.me/" target="_blank" rel="noopener">碎念</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/pgo0r.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.chenwenguan.com/" target="_blank" rel="noopener">陈文管的博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/kk82a.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.lxlinux.net/" target="_blank" rel="noopener">良许Linux教程网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/lj0t2.jpg">
                  </span>
                  <span class="link">
                    <a href="https://tanqingbo.cn/" target="_blank" rel="noopener">IT码农</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/i8cdr.png">
                  </span>
                  <span class="link">
                    <a href="https://junyiseo.com/" target="_blank" rel="noopener">均益个人博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://cdn.cuiqingcai.com/chwv2.png">
                  </span>
                  <span class="link">
                    <a href="https://brucedone.com/" target="_blank" rel="noopener">大鱼的鱼塘</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://www.91vps.com/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="http://www.91vps.com/" target="_blank" rel="noopener">91VPS</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://webpage.qidian.qq.com/qidian/chatv3-gray/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="https://www.qg.net/" target="_blank" rel="noopener">青果网络</a>
                  </span>
                </li>
              </ul>
            </div>
            <div class="sidebar-panel sidebar-panel-tags sidebar-panel-active">
              <h4 class="name"> 标签云 </h4>
              <div class="content">
                <a href="/tags/2022/" style="font-size: 20px;">2022</a> <a href="/tags/2048/" style="font-size: 10px;">2048</a> <a href="/tags/ADSL/" style="font-size: 10px;">ADSL</a> <a href="/tags/API/" style="font-size: 16px;">API</a> <a href="/tags/Ajax/" style="font-size: 12px;">Ajax</a> <a href="/tags/Bootstrap/" style="font-size: 11px;">Bootstrap</a> <a href="/tags/Bug/" style="font-size: 10px;">Bug</a> <a href="/tags/CDN/" style="font-size: 10px;">CDN</a> <a href="/tags/CQC/" style="font-size: 10px;">CQC</a> <a href="/tags/CSS/" style="font-size: 10px;">CSS</a> <a href="/tags/CSS-%E5%8F%8D%E7%88%AC%E8%99%AB/" style="font-size: 10px;">CSS 反爬虫</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/ChatGPT/" style="font-size: 10px;">ChatGPT</a> <a href="/tags/Cookie/" style="font-size: 10px;">Cookie</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Eclipse/" style="font-size: 11px;">Eclipse</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/FTP/" style="font-size: 10px;">FTP</a> <a href="/tags/Flux/" style="font-size: 10px;">Flux</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/GitHub/" style="font-size: 13px;">GitHub</a> <a href="/tags/HTML5/" style="font-size: 10px;">HTML5</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/Hailuo/" style="font-size: 10px;">Hailuo</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Hook/" style="font-size: 10px;">Hook</a> <a href="/tags/IP/" style="font-size: 10px;">IP</a> <a href="/tags/IT/" style="font-size: 10px;">IT</a> <a href="/tags/JSON/" style="font-size: 10px;">JSON</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/JavaScript/" style="font-size: 14px;">JavaScript</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/LOGO/" style="font-size: 10px;">LOGO</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Luma/" style="font-size: 10px;">Luma</a> <a href="/tags/MIUI/" style="font-size: 10px;">MIUI</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/Midjourney/" style="font-size: 11px;">Midjourney</a> <a href="/tags/MongoDB/" style="font-size: 11px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Mysql/" style="font-size: 10px;">Mysql</a> <a href="/tags/NBA/" style="font-size: 10px;">NBA</a> <a href="/tags/Nexior/" style="font-size: 10px;">Nexior</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/OpenCV/" style="font-size: 10px;">OpenCV</a> <a href="/tags/PHP/" style="font-size: 11px;">PHP</a> <a href="/tags/PPT/" style="font-size: 10px;">PPT</a> <a href="/tags/PS/" style="font-size: 10px;">PS</a> <a href="/tags/Pathlib/" style="font-size: 10px;">Pathlib</a> <a href="/tags/PhantomJS/" style="font-size: 10px;">PhantomJS</a> <a href="/tags/Playwright/" style="font-size: 10px;">Playwright</a> <a href="/tags/Python/" style="font-size: 17px;">Python</a> <a href="/tags/Python-%E7%88%AC%E8%99%AB/" style="font-size: 18px;">Python 爬虫</a> <a href="/tags/Python3/" style="font-size: 11px;">Python3</a> <a href="/tags/Python3%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B/" style="font-size: 12px;">Python3爬虫教程</a> <a href="/tags/Pythonic/" style="font-size: 10px;">Pythonic</a> <a href="/tags/Python%E7%88%AC%E8%99%AB/" style="font-size: 19px;">Python爬虫</a> <a href="/tags/Python%E7%88%AC%E8%99%AB%E4%B9%A6/" style="font-size: 12px;">Python爬虫书</a> <a href="/tags/Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B/" style="font-size: 15px;">Python爬虫教程</a> <a href="/tags/QQ/" style="font-size: 10px;">QQ</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/ReCAPTCHA/" style="font-size: 10px;">ReCAPTCHA</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Riffusion/" style="font-size: 10px;">Riffusion</a> <a href="/tags/SAE/" style="font-size: 10px;">SAE</a> <a href="/tags/SSH/" style="font-size: 10px;">SSH</a> <a href="/tags/SVG/" style="font-size: 10px;">SVG</a> <a href="/tags/Scrapy-redis/" style="font-size: 10px;">Scrapy-redis</a> <a href="/tags/Scrapy%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 10px;">Scrapy分布式</a> <a href="/tags/Selenium/" style="font-size: 11px;">Selenium</a> <a href="/tags/Session/" style="font-size: 10px;">Session</a> <a href="/tags/Shell/" style="font-size: 10px;">Shell</a> <a href="/tags/Suno/" style="font-size: 10px;">Suno</a> <a href="/tags/TKE/" style="font-size: 10px;">TKE</a> <a href="/tags/TXT/" style="font-size: 10px;">TXT</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/Ubuntu/" style="font-size: 11px;">Ubuntu</a> <a href="/tags/VS-Code/" style="font-size: 10px;">VS Code</a> <a href="/tags/Veo/" style="font-size: 10px;">Veo</a> <a href="/tags/Vercel/" style="font-size: 10px;">Vercel</a> <a href="/tags/Vs-Code/" style="font-size: 10px;">Vs Code</a> <a href="/tags/Vue/" style="font-size: 11px;">Vue</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a> <a href="/tags/Webpack/" style="font-size: 10px;">Webpack</a> <a href="/tags/Web%E7%BD%91%E9%A1%B5/" style="font-size: 10px;">Web网页</a> <a href="/tags/Windows/" style="font-size: 10px;">Windows</a> <a href="/tags/Winpcap/" style="font-size: 10px;">Winpcap</a> <a href="/tags/WordPress/" style="font-size: 13px;">WordPress</a> <a href="/tags/XPath/" style="font-size: 12px;">XPath</a> <a href="/tags/Youtube/" style="font-size: 11px;">Youtube</a> <a href="/tags/acedata/" style="font-size: 12px;">acedata</a> <a href="/tags/aiohttp/" style="font-size: 10px;">aiohttp</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/ansible/" style="font-size: 10px;">ansible</a> <a href="/tags/api/" style="font-size: 13px;">api</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/cocos2d-x/" style="font-size: 10px;">cocos2d-x</a> <a href="/tags/dummy-change/" style="font-size: 10px;">dummy change</a> <a href="/tags/e6/" style="font-size: 10px;">e6</a> <a href="/tags/fitvids/" style="font-size: 10px;">fitvids</a>
              </div>
              <script>
                const tagsColors = ['#00a67c', '#5cb85c', '#d9534f', '#567e95', '#b37333', '#f4843d', '#15a287']
                const tagsElements = document.querySelectorAll('.sidebar-panel-tags .content a')
                tagsElements.forEach((item) =>
                {
                  item.style.backgroundColor = tagsColors[Math.floor(Math.random() * tagsColors.length)]
                })

              </script>
            </div>
          </div>
        </aside>
        <div id="sidebar-dimmer"></div>
      </div>
    </main>
    <footer class="footer">
      <div class="footer-inner">
        <div class="copyright">
          <span class="author" itemprop="copyrightHolder">崔庆才丨静觅</span> &copy; <span itemprop="copyrightYear">2025</span>
          <span class="with-love">
            <i class="fa fa-heart"></i>
          </span>
          <a href="https://cuiqingcai.com/sitemap.xml" style="display:none" title="爬虫教程" target="_blank"><strong>爬虫教程</strong></a>
          <a href="https://cuiqingcai.com/sitemap.html" style="display:none" title="爬虫教程" target="_blank"><strong>爬虫教程</strong></a>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-chart-area"></i>
          </span>
          <span title="站点总字数">3.3m</span>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-coffee"></i>
          </span>
          <span title="站点阅读时长">49:35</span>
        </div>
        <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动 </div>
        <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备18015597号-1 </a>
        </div>
        <script>
          (function ()
          {
            function leancloudSelector(url)
            {
              url = encodeURI(url);
              return document.getElementById(url).querySelector('.leancloud-visitors-count');
            }

            function addCount(Counter)
            {
              var visitors = document.querySelector('.leancloud_visitors');
              var url = decodeURI(visitors.id);
              var title = visitors.dataset.flagTitle;
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                if (results.length > 0)
                {
                  var counter = results[0];
                  leancloudSelector(url).innerText = counter.time + 1;
                  Counter('put', '/classes/Counter/' + counter.objectId,
                  {
                    time:
                    {
                      '__op': 'Increment',
                      'amount': 1
                    }
                  }).catch(error =>
                  {
                    console.error('Failed to save visitor count', error);
                  });
                }
                else
                {
                  Counter('post', '/classes/Counter',
                  {
                    title,
                    url,
                    time: 1
                  }).then(response => response.json()).then(() =>
                  {
                    leancloudSelector(url).innerText = 1;
                  }).catch(error =>
                  {
                    console.error('Failed to create', error);
                  });
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }

            function showTime(Counter)
            {
              var visitors = document.querySelectorAll('.leancloud_visitors');
              var entries = [...visitors].map(element =>
              {
                return decodeURI(element.id);
              });
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url:
                {
                  '$in': entries
                }
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                for (let url of entries)
                {
                  let target = results.find(item => item.url === url);
                  leancloudSelector(url).innerText = target ? target.time : 0;
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }
            let
            {
              app_id,
              app_key,
              server_url
            } = {
              "enable": true,
              "app_id": "6X5dRQ0pnPWJgYy8SXOg0uID-gzGzoHsz",
              "app_key": "ziLDVEy73ne5HtFTiGstzHMS",
              "server_url": "https://6x5drq0p.lc-cn-n1-shared.com",
              "security": false
            };

            function fetchData(api_server)
            {
              var Counter = (method, url, data) =>
              {
                return fetch(`${api_server}/1.1${url}`,
                {
                  method,
                  headers:
                  {
                    'X-LC-Id': app_id,
                    'X-LC-Key': app_key,
                    'Content-Type': 'application/json',
                  },
                  body: JSON.stringify(data)
                });
              };
              if (CONFIG.page.isPost)
              {
                if (CONFIG.hostname !== location.hostname) return;
                addCount(Counter);
              }
              else if (document.querySelectorAll('.post-title-link').length >= 1)
              {
                showTime(Counter);
              }
            }
            let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;
            if (api_server)
            {
              fetchData(api_server);
            }
            else
            {
              fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id).then(response => response.json()).then((
              {
                api_server
              }) =>
              {
                fetchData('https://' + api_server);
              });
            }
          })();

        </script>
      </div>
      <div class="footer-stat">
        <span id="cnzz_stat_icon_1279355174"></span>
        <script type="text/javascript">
          document.write(unescape("%3Cspan id='cnzz_stat_icon_1279355174'%3E%3C/span%3E%3Cscript src='https://v1.cnzz.com/z_stat.php%3Fid%3D1279355174%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));

        </script>
      </div>
    </footer>
  </div>
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/js/utils.js"></script>
  <script src="/.js"></script>
  <script src="/js/schemes/pisces.js"></script>
  <script src="/.js"></script>
  <script src="/js/next-boot.js"></script>
  <script src="/.js"></script>
  <script>
    (function ()
    {
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x = document.getElementsByTagName("link");
      //Find the last canonical URL
      if (x.length > 0)
      {
        for (i = 0; i < x.length; i++)
        {
          if (x[i].rel.toLowerCase() == 'canonical' && x[i].href)
          {
            canonicalURL = x[i].href;
          }
        }
      }
      //Get protocol
      if (!canonicalURL)
      {
        curProtocol = window.location.protocol.split(':')[0];
      }
      else
      {
        curProtocol = canonicalURL.split(':')[0];
      }
      //Get current URL if the canonical URL does not exist
      if (!canonicalURL) canonicalURL = window.location.href;
      //Assign script content. Replace current URL with the canonical URL
      ! function ()
      {
        var e = /([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,
          r = canonicalURL,
          t = document.referrer;
        if (!e.test(r))
        {
          var n = (String(curProtocol).toLowerCase() === 'https') ? "https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif" : "//api.share.baidu.com/s.gif";
          t ? (n += "?r=" + encodeURIComponent(document.referrer), r && (n += "&l=" + r)) : r && (n += "?l=" + r);
          var i = new Image;
          i.src = n
        }
      }(window);
    })();

  </script>
  <script src="/js/local-search.js"></script>
  <script src="/.js"></script>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">
  <script>
    NexT.utils.loadComments(document.querySelector('#gitalk-container'), () =>
    {
      NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () =>
      {
        var gitalk = new Gitalk(
        {
          perPage: : 100,
          clientID: '4c86ce1d7c4fbb3b277c',
          clientSecret: '4927beb0f90e2c07e66c99d9d2529cf3eb8ac8e4',
          repo: 'Blog',
          owner: 'germey',
          admin: ['germey'],
          id: '574b19b664ea13bb04725d6c939cb907',
          language: 'zh-CN',
          distractionFreeMode: true
        });
        gitalk.render('gitalk-container');
      }, window.Gitalk);
    });

  </script>
</body>

</html>
